# A.市场模拟
考察由10,20,30,40,50个提供商组成的HDT,以测试提出的基于学习的拍卖算法的性能
- 假设提供者和观看者数量相同,更好的评价拍卖商的表现
	- 通过赢者决定规则
		- 第一批w对候选将会在小于等于买卖双方最小值的情况下,进行匹配和交易
			- 其他的买卖者无法产生社会福利
		- 当没有活跃的买卖者时,拍卖商停止拍卖
			- 意味着当买卖双方数量相同,拍卖会进行更多回合
	- 验证学习为基础的拍卖商在增加社会福利和降低成本方面的有效性,使用买卖者数量相同的市场,可以得到更好的检验

- 买家的兴趣向量从以1为中心,方差为4的多元正态分布中采样
- 对视觉表现的质量需求从客观意见部分统一抽样
	- [[Market Design And System Model#^247a10]]
- 请求访问的时间为3-30min之间
- 兴趣衰减率$j_a$ 从以1为中心,方差为4的多元正态分布中采样
- 提供商的基本速率,CPU周期,CPU频率,频谱效率从定义域{1,2,3}中均匀采样
- 默认提供的角分辨率16,持续时间为15min
- 模型中的演员评论家模型学习率均为0.001,折扣因子为0.5
- 策略在每次迭代更新
	- 2048迭代 10个epoch

# B.拍卖方法
实验中使用了三种拍卖方法
1.  Vanilla-DDA:拍卖商根据最小价格间隔调整拍卖时钟
2. Random-DDA:拍卖商随机选择1-20个最低价格间隔调整拍卖时钟
3. DRL-DDA:拍卖商根据训练的DRL模型调整拍卖时钟
DRL在不同市场规模下的趋势
![[Pasted image 20221112180218.png]]
模型可以在不同市场规模下收敛,但是收敛速度不同
市场规模越大,收敛越慢
市场参与者增加,拍卖商需要做出更多决定
![[Pasted image 20221112180240.png]]![[Pasted image 20221112180250.png]]
- 随着迭代次数增加,DRL-DDA竞拍商以增大拍卖信息交换成本为代价获得更大的社会福利
	- 这是由于奖励函数的设置导致模型被训练通过应用拍卖信息最小化遗憾
	- 社会福利随着遗憾减少增加,表明进行更多信息交换,随之成本增大

# 性能验证
不同市场规模,不同方法的性能
![[Pasted image 20221112181108.png]]
DRL比RandomDDA更好,但是比Vanilla差
Vanilla是0遗憾方法,能达到最大社会福利
DRL比Vanilla少10%的社会福利

![[Pasted image 20221112181145.png]]
比较了拍卖方法中使用的信息交换成本
Random最低,积极调整时钟很快完成拍卖
DRL是Vanilla的一半,表明使用一个大于Vanilla的最低价格间隔调整拍卖时钟

牺牲一部分社会福利,换取更小的拍卖信息交换成本和更快的拍卖过程