---
UID: 20231205155846
aliases: null
source: null
cssclass: null
created: 2023-12-05T00:00:00.000Z
date updated: 2023-12-05 15:58
---

## ✍内容

## 1、背景

前段时间想看看大模型的分布式训练框架（比如 Megatron）是咋做的，然后发现需要对 Transformer 的细节更加了解，所以就打算看看 Transformer 开山之作《Attention Is All You Need》[1]，并手动实现一遍。

过程：

- 一边读论文 [1]，一边看李沐的讲解视频 [2]；
- 遇到一些细节不理解的，比如 Multi-Head 具体是啥操作，或者每个 Attention 操作的 query、key、value 是咋来的，就 google 一下或者问 ChatGPT 4；还有看了文章 [4] 和 [5]（推荐！）
- 实现之前，先看了一遍 harvard nlp 团队 [3] 和 github 上 hyunwoongko 的实现。**hyunwoongko 是完全参照了原文实现，而 harvard nlp 团队是基于文章发表后的一些新发现，做了微调**，比如 Post-LM 改成 Pre-LM（参考文章 [6]），还有对 Attention 操作中 softmax 之后、乘以 V 之前的中间结果，加了 dropout 操作，这些操作是在后面的研究中发现可以使得训练更稳定的微调。
- 然后自己动手！
- 最后跑了个简单的示例任务。

harvard nlp 团队给的实现里面，整体很强，不过很多地方没有额外的解释，比如有一些细节是和原文不一样的，直接给出了微调后的实现版本，不仔细留意都可能发现不了。另外还有就是示例任务写得有点问题，我改了下～

小总结：

**本文在以上工作的基础上，补充更多的图文解释和代码注释（比如矩阵操作的图画得更清晰准确）**，尝试帮助大家更好地理解。

本文代码：[https://github.com/NgCafai/transformer/tree/master](https://link.zhihu.com/?target=https%3A//github.com/NgCafai/transformer/tree/master)

### Tips

1、本文的讲述顺序是和论文的顺序基本一致，即先讲整体模型的结构和实现，再讲各个组成部分。动手实现的时候，可以考虑从各个基础组件开始，再往上一层层搭建。

2、遇到不会的地方，善用 ChatGPT！真的好用！

## 2、模型实现

### 整体结构

先上经典图：

![](https://pic4.zhimg.com/v2-2e89d1931ff775e7f1818fad509cdfeb_b.jpg)

实现：

```
class Transformer(nn.Module):
    """
    The Transformer model: A standard Encoder-Decoder architecture.
    """
    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):
        """
        Args:
            encoder: nn.Module, a stack of N EncoderLayer
            decoder: nn.Module, a stack of N DecoderLayer
            src_embed: nn.Sequential, composed of Embeddings and PositionalEncoding, for input sequence
            tgt_embed: nn.Sequential, composed of Embeddings and PositionalEncoding, for output sequence
            generator: nn.Module, used to predict the next token
        """
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.src_embed = src_embed
        self.tgt_embed = tgt_embed
        self.generator = generator

    def forward(self, src, tgt, src_mask, tgt_mask):
        """
        Args:
            src: (batch_size, seq_len_src)
            tgt: (batch_size, seq_len_tgt)
            src_mask: (batch_size, 1, seq_len_src)
            tgt_mask: (batch_size, seq_len_tgt, seq_len_tgt)
        Returns:
            output: (batch_size, seq_len, d_model)
        """
        return self.do_decode(self.do_encode(src, src_mask), src_mask, tgt, tgt_mask)

    def do_encode(self, src, src_mask):
        return self.encoder(self.src_embed(src), src_mask)
    
    def do_decode(self, memory, src_mask, tgt, tgt_mask):
        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)
```

Generator 用来将模型输出的 embeding 转换为对下一个词的预测（在整体词表上的概率分布），对应图中右上角的 Linear 和 Softmax 步骤：

```
class Generator(nn.Module):
    """
    Define standard linear + softmax generation step.
    """
    def __init__(self, d_model, vocab_size):
        """
        Args:
            vocab_size: size of the vocabulary, that is, total number of unique tokens
        """
        super().__init__()
        self.linear = nn.Linear(d_model, vocab_size)

    def forward(self, x):
        return nn.functional.log_softmax(self.linear(x), dim=-1)
```

### Encoder

Encoder 对应论文 [1] Figure 1 左边的那个大框，由 N 个 EncoderLayer 组成～

![](https://pic4.zhimg.com/v2-a55ecf9960dabf621dbe99e9da4da14f_b.jpg)

```
class Encoder(nn.Module):
    """
    Core encoder is a stack of N EncoderLayer.
    """
    def __init__(self, layer, N):
        super().__init__()
        self.layers = nn.ModuleList([copy.deepcopy(layer) for _ in range(N)])
        self.norm = LayerNorm(layer.d_model)

    def forward(self, x, mask):
        """
        Pass the input (and mask) through each layer in turn.
        """
        for layer in self.layers:
            x = layer(x, mask)
        return self.norm(x)
```

EncoderLayer 就是论文 [1] Figure 1 左边的画出来的一个模块，由两个 sublayer 组成：

- SubLayer 1：先进行 Multi-Head Attention，再套一层 Residual + LayerNorm；
- SubLayer 2：先进行 Feed Forward（就是一个全链接网络），再套一层 Residual + LayerNorm；

注意：按论文中描述的操作，**SubLayer** 中是先进行 Multi-Head Attention 或者 Feed Forward，然后 Redsial，最后才进行 LayerNorm，这种方式称为 Post LN；后续的研究中，发现进行第一步操作前，先做一次 LayerNorm，**效果会更好**，称为 **Pre LN**。两个方式的执行路径是这样子：

![](https://pic3.zhimg.com/v2-81c7dcbd4dde90795145fdaf2f0f1ce6_b.jpg)

```
class EncoderLayer(nn.Module):
    """
    Encoder is made up of self-attn and feed forward.
    """
    def __init__(self, d_model, attention, feed_forward, dropout_prob):
        super().__init__()
        self.d_model = d_model
        self.attention = attention
        self.feed_forward = feed_forward
        self.sub_layers = nn.ModuleList([SubLayer(d_model, dropout_prob) for _ in range(2)])

    def forward(self, x, mask):
        """
        Args:
            x: (batch_size, seq_len, d_model)
            mask: 
        Returns:
            out: (batch_size, seq_len, d_model)
        """
        x = self.sub_layers[0](x, lambda x: self.attention(x, x, x, mask))
        x = self.sub_layers[1](x, self.feed_forward)
        return x
```

SubLayer 实现（Pre LN 方式）：

```
class SubLayer(nn.Module):
    """
    Do pre-layer normalization for input, and then run multi-head attention or feed forward,
    and finally do the residual connection.
    """
    def __init__(self, d_model, dropout_prob=0.1):
        super().__init__()
        self.norm = LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout_prob)

    def forward(self, x, main_logic):
        # main_logic: multi-head attention or feed forward
        x_norm = self.norm(x)
        return x + self.dropout(main_logic(x_norm))
```

```
class LayerNorm(nn.Module):
    """
    Layer normalization module.
    """
    def __init__(self, d_model, eps=1e-6):
        super().__init__()
        self.gamma = nn.Parameter(torch.ones(d_model))
        self.beta = nn.Parameter(torch.zeros(d_model))
        self.eps = eps

    def forward(self, x):
        mean = x.mean(-1, keepdim=True)
        std = x.std(-1, keepdim=True)
        return self.gamma * (x - mean) / (std + self.eps) + self.beta
```

### Decoder

Decoder 对应论文 [1] Figure 1 右边边的那个大框，由 N 层 DecoderLayer 组成。

![](https://pic3.zhimg.com/v2-f5225435bd38af9786172d0619f4b41a_b.jpg)

实现：\
（memory 指的是 Encoder 的输出）

```
class Decoder(nn.Module):
    """
    Core decoder is a stack of N DecoderLayer.
    """
    def __init__(self, layer, N):
        super().__init__()
        self.layers = nn.ModuleList([copy.deepcopy(layer) for _ in range(N)])
        self.norm = LayerNorm(layer.d_model)

    def forward(self, x, memory, src_mask, tgt_mask):
        # memory is the output of the Encoder
        for layer in self.layers:
            x = layer(x, memory, src_mask, tgt_mask)
        return self.norm(x)
```

DecoderLayer 的结构和 EncoderLayer 类似，主要差别是中间多了一层 Attention 操作（对应下面实现中的 src_attention），其 key 和 value 来自 Encoder 的输出，query 来自 target sequence：

```
class DecoderLayer(nn.Module):
    """
    Decoder is made of self-attn, src-attn, and feed forward.
    """
    def __init__(self, d_model, self_attention, src_attention, feed_forward, dropout_prob):
        super().__init__()
        self.d_model = d_model
        self.self_attention = self_attention
        self.src_attention = src_attention
        self.feed_forward = feed_forward
        self.sub_layers = nn.ModuleList([SubLayer(d_model, dropout_prob) for _ in range(3)])

    def forward(self, x, memory, src_mask, tgt_mask):
        """
        Args:
            x: (batch_size, seq_len, d_model)
            memory: (batch_size, seq_len, d_model)
            src_mask: 
            tgt_mask:
        Returns:
            out: (batch_size, seq_len, d_model)
        """
        # self-attention: query, key, value are all from x
        x = self.sub_layers[0](x, lambda x: self.self_attention(x, x, x, tgt_mask))
        # src-attention: query is from x, while key and value are from the output of the Encoder
        x = self.sub_layers[1](x, lambda x: self.src_attention(x, memory, memory, src_mask))
        x = self.sub_layers[2](x, self.feed_forward)
        return x
```

masking：

输出一个下图这样子的矩阵；用于 Decoder 中的第一个 Multi-Head Attention 操作，目的是保证训练时，target sequence 中每个位置只能看到自己及前面位置的信息：

![](https://pic2.zhimg.com/v2-7fdce4a98acd79a06ce90cff1f8e7599_b.jpg)

```
def subsequent_mask(size):
    "Mask out subsequent positions"
    attention_shape = (1, size, size)
    subsequent_mask = torch.triu(torch.ones(attention_shape), diagonal=1).type(torch.uint8)
    return subsequent_mask == 0
```

### Attention

实现论文[1] 3.2 中提到的 Attention 部分。

![](https://pic4.zhimg.com/v2-116b3f4a5d2f93d3ef19f95c5548fedb_b.jpg)

**先是左边的 Scaled Dot-Product Attention**，公式：

\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V

对应的矩阵操作：

![](https://pic2.zhimg.com/v2-3cfe8adbe2e8a6d192d13cd35ac3cde9_b.jpg)

Q、K、V 对应 3 个序列，每个序列分别包含 n、m、m 个词（这里 n 和 m 不是具体的训练参数，只是为了说明各个矩阵的相对大小，比如 K 和 V 的词的数量是一样的），每个词用一个 d_k 维的向量表示。（论文中 V 是用了 d_v 维的向量，但一般 d_k 和 d_v 相等）

一般 K 和 V 来自同一个序列。

Attention 的作用是将 Q 中每个向量，和 K 中的每个向量，分别做内积，***表示这两个向量的相似程度***，得到 scores 矩阵。 scores_{i, j} 就表示 q_i 和 k_j 的内积。

***result 中的每一行，相当于是 V 中所有行的加权之和***。比如 result 中的第 i 行，就是以 scores 中的第 i 行为权重，乘以整个 V。

***具体实现时***，还会对 scores 做一下 dropout。这是原文没有提到的，但是后来的实践中发现这么做的话训练效果更好；另外，**为了提高效率，会对所有 head 的 Attention 操作合在一起做**：

```
def scaled_dot_product_attention(query, key, value, mask=None, dropout=None):
    """
    Args:
        query: (batch_size, num_heads, seq_len_q, d_k), given sequence that we focus on
        key: (batch_size, num_heads, seq_len_k, d_k), the sequence to check relevance with query
        value: (batch_size, num_heads, seq_len_v, d_k), seq_len_k == seq_len_v, usually value and key come from the same source
        mask: for encoder, mask is [batch_size, 1, 1, seq_len_k], for decoder, mask is [batch_size, 1, seq_len_q, seq_len_k]
        dropout: nn.Dropout(), optional
    Returns:
        output: (batch_size, num_heads, seq_len_q, d_v), attn: (batch_size, num_heads, seq_len_q, seq_len_k)
    """
    d_k = query.size(-1)
    # size of scores: (batch_size, num_heads, seq_len_q, seq_len_k)
    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k) 
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)
    scores = scores.softmax(dim=-1)

    if dropout is not None:
        scores = dropout(scores)

    return torch.matmul(scores, value), scores
```

---

**接下来是右边的 MultiHead 操作**，公式：

\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \text{head}_2, \ldots, \text{head}_h)W^O \\\ \text{where } \text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)

参数矩阵的维度： W_i^Q \in \mathbb{R}^{d_{\text{model}} \times d_k}, W_i^K \in \mathbb{R}^{d_{\text{model}} \times d_k}, W_i^V \in \mathbb{R}^{d_{\text{model}} \times d_v}, W^O \in \mathbb{R}^{h d_v \times d_{\text{model}}}

其中 h 表示 head 的数量， d_k = d_v = d_{model} / h

这里的 Q、K、V 对应 3 个序列，每个序列分别包含 n、m、m 个词（这里 n 和 m 不是具体的训练参数，只是为了说明各个矩阵的相对大小，比如 K 和 V 的词的数量是一样的），序列中每个词的维度则是 d_{model}（即输入和输出的词的 embedding 的纬度）。

Q、K、V 分别乘以对应的 W 矩阵，相当于将每个词的维度从 d_{model} 投影为 d_k。

经过 Attention 操作后，得到的每个 head 的维度为 n \times d_k ，然后拼接成 n \times d_{model} 的矩阵。

**来看图**！

每个 head_i 的计算过程：

![](https://pic4.zhimg.com/v2-8f86544c626819a222d1f855bb78c5d7_b.jpg)

然后拼接，再做一次线性操作：

![](https://pic4.zhimg.com/v2-984ab19e8d14860b2f1b620b370667a7_b.jpg)

***在模型的不同部分，Q、K、V 的来源不一样***。

比如论文 [1] Figure 1 中左边 Encoder，第一层 EncoderLayer 的输入为 （input embedding + positional encoding），**记为 X**。X 为 n \times d_{model} 的矩阵，n 为 input 序列的长度（词的数量）。

**第一层 EncoderLaye**r 的 Multi-Head Attention 的 **Q、K、V 都是 X**。

然后每一层 EncoderLayer 的输出，都作为下一层 EncoderLayer 的 Q、K、V。

**每一层 DecoderLayer 也是类似，主要区别是中间的 Multi-Head Attention 的 K、V 都是来自最后一层 EncoderLayer 的输出**。

Multi-Head Attention 代码：

```
class MultiHeadAttention(nn.Module):
    def __init__(self, h, d_model, dropout_prob=0.1):
        """
        Args:
            h: number of heads
            d_model: dimension of the vector for each token in input and output
            dropout_prob: probability of dropout
        """
        super().__init__()
        self.d_k = d_model // h
        self.h = h
        # corresponding to W^Q, W^K, W^V (matrix for each head are concatenated for efficiency)   
        # and W^O in the paper
        self.linears = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(4)])
        self.dropout = nn.Dropout(dropout_prob)
        
    def forward(self, query, key, value, mask=None):
        """
        Args:
            query: (batch_size, seq_len_q, d_model)
            key: (batch_size, seq_len_k, d_model)
            value: (batch_size, seq_len_v, d_model), seq_len_k == seq_len_v
            mask: 
        Returns:
            output: (batch_size, seq_len_q, d_model)
            attn: (batch_size, num_heads, seq_len_q, seq_len_k)
        """
        if mask is not None:
            mask = mask.unsqueeze(1)
        n_batches = query.size(0)

        # 1. linear projection for query, key, value
        #    after this step, the shape of each is (batch_size, h, seq_len, d_k)
        query, key, value = [linear(x).view(n_batches, -1, self.h, self.d_k).transpose(1, 2) for linear, x in zip(self.linears, (query, key, value))]

        # 2. scaled dot product attention
        #    out: (batch_size, h, seq_len_q, d_k)
        out, _  = scaled_dot_product_attention(query, key, value, mask, self.dropout)

        # 3. "Concat" using a view and apply a final linear
        out = (
            out.transpose(1, 2)
            .contiguous()
            .view(n_batches, -1, self.h * self.d_k)
        )
        out = self.linears[-1](out)

        del query, key, value
        return out
```

### Feed-Forward Network

一个全链接层：

\text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2

```
class PositionWiseFeedForward(nn.Module):
    """
    Implements FFN equation.
    """
    def __init__(self, d_model, d_ff, dropout_prob=0.1):
        super().__init__()
        self.linear1 = nn.Linear(d_model, d_ff)
        self.linear2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout_prob)

    def forward(self, x):
        return self.linear2(self.dropout(nn.functional.relu(self.linear1(x))))
```

### Embedding

```
class Embeddings(nn.Module):
    def __init__(self, d_model, vocab_size):
        """
        Args:
            vocab_size: size of the vocabulary, that is, total number of unique tokens
        """
        super().__init__()
        self.lookup_table = nn.Embedding(vocab_size, d_model)
        self.d_model = d_model
    
    def forward(self, x):
        return self.lookup_table(x) * math.sqrt(self.d_model)
```

### Positional Encoding

公式：

PE_{(pos,2i)} = \sin\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)

PE_{(pos,2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)

来看看分母： 1 / 10000^{2i/d_{model}} ，注意 2i / d_{model} 是一个小数，一般计算库实现这种指数为小数的计算时，都会转换为 exp() 和 log() 来计算。

对于上面这个例子，就是这样子计算： 10000^{-\frac{2i}{d_{\text{model}}}} \rightarrow e^{-\frac{2i \cdot \log(10000)}{d_{\text{model}}}} （因为 e^{log(10000)} = 10000 ）

实现的时候，先把指数统一算好，然后套一层 exp() 操作，这样计算效率比较高。

```
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, dropout_prob, max_len=5000):
        super().__init__()
        self.dropout = nn.Dropout(dropout_prob)

        # Compute the positional encodings once in log space.
        positional_encodings = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1) # size: (max_len, 1)

        # Equivalent to 1 / (10000 ^ (2i / d_model)) in the paper
        div_term = torch.exp(
            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)
        ) # size: (d_model / 2,)

        positional_encodings[:, 0::2] = torch.sin(position * div_term)
        positional_encodings[:, 1::2] = torch.cos(position * div_term)

        positional_encodings = positional_encodings.unsqueeze(0) # size: (1, max_len, d_model)
        self.register_buffer('positional_encodings', positional_encodings)

    def forward(self, x):
        x = x + self.positional_encodings[:, :x.size(1)]
        return self.dropout(x)
```

### 创建模型

给定超参，创建模型：

```
def make_model(src_vocab_size, tgt_vocab_size, N=6, d_model=512, d_ff=2048, h=8, dropout_prob=0.1, max_len=5000):
    """
    Helper: Construct a model from hyperparameters.
    """
    func_copy = copy.deepcopy
    attention = MultiHeadAttention(h, d_model, dropout_prob)
    feed_forward = PositionWiseFeedForward(d_model, d_ff, dropout_prob)
    positional_encoding = PositionalEncoding(d_model, dropout_prob, max_len)
    model = Transformer(
        encoder=Encoder(EncoderLayer(d_model, func_copy(attention), func_copy(feed_forward), dropout_prob), N),
        decoder=Decoder(DecoderLayer(d_model, func_copy(attention), func_copy(attention), func_copy(feed_forward), dropout_prob), N),
        src_embed=nn.Sequential(Embeddings(d_model, src_vocab_size), func_copy(positional_encoding)),
        tgt_embed=nn.Sequential(Embeddings(d_model, tgt_vocab_size), func_copy(positional_encoding)),
        generator=Generator(d_model, tgt_vocab_size)
    )

    # Initialize parameters with Glorot / fan_avg.
    for p in model.parameters():
        if p.dim() > 1:
            nn.init.xavier_uniform_(p)
    return model
```

### 简单测试

写个简单的推理步骤，测试下前面的代码逻辑有没有 bug：

```
def inference_test():
    vocab = ['<pad>', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10']
    vocab_size = len(vocab)

    test_model = make_model(vocab_size, vocab_size, N=2)
    test_model.eval()

    src = torch.LongTensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]]) # (batch_size, seq_len)
    src_mask = torch.ones(1, 1, 10) # (batch_size, 1, seq_len)

    memory = test_model.do_encode(src, src_mask)
    ys = torch.zeros(1, 1).type_as(src)

    for i in range(9):
        out = test_model.do_decode(memory, src_mask, ys, subsequent_mask(ys.size(1)).type_as(src))
        prob = test_model.generator(out[:, -1])
        _, next_word = torch.max(prob, dim=1)
        next_word = next_word.data[0]
        ys = torch.cat([ys, torch.empty(1, 1).type_as(src).fill_(next_word)], dim=1)

    print("Example Untrained Model Prediction: ", ys)


def run_tests():
    for _ in range(10):
        inference_test()


if __name__ == "__main__":
    run_tests()
```

Q：为什么会有 src_mask？

A：训练时，每个样本的输入、输出序列的长度不一致，然后就通过 padding 填成一样的大小。src_mask 一般用于标记出输入序列中的 padding 位置，然后在 Attention 操作中将 scores（Q * K 的结果）的对应位置设为负无穷，来消除 padding 的影响。

## 3、训练配置和流程

对应论文 [1] 的第 5 部分 - Training。

### Optimizer

learning rate 按下面的公式来变化：

lrate = d_{\text{model}}^{-0.5} \cdot \min(step\\_num^{-0.5}, step\\_num \cdot warmup\\_steps^{-1.5})

例如，在 d_{model} 分别为 256、512、512，warmup step 分别为 4000、4000、8000 的情况下，learning rate 的变化如下：

![](https://pic3.zhimg.com/v2-dd6d9db6a89b9de79ad39c5563a598d2_b.jpg)

```
def learning_rate(step, model_size, factor, warmup):
    """
    We have to default the step to 1 for LambdaLR function
    to avoid zero raising to negative power.
    """
    if step == 0:
        step = 1
    return factor * (model_size ** (-0.5) * min(step ** (-0.5), step * warmup ** (-1.5)))
```

### Regularization

***Label Smoothing***

这一步是为了提高模型的泛化能力～ 使用了 Kullback–Leibler (KL) divergence 的方式实现。

```
class LabelSmoothing(nn.Module):
    """
    Implement label smoothing using KL divergence.
    """
    def __init__(self, size, padding_idx, smoothing=0.0):
        """
        Args:
            size: number of classes
            padding_idx: index of padding token
            smoothing: smoothing rate
        """
        super().__init__()
        self.criterion = nn.KLDivLoss(reduction="sum")
        self.padding_idx = padding_idx
        self.confidence = 1.0 - smoothing
        self.smoothing = smoothing
        self.size = size
        self.true_dist = None

    def forward(self, x, target):
        assert x.size(1) == self.size
        true_dist = x.clone()
        true_dist.fill_(self.smoothing / (self.size - 2))
        true_dist.scatter_(1, target.unsqueeze(1), self.confidence)
        true_dist[:, self.padding_idx] = 0
        mask = torch.nonzero(target == self.padding_idx)
        if mask.dim() > 0:
            true_dist.index_fill_(0, mask.squeeze(), 0.0)
        self.true_dist = true_dist
        return self.criterion(x, true_dist)
```

### Batch and Masking

通过 Batch 对象来管理训练过程每个 batch 的输入、输出序列，以及对应的 mask。

```
class Batch:
    """
    Object for holding a batch of src and target sentences for training,
    as well as constructing masks.
    """
    def __init__(self, src, tgt=None, pad_idx=2):
        self.src = src
        self.src_mask = (src != pad_idx).unsqueeze(-2)
        if tgt is not None:
            self.tgt = tgt[:, :-1] 
            self.tgt_mask = self.make_std_mask(self.tgt, pad_idx)
            self.tgt_y = tgt[:, 1:] # used for loss calculation
            self.ntokens = (self.tgt_y != pad_idx).data.sum() # used for loss calculation

    @staticmethod
    def make_std_mask(tgt, pad_idx):
        """
        Create a mask to hide padding and future words.
        """
        tgt_mask = (tgt != pad_idx).unsqueeze(-2)
        tgt_mask = tgt_mask & subsequent_mask(tgt.size(-1)).type_as(tgt_mask)
        return tgt_mask
```

### Epoch

run_epoch(): 对整个训练集完成一轮训练

```
class TrainState:
    """
    Track number of steps, examples, and tokens processed.
    """
    step: int = 0 # Steps in the current epoch
    accum_step: int = 0 # Number of gradient accumulation steps
    samples: int = 0 # Total number of examples used
    tokens: int = 0 # Total number of tokens processed


def run_epoch(
    data_iter,
    model,
    loss_compute,
    optimizer,
    scheduler,
    mode="train",
    accum_iter=1,
    train_state=TrainState(),
):
    """
    Train a single epoch.
    """
    start = time.time()
    total_tokens = 0
    total_loss = 0
    tokens = 0
    n_accum = 0

    for i, batch in enumerate(data_iter):
        out = model.forward(batch.src, batch.tgt, batch.src_mask, batch.tgt_mask)
        loss, loss_node = loss_compute(out, batch.tgt_y, batch.ntokens)
        
        if mode == "train" or mode == "train+log":
            loss_node.backward()
            train_state.step += 1
            train_state.samples += batch.src.shape[0]
            train_state.tokens += batch.ntokens

            # implement as gradient accumulation - often used when the available hardware 
            # cannot handle the desired batch size due to memory constraints.
            if i % accum_iter == 0:
                optimizer.step()
                optimizer.zero_grad(set_to_none=True)
                n_accum += 1
                train_state.accum_step += 1

            scheduler.step()
        
        total_loss += loss
        total_tokens += batch.ntokens
        tokens += batch.ntokens

        if i % 40 == 1 and (mode == "train" or mode == "train+log"):
            lr = optimizer.param_groups[0]["lr"]
            elapsed = time.time() - start
            print(
                (
                    "Epoch Step: %6d | Accumulation Step: %3d | Loss: %6.2f | "
                    + "Tokens per Sec: %7.1f | Learning Rate: %6.1e"
                )
                % ( i, n_accum, loss / batch.ntokens, tokens / elapsed, lr)
            )
            start = time.time()
            tokens = 0
        
        del loss, loss_node

    return total_loss / total_tokens, train_state
            
```

## 4、示例任务

harvard nlp 团队给的示例任务中，配置有点问题，于是我改了下，变成针对字母 'a' 至 'k' 的 copy task（即输入为字母序列，输出为相同的字母序列），加上了常见的 <pad>, <start>, <end> 等 symbol。

```
"""
A simple copy task: Given a random set of input symbols from a small vocabulary, the goal is to generate back those same symbols.
"""
import torch
from torch.optim.lr_scheduler import LambdaLR
from train import *
from models.transformer import make_model
from models.utils import *

def data_gen(vocab, batch_size, n_batches, max_len=8):
    """
    Generate random data for a src-tgt copy task.
    """
    for i in range(n_batches):
        start_symbol_idx = vocab['<start>']
        end_symbol_idx = vocab['<end>']
        # Create a tensor with random integers between (start_symbol_idx + 1) and (end_symbol_idx - 1), size: (batch_size, 10)
        data = torch.randint(start_symbol_idx + 1, end_symbol_idx, size=(batch_size, max_len))

        data[:, 0] = start_symbol_idx
        data[:, -1] = end_symbol_idx
        src = data.requires_grad_(False).clone().detach()
        tgt = data.requires_grad_(False).clone().detach()
        yield Batch(src, tgt, pad_idx=vocab['<pad>'])


class SimpleLossCompute:
    """
    A simple loss compute and train function.
    """
    def __init__(self, generator, criterion):
        """
        Args:
            generator: nn.Module, used to generate the probability distribution over the target vocabulary

        """
        self.generator = generator
        self.criterion = criterion

    def __call__(self, x, y, norm):
        x = self.generator(x)
        loss = (
            self.criterion(
                x.contiguous().view(-1, x.size(-1)), y.contiguous().view(-1)
            )
            / norm
        )
        return loss.data * norm, loss


def greedy_decode(model, src, src_mask, max_len, start_symbol):
    """
    Predicts a translation using greedy decoding for simplicity.
    """
    memory = model.do_encode(src, src_mask)
    ys = torch.zeros(1, 1).fill_(start_symbol).type_as(src.data)
    for i in range(max_len - 1):
        out = model.do_decode(
            memory, src_mask, ys, subsequent_mask(ys.size(1)).type_as(src.data)
        )
        prob = model.generator(out[:, -1])
        _, next_word = torch.max(prob, dim=1)
        next_word = next_word.data[0]
        ys = torch.cat(
            [ys, torch.zeros(1, 1).type_as(src.data).fill_(next_word)], dim=1
        )
    return ys


def example_simple_model():
    # Example copy task for symbols: 'a' to 'k'
    vocab = {
        '<pad>': 0,  # Padding token
        '<start>': 1,  # Start of sequence token
        'a': 2,
        'b': 3,
        'c': 4,
        'd': 5,
        'e': 6,
        'f': 7,
        'g': 8,
        'h': 9,
        'i': 10,
        'j': 11,
        'k': 12,
        '<end>': 13  # End of sequence token
    }
    vocab_size = len(vocab)  # Should be 10 in this case

    criterion = LabelSmoothing(size=vocab_size, padding_idx=vocab['<pad>'], smoothing=0.0)
    model = make_model(vocab_size, vocab_size, N=2)

    optimizer = torch.optim.Adam(model.parameters(), lr=0.5, betas=(0.9, 0.98), eps=1e-9)
    lr_scheduler = LambdaLR(
        optimizer=optimizer,
        lr_lambda=lambda step: learning_rate(
            step, model_size=model.src_embed[0].d_model, factor=1.0, warmup=400
        ),
    )

    batch_size = 80
    for epoch in range(20):
        model.train()
        run_epoch(
            data_gen(vocab, batch_size=batch_size, n_batches=20),
            model,
            SimpleLossCompute(model.generator, criterion),
            optimizer,
            lr_scheduler,
            mode="train",
        )

        model.eval()

        run_epoch(
            data_gen(vocab, batch_size=batch_size, n_batches=5),
            model,
            SimpleLossCompute(model.generator, criterion),
            DummyOptimizer(),
            DummyScheduler(),
            mode="eval",
        )
    
    model.eval()
    max_len = 8
    src_sentence = ['<start>', 'a', 'b', 'c', 'i', 'j', 'k', '<end>']
    src = torch.LongTensor([[vocab[word] for word in src_sentence]])
    src_mask = torch.ones(1, 1, max_len)
    prediction = greedy_decode(model, src, src_mask, max_len, start_symbol=vocab['<start>'])

    idx_to_word = {idx: word for word, idx in vocab.items()}
    prediction_sentence = [idx_to_word[idx.item()] for idx in prediction[0]]
    print("Example Trained Model Prediction: ", prediction_sentence)


if __name__ == "__main__":
    example_simple_model()
```

训练过程和结果：

![](https://pic4.zhimg.com/v2-c9fa4cf54c86a279bbd01d7278017107_b.jpg)

##

总结

1、本文的目标是弄清 transformer 的结构，以及训练和推理流程，所以就不去弄翻译任务了，后面有需要再更新；

2、Transformer 相比于 RNN，其中一个很突出的优点是很多地方可以做并行优化，比如里面很多计算可以合并为一个大的矩阵运算（Attention 那里），而矩阵运算是 cuda 算子中算是优化得最好的操作之一了。还有就是在训练的时候，每个 output token 的输出不需要等前一 token 算出来（RNN 就需要），而是可以直接算出整个 output sequence，然后算 loss，这样效率就高太多了；

3、接下来就是去研究 Megatron，还有看 GPT 1 到 GPT 4 的相关论文！

Reference:

[1] Attention Is All You Need: [https://arxiv.org/pdf/1706.03762.pdf](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1706.03762.pdf)

[2] 李沐的论文讲解视频：[https://www.bilibili.com/video/BV1pu411o7BE](https://link.zhihu.com/?target=https%3A//www.bilibili.com/video/BV1pu411o7BE)

[3] harvard nlp 团队的实现：[http://nlp.seas.harvard.edu/annotated-transformer/#hardware-and-schedule](https://link.zhihu.com/?target=http%3A//nlp.seas.harvard.edu/annotated-transformer/%23hardware-and-schedule)

[4] 知乎文章：[初识CV：Transformer模型详解（图解最完整版）](https://zhuanlan.zhihu.com/p/338817680)

[5] [The Illustrated Transformer](https://link.zhihu.com/?target=http%3A//jalammar.github.io/illustrated-transformer/)

[6] 知乎文章：[叮当猫：【DL&NLP】再谈Layer-Norm：Pre-LN、Post-LN、DeepNorm](https://zhuanlan.zhihu.com/p/480783670)

[7] On Layer Normalization in the Transformer Architecture: [https://openreview.net/pdf?id=B1x8anVFPr](https://link.zhihu.com/?target=https%3A//openreview.net/pdf%3Fid%3DB1x8anVFPr)
