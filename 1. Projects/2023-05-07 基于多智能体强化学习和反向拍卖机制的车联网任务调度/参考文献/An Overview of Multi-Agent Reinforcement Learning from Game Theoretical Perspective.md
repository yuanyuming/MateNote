---
aliases: 'Abstract'
tags: 
cssclass:
source:
created: "2023-08-01 09:45"
updated: "2023-08-24 11:42"
---

# Abstract

Following the remarkable success of the AlphaGO series, 2019 was a booming year that witnessed significant advances in multi-agent reinforcement learning (MARL) techniques. MARL corresponds to the learning problem in a multi-agent system in which multiple agents learn simultaneously. It is an interdisciplinary domain with a long history that includes game theory, machine learning, stochastic control, psychology, and optimisation. Although MARL has achieved considerable empirical success in solving real-world games, there is a lack of a self-contained overview in the literature that elaborates the game theoretical foundations of modern MARL methods and summarises the recent advances. In fact, the majority of existing surveys are outdated and do not fully cover the recent developments since 2010. In this work, we provide a monograph on MARL that covers both the fundamentals and the latest developments in the research frontier.

Our work is separated into two parts. From _§_[1] to _§_[4], we present the selfcontained fundamental knowledge of MARL, including problem formulations, basic solutions, and existing challenges. Specifically, we present the MARL formulations through two representative frameworks, namely, stochastic games and extensiveform games, along with different variations of games that can be addressed. The goal of this part is to enable the readers, even those with minimal related background, to grasp the key ideas in MARL research. From _§_[5] to _§_[9], we present an overview of recent developments of MARL algorithms. Starting from new taxonomies for MARL methods, we conduct a survey of previous survey papers. In later sections, we highlight several modern topics in MARL research, including Q-function factorisation, multi-agent soft learning, networked multi-agent MDP, stochastic potential games, zero-sum continuous games, online MDP, turn-based stochastic games, policy space response oracle, approximation methods in generalsum games, and mean-field type learning in games with infinite agents. Within each topic, we select both the most fundamental and cutting-edge algorithms.

The goal of our monograph is to provide a self-contained assessment of the current state-of-the-art MARL techniques from a game theoretical perspective. We expect this work to serve as a stepping stone for both new researchers who are about to enter this fast-growing domain and existing domain experts who want to obtain a panoramic view and identify new directions based on recent advances.


# 1. Introduction

Machine learning can be considered as the process of converting data into knowledge ([Shalev-Shwartz and Ben-David], [2014]). The input of a learning algorithm is training data (for example, images containing cats), and the output is some knowledge (for example, rules about how to detect cats in an image). This knowledge is usually represented as a computer program that can perform certain task(s) (for example, an automatic cat detector). In the past decade, considerable progress has been made by means of a special kind of machine learning technique: deep learning ([LeCun et al.], [2015]). One of the critical embodiments of deep learning is different kinds of deep neural networks (DNNs) ([Schmidhuber], [2015]) that can find disentangled representations ([Bengio], [2009]) in high-dimensional data, which allows the software to train itself to perform new tasks rather than merely relying on the programmer for designing hand-crafted rules. An uncountable number of breakthroughs in real-world AI applications have been achieved through the usage of DNNs, with the domains of computer vision ([Krizhevsky et al.], [2012]) and natural language processing ([Brown et al.], [2020]; [Devlin et al.], [2018]) being the greatest beneficiaries.

![[Pasted image 20230801095154.png]]
**Figure 1:** Modern AI applications are being transformed from pure feature recognition (for example, detecting a cat in an image) to decision making (driving through a traffic intersection safely), where interaction among multiple agents inevitably occurs. As a result, each agent has to behave strategically. Furthermore, the problem becomes more challenging because current decisions influence future outcomes.

In addition to feature recognition from existing data, modern AI applications often require computer programs to make decisions based on acquired knowledge (see Figure[1]). To illustrate the key components of decision making, let us consider the real-world example of controlling a car to drive safely through an intersection. At each time step, a robot car can move by steering, accelerating and braking. The goal is to safely exit the intersection and reach the destination (with possible decisions of going straight or turning left/right into another lane). Therefore, in addition to being able to detect objects, such as traffic lights, lane markings, and other cars (by converting data to knowledge), we aim to find a steering policy that can control the car to make a sequence of manoeuvres to achieve the goal (making decisions based on the knowledge gained). In a decision-making setting such as this, two additional challenges arise:

1. First, during the decision-making process, at each time step, the robot car should consider not only the immediate value of its current action but also the consequences of its current action in the future. For example, in the case of driving through an intersection, it would be detrimental to have a policy that chooses to steer in a “safe” direction at the beginning of the process if it would eventually lead to a car crash later on.
2. Second, to make each decision correctly and safely, the car must also consider other cars’ behaviour and act accordingly. Human drivers, for example, often predict in advance other cars’ movements and then take strategic moves in response (like giving way to an oncoming car or accelerating to merge into another lane).

The need for an adaptive decision-making framework, together with the complexity of addressing multiple interacting learners, has led to the development of multi-agent RL. Multi-agent RL tackles the sequential decision-making problem of having multiple intelligent agents that operate in a shared stochastic environment, each of which targets to maximise its long-term reward through interacting with the environment and other agents. Multi-agent RL is built on the knowledge of both multi-agent systems (MAS) and RL. In the next section, we provide a brief overview of (single-agent) RL and the research developments in recent decades.

#### 1.1 A Short History of RL

RL is a sub-field of machine learning, where agents learn how to behave optimally based on a trial-and-error procedure during their interaction with the environment. Unlike supervised learning, which takes labelled data as the input (for example, an image labelled with cats), RL is goal-oriented: it constructs a learning model that learns to achieve the optimal long-term goal by improvement through trial and error, with the learner having no labelled data to obtain knowledge from. The word “reinforcement” refers to the learning mechanism since the actions that lead to satisfactory outcomes are reinforced in the learner’s set of behaviours.

![[Pasted image 20230801095432.png]]
**Figure 2:** The success of the AlphaGo series marks the maturation of the single-agent decisionmaking process. The year 2019 was a booming year for MARL techniques; remarkable progress was achieved in solving immensely challenging multi-player realstrategy video games and multi-player incomplete-information poker games.

Historically, the RL mechanism was originally developed based on studying cats’ behaviour in a puzzle box ([Thorndike], [1898]). [Minsky] ([1954]) first proposed the computational model of RL in his Ph.D. thesis and named his resulting analog machine the _stochastic neural-analog reinforcement calculator_. Several years later, he first suggested the connection between dynamic programming ([Bellman], [1952]) and RL ([Minsky], [1961]). In 1972, [Klopf] ([1972]) integrated the trial-and-error learning process with the finding of _temporal_ _difference_ _(TD)_ learning from psychology. TD learning quickly became indispensable in scaling RL for larger systems. On the basis of dynamic programming and TD learning, [Watkins and Dayan] ([1992]) laid the foundations for present day RL using the Markov decision process (MDP) and proposing the famous Q-learning method as the solver. As a dynamic programming method, the original Q-learning process inherits Bellman’s “curse of dimensionality” ([Bellman], [1952]), which strongly limits its applications when the number of state variables is large. To overcome such a bottleneck, [Bertsekas and] [Tsitsiklis] ([1996]) proposed approximate dynamic programming methods based on neural networks. More recently, [Mnih et al.] ([2015]) from DeepMind made a significant breakthrough by introducing the deep Q-learning (DQN) architecture, which leverages the representation power of DNNs for approximate dynamic programming methods. DQN has demonstrated human-level performance on 49 Atari games. Since then, deep RL techniques have become common in machine learning/AI and have attracted considerable attention from the research community.

RL originates from an understanding of animal behaviour where animals use trialand-error to reinforce beneficial behaviours, which they then perform more frequently. During its development, computational RL incorporated ideas such as optimal control theory and other findings from psychology that help mimic the way humans make decisions to maximise the long-term profit of decision-making tasks. As a result, RL methods can naturally be used to train a computer program (an agent) to a performance level comparable to that of a human on certain tasks. The earliest success of RL methods against human players can be traced back to the game of backgammon ([Tesauro], [1995]). More recently, the advancement of applying RL to solve sequential decision-making problems was marked by the remarkable success of the AlphaGo series ([Silver et al.], [2016], [2018], [2017]), a self-taught RL agent that beats top professional players of the game GO, a game whose search space (10761 possible games) is even greater than the number of atoms in the universe[1].

In fact, the majority of successful RL applications, such as those for the game GO[2], robotic control ([Kober et al.], [2013]), and autonomous driving ([Shalev-Shwartz et al.], [2016]), naturally involve the participation of multiple AI agents, which probe into the realm of MARL. As we would expect, the significant progress achieved by single-agent RL methods – marked by the 2016 success in GO – foreshadowed the breakthroughs of multi-agent RL techniques in the following years.

#### 1.2 2019: A Booming Year for MARL

2019 was a booming year for MARL development as a series of breakthroughs were made in immensely challenging multi-agent tasks that people used to believe were impossible to solve via AI. Nevertheless, the progress made in the field of MARL, though remarkable, has been overshadowed to some extent by the prior success of AlphaGo ([Chalmers], [2020]). It is possible that the AlphaGo series ([Silver et al.], [2016], [2018], [2017]) has largely fulfilled people’s expectations for the effectiveness of RL methods, such that there is a lack of interest in further advancements in the field. The ripples caused by the progress of MARL were relatively mild among the research community. In this section, we highlight several pieces of work that we believe are important and could profoundly impact the future development of MARL techniques.

One popular test-bed of MARL is StarCraft II ([Vinyals et al.], [2017]), a multi-player real-time strategy computer game that has its own professional league. In this game, each player has only limited information about the game state, and the dimension of the search space is orders of magnitude larger than that of GO (1026 possible choices for every move). The design of effective RL methods for StarCraft II was once believed to be a long-term challenge for AI ([Vinyals et al.], [2017]). However, a breakthrough was accomplished by AlphaStar in 2019 ([Vinyals et al.], [2019a]), which has exhibited grandmaster-level skills by ranking above 99_._8% of human players. Another prominent video game-based test-bed for MARL is Dota2, a zero-sum game played by two teams, each composed of five players. From each agent’s perspective, in addition to the difficulty of incomplete information (similar to StarCraft II), Dota2 is more challenging, in the sense that both cooperation among team members and competition against the opponents must be considered. The OpenAI Five AI system ([Pachocki et al.], [2018]) demonstrated superhuman performance in Dota2 by defeating world champions in a public e-sports competition.

(1There are an estimated 1082 atoms in the universe. If one had one trillion computers, each processing one trillion states per second for one trillion years, one could only reach 1043 states.
2Arguably, AlphaGo can also be treated as a multi-agent technique if we consider the opponent in self-play as another agent.)

In addition to StarCraft II and Dota2, [Jaderberg et al.] ([2019]) and [Baker et al.] ([2019a]) showed human-level performance in capture-the-flag and hide-and-seek games, respectively. Although the games themselves are less sophisticated than either StarCraft II or Dota2, it is still non-trivial for AI agents to master their tactics, so the agents’ impres-sive performance again demonstrates the efficacy of MARL. Interestingly, both authors reported emergent behaviours induced by their proposed MARL methods that humans can understand and are grounded in physical theory.

![[Pasted image 20230801095802.png]]
**Figure 3:** Diagram of a single-agent MDP (left) and a multi-agent MDP (right).

One last remarkable achievement of MARL worth mentioning is its application to the poker game Texas hold’ em, which is a multi-player extensive-form game with incomplete information accessible to the player. Heads-up (namely, two player) no-limit hold’em hasmore than 6 _×_ 10161 information states. Only recently have ground-breaking achieve-ments in the game been made, thanks to MARL. Two independent programs, _DeepStack_ ([Moravˇc´ık et al.], [2017]) and _Libratus_ ([Brown and Sandholm], [2018]), are able to beat professional human players. Even more recently, Libratus was upgraded to Pluribus ([Brown] [and Sandholm], [2019]) and showed remarkable performance by winning over one million dollars from five elite human professionals in a no-limit setting.

For a deeper understanding of RL and MARL, mathematical notation and deconstruction of the concepts are needed. In the next section, we provide mathematical formulations for these concepts, starting from single-agent RL and progressing to multi-agent RL methods.

# 2 Single-Agent RL

Through trial and error, an RL agent attempts to find the optimal policy to maximise its long-term reward. This process is formulated by Markov Decision Processes.

#### 2.1 Problem Formulation: Markov Decision Process

**Definition 1 (Markov Decision Process)** $A_n$ _MDP can be described by a tuple of key_ _elements_ _(_S_,_ A_,_ $P_,$ _R, γ分._

- S: the set of environmental states._
- A: the set of agent’s possible actions._
- $P$ : S _×_ A _→_ _∆_(S)_: for each time step t_ ∈ N_, given agent’s action a_ ∈ A_, the transition probability from a state s_ ∈ S _to the state in the next time step $s_I ∈ S$.
- $R$ : S _×_ A _×_ S _→_ R_: the reward function that returns a scalar value to the agent for a transition from s to s_I_ _as a result of action a. The rewards have absolute values_ _uniformly bounded by R_max_._
- $γ$ ∈ [0_,_ 1] _is the discount factor that represents the value of time._
At each time step $t$, the environment has a state $s_t$. The learning agent observes this state[3] and executes an action $a_t$. The action makes the environment transition into the next state _st_+1 _∼_ _P_ (_·|_st, at_), and the new environment returns an immediate reward _R_(_st,_ _at,_ _st_+1) to the agent. The reward function can be also written as $R$ : S _×_ A _→_ R, which is interchangeable with _R_ : S _×_ A _×_ S _→_ R (see [Van Otterlo and Wiering] ([2012]), page 10). The goal of the agent is to solve the MDP: to find the optimal policy that maximises the reward over time. Mathematically, one common objective is for the agent to find a Markovian (i.e., the input depends on only the current state) and stationary (i.e., function form is time-independent) policy function[4] _π_ : S _→_ _∆_(A), with _∆_(_·_) denoting the probability simplex, which can guide it to take sequential actions such that the discounted cumulative reward is maximised:
$$\mathbb{E}_{s_{t+1}\sim P(\cdot |s_{t},a_{t})}\left[\sum_{t>0}\gamma^{t}R\left(s_{t},a_{t},s_{t+1}\right) \left|a_{t}\sim\pi\left(\cdot\right|s_{t}\right),s_{0}\right]~.\tag1$$

(3The agent can only observe part of the full environment state. The partially observable setting is introduced in Denition 7 as a special case of Dec-PODMP.
4Such an optimal policy exists as long as the transition function and the reward function are both Markovian and stationary (Feinberg, 2010).)

Another common mathematical objective of an MDP is to maximise the time-average reward:
$$\operatorname*{lim}_{T\rightarrow\infty}\mathbb{E}_{s_{t+1}\sim P(1)s_{t,a t}}\left[\frac{1}{T}\sum_{t=0}^{T-1}R(s_{t},a_{t},s_{t+1}\left|a_{t}\sim\pi\left(\cdot\mid s_{t}\right),s_{0}\right.\right],\tag2$$

which we do not consider in this work and refer to [Mahadevan] ([1996]) for a full analysis of the objective of time-average reward.

Based on the objective function of Eq. ([1]), under a given policy $π$, we can define the state-action function (namely, the Q-function, which determines the expected return from undertaking action $a$ in state _s_) and the value function (which determines the return associated with the policy in state _s_) as:
$$
Q^{\pi}(s,a)=\mathbb{E}^{\pi}\left[\sum_{t\geq0}\gamma^{t}R\left(s_{t},a_{t},s_{t+1}\right)|a_{0}=a,s_{0}=s\right],\forall s\in\mathbb{S},a\in\mathbb{A}\tag3
$$
$$V^{\pi}(s)=\mathbb{E}^{\pi}\left[\sum_{t>0}\gamma^{t}R\left(s_{t},a_{t},s_{t+1}\right)|s_{0}=s\right],\forall s\in\mathbb{Q}\tag4$$

where E_π_ is the expectation under the probability measure P_π_ over the set of infinitely long state-action trajectories $τ$ = (_s_0_, a_0_, s_1_, a_1_, _) and where P_π_ is induced by state transition probability $P$ , the policy $π$, the initial state $s$ and initial action $a$ (in the case of the Q-function). The connection between the Q-function and value function is
$V^{\pi}(s)=\mathbb{E}_{a\sim\pi(\cdot)s}[Q^{\pi}(s,a)]$ and $Q^{\pi}=\mathbb{E}_{s^{\prime}\sim P(\cdot|s,a)}[R(s,a,s^{\prime})+V^{\pi}(s^{\prime})].$


#### 2.2 Justification of Reward Maximisation

The current model for RL, as given by Eq. ([1]), suggests that the expected value of a single reward function is sufficient for any problem we want our “intelligent agents” to solve. The justification for this idea is deeply rooted in the _von Neumann-Morgenstern (VNM) utility theory_ ([Von Neumann and Morgenstern], [2007]). This theory essentially proves that an agent is _VNM-rational_ if and only if there exists a real-valued utility (or, reward) function such that every preference of the agent is characterised by maximising the single expected reward. The VNM utility theorem is the basis for the well-known _expected_ _utility theory_ ([Schoemaker], [2013]), which essentially states that _rationality_ can be modelled as maximising an expected value. Specifically, the VNM utility theorem provides both necessary and sufficient conditions under which the expected utility hypothesis holds. In other words, rationality is equivalent to VNM-rationality, and it is safe to assume an intelligent entity will always choose the action with the highest expected utility in any complex scenarios.

Admittedly, it was accepted long before that some of the assumptions on rationality could be violated by real decision-makers in practice ([Gigerenzer and Selten], [2002]). In fact, those conditions are rather taken as the “axioms” of rational decision making. In the case of the multi-objective MDP, we are still able to convert multiple objectives into a single-objective MDP with the help of a _scalarisation function_ through a two-timescale process; we refer to [Roijers et al.] ([2013]) for more details.

#### 2.3 Solving Markov Decision Processes

One commonly used notion in MDPs is the (discounted-normalised) occupancy measure _µπ_(_s,_ _a_), which uniquely corresponds to a given policy $π$ and vice versa ([Syed et al.], [2008], Theorem 2), defined by
$$\begin{array}{ll}\mu^{\pi}(s,a)&=\mathbb{E}_{s_{t}\sim P,a_{t}\sim\pi}\left[(1-\gamma)\sum_{t>0}\gamma^{t}\mathbb{1}_{(s_{t}=s\wedge a_{t}=a)}\right].\\ &{{=(1-\gamma)\sum_{t>0}\gamma^{t}\mathbb{P}^{\pi}(s_{t}=s,a_{t}=a),}}\end{array}\qquad\qquad(5)$$

where 1 is an indicator function. Note that in Eq. ([5]), $P$ is the state transitional probability and P_π_ is the probability of specific state-action pairs when following stationary policy $π$. The physical meaning of _µπ_(_s,_ _a_) is that of a probability measure that counts the expected discounted number of visits to the individual admissible state-action pairs. Correspondingly, _µπ_(_s_) = $a$ _µπ_(_s,_ _a_) is the discounted state visitation frequency, i.e., the stationary distribution of the Markov process induced by $π$. With the occupancy measure, we can write Eq. ([4]) as an inner product of _V π_(_s_) = 1 _µπ_(_s,_ _a_)_, R_(_s, a_) .1_−_γ_

This implies that solving an MDP can be regarded as solving a linear program (LP) of max_µ_ _µ_(_s, a_)_, R_(_s, a_) , and the optimal policy is then
$$
\pi^{*}(a|s)=\mu^{*}(s,a)/\mu^{*}(s)\tag6
$$


However, this method for solving the MDP remains at a textbook level, aiming to offer theoretical insights but lacking practically in the case of a large-scale LP with millions of variables ([Papadimitriou and Tsitsiklis], [1987]). When the state-action space of an MDP is continuous, LP formulation cannot help solve either.

In the context of optimal control ([Bertsekas], [2005]), dynamic-programming approaches, such as policy iteration and value iteration, can also be applied to solve for the optimal policy that maximises Eq. ([3]) & Eq. ([4]), but these approaches require knowledge of the exact form of the model: the transition function $P$ (_·|_s, a_), and the reward function _R_(_s, a, s_I_) .

On the other hand, in the setting of RL, the agent learns the optimal policy by a trial-and-error process during its interaction with the environment rather than using prior knowledge of the model. The word “learning” essentially means that the agent turns its experience gained during the interaction into knowledge about the model of the environment. Based on the solution target, either the optimal policy or the optimal value function, RL algorithms can be categorised into two types: value-based methods and policy-based methods.

#### 2.3.1 Value-Based Methods

For all MDPs with finite states and actions, there exists at least one deterministic stationary optimal policy ([Sutton and Barto], [1998]; [Szepesv´ari], [2010]). Value-based methods are introduced to find the optimal Q-function _Q_∗_ that maximises Eq. ([3]). Correspondingly, the optimal policy can be derived from the Q-function by taking the greedy action of _π_∗_ = arg max_a_ _Q_∗_(_s, a_). The classic Q-learning algorithm ([Watkins and Dayan], [1992]) approximates _Q_∗_ by _Q_ˆ, and updates its value via temporal-difference learning ([Sutton], [1988]).
$$
\underbrace{\hat{Q}(s_t,a_t)}_{\text{new value}}\leftarrow\underbrace{\hat{Q}(s_t,a_t)}_{\text{old value}}+\underbrace{\alpha}_{\text{learning rate}}\cdot\overbrace{\left(\underbrace{R_t+\gamma\cdot\max_{a\in\mathbb{A}}\hat{Q}(s_{t+1},a)}_{\text{temporal difference target}}-\underbrace{\hat{Q}(s_t,a_t)}_{\text{old value}}\right)}^{\text{temporal difference error}}\tag7
$$


Theoretically, given the Bellman optimality operator **H**_∗_, defined by
$$(\mathbf{H}^{*}Q)(s,a)=\sum_{s^{\prime}}P(s^{\prime}|s,a)\left[R(s,a,s^{\prime})+\gamma\operatorname*{max}_{b\in\mathbb{A}}Q(s,b)\right],\tag8$$


we know it is a contraction mapping and the optimal Q-function is the unique[5] fixed point, i.e., **H**_∗_(_Q_∗_) = _Q_∗_. The Q-learning algorithm draws random samples of (_s, a, R, s_I_) in Eq. ([7]) to approximate Eq. ([8]), but is still guaranteed to converge to the optimal Qfunction ([Szepesv´ari and Littman], [1999]) under the assumptions that the state-action sets are discrete and finite and are visited an infinite number of times. [Munos and Szepesv´ari] ([2008]) extended the convergence result to a more realistic setting by deriving the high probability error bound for an infinite state space with a finite number of samples.

Recently, [Mnih et al.] ([2015]) applied neural networks as a function approximator for the Q-function in updating Eq. ([7]). Specifically, DQN optimises the following equation:
$$\operatorname*{min}_{\theta}\mathbb{E}_{s(s_{t},a_{t},s_{t+1})\sim D}\left[\left(R_{t}+\gamma\operatorname*{max}_{a\in\mathbb{A}}Q_{\theta^{-}}\left(s_{t+1},a\right)-Q_{\theta}\left(s_{t},a_{t}\right)\right)^{2}\right].\tag9$$

The neural network parameters $θ$ is fitted by drawing i.i.d. samples from the replay buffer $D$ and then updating in a supervised learning fashion. _Qθ_−_ is a slowly updated target network that helps stabilise training. The convergence property and finite sample analysis of DQN have been studied by [Yang et al.] ([2019c]).

**2.3.1** **Policy-Based Methods**

Policy-based methods are designed to directly search over the policy space to find the optimal policy _π_∗_. One can parameterise the policy expression _π_∗_ $≈$ _πθ_(_·|_s_) and update the parameter $θ$ in the direction that maximises the cumulative reward $θ$ ← $θ$ +_α_∇_θ_V_ _π_θ_ (_s_) to find the optimal policy. However, the gradient will depend on the unknown effects of policy changes on the state distribution. The famous policy gradient (PG) theorem ([Sutton] [et al.], [2000]) derives an analytical solution that does not involve the state distribution, that is:
$$\nabla_{\theta}V^{\pi_{\theta}}(s)=\mathbb{E}_{s\sim\mu^{\pi_{\theta}}(.),a\sim\pi_{\theta}(.|s)}{\Big[}\nabla_{\theta}\log\pi_{\theta}(a|s)\cdot Q^{\pi_{\theta}}(s,a){\Big]}\tag{10}$$

where _µπ_θ_ is the state occupancy measure under policy $π_θ$ and $∇$ log _πθ_(_a_|_s_) is the updating score of the policy. When the policy is deterministic and the action set is continuous, one obtains the deterministic policy gradient (DPG) theorem ([Silver et al.], [2014]) as
$$\nabla_{\theta}V^{\pi_{\theta}}(s)=\mathbb{E}_{s\sim\mu^{\pi_{\theta}}(\cdot)}\left[\nabla_{\theta}\pi_{\theta}(a|s)\cdot\nabla_{a}Q^{\pi_{\theta}}(s,a)|_{a=\pi_{\theta}(s)}\right].\tag{11}$$

(5Note that although the optimal Q-function is unique, its corresponding optimal policies may have multiple candidates.)

A classic implementation of the PG theorem is REINFORCE ([Williams], [1992]), which uses a sample return _R_t_ = 2_T_ _γ_i_−_t_r_i_ to estimate _Q_π_θ_. Alternatively, one can use a model of $Q_ω$ (also called _critic_) to approximate the true _Qπ_θ_ and update the parameter $ω$ via TD learning. This approach gives rise to the famous actor-critic methods ([Konda] [and Tsitsiklis], [2000]; [Peters and Schaal], [2008]). Important variants of actor-critic methods include trust-region methods ([Schulman et al.], [2015], [2017]), PG with optimal baselines ([Weaver and Tao], [2001]; [Zhao et al.], [2011]), soft actor-critic methods ([Haarnoja et al.], [2018]), and deep deterministic policy gradient (DDPG) methods ([Lillicrap et al.], [2015]).

![[Pasted image 20230801100558.png]]
**Figure 4:** A snapshot of stochastic time in the intersection example. The scenario is abstracted such that there are two cars, with each car taking one of two possible actions: to yield or to rush. The outcome of each joint action pair is represented by a normalform game, with the reward value for the row player denoted in red and that for the column player denoted in black. The Nash equilibria (NE) of this game are (rush, yield) and (yield, rush). If both cars maximise their own reward selfishly without considering the others, they will end up in an accident.

# 3 Multi-Agent RL

In the multi-agent scenario, much like in the single-agent scenario, each agent is still trying to solve the sequential decision-making problem through a trial-and-error procedure. The difference is that the evolution of the environmental state and the reward function that each agent receives is now determined by all agents’ joint actions (see Figure [3]). As a result, agents need to take into account and interact with not only the environment but also other learning agents. A decision-making process that involves multiple agents is usually modelled through a stochastic game ([Shapley], [1953]), also known as a Markov game ([Littman], [1994]).

#### 3.1 Problem Formulation: Stochastic Game

**Definition 2 (Stochastic Game)** _A stochastic game can be regarded as a multi-player_[6] _extension to the MDP in Definition_ [_1_]_. Therefore, it is also defined by a set of key elements_ _(_N,_ S_,_ _{_A_i_}_i_∈{_1_,,N_ }, P,_ _{_R_i_}_i_∈{_1_,,N_ }, γ_分_._

- $N_:$ _the number of agents, N_ = 1 _degenerates_ _to a single-agent MDP, N_ _»_ 2 _is referred as many-agent cases in this paper._
- S_: the set of environmental states shared by all agents._
- A_i_:_ _the set of actions of agent i. We denote_ A := A1 _× · · · ×_ A_N_ _._
- $P$ : S _×_ A _→_ _∆_(S)_: for each time step t_ ∈ N_, given agents’ joint actions_ **_a_** ∈ A_, the transition probability from state s_ ∈ S _to state s_I_ ∈ S _in the next time step._
- $R_i$ : S _×_ A _×_ S _→_ R_: the reward function that returns a scalar value to the i−th agent for a transition from_ (_s,_ **_a_**) _to s_I_._ _The rewards have absolute values uniformly_ _bounded_ _by R_max_._
- $γ$ ∈ [0_,_ 1] _is the discount factor that represents the value of time._
We use the superscript of (_·_i_,_ _·_−_i_) (for example, **_a_** = (_ai, a_−_i_)), when it is necessary to distinguish between agent $i$ and all other _N_ _−_ 1 opponents.

Ultimately, the stochastic game (SG) acts as a framework that allows simultaneous moves from agents in a decision-making scenario[7]. The game can be described sequentially, as follows: At each time step $t$, the environment has a state $s_t$, and given $s_t$, each agent executes its action $a_i$, simultaneously with all other agents. The joint action from all agents makes the environment transition into the next state _st_+1 _∼_ _P_ (_·|_st,_ **_a_**_t_); then, the environment determines an immediate reward _Ri_(_st, **a**t, st_+1) for each agent. As seen in the single-agent MDP scenario, the goal of each agent $i$ is to solve the SG. In other words, each agent aims to find a behavioural policy (or, a mixed strategy[8] in game theory terminology ([Osborne and Rubinstein], [1994])), that is, $π_i$ ∈ $Π_i$ : S _→_ _∆_(A_i_) that can guide the agent to take sequential actions such that the discounted cumulative reward[9] in Eq. ([12]) is maximised. Here, _∆_(_·_) is the probability simplex on a set. In game theory, $π_i$ is also called a pure strategy (vs a mixed strategy) if _∆_(_·_) is replaced by a Dirac measure.
![[Pasted image 20230801100858.png]]

(6Player is a common word used in game theory; agent is more commonly used in machine learning. We do not discriminate between their usages in this work. The same holds for strategy vs policy and utility/payovs reward. Each pair refers to the game theory usage vs machine learning usage.
7Extensive-form games allow agents to take sequential moves; the full description can be found in (Shoham and Leyton-Brown, 2008, Chapter 5).
8Abehavioural policy refers to a function map from the history (s0ai 0s1ai 1 st 1) to an action. The policy is typically assumed to be Markovian such that it depends on only the current state st rather than the entire history. A mixed strategy refers to a randomisation over pure strategies (for example, the actions). In SGs, the behavioural policy and mixed policy are exactly the same. In extensive-form games, they are dierent, but if the agent retains the history of previous actions and states (has perfect recall), each behavioural strategy has a realisation-equivalent mixed strategy, and vice versa (Kuhn, 1950a).
9Similar to single-agent MDP, we can adopt the objective of time-average rewards.)

Comparison of Eq. ([12]) with Eq. ([4]) indicates that the optimal policy of each agent is influenced by not only its own policy but also the policies of the other agents in the game. This scenario leads to fundamental differences in the _solution concept_ between single-agent RL and multi-agent RL.

#### 3.2 Solving Stochastic Games

An SG can be considered as a sequence of normal-form games, which are games that can be represented in a matrix. Take the original intersection scenario as an example (see Figure [4]). A snapshot of the SG at time $t$ (stage game) can be represented as a normal-form game in a matrix format. The rows correspond to the action set A1 for agent 1, and the columns correspond to the action set A2 for agent 2. The values of the matrix are the rewards given for each of the joint action pairs. In this scenario, if both agents care only about maximising their own possible reward with no consideration of other agents (the solution concept in a single-agent RL problem) and choose the action to rush, they will reach the outcome of crashing into each other. Clearly, this state is unsafe and is thus sub-optimal for each agent, despite the fact that the possible reward was the highest for each agent when rushing. Therefore, to solve an SG and truly maximise the cumulative reward, each agent must take strategic actions with consideration of others when determining their policies.

Unfortunately, in contrast to MDPs, which have polynomial time-solvable linearprogramming formulations, solving SGs usually involves applying Newton’s method for solving nonlinear programs. However, there are two special cases of two-player general-sum discounted-reward SGs that can still be written as LPs ([Shoham and Leyton-Brown], [2008], Chapter 6.2)[10]. They are as follows:

- _single-controller SG_ : the transition dynamics are determined by a single player, i.e., _P_ (_·|_**_a_**_,_ _s_) = _P_ (_·|_a_i_,_ _s_) if the i-th index in the vector **_a_** is **_a_**[_i_] = _a_i_,_ _∀_s_ ∈ S_,_ _∀_**_a_** ∈ A.
- _separable_ _reward_ _state independent transition (SR-SIT) SG_ : the states and the actions have independent effects on the reward function and the transition function depends on only the joint actions, i.e., _∃_α_ : S _→_ R_, β_ : A _→_ R such that these two conditions hold: 1) _R_i_(_s,_ **_a_**) = _α_(_s_) + _β_(**_a_**)_,_ _∀_i_ ∈ _{_1_,_ _,_ _N_ },_ _∀_s_ ∈ S_,_ _∀_**_a_** ∈ A_,_ and 2) _P_ (_·|_s_I_,_ **_a_**) = _P_ (_·|_s,_ **_a_**)_,_ _∀_**_a_** ∈ A_,_ _∀_s,_ _s_I_ ∈ S.
#### 3.2.1 Value-Based MARL Methods

The single-agent Q-learning update in Eq. ([7]) still holds in the multi-agent case. In the _t_-th iteration, for each agent $i$, given the transition data (_s_t_,_ **_a_**_t_,_ _R_i_,_ _s_t_+1)｝ _t_≥_0 sampled from the replay buffer, it updates only the value of _Q_(_st,_ **_a_**_t_) and keeps the other entries of the Q-function unchanged. Specifically, we have
![[Pasted image 20230801101109.png]]

Compared to Eq. ([7]), the max operator is changed to **eval**_i_ _{_Qi_(_st_+1_,_ _·_)_}_i_∈{_1_,,N_}_ in Eq. ([13]) to reflect the fact that each agent can no longer consider only itself but must evaluate the situation of the stage game at time step $t+ 1$ by considering all agents’ interests, as represented by the set of their Q-functions. Then, the optimal policy can be **solved** by **solve** i_ _{_Qi_(_st_+1_,_ _·_)_}_i_∈{_1_,,N_}_ = _πi,_∗_. Therefore, we can further write the evaluation operator as
![[Pasted image 20230801101139.png]]

In summary, **solve**_i_ returns agent _i_I_s part of the optimal policy at some equilibrium point (not necessarily corresponding to its largest possible reward), and **eval**_i_ gives agent _i_’s expected long-term reward under this equilibrium, assuming all other agents agree to play the same equilibrium.

(10According to Filar and Vrieze (2012) [Section 3.5], single-controller SG is solvable in polynomial time only under zero-sum cases rather than general-sum cases, which contradicts the result in Shoham and Leyton-Brown (2008) [Chapter 6.2], and we believe Shoham and Leyton-Brown (2008) made a typo.)

#### 3.2.2 Policy-Based MARL Methods

The value-based approach suffers from the curse of dimensionality due to the combinatorial nature of multi-agent systems (for further discussion, see Section [4.1]). This characteristic necessitates the development of policy-based algorithms with function approximations. Specifically, each agent learns its own optimal policy _π_i_i_ : S _→_ _∆_(A_i_) by updating the parameter $θ_i$ of, for example, a neural network. Let _θ_ = (_θi_)_i_∈{_1_,,N_}_ represent the collection of policy parameters for all agents, and let **_π_**_θ_ := _i_∈{_1_,,N__}__π_i_i_(_a_i_|_s_) be the joint policy. To optimise the parameter $θ_i$, the policy gradient theorem in Section [2.3.2] can be extended to the multi-agent context. Given agent _i_’s objective function_J_i_(_θ_)= E_s_∼_P,_**_a_**_∼_**_π_** [ 2 _t_≥_0 _γ_t_R_i_－, we have:
![[Pasted image 20230801101309.png]]

Considering a continuous action set with a deterministic policy, we have the multi-agent deterministic policy gradient (MADDPG) ([Lowe et al.], [2017]) written as
![[Pasted image 20230801101331.png]]

Note that in both Eqs. ([15]) & ([16]), the expectation over the joint policy **_π_**_θ_ implies that other agents’ policies must be observed; this is often a strong assumption for many real-world applications.

#### 3.2.3 Solution Concept of the Nash Equilibrium

Game theory plays an essential role in multi-agent learning by offering so-called _solution concepts_ that describe the outcomes of a game by showing which strategies will finally be adopted by players. Many types of solution concepts exist for MARL (see Section [4.2]), among which the most famous is probably the Nash equilibrium (NE) in non-cooperative game theory ([Nash], [1951]). The word “non-cooperative” does not mean agents cannot collaborate or have to fight against each other all the time, it merely means that each agent maximises its own reward independently and that agents cannot group into coalitions to make collective decisions.

In a normal-form game, the NE characterises an equilibrium point of the joint strategy profile (_π_1_,_∗_, , πN,_∗_), where each agent acts according to their **best response** to the others. The best response produces the optimal outcome for the player once all other players’ strategies have been considered. Player _i_’s best response[11] to _π_−_i_ is a set of policies in which the following condition is satisfied.
![[Pasted image 20230801101422.png]]

NE states that if all players are perfectly rational, none of them will have a motivation to deviate from their best response _πi,_∗_ given others are playing _π_−_i,_∗_. Note that NE is defined in terms of the best response, which relies on relative reward values, suggesting that the exact values of rewards are not required for identifying NE. In fact, NE is invariant under positive affine transformations of a players’ reward functions. By applying Brouwer’s fixed point theorem, [Nash] ([1951]) proved that a mixed-strategy NE always exists for any games with a finite set of actions. In the example of driving through an intersection in Figure [4], the NE are (_yield, rush_) and (_rush, yield_).

For a SG, one commonly used equilibrium is a stronger version of the NE, called the Markov perfect NE ([Maskin and Tirole], [2001]), which is defined by:

**Definition 3 (Nash Equilibrium for Stochastic Game)** _A Markovian strategy profile_ **_π_**_∗_ = (_πi,_∗_,_ _π_−_i,_∗_) _is a Markov perfect NE of a SG defined in Definition_ [_2_] _if the following_ _condition holds_
![[Pasted image 20230801101452.png]]

“Markovian” means the Nash policies are measurable with respect to a particular partition of possible histories (usually referring to the last state). The word “perfect” means that the equilibrium is also subgame-perfect ([Selten], [1965]) regardless of the starting state. Considering the sequential nature of SGs, these assumptions are necessary, while still maintaining generality. Hereafter, the Markov perfect NE will be referred to as NE.

(11Best responses may not be unique; if a mixed-strategy best response exists, there must be at least one best response that is also a pure strategy.
12Note that this is dierent from a single-agent MDP, where a single, purestrategy optimal policy always exists. A simple example is the rock-paper-scissors game, where none of the pure strategies is the NE and the only NE is to mix between the three equally.
13Average-reward SGs entail more subtleties because the limit of Eq. (2) in the multi-agent setting may be a cycle and thus not exist. Instead, NE are proved to exist on a special class of irreducible SGs, where every stage game can be reached regardless of the adopted policy.
14The class of NP-complete is not suitable to describe the complexity of solving the NE because the NE is proven to always exist (Nash, 1951), while a typical NP-complete problem the travelling salesman problem (TSP), for example searches for the solution to the question: Given a distance matrix and a budget B, nd a tour that is cheaper than B, or report that none exists (Daskalakis et al., 2009).")

A mixed-strategy NE[12] always exists for both discounted and average-reward[13] SGs ([Filar and Vrieze], [2012]), though they may not be unique. In fact, checking for uniqueness is $N_P$ -hard ([Conitzer and Sandholm], [2002]). With the NE as the solution concept of optimality, we can re-write Eq. ([14]) as:
![[Pasted image 20230801101612.png]]

In the above equation, **Nash**_i_(_·_) = _πi,∗_ computes the NE of agent _i_’s strategy, and _V i s, {_**Nashi**_}i∈{_1_,,N}_ is the expected payoff for agent $i$ from state $s$ onwards under this equilibrium. Eq. ([19]) and Eq. ([13]) form the learning steps of Nash Q-learning ([Hu] [et al.], [1998]). This process essentially leads to the outcome of a learnt set of optimal policies that reach NE for every single-stage game encountered. In the case when NE is not unique, Nash-Q adopts hand-crafted rules for equilibrium selection (e.g., all players choose the first NE). Furthermore, similar to normal Q-learning, the Nash-Q operator defined in Eq. ([20]) is also proved to be a contraction mapping, and the stochastic updating rule provably converges to the NE for all states when the NE is unique:
![[Pasted image 20230801101638.png]]

The process of finding a NE in a two-player general-sum game can be formulated as a linear complementarity problem (LCP), which can then be solved using the _LemkeHowson_ algorithm ([Shapley], [1974]). However, the exact solution for games with more than three players is unknown. In fact, the process of finding the NE is computationally demanding. Even in the case of two-player games, the complexity of solving the NE is _PPAD_-hard (polynomial parity arguments on directed graphs) ([Chen and Deng], [2006]; [Daskalakis et al.], [2009]); therefore, in the worst-case scenario, the solution could take time that is exponential in relation to the game size. This complexity[14] prohibits any brute force or exhaustive search solutions unless $P=NP$ (see Figure [5]). As we would expect, the NE is much more difficult to solve for general SGs, where determining whether a pure-strategy NE exists is _PSPACE_-hard. Even if the SG has a finite-time horizon, the calculation remains $N_P$ -hard ([Conitzer and Sandholm], [2008]). When it comes to approximation methods to _E_-NE, the best known polynomially computable algorithm can achieve $E$ = 0_._3393 on bimatrix games ([Tsaknakis and Spirakis], [2007]); its approach is to turn the problem of finding NE into an optimisation problem that searches for a stationary point.

![[Pasted image 20230801101744.png]]
**Figure 5:** The landscape of different complexity classes. Relevant examples are 1) solving the NE in a two-player zero-sum game, $P$ -complete ([Neumann], [1928]), 2) solvingthe NE in a general-sum game, _PPAD_-hard ([Daskalakis et al.], [2009]), 3) checkingthe uniqueness of the NE, $N_P$ -hard ([Conitzer and Sandholm], [2002]), 4) checking whether a pure-strategy NE exists in a stochastic game, _PSPACE_-hard ([Conitzer] [and Sandholm], [2008]), and 5) solving Dec-POMDP, _NEXPTIME_-hard ([Bernstein] [et al.], [2002]).

#### 3.2.4 Special Types of Stochastic Games

To summarise the solutions to SGs, one can think of the “master” equation

**Normal-form game solver** + **MDP solver** = **Stochastic game solver,**

which was first summarised by [Bowling and Veloso] ([2000]) (in Table 4). The first term refers to solving an equilibrium (NE) for the stage game encountered at every time step. It assumes the transition and reward function is known. The second term refers to applying a RL technique (such as Q-learning) to model the temporal structure in the sequential decision-making process. It assumes to only receive observations of the transition and reward function. The combination of the two gives a solution to SGs, where agents reach a certain type of equilibrium at each and every time step during the game.

Since solving general SGs with NE as the solution concept for the normal-form game is computationally challenging, researchers instead aim to study special types of SGs that have tractable solution concepts. In this section, we provide a brief summary of these special types of games.

![[Pasted image 20230801101840.png]]
**Figure 6:** Venn diagram of different types of games in the context of POSGs. The intersection of SG and Dec-POMDP is the team game. In the upper-half SG, we have MDP $⊂$ team games $⊂$ potential games $⊂$ identical-interest games $⊂$ SGs, and zero-sum games $⊂$ SGs. In the bottom-half Dec-POMDP, we have MDP $⊂$ team games $⊂$ Dec-MDP $⊂$ Dec-POMDPs, and MDP $⊂$ POMDP $⊂$ Dec-POMDP. We refer to Sections ([3.2.4] & [3.2.5]) for detailed definitions of these games.

**Definition 4 (Special Types of Stochastic Games)** _Given the general form of SG in Definition_ [_2_]_, we have the following special cases:_

- **_normal-form game/repeated_** **_game_**_:_ _|_S_|_ = 1_, see the example in Figure_ [_4_]_. These games have only a single state. Though not theoretically grounded, it is_practically easier to solve a small-scale SG._
- **_identical-interest setting_**[15]_:_ _agents share the same learning objective, which we_ _denote as_ R_. Since all agents are treated independently, each agent can safely choose_ _the action that maximises its own reward. As a result, single-agent RL algorithms_ _can_ $b_e$ _applied safely, and a decentralised method developed. Several types of SGs_ _fall_ _into this category._
– **_team_** **_games/fully cooperative games/multi-agent MDP (MMDP)_**_:_ _agents are assumed to be homogeneous and interchangeable, so importantly,_ _they share the same reward function_[16]_,_ R = _R_1 = _R_2 = _· · ·_ = $R_N$ _._

– **_team-average_** **_reward_** **_games/networked multi-agent MDP (M-MDP)_**_:_ _agents can have different reward functions, but they share the same objective,_![[Pasted image 20230801101944.png]]
– **_stochastic potential_** **_games_**_: agents can have different reward functions, but their mutual interests are described by a shared potential function_ R = $φ_,$ _define_d_ $a_s$ _φ_ : S_×_A _→_ R _such_ _that_ _∀_(_a_i_,_ _a_−_i_)_,_ (_b_i_,_ _a_−_i_) ∈ A_,_ _∀_i_ ∈ _{_1_,_ _,_ _N_ },_ _∀_s_ ∈ S _and the following equation holds:_
![[Pasted image 20230801102015.png]]
_Games of this type are guaranteed to have a pure-strategy NE (_[_Mguni_]_,_ [_2020_]_)._ _Moreover,_ _potential games degenerate to team games if one chooses the reward_ _function to be a potential function._

**_zero-sum setting_**_: agents share opposite interests and act competitively, and each agent optimises against the worst-case scenario. The NE in a zero-sum setting can be solved using a linear program (LP) in polynomial time because of the minimax theorem developed by_ [_Neumann_] _(_[_1928_]_). The idea of min-max values is also related to robustness in machine learning. We can subdivide the zero-sum setting as follows:_
– **_two-player_** **_constant-sum games_**_:_ _R_1(_s,_ $a_,$ _s_I_) + _R_2(_s,_ $a_,$ _s_I_) = $c_,$ _∀_(_s,_ $a_,$ _s_I_)_,_ _where_ _c is a constant and usually c_ = 0_. For cases when c_ _/_= 0_, one can always subtract the constant c for all payoff entries to make the game zero-sum._

– **_two-team competitive games_**_:_ _two teams compete against each other, with team sizes N_1 _and N_2_._ _Their reward functions are:_
![[Pasted image 20230801102746.png]]
_Team members within a team share the same objective of either_
![[Pasted image 20230801102820.png]]
or
![[Pasted image 20230801102828.png]]

– **_harmonic games_**_: Any normal-form game can be decomposed into a potential game plus a harmonic game (_[_Candogan et al._]_,_ [_2011_]_). A harmonic game (for_ _example, rock-paper-scissors) can be regarded as a general class of zero-sum_ _games with a harmonic property. Let_ _∀_**_p_** ∈ A $b_e$ _a joint pure-strategy profile,_ _and let_ A[_−_i_] = _{_**_q_** ∈ A : **_q_**_i_ **_p_**_i_,_ **_q_**_−_i_ = **_p_**_−_i_}_ $b_e$ _the set of strategies that differ_ _from_ **_p_** _on agent i; then, the harmonic property is:_

![[Pasted image 20230801102900.png]]

- **_linear-quadratic (LQ) setting_**_: the transition model follows linear dynamics, and the reward function is quadratic with respect to the states and actions. Compared to a black-box reward function, LQ games offer a simple setting. For example, actor-critic methods are known to facilitate convergence to the NE of zero-sum LQ games (_[_Al-Tamimi_ _et al._]_,_ [_2007_]_). Again, the LQ setting can be subdivided as follows:_
– **_two-player zero-sum LQ games_**_: Q_ ∈ R_|_S_|_, U_ 1 ∈ R_|_A1_|_ _and W_ 2 ∈ R_|_A2_|_ _are_ _the known cost matrices for the state and action spaces, respectively, while the_ _matrices A_ ∈ R_|_S_|×|_S_|_, B_ ∈ R_|_S_|×|_A1_|_, C_ ∈ R_|_S_|×|_A2_|_ _are_ _usually unknown to the_ _agent:_
![[Pasted image 20230801102934.png]]

**_multi-player general-sum LQ games_**_: the difference with respect to a twoplayer game is that the summation of the agents’ rewards does not necessarily equal zero:_
![[Pasted image 20230801102952.png]]

(15In some of the literature on this topic, identical-interest games are equivalent to team games. Here, we refer to this type of game as a more general class of games that involve a shared objective function that all agents collectively optimise, although their individual reward functions can still be dierent.
16In some of the literature on this topic (for example, Wang and Sandholm (2003)), agents are assumed to receive the same expected reward in a team game, which means in the presence of noise, dierent agents may receive dierent reward values at a particular moment.)

#### 3.2.5 Partially Observable Settings

A partially observable stochastic game (POSG) assumes that agents have no access to the exact environmental state but only an observation of the true state through an observation function. Formally, this scenario is defined by:

**Definition 5 (partially-observable stochastic games)** _A POSG is defined by the set_

_(_N_,_ S_,_ _{_A_i_}_i_∈{_1_,,N_}_,_ _P_,_ _{_Ri_}_i_∈{_1_,,N_}_,_ _γ_,_ _{_O_i_}_i_∈{_1_,,N_}_,_ _O_分_._ _In addition to the SG defined in_ _new_l_y_ _added_

_Definition_ [_2_]_, POSGs add the following terms:_
![[Pasted image 20230801103054.png]]
![[Pasted image 20230801103106.png]]

Although the added partial-observability constraint is common in practice for many real-world applications, theoretically it exacerbates the difficulty of solving SGs. Even in the simplest setting of a two-player fully cooperative finite-horizon game, solving a POSG is _NEXP_ -hard (see Figure [5]), which means it requires super-exponential time to solve in the worst-case scenario ([Bernstein et al.], [2002]). However, the benefits of studying games in the partially observable setting come from the algorithmic advantages. Centralised-training-with-decentralised-execution methods ([Foerster et al.], [2017a]; [Lowe] [et al.], [2017]; [Oliehoek et al.], [2016]; [Rashid et al.], [2018]; [Yang et al.], [2020]) have achieved many empirical successes, and together with DNNs, they hold great promise.

A POSG is one of the most general classes of games. An important subclass of POSGs is decentralised partially observable MDP (Dec-POMDP), where all agents share the same reward. Formally, this scenario is defined as follows:

**Definition 6 (Dec-POMDP)** _A Dec-POMDP is a special type of POSG defined in Definition_ [_5_] _with R_1 = _R_2 = _· · ·_ = _RN ._

Dec-POMDPs are related to single-agent MDPs through the partial observability condition, and they are also related to stochastic team games through the assumption of identical rewards. In other words, versions of both single-agent MDPs and stochastic team games are particular types of Dec-POMDPs (see Figure [6]).

**Definition 7 (Special types of Dec-POMDPs)** _The following games are special types_ _of Dec-POMDPs._

- **_partially observable MDP (POMDP)_**_: there is only one agent of interest, N_ = 1_. This scenario is equivalent to a single-agent MDP in Definition_ [_1_] _with a partial-observability constraint._
- **_decentralised_** **_MDP (Dec-MDP)_**_:_ _the agents in a Dec-MDP have joint full observability. That is, if all agents share their observations, they can recover the state of the Dec-MDP unanimously. Mathematically, we have_ _∀_**_o_** ∈ O_,_ _∃_s_ ∈ S _such_ _that_ P(_St_ = _s_|_O_t_ = **_o_**) = 1_._
- **_fully_** **_cooperative_** **_stochastic games_**_: assuming each agent has full observability,_ _∀_i_ = _{_1_,_ _,_ _N_ },_ _∀_o_i_ ∈ _O_i_,_ _∃_s_ ∈ S _such_ _that_ P(_S_t_ = _s_|_O_t_ = _o_i_) = 1_._ _The fullycooperative SG from Definition_ [_4_] _is a type of Dec-POMDP._
I conclude Section [3] by presenting the relationships between the many different types of POSGs through a Venn diagram in Figure [6].

#### 3.3 Problem Formulation: Extensive-Form Game

An SG assumes that a game is represented as a large table in each stage where the rows and columns of the table correspond to the actions of the two players[17]. Based on the big table, SGs model the situations in which agents act simultaneously and then receive their rewards. Nonetheless, for many real-world games, players take actions alternately. Poker is one class of games in which who plays first has a critical role in the players’ decision-making process. Games with alternating actions are naturally described by an extensive-form game (EFG) ([Osborne and Rubinstein], [1994]; [Von Neumann and] [Morgenstern], [1945]) through a tree structure. Recently, [Kovaˇr´ık et al.] ([2019]) has made a significant contribution in unifying the framework of EFGs and the framework of POSGs. Figure [7] shows the game tree of two-player Kuhn poker ([Kuhn], [1950b]). In Kuhn poker, the dealer has three cards, a King, Queen, and Jack (King_>_Queen_>_Jack), each player is dealt one card (the orange nodes in Figure [7]), and the third card is put asideunseen. The game then develops as follows.

- Player one acts first; he/she can _check_ or _bet_.
- If player one _checks_, then player two decides to _check_ or _bet_.
- If player two _checks_, then the higher card wins 1$ from the other player.
- If player two _bets_, then player one can _fold_ or _call_.
- If player one _folds_, then player two wins 1$ from player one.
- If player one _calls_, then the higher card wins 2$ from the other player.
- If player one _bets_, then player two decides to _fold_ or _call_.
- If player two _folds_, then player one wins 1$ from player two.
- If player two calls, then the higher card wins 2$ from the other player.
An important feature of EFGs is that they can handle imperfect information for multiplayer decision making. In the example of Kuhn poker, the players do not know which card the opponent holds. However, unlike Dec-POMDP, which also models imperfect information in the SG setting but is intractable to solve, EFG, represented in an equivalent sequence form, can be solved by an LP in polynomial time in terms of game states ([Koller] [and Megiddo], [1992]). In the next section, we first introduce EFG and then consider the sequence form of EFG.

![[Pasted image 20230801103312.png]]
**Figure 7:** Game tree of two-player Kuhn poker. Each node (i.e., circles, squares and rectangles) represents the choice of one player, each edge represents a possible action, and the leaves (i.e., diamond) represent final outcomes over which each player has a reward function (only player one’s reward is shown in the graph since Kuhn poker is a zerosum game). Each player can observe only their own card; for example, when player one holds a Jack, it cannot tell whether player two is holding a Queen or a King, so the choice nodes of player one in each of the two scenarios stay within the same information set.

**Definition 8 (Extensive-form Game)** $A_n$ _(imperfect-information) EFG can be described by a tuple of key elements_ _(_N_,_ A_,_ H_,_ T_,_ _{_Ri_}_i_∈{_1_,,N_}_,_ _χ, ρ, P,_ _{_S_i_}_i_∈{_1_,,N_}_分_._

- $N_:$ _the number of players. Some EFGs involve a special player called “chance”,_ _which has a fixed stochastic policy that represents the randomness of the environ-_ _ment. For example, the chance player in Kuhn poker is the dealer, who distributes cards to the players at the beginning._
- A_: the (finite) set of all agents’ possible actions._
- H_: the (finite) set of non-terminal choice nodes._
- T_: the (finite) set of terminal choice nodes, disjoint from_ H_._
- _χ_ : H _→_ 2_|_A_|_ _is the action function that assigns a set of valid actions to each choice_ _node._
- _ρ_ : H _→ {_1_, , N_ _}_ _is the player indicating function that assigns, to each nonterminal node, a player who is due to choose an action at that node._
- _P_ : H _×_ A _→_ H _∪_ T _is the transition function that maps a choice node and an action to a new choice/terminal node such that_ _∀_h_1_,_ _h_2 ∈ H _and_ _∀_a_1_,_ _a_2 ∈ A_, if P_ (_h_1_,_ _a_1) = _P_ (_h_2_,_ _a_2)_,_ _then h_1 = _h_2 _and a_1 = _a_2_._
- $R_i$ : T _→_ R _is a real-valued reward function for player i on the terminal node. Kuhn_ _poker_ _is a zero-sum game since R_1 + _R_2 = 0_._
- _a set of equivalence classes/partitions_ S_i_= (_S_i_,_ _,_ _S_i_i_) _for_ _agent_ _i_ $o_n$ _{_h_ ∈ H : _ρ_(_h_) = _i_}_ _with_ _the_ _pr_op_erty_ _that_ _∀_j_ ∈ _{_1_,_ _,_ _k_i_}_,_ _∀_h,_ _h_I_ ∈ _S_i_,_ $w_e$ _have_ _χ_(_h_) = _χ_(_h_I_) and ρ(h) = ρ(h_I). The set Si is also called an information state. The physical _meaning of the information state is that the choice nodes of an information state are indistinguishable. In other words, the set of valid actions and agent identities for the choice nodes within an information state are the same; one can thus use_ _χ_(_Si_)_, ρ_(_Si_) _to denote χ_(_h_)_, ρ_(_h_)_,_ _∀_h_ ∈ _Si._
Inclusion of the information sets in EFG helps to model the imperfect-information cases in which players have only partial or no knowledge about their opponents. In the case of Kuhn poker, each player can only observe their own card. For example, when player one holds a Jack,it cannot tell whether player two is holding a Queen or a King, so the choice nodes of player one under each of the two scenarios (Queen or King) stay within the same information set. Perfect-information EFGs (e.g., GO or chess) are a special case where the information set is a singleton, i.e., _|_S_i_|_ = 1_,_ _∀_j_, so a choice node can be equated to the unique history that leads to it. Imperfect-information EFGs (e.g., Kuhn poker or Texas hold’em) are those in which there exists $i_,$ _j_ such that _|_S_i_|_ _≥_ 1, so the information state can represent more than one possible history. However, with the assumption of perfect recall (described later), the history that leads to an information state is still unique.

(17A multi-player game is represented as a high-dimensional tensor in an SG.)

#### 3.3.1 Normal-Form Representation

A (simultaneous-move) NFG can be equivalently transformed into an imperfect-information EFG[18] ([Shoham and Leyton-Brown], [2008]) [Chapter 5]. Specifically, since the choices of actions by other agents are unknown to the central agent, this could potentially leads to different histories (triggered by other agents) that can be aggregated into one information state for the central agent.

On the other direction, an imperfect-information EFG can also be transformed into an equivalent NFG in which the pure strategies of each agent _i_ are defined by the Cartesian product n _S_i_∈_S _i_ _χ_(_Si_), which is a complete specification[19] of which action to take at every information state of that agent. In the Kuhn poker example, one pure strategy for player one can be check-bet-check-fold-call-fold; altogether, player one has 26 = 64 pure strategies, corresponding to 3 _×_ 23 = 24 pure strategies for the chance node and 26 = 64 pure strategies for player two. The mixed strategy of each player is then a distribution over all its pure strategies. In this way, the NE in NFG in Eq. ([17]) can still be applied to the EFG, and the NE of an EFG can be solved in two steps: first, convert the EFG into an NFG; second, solve the NE of the induced NFG by means of the Lemke-Howson algorithm ([Shapley], [1974]). If one further restricts the action space to be state-dependent and adopts the discounted accumulated reward at the terminal node, then the EFG recovers to an SG. While the NE of an EFG can be solved through its equivalent normal form, the computational benefit can be achieved by dealing with the extensive form directly; this motivates the adoption of the sequence-form representation of EFGs.

(18Note that this transformation is not unique, but they share the same equilibria as the original game. Moreover, this transformation from NFG to EFG does not hold for perfect-information EFGs.
19One subtlety of the pure strategy is that it designates a decision at each choice node, regardless of whether it is possible to reach that node given the other choice nodes.)

#### 3.3.2 Sequence-Form Representation

Solving EFGs via the NFG representation, though universal, is inefficient because the size of the induced NFG is exponential in the number of information states. In addition, the NFG representation does not consider the temporal structure of games. One way to address these problems is to operate on the sequence form of the EFG, also known as the realisation-plan representation, the size of which is only linear in the number of game states and is thus exponentially smaller than that of the NFG. Importantly, this approach enables polynomial-time solutions to EFGs ([Koller and Megiddo], [1992]).

In the sequence form of EFGs, the main focus shifts from mixed strategies to _be-_ _havioural strategies_ in which, rather than randomising over complete pure strategies, the agents randomise independently at each information state Si ∈ S_i_, i.e., πi : S_i_ →_∆ χ_(_Si_) . With the help of behavioural strategies, the key insight of the sequence form is that rather than building a player’s strategy around the notion of pure strategies that can be exponentially many, one can build the strategy based on the paths in the game tree from the root to each node.

In general, the expressive power of behavioural strategy and mixed strategy are noncomparable. However, if the game has _perfect_ _recall_, which intuitively[20] means that each agent remembers all his historical moves in different information states precisely, then the behavioural strategy and mixed strategy are somehow equivalent. Specifically, suppose all choice nodes in an information state share the same history that led to them (otherwise the agent can distinguish between the choice nodes). In that case, the well-known Kuhn’s theorem ([Kuhn], [1950a]) guarantees that the expressive power of behavioural strategies and that of mixed strategies coincides in the sense that they induce the same probability on outcomes for games of perfect recall. As a result, the set of NE does not change if one considers only behavioural strategies. In fact, the sequence-form representation is primarily useful for describing imperfect-information EFGs of perfect recall, written as:

**Definition 9 (Sequence-form Representation)** _The sequence-form representation of_ _an imperfect-information EFG, defined in Definition_ [_8_]_, of perfect recall is described by_
![[Pasted image 20230801103621.png]]

- $N_:$ _the number of agents, including the chance node, if any, denoted by c._
- **_Σ_** = n_N_ _Σ_i_:_ _where Σi is the set of sequences available to agent i. A sequence of_ _actions of player i, σi_ ∈ _Σi,_ _defined by a choice node h_ ∈ H _∪_ T_, is the ordered set of player i’s actions that has been taken from the root to node h. Let_ ∅ $b_e$ _the sequence that corresponds to the root node._
_Note that other players’ actions are not part of agent i’s sequence. In the example of Kuhn poker, Σc_ = _{_∅_, Jack, Queen, King, Jack-Queen, Jack-King, Queen-Jack,_ _Queen-King, King-Jack, King-Queen_}_, Σ_1 = _{_∅_, check, bet, check-fold, check-bet_}_,_ _and Σ_2 = _{_∅_, check, bet, fold, call_}_._

- _πi:_ S_i_ _→_ _∆_ _χ_(_Si_) _is the behavioural policy that assigns a probability of taking a valid action ai_ ∈ _χ_(_Si_) _at an information state Si_ ∈ S_i_._ _This policy randomises independently over different information states. In the example of Kuhn poker, each player has six information states; their behavioural strategy is therefore a list of six independent probability distributions._
- _µπ_i_ _: Σi_ _→_ [0_,_ 1] _is the realisation plan that provides the realisation probability,_i.e.,_ _µπ_i_ (_σi_) = n_c_∈_σ_i_πi_(_c_)_, that a sequence σi_ ∈ _Σi would arise under a given_ _behavioural_ _policy πi of player i. In the Kuhn poker case, the realisation probability that player one chooses the sequence of check and then fold is µπ_1 (_check-fold_) = _π_1(_check_) _×_ _π_1(_fold_)_.
Based_ _on the realisation plan, one can recover the underlying behavioural strategy_[21] _(an idea similar to Eq. (_[_6_]_)). To do so, we need three additional pieces of notation._ _Let_ Seq : S_i_ _→_ $Σ_i$ _return the sequence σi_ ∈ $Σ_i$ _that leads to a given information state Si_ ∈ S_i_._ _Since_ _the game assumes perfect recall,_ Seq(_Si_) _is known to be unique. Let σiai denote a sequence that consists of the sequence σi followed by the single_ _action_ _a_i_._ _Since there are many possible actions ai to choose, let_ Ext : _Σ_i_ _→_ 2_Σ_i_ _denote the set of all possible sequences that extend the given sequence by taking one_ _additional action. It is trivial to see that sequences that include a terminal node cannot be extended, i.e.,_ Ext(_T_ ) = _∅_. Finally, we can write the behavioural policy_ _πi for an information state Si as_
![[Pasted image 20230801104540.png]]

- $G_i$ : **_Σ_** _→_ R _is the reward function for agent i given by Gi_(**_σ_**) = _Ri_(_T_ ) _if a terminal node T_ ∈ T _is reached when each player plays their part of the sequence in_ **_σ_** ∈ **_Σ_**_,_ _and Gi_(**_σ_**) = 0 _if non-terminal nodes are reached._
_Note that since each payoff that corresponds to a terminal node is stored only once in the sequence-form representation (due to the perfect recall, each terminal node_ _has only one sequence that leads to it), compared to the normal-form representation,_ _which is a Cartesian product over all information sets for each agent and is thus exponential in size, the sequence form is only linear in the size of the EFG. In the_ _example of Kuhn poker, the normal-form representation is a tensor with_ 64_×_64_×_32 _elements, while in the sequence-form representation, since there are_ 30 _terminal nodes and each node has only one unique sequence leading to it, the payoff tensor_ _has only_ 30 _elements (plus_ ∅ _for each player)._

- _Ci:_ _is a set of linear constraints on the realisation probability of µπ_i_ _. Under the notations of_ Seq _and_ Ext _defined in the bullet points of µπ_i_ _, we know the realisation_ _plan must meet the condition that_
![[Pasted image 20230801104609.png]]
_The first constraint requires that µπ_i_ _is a proper probability distribution. In addition,_ _the second constraint in Eq. (_[_25_]_) indicates that in order for a realisation plan to be valid to recover a behavioural strategy, at each information state of agent i, the probability of reaching that information state must equal the summation of the realisation probabilities of all the extended sequences. In the example of Kuhn poker, we have C_1 _for player one by µπ_1 (_check_) = _µπ_1 (_check-fold_) + _µπ_1 (_check-call_)_._

(21Empirically, it is often the case that working on the realisation plan of a behavioural strategy is more computationally friendly than working on the behavioural strategy directly.)

#### 3.4 Solving Extensive-Form Games

In the sequence-form EFG, given a joint (behavioural) policy **_π_** = (_π_1_,_ _,_ $π_N$ ), we can write the realisation probability of agents reaching a terminal node _T_ ∈ T, assuming the sequence that leads to the node _T_ is **_σ_**_T_ , in which each player, including the chance player, follows its own path $σ_i$ as
![[Pasted image 20230801104650.png]]

The expected reward for agent _i_, which covers all possible terminal nodes following the joint policy **_π_**, is thus given by Eq. ([27]).
![[Pasted image 20230801104726.png]]

If we denote the expected reward by _Ri_(**_π_**) for simplicity, then the solution concept of NE for the EFG can be written as
![[Pasted image 20230801104744.png]]

#### 3.4.1 Perfect-Information Games

Every finite perfect-information EFG has a pure-strategy NE ([Zermelo and Borel], [1913]). Since players take turns and every agent sees everything that has occurred thus far, it is unnecessary to introduce randomness or mixed strategies into the action selection. However, the NE can be too weak of a solution concept for the EFG. In contrast to that in NFGs, the NE in EFGs can represent _non-credible threats_, which represent the situation where the Nash strategy is not executed as claimed if agents truly reach that decision node. A refinement of the NE in the perfect-information EFG is a _subgameperfect equilibrium_ (SPE). The SPE rules out non-credible threats by picking only the NE that is the best response at every subgame of the original game.

The fundamental principle in solving the SPE is _backward_ _induction_, which identifies the NE from the bottom-most subgame and assumes those NE will be played as considers increasingly large trees. Specifically, backward induction can be implemented through a depth-first search algorithm on the game tree, which requires time that is only linear in the size of the EFG. In contrast, finding NE in NFG is known to be _PPAD_-hard, let alone the NFG representation is exponential in the size of an EFG.

In the case of two-player zero-sum EFGs, backward induction needs to propagate only a single payoff from the terminal node to the root node in the game tree. Furthermore, due to the strictly opposing interests between players, one can further _prune_ the backward induction process by recognising that certain subtrees will never be reached in NE, even without examining those subtree nodes[22], which leads to the well-known Alpha-BetaPruning algorithm ([Shoham and Leyton-Brown], [2008], Chapter 5.1). For games with very deep game trees, such as Chess or GO, a common approach is to search only nodes up to certain depths and use an approximate value function to estimate those nodes’ value without roll outing to the end ([Silver et al.], [2016]).

Finally, backward induction can identify one NE in linear time; yet, it does not provide an effective way to find all NE. A theoretical result suggests that finding all NE in a two-player perfect-information EFG (not necessarily zero-sum) requires _O_(_|_T_|_3), which is still tractable ([Shoham and Leyton-Brown], [2008], Theorem 5.1.6).

#### 3.4.2 Imperfect-Information Games

By means of the sequence-form representation, one can write the solution of a two-player EFG as a LP. Given a fixed behavioural strategy of player two, in the form of realisation plan _µπ_2 , the best response for player one can be written as
![[Pasted image 20230801104826.png]]

subject to the constraints in Eq. ([25]). In NE, player one and player two form a mutual best response. However, if we treat both _µπ_1 and _µπ_2 as variables, then the objective becomes nonlinear. The key to address this issue is to adopt the dual form of the LP ([Koller and Megiddo], [1996]), which is written as
![[Pasted image 20230801104842.png]]
where _I_ : $Σ_i$ _→_ S_i_ is a mapping function that returns the information set[23] encountered when the final action in $σ_i$ was taken. With slight abuse of notation, we let _I_ Ext(_σ_1) [24] denote the set of final information states encountered in the set of the extension of $σ_i$. The variable _v_ represents, given _µπ_2 , player one’s expected reward under its own realisation plan _µ_ , and _v_I_1_ can be considered as the part of this expected utility in the subgame starting from information state _I_I_. Note that the constraint needs to hold for every sequence of player one.

In the dual form of best response in Eq. ([29]), if one treats _µπ_2 as an optimising variable rather than a constant, which means _µπ_2 must meet the requirements in Eq.([25]) to be a proper realisation plan, then the LP formulation for a two-player zero-sum EFG can be written as follows.
![[Pasted image 20230801104934.png]]

Player two’s realisation plan is now selected to minimise player one’s expected utility. Based on the minimax theorem ([Von Neumann and Morgenstern], [1945]), we know this process will lead to a NE. Notably, though the zero-sum EFG and zero-sum SG (see the formulation in Eq. ([51])) both adopt the LP formulation to solve the NE and can be solved in polynomial time, the size of the representation for the game itself is very different. If one chooses first to transform the EFG into an NFG presentation and then solve it by LP, then the time complexity would in fact become exponential in the size of the original EFG.

(22This occurs, for example, in the case that the worst case of one player in one subgame is better than the best case of that player in another subgame.)

The solution to a two-player general-sum EFG can also be formulated using an approach similar to that used for the zero-sum EFG. The difference is that there will be no objective function such as Eq. ([30]) since in the general-sum context, one agent’s reward can no longer be determined based on the other player’s reward. The LP with only Eqs. ([31] [33]) thus becomes a constraint satisfaction problem. Specifically, one would need to repeat Eqs. ([31] [33]) twice to consider each player independently. One final subtlety required in solving the two-player general-sum EFG is that to ensure _v_1 and _v_2 are bounded[25], a _complementary slack condition_ must be further imposed; we have _∀_σ_1 ∈ _Σ_1 (vice versa _∀_σ_2 ∈ _Σ_2 for player two):
![[Pasted image 20230801105026.png]]
The above condition indicates that for each player, either the sequence $σ_i$ is never played, i.e., _µπ_i_ (_σi_) = 0, or all sequences that are played by that player with positive probability must induce the same expected payoff such that $v_i$ takes arbitrarily large values, thus being bounded. Eqs. ([31] [33]), together with Eq. ([34]), turns the solution to the NE into an LCP problem that can be solved by the generalised Lemke-Howson method ([Lemke] [and Howson], [1964]). Although in the worst case, polynomial time complexity cannot be achieved, as can for zero-sum games, this approach is still exponentially faster than running the Lemke-Howson method to solve the NE in a normal-form representation.

For a perfect-information EFG, recall that the SPE is a more informative solution concept than NE. Extending SPE to the imperfect-information scenario is therefore valuable. However, such an extension is non-trivial because a well-defined notion of a subgame is lacking. However, for EFGs with perfect recall, the intuition of subgame perfection can be effectively extended to a new solution concept, named the sequential equilibrium (SE) ([Kreps and Wilson], [1982]), which is guaranteed to exist and coincides with the SPE if all players in the game have perfect information.

# 4 Grand Challenges of MARL

Compared to single-agent RL, multi-agent RL is a general framework that better matches the broad scope of real-world AI applications. However, due to the existence of multiple agents that learn simultaneously, MARL methods pose more theoretical challenges, in addition to those already present in single-agent RL. Compared to classic MARL settings where there are usually two agents, solving a many-agent RL problem is even more challenging. As a matter of fact, ○1 **the** **combinatorial complexity**, ○2 **the multi-****dimensional** **learning objectives**, and ○3 **the issue of non-stationarity** all result in the majority of MARL algorithms being capable of solving games with ○4 **only two** **players**, in particular, two-player zero-sum games. In this section, I will elaborate each of the grand challenge in many-agent RL.

(25Since the constraints are linear, they remain satised when both v1 and v2 are increased by the same constant to any arbitrarily large values.)

#### 4.1 The Combinatorial Complexity

In the context of multi-agent learning, each agent has to consider the other opponents’ actions when determining the best response; this characteristic is deeply rooted in each agent’s reward function and for example is represented by the joint action **_a_** in their Qfunction _Qi_(_s,_ **_a_**) in Eq. ([13]). The size of the joint action space, _|_A_|_N_ , grows exponentially with the number of agents and thus largely constrains the scalability of MARL methods. Furthermore, the combinatorial complexity is worsened by the fact that solving a NE in game theory is _PPAD_-hard, even for two-player games. Therefore, for multi-player general-sum games (neither team games nor zero-sum games), it is non-trivial to find an applicable solution concept.

One common way to address this issue is by assuming specific factorised structures on action dependency such that the reward function or Q-function can be significantly simplified. For example, a graphical game assumes an agent’s reward is affected by only its neighbouring agents, as defined by the graph from ([Kearns], [2007]). This assumption leads directly to a polynomial-time solution for the computation of a NE in specific tree graphs ([Kearns et al.], [2013]), though the scope of applications is somewhat limited beyond this specific scenario.

Recent progress has also been made toward leveraging particular neural network architectures for Q-function decomposition ([Rashid et al.], [2018]; [Sunehag et al.], [2018]; [Yang] [et al.], [2020]). In addition to the fact that these methods work only for the team-game setting, the majority of them lack theoretical backing. There remain open questions to answer, such as understanding the representational power (the approximation error) of the factorised Q-functions in a multi-agent task and how factorisation itself can be learnt from scratch.

#### 4.2 The Multi-Dimensional Learning Objectives

Compared to single-agent RL, where the only goal is to maximise the learning agent’s long-term reward, the learning goals in MARL are naturally multi-dimensional, as the objective of all agents are not necessarily aligned by one metric. [Bowling and Veloso] ([2001], [2002]) proposed to classify the goals of the learning task into two types: **rationality** and **convergence**. Rationality ensures an agent takes the best possible response to the opponents when they are stationary, and convergence ensures the learning dynamics eventually lead to a stable policy against a given class of opponents. Reaching both rationality and convergence gives rise to reaching the NE.

In terms of rationality, the NE characterises a fixed point of a joint optimal strategy profile from which no agents would be motivated to deviate as long as they are all perfectly rational. However, in practice, an agent’s rationality can easily be bound by either cognitive limitations and/or the tractability of the decision problem. In these scenarios, the rationality assumption can be relaxed to include other types of solution concepts, such as the recursive reasoning equilibrium, which results from modelling the reasoning process recursively among agents with finite levels of hierarchical thinking (for example, an agent may reason in the following way: I believe that you believe that I believe ) ([Wen] [et al.], [2019], [2018]); best response against a target type of opponent ([Powers and Shoham], [2005b]); the mean-field game equilibrium, which describes multi-agent interactions as a two-agent interaction between each agent itself and the population mean ([Guo et al.], [2019]; [Yang et al.], [2018a],[b]); evolutionary stable strategies, which describe an equilibrium strategy based on its evolutionary advantage of resisting invasion by rare emerging mutant strategies ([Bloembergen et al.], [2015]; [Maynard Smith], [1972]; [Tuyls and Now´e], [2005]; [Tuyls] [and Parsons], [2007]); Stackelberg equilibrium ([Zhang et al.], [2019a]), which assumes specific sequential order when agents take decisions; and the robust equilibrium (also called the trembling-hand perfect equilibrium in game theory), which is stable against adversarial disturbance ([Goodfellow et al.], [2014b]; [Li et al.], [2019b]; [Yabu et al.], [2007]).

In terms of convergence, although most MARL algorithms are contrived to converge to the NE, the majority either lack a rigorous convergence guarantee ([Zhang et al.], [2019b]), potentially converge only under strong assumptions such as the existence of a unique NE ([Hu and Wellman], [2003]; [Littman], [2001b]), or are provably non-convergent in all cases ([Mazumdar et al.], [2019a]). [Zinkevich et al.] ([2006]) identified the non-convergent behaviour of value-iteration methods in general-sum SGs and instead proposed an alternative solu-tion concept to the NE – _cyclic equilibria_ – that value-based methods converge to. The concept of no regret (also called the Hannan consistency in game theory ([Hansen et al.], [2003])), measures convergence by comparison against the best possible strategy in hindsight. This was also proposed as a new criterion to evaluate convergence in zero-sum self-plays ([Bowling], [2005]; [Hart and Mas-Colell], [2001]; [Zinkevich et al.], [2008]). In twoplayer zero-sum games with a non-convex non-concave loss landscape (training GANs ([Goodfellow et al.], [2014a])), gradient-descent-ascent methods are found to reach a Stackelberg equilibrium ([Fiez et al.], [2019]; [Lin et al.], [2019]) or a local differential NE ([Mazumdar] [et al.], [2019b]) rather than the general NE.

Finally, although the above solution concepts account for convergence, building a convergent objective for MARL methods with DNNs remains an uncharted area. This is partly because the global convergence result of a single-agent deep RL algorithm, for example, neural policy gradient methods ([Liu et al.], [2019]; [Wang et al.], [2019]) and neural TD learning algorithms ([Cai et al.], [2019b]), has not been extensively studied yet.

#### 4.3 The Non-Stationarity Issue

The most well-known challenge of multi-agent learning versus single-agent learning is probably the non-stationarity issue. Since multiple agents concurrently improve their policies according to their own interests, from each agent’s perspective, the environmental dynamics become non-stationary and challenging to interpret when learning. This problem occurs because the agent itself cannot tell whether the state transition – or the change in reward – is an actual outcome due to its own action or if it is due to its opponent’s explorations. Although learning independently by completely ignoring the other agents can sometimes yield surprisingly powerful empirical performance ([Matignon] [et al.], [2012]; [Papoudakis et al.], [2020]), this approach essentially harms the stationarity assumption that supports the theoretical convergence guarantee of single-agent learning methods ([Tan], [1993]). As a result, the Markovian property of the environment is lost, and the state occupancy measure of the stationary policy in Eq. ([5]) no longer exists. For example, the convergence result of single-agent policy gradient methods in MARL is provably non-convergent in simple linear-quadratic games ([Mazumdar et al.], [2019b]).

The non-stationarity issue can be further aggravated by TD learning, which occurs with the replay buffer that most deep RL methods currently adopt ([Foerster et al.], [2017b]). In single-agent TD learning (see Eq. ([9])), the agent bootstraps the current estimate of the TD error, saves it in the replay buffer, and samples the data in the replay buffer to update the value function. In the context of multi-agent learning, since the value function for one agent also depends on other agents’ actions, the bootstrap process in TD learning also requires sampling other agents’ actions, which leads to two problems. First, the sampled actions barely represent the full behaviour of other agents’ underlying policies across different states. Second, an agent’s policy can change during training, so the samples in the replay buffer can quickly become outdated. Therefore, the dynamics that yielded the data in the agent’s replay buffer must be constantly updated to reflect the current dynamics in which it is learning. This process further exacerbates the non-stationarity issue.

In general, the non-stationarity issue forbids the reuse of the same mathematical tool for analysing single-agent algorithms in the multi-agent context. However, one exception exists: the identical-interest game in Definition [4]. In such settings, each agent can safely perform selfishly without considering other agents’ policies since the agent knows the other agents will also act in their own interest. The stationarity is thus maintained, so single-agent RL algorithms can still be applied.

![[Pasted image 20230801105956.png]]
**Figure 8:** The scope of multi-agent intelligence, as described here, consists of three pillars. Deep learning serves as a powerful function approximation tool for the learning process. Game theory provides an effective approach to describe learning outcomes. RL offers a valid approach to describe agents’ incentives in multi-agent systems.

#### 4.4 The Scalability Issue when _N_ _»_ 2

Combinatorial complexity, multi-dimensional learning objectives, and the issue of nonstationarity all result in the majority of MARL algorithms being capable of solving games with only two players, in particular, two-player zero-sum games ([Zhang et al.], [2019b]). As a result, solutions to general-sum settings with more than two agents (for example, the many-agent problem) remain an open challenge. This challenge must be addressed from all three perspectives of multi-agent intelligence (see Figure ([8])): game theory, which provides realistic and tractable solution concepts to describe learning outcomes of a many-agent system; RL algorithms, which offer provably convergent learning algorithms that can reach stable and rational equilibria in the sequential decision-making process; and finally deep learning techniques, which provide the learning algorithms expressive function approximators.

# 5 A Survey of MARL Surveys

In this section, I provide a non-comprehensive review of MARL algorithms. To begin, I introduce different taxonomies that can be applied to categorise prior approaches. Given multiple high-quality, comprehensive surveys on MARL methods already exist, a survey of those surveys is provided. Based on the proposed taxonomy, I review related MARL algorithms, covering works on identical interest games, zero-sum games, and games with an infinite number of players. This section is written to be selective, focusing on the algorithms that have theoretical guarantees and less focus on those with only empirical success or those that are purely driven by specific applications.

#### 5.1 Taxonomy of MARL Algorithms

One significant difference between the taxonomy of single-agent RL algorithms and MARL algorithms is that in the single-agent setting, since the problem is unanimously defined, the taxonomy is driven mainly by the type of solution ([Kaelbling et al.], [1996]; [Li], [2017]), for example, model-free vs model-based, on-policy vs off-policy, TD learning vs MonteCarlo methods. By contrast, in the multi-agent setting, due to the existence of multiple learning objectives (see Section [4.2]), the taxonomy is driven mainly by the type of problem rather than the solution. In fact, asking the right question for MARL algorithms is itself a research problem, which is referred to as the problem problem ([Balduzzi et al.], [2018b]; [Shoham et al.], [2007]).

**Based on Stage Games Types.** Since the solution concept varies considerably according to the game type, one principal component of the MARL taxonomy is the nature of stage games. A common division[26] includes team games (more generally, potential games), zero-sum games (more generally, harmonic games), and a mixed setting of the two games, namely, general-sum games. Other types of “exotic” games, such as potential games ([Monderer and Shapley], [1996]) and mean-field games ([Lasry and Lions], [2007]), that originate from non-game-theoretical research domains exist and have recently attracted tremendous attention. Based on the type of stage game, the taxonomy can be further enriched by how many times they are played. A repeated game is where one stage game is played repeatedly without considering the state transition. An SG is a sequence of stage games, which can be infinitely long, with the order of the games to play determined by the state-transition probability. Since solving a general-sum SG is at least _PSPACE_hard ([Conitzer and Sandholm], [2002]), MARL algorithms usually have a clear boundary on what types of game they can solve. For general-sum games, there are few MARL algorithms that have a provable convergence guarantee without strong, even unrealistic, assumptions (e.g., the NE is unique) ([Shoham et al.], [2007]; [Zhang et al.], [2019b]).

**Based on Level of Local Knowledge.** The assumption on the level of local knowledge, i.e., what agents can and cannot know during training and execution time, is another major component to differentiate MARL algorithms. Having access to different levels of local knowledge leads to different local behaviours by agents and various levels of difficulty in developing theoretical analysis. I list the common assumptions that most MARL methods adopt in Table ([1]). The seven levels of assumptions are ranked based on how strong, or unrealistic, they are in general. The two extreme cases are that the agent can observe nothing apart from itself and that the agent knows the equilibrium point, i.e., the direct answer of the game. Among the multiple levels, the nuance between level 0 and level 1, which has been mainly investigated in the online learning literature, is referred to as the _bandit_ setting vs _full-information_ setting. In addition, knowledge of the agents’ exact policy/reward function forms is a much stronger assumption than being able to observe their sampled actions/rewards. In fact, knowing the exact policy parameters of other agents in most cases are only possible in simulations. Furthermore, from an applicability perspective, observing other agents’ rewards is also more unrealistic than observing their actions.

**Table 1:** Common assumptions on the level of local knowledge made by MARL algorithms.
![[Pasted image 20230801110121.png]]

(26Such a division is complementary because any multi-player normal-form game can be decomposed into a potential game plus a harmonic game (Candogan et al., 2011) (also see Denition 4); in the two-player case, it corresponds to a team game plus a zero-sum game.)

![[Pasted image 20230801110150.png]]
**Figure 9:** Common learning paradigms of MARL algorithms. (1) Independent learners with shared policy. (2) Independent learners with independent policies (i.e., denoted by the difference in wheels). (3) Independent learners with shared policy within a group.(4) One central controller controls all agents: agents can exchange information with any other agents at any time. (5) Centralised training with decentralised execution (CTDE): only during training, agents can exchange information with others; during execution, they act independently. (6) Decentralised training with networked agents: during training, agents can exchange information with their neighbours in the network; during execution, they act independently.

**Based on Learning Paradigms.** In addition to various levels of local knowledge, MARL algorithms can be classified based on the learning paradigm, as shown in Figure [9]. For example, the 4th learning paradigm addresses multi-agent problems by building a single-agent controller, which takes the joint information from all agents as inputs and outputs the joint policies for all agents. In this paradigm, agents can exchange any information with any other opponent through the central controller. The information that can be exchanged depends on the assumptions about the level of local knowledge described in Table ([1]), e.g., private observations from each agent, the reward value, or policy parameters for each agent. The 5th learning paradigm allows agents to exchange information with other agents only during training; during execution, each agent has to act in a decentralised manner, making decisions based on its own observations only. The 6th paradigm can be regarded as a particular case of Paradigm 5 in that agents are assumed to be interconnected via a (time-varying) network such that information can still spread across the whole network if agents communicate with their neighbours. The most general case is Paradigm 2, where agents are fully decentralised, with no information exchange of any kind allowed at any time, and each agent executes its own policy. Relaxation of Paradigm 2 yields the 1st and the 3rd paradigms, where the agents, although they cannot exchange information, share a single set of policy parameters, or, within a pre-defined group, share a single set of policy parameters.

**Based on Five AI Agendas.** In order for MARL researchers to be specific about the problem being addressed and the associated evaluation criteria, [Shoham et al.] ([2007]) identified five coherent agendas for MARL studies, each of which has a clear motivation and success criterion. Though proposed more than a decade ago, these five distinct goals are still useful in evaluating and categorising recent contributions. I, therefore, choose to incorporate them into the taxonomy of MARL algorithms.

#### 5.2 A Survey of Surveys

A multi-agent system (MAS) is a generic concept that could refer to many different domains of research across different academic subjects; general overviews are given by [Weiss] ([1999]), [Wooldridge] ([2009]), and [Shoham and Leyton-Brown] ([2008]). Due to the many possible ways of categorising multi-agent (reinforcement) learning algorithms, it is impossible to have a single survey that includes all relevant works considering all directions of categorisations. In the past two decades, there has been no lack of survey papers that summarise the current progress of specific categories of multi-agent learning research. In fact, there are so many that these surveys themselves deserve a comprehensive review. Before proceeding to review MARL algorithms based on the proposed taxonomy in Section [5.1], in this section, I provide an overview of relevant surveys that study multiagent systems from the machine learning, in particular, the RL, perspective.

One of the earliest studies that surveyed MASs in the context of machine learning/AI was published by [Stone and Veloso] ([2000]): the research works up to that time were summarised into four major scenarios considering whether agents were homogeneous or heterogeneous and whether or not agents were allowed to communicate with each other. [Shoham et al.] ([2007]) considered the game theory and RL perspective and introspectively asked the question of “if multi-agent learning is the answer, what is the question?”. Upon failing to find a single answer, [Shoham et al.] ([2007]) proposed the famous five AI agendas for future research work to address. [Stone] ([2007]) tried to answer Shoham’s question by emphasising that MARL can be more broadly framed than through game theoretic terms, and he noted that how to apply the MARL technique remains an open question, rather than being an answer, in contrast to the suggestion of [Shoham et al.] ([2007]). The survey of [Tuyls and Weiss] ([2012]) also reflected on Stone’s viewpoint; they believed that the entanglement of only RL and game theory is too narrow in its conceptual scope, and MARL should embrace other ideas, such as transfer learning ([Taylor and Stone], [2009]), swarm intelligence ([Kennedy], [2006]), and co-evolution ([Tuyls and Parsons], [2007]).

**Table 2:** Summary of the five agendas for multi-agent learning research [Shoham et al.] ([2007]).
![[Pasted image 20230801110332.png]]

[Panait and Luke] ([2005]) investigated the cooperative MARL setting; instead of considering only reinforcement learners, they reviewed learning algorithms based on the division of _team_ _learning_ (i.e., applying a single learner to search for the optimal joint behaviour for the whole team) and _concurrent_ _learning_ (i.e., applying one learner per agent), which includes broader areas of evolutionary computation, complex systems, etc. [Matignon] [et al.] ([2012]) surveyed the solutions for fully-cooperative games only; in particular, they focused on evaluating independent RL solutions powered by Q-learning and its many variants. [Jan’t Hoen et al.] ([2005]) conducted an overview with a similar scope; moreover, they extended the work to include fully competitive games in addition to fully cooperative games. [Bu¸soniu et al.] ([2010]), to the best of my knowledge, presented the first comprehensive survey on MARL techniques, covering both value iteration-based and policy search-based methods, together with their strengths and weaknesses. In their survey, they considered not only fully cooperative or competitive games but also the effectiveness of different algorithms in the general-sum setting. [Now´e et al.] ([2012]), in the 14th chapter, addressed the same topic as [Bu¸soniu et al.] ([2010]) but with a much narrower coverage of multi-agent RL algorithms.

[Tuyls and Now´e] ([2005]) and [Bloembergen et al.] ([2015]) both surveyed the dynamic models that have been derived for various MARL algorithms and revealed the deep connection between evolutionary game theory and MARL methods. We refer to Table 1 in [Tuyls and Now´e] ([2005]) for a summary of this connection.

[Hernandez-Leal et al.] ([2017]) provided a different perspective on the taxonomy of how existing MARL algorithms cope with the issue of non-stationarity induced by opponents. On the basis of the opponent and environment characteristics, they categorised the MARL algorithms according to the type of opponent modelling.

[Da Silva and Costa] ([2019]) introduced a new perspective of reviewing MARL algorithms based on how knowledge is reused, i.e., transfer learning. Specifically, they grouped the surveyed algorithms into _intra-agent_ and _inter-agent_ methods, which correspond to the reuse of knowledge from experience gathered from the agent itself and that acquired from other agents, respectively.

Most recently, deep MARL techniques have received considerable attention. [Nguyen] [et al.] ([2020]) surveyed how deep learning techniques were used to address the challenges in multi-agent learning, such as partial observability, continuous state and action spaces, and transfer learning. [OroojlooyJadid and Hajinezhad] ([2019]) reviewed the application of deep MARL techniques in fully cooperative games: the survey on this setting is thorough. [Hernandez-Leal et al.] ([2019]) summarised how the classic ideas from traditional MAS research, such as emergent behaviour, learning communication, and opponent modelling, were incorporated into deep MARL domains, based on which they proposed a new cate-gorisation for deep MARL methods. [Zhang et al.] ([2019b]) performed a selective survey on MARL algorithms that have theoretical convergence guarantees and complexity analysis. To the best of my knowledge, their review is the only one to cover more advanced topics such as decentralised MARL with networked agents, mean-field MARL, and MARL for stochastic potential games.

On the application side, [Mu¨ller and Fischer] ([2014]) surveyed 152 real-world applications in various sectors powered by MAS techniques. [Campos-Rodriguez et al.] ([2017]) reviewed the application of multi-agent techniques for automotive industry applications, such as traffic coordination and route balancing. [Derakhshan and Yousefi] ([2019]) focused on real-world applications for wireless sensor networks, [Shakshuki and Reid] ([2015]) studied multi-agent applications for the healthcare industry, and [Kober et al.] ([2013]) investigated the application of robotic control and summarised profitable RL approaches that can be applied to robots in the real world.

# 6 Learning in Identical-Interest Games

The majority of MARL algorithms assume that agents collaborate with each other to achieve shared goals. In this setting, agents are usually considered homogeneous and play an interchangeable role in the environmental dynamics. In a two-player normalform game or repeated game, for example, this means the payoff matrix is symmetrical.

#### 6.1 Stochastic Team Games

One benefit of studying identical interest games is that single-agent RL algorithms with a theoretical guarantee can be safely applied. For example, in the team game[27] setting, since all agents’ rewards are always the same, the Q-functions are identical among all agents. As a result, one can simply apply the single-agent RL algorithms over the joint action space **_a_** ∈ A, equivalently, Eq. ([14]) can be written as
![[Pasted image 20230801110441.png]]

(27The terms Markov team games, stochastic team games, and dynamic team games are interchangeably used across dierent domains of the literature.)

[Littman] ([1994]) first studied this approach in SGs. However, one issue with this approach is that when multiple equilibria exist (e.g., a normal-form game with reward _R_ = 0_,_ 0 2_,_ 2 ), unless the selection process is coordinated among agents, the agents’ 2_,_ 2 0_,_ 0 optimal policy can end up with a worse scenario even though their value functions have reached the optimal values. To address this issue, [Claus and Boutilier] ([1998b]) proposed to build belief models about other agents’ policies. Similar to fictitious play ([Berger], [2007]), each agent chooses actions in accordance with its belief about the other agents. Empirical effectiveness, as well as convergence, have been reported for repeated games; however, the convergent equilibrium may not be optimal. In solving this problem, [Wang] [and Sandholm] ([2003]) proposed optimal adaptive learning (OAL) methods that provably converge to the optimal NE almost surely in any team SG. The main novelty of OAL is that it learns the game structure by building so-called _weakly acyclic games_ that eliminate all the joint actions with sub-optimal NE values and then applies adaptive play ([Young], [1993]) to address the equilibrium selection problem for weakly acyclic games specifically. Following this approach, [Arslan and Yu¨ksel] ([2016]) proposed decentralised Q-learning algorithms that, under the help of two-timescale analysis ([Leslie et al.], [2003]), converge to an equilibrium policy for weakly acyclic SGs. To avoid sub-optimal equilibria for weakly acyclic SGs, [Yongacoglu et al.] ([2019]) further refined the decentralised Q-learners and derived theorems with stronger almost-surely convergence guarantees for optimal policies.

#### 6.1.1 Solutions via Q-function Factorisation

Another vital reason that team games have been repeatedly studied is that solving team games is a crucial step in building distributed AI (DAI) ([Gasser and Huhns], [2014]; [Huhns], [2012]). The logic is that if each agent only needs to maintain the Q-function of _Qi_(_s, ai_), which depends on the state and local action $a_i$, rather than joint action **_a_**, then the combinatorial nature of multi-agent problems can be avoided. Unfortunately, [Tan] ([1993]) previously noted that such independent Q-learning methods do not converge in team games. [Lauer and Riedmiller] ([2000]) reported similar negative results; however, when the state transition dynamics are deterministic, independent learning through distributed Qlearning can still obtain a convergence guarantee. No additional expense is needed in comparison to the non-distributed case for computing the optimal policies.

Factorised MDPs ([Boutilier et al.], [1999]) are an effective way to avoid exponential blowups. For a coordination task, if the joint-Q function can be naturally written as
![[Pasted image 20230801110916.png]]
then the nested structure can be exploited. For example, _Q_1 and _Q_3 are irrelevant in finding the optimal _a_4; thus, given _a_4, _Q_1 becomes irrelevant for optimising _a_3. Given _a_3_, a_4, one can then optimise _a_1_, a_2. Inspired by this result, [Guestrin et al.] ([2002a],[b]); [Kok] [and Vlassis] ([2004]) studied the idea of _coordination_ _graphs_, which combine value function approximation with a message-passing scheme by which agents can efficiently find the globally optimal joint action.

However, the coordination graph may not always be available in real-world applications; thus, the ideal approach is to let agents learn the Q-function factorisation from the tasks automatically. Deep neural networks are an effective way to learn such factorisations. Specifically, the scope of the problem is then narrowed to the so-called _decen-_ _tralisable tasks_ in the Dec-POMDP setting, that is, _∃_ $Q_i$ _i_∈{_1_,,N_ _}_ _∀_**_o_** ∈ O_,_ **_a_** ∈ A, the following condition holds.

![[Pasted image 20230801110952.png]]
Eq. ([36]) suggests that a task is decentralisable only if the local maxima on the individual value function per every agent amounts to the global maximum on the joint value function. Different structural constraints, enforced by particular neural architectures, have been proposed to satisfy this condition. For example, VDN ([Sunehag et al.], [2018]) maintains an additivity structure by making _Q_**_π_**(**_o_**_,_ **_a_**) := 2_N_ _Q_i_(_o_i_,_ _a_i_). QMIX ([Rashid et al.], [2018]) adopts a monotonic structure by means of a mixing network to ensure _∂Q_**_π_** (**_o_**_,_**_a_**) _∂_Q_i_(_o_i_,a_i_)\||_{_1_, , N_ _}_. QTRAN ([Son et al.], [2019]) introduces a more rigorous learning objective on top of QMIX that proves to be a sufficient condition for Eq. ([36]). However, these structure constraints heavily depend on specially designed neural architectures, which makes understanding the representational power (i.e., the approximation error) of the above methods almost infeasible. Another drawback is that the structure constraint also damages agents’ efficient exploration during training. To mitigate these issues, [Yang et al.] ([2020]) proposed Q-DPP, which eradicates the structure constraints by approximating the Q-function through a _determinantal point process (DPP)_ ([Kulesza et al.], [2012]). DPP pushes agents to explore and acquire diverse behaviours; consequently, it leads to natural decomposition of the joint Q-function with no need for _a priori_ structure constraints. In fact, VDN/QMIX/QTRAN prove to be the exceptional cases of Q-DPP.

![[Pasted image 20230801111047.png]]
**Figure 10:** Graphical model of the _level_-_k_ reasoning model ([Wen et al.], [2019]). The red part is the equivalent graphical model for the multi-agent learning problem. The blue part corresponds to the recursive reasoning steps. Subscript $a_∗$ stands for the level of thinking, not the time step. The opponent policies are approximated by _ρ−_i_. The omitted _level_-0 model considers opponents that are fully randomised. Agent _i_ rolls out the recursive reasoning about opponents in its mind (blue area). In the recursion, agents with higher-level beliefs take the best response to the lower-level agents. The higher-level models conduct all the computations that the lower-level models have done, e.g., the _level_-2 model contains the _level_-1 model by integrating out _π_i_ (_a_i_|_s_).

#### 6.1.2 Solutions via Multi-Agent Soft Learning

In single-agent RL, the process of finding the optimal policy can be equivalently transformed into a probabilistic inference problem on a graphical model ([Levine], [2018]). The pivotal insight is that by introducing an additional binary random variable _P_ (_O_ =1_|_st, at_) _∝_ exp(_R_(_st, at_)), which denotes the _optimality_ of the state-action pair at time step _t_, one can draw an equal connection between searching the optimal policies by RL methods and computing the marginal probability of _p_(_O_i_ = 1) by probabilistic inference methods, such as message passing or variational inference ([Blei et al.], [2017]). This equivalence between optimal control and probabilistic inference also holds in the multi-agent setting ([Grau-Moya et al.], [2018]; [Shi et al.], [2019]; [Tian et al.], [2019]; [Wen et al.], [2019], [2018]). In the context of SG (see the red part in Figure [10]), the optimality variable for each agent i is dened by pOi t = 1 Oi t = 1 i t expri stai tai t , which implies that the optimality of trajectory _τ_i_ = (_s_0_,_ _a_i_ _,_ _a_−_0_i_,_ _,_ _s_t_,_ _a_i_,_ _a_−_t__i_) depends on whether agent\||each agent _i_ is defined by _p_ ,_O_i_= 1_|O_t_−_i_= 1_,_ _τ_i_）_∝_ exp ,_r_i_ ,_s_t_,_ _a_i_,_ _a_t_−_i_））, which implies _i_ acts according to its best response against other agents, and _O_t_−_i_ = 1 indicates that all other agents are perfectly rational and attempt to maximise their rewards. Therefore,from each agent’s perspective, its objective becomes maximising _p_(_O_i_ As we assume no knowledge of the optimal policies and the model of the environment, we treat states and actions as latent variables and apply variational inference ([Blei et al.], [2017]) to approximate this objective, which leads to
![[Pasted image 20230801111309.png]]

One major dierence from traditional RL is the additional entropy term28 in Eq. (37). Under this new objective, the value function is written as V i(s) = E Qi(stai tai t ) log (ai tai t st) , and the corresponding optimal Bellman operator is
![[Pasted image 20230801111342.png]]

This process is called _soft learning_ because log **_a_** exp

(28Soft learning is also called maximum-entropy RL (Haarnoja et al., 2018).)

One substantial benefit of developing a probabilistic framework for multi-agent learning is that it can help model the _bounded_ _rationality_ ([Simon], [1972]). Instead of assuming perfect rationality and agents reaching NE, bounded rationality accounts for situations in which rationality is compromised; it can be constrained by either the difficulty of the decision problem or the agents’ own cognitive limitations. One intuitive example is the psychological experiment of the Keynes beauty contest ([Keynes], [1936]), in which all players are asked to guess a number between 0 and 100 and the winner is the person whose number is closest to the 1_/_2 of the average number of all guesses. Readers are recommended to pause here and think about which number you would guess. Although the NE of this game is 0, the majority of people guess a number between 13 and 25 ([Coricelli] [and Nagel], [2009]), which suggests that human beings tend to reason only by 1-2 levels of recursion in strategic games [Camerer et al.] ([2004]), i.e., “I believe how you believe how I believe”.

[Wen et al.] ([2018]) developed the first MARL powered reasoning model that accounts for bounded rationality, which they called _probabilistic recursive reasoning_ (PR2). The key idea of PR2 is that a dependency structure is assumed when splitting the joint policy **_π_**_θ_, written by
![[Pasted image 20230801111521.png]]

that is, the opponent is considering how the learning agent is going to affect its actions, i.e., a Level-1 model. The unobserved opponent model is approximated by a best-fit model _ρ_θ_−_i_ when optimising Eq. ([37]). In the team game setting, since agents’ ob-jectives are fully aligned, the optimal _ρ_φ_−_i_ has a closed-form solution _ρ_−_i_ (_a_−_i_|_s,_ _a_i_) _∝_exp (_Qi_(_s, ai, a_−_i_) _−_ _Qi_(_s, ai_)). Following the direction of recursive reasoning, [Tian et al.] ([2019]) proposed an algorithm named ROMMEO that splits the joint policy by
![[Pasted image 20230801111552.png]]

in which a Level-1 model is built from the learning agent’s perspective. [Grau-Moya et al.] ([2018]); [Shi et al.] ([2019]) introduced a Level-0 model where no explicit recursive reasoning is considered.
![[Pasted image 20230801111610.png]]

However, they generalised the multi-agent soft learning framework to include the zerosum setting. [Wen et al.] ([2019]) recently proposed a mixture of hierarchy Level-_k_ models in which agents can reason at different recursion levels, and higher-level agents make the best response to lower-level agents (see the blue part in Figure [10]). They called this method _generalised recursive reasoning_ (GR2).
![[Pasted image 20230801111634.png]]

In GR2, practical multi-agent soft actor-critic methods with convergence guarantee were introduced to make large-_K_ reasoning tractable.

#### 6.2 Dec-POMDPs

Dec-POMDP is a stochastic team game with partial observability. However, optimally solving Dec-POMDPs is a challenging combinatorial problem that is _NEXP_ -complete ([Bernstein et al.], [2002]). As the horizon increases, the doubly exponential growth in the number of possible policies quickly makes solution methods intractable. Most of the solution algorithms for Dec-POMDPs, including the above VDN/QMIX/QTRAN/Q-DPP, are based on the learning paradigm of centralised training with decentralised execution (CTDE) ([Oliehoek et al.], [2016]). CTDE methods assume a centralised controller that can access observations across all agents during training. A typical implementation is through a centralised critic with a decentralised actor ([Lowe et al.], [2017]). In representing agents’ local policies, stochastic finite-state controllers and a correlation device are commonly applied ([Bernstein et al.], [2009]). Through this representation, Dec-POMDP can be formulated as non-linear programmes ([Amato et al.], [2010]); this process allows the use of a wide range of off-the-shelf optimisation algorithms. [Dibangoye and Buffet] ([2018]); [Dibangoye et al.] ([2016]); [Szer et al.] ([2005]) introduced the transformation from Dec-POMDP into a continuous-state MDP, named the _occupancy-state_ _MDP (oMDP)_. The occupancy state is essentially a distribution over hidden states and the joint histories of observation-action pairs. In contrast to the standard MDP, where the agent learns an optimal value function that maps histories (or states) to real values, the learner in oMDP learns an optimal value function that maps occupancy states and joint actions to real values (they call the corresponding policy a _plan_). These value functions in oMDP are piece-wise linear and convex. Importantly, the benefit of restricting attention on the occupancy state is that the resulting algorithms are guaranteed to converge to a near-optimal plan for any finite Dec-POMDP with a probability of one, while traditional RL methods, such as REINFORCE, may only converge towards a local optimum.

In addition to CTDE methods, famous approximation solutions to Dec-POMDP include the Monte Carlo policy iteration method ([Wu et al.], [2010]), which enjoys linear-time complexity in terms of the number of agents, planning by maximum-likelihood methods ([Toussaint et al.], [2008]; [Wu et al.], [2013]), which easily scales up to thousands of agents, and a method that decentralises POMDP by maintaining shared memory among agents ([Nayyar et al.], [2013]).

#### 6.3 Networked Multi-Agent MDPs

A rapidly growing area in the optimisation domain for addressing decentralised learning for cooperative tasks is the networked multi-agent MDP (M-MDP). In the context of M-MDP, agents are considered heterogeneous rather than homogeneous; they have different reward functions but still form a team to maximise the team-average reward _Ri_(_s,_ **_a_**_, s_I_). Furthermore, in M-MDP, the centralised controller is assumed to be non-existent; instead, agents can only exchange information with their neighbours in a time-varying communication network defined by $G_t$ = ([_N_ ]_, Et_), where $E_t$ represents the set of all communicative links between any two of the _N_ neighbouring agents at time step _t_. The states and joint actions are assumed to be globally observable, but each agent’s reward is only locally observable to itself. Compared to stochastic team games, this setting is believed to be more realistic for real-world applications such as smart grids ([Dall’Anese et al.], [2013]) or transport management ([Adler and Blue], [2002]).

The cooperative goal of the agents in M-MDP is to maximise the team average cumulative discounted reward obtained by all agents over the network, that is,
![[Pasted image 20230801111741.png]]

Accordingly, under the joint policy **_π_** = n_i∈{_1_,,N}πi_(_ai|s_), the Q-function is defined as
![[Pasted image 20230801111812.png]]

To optimise Eq. ([50]), the optimal Bellman operator is written as
![[Pasted image 20230801111828.png]]

However, since agents can know only their own reward, they do not share the estimation of the Q function but rather maintain their own copy. Therefore, from each agent’s perspective, the individual optimal Bellman operator is written as
![[Pasted image 20230801111842.png]]

To solve the optimal joint policy **_π_**_∗_, the agents must reach **consensus** over the global optimal policy estimation, that is, if _Q_1 = _· · ·_ = $Q_N$ = _Q_∗_, we know
![[Pasted image 20230801111911.png]]

To satisfy Eq. ([47]), [Zhang et al.] ([2018b]) proposed a method based on neural fitted-Q iteration (FQI) ([Riedmiller], [2005]) in the batch RL setting ([Lange et al.], [2012]). Specifically, let _F_θ_ denote the parametric function class of neural networks that approximate Q-functions, let _D_ = _{_(_sk,_ **_a_**_i_ _, s_I_ )_}_ be the replay buffer that contains all the transition data available to all agents, and let _{_Ri_ _}_ be the local reward known only to each agent.

The objective of FQI can be written as
![[Pasted image 20230801111947.png]]

In each iteration, _K_ samples are drawn from _D_. Since $y_i$ is known only to each agent _i_, Eq. ([48]) becomes a typical consensus optimisation problem (i.e., consensus must be reached for _θ_) ([Nedic and Ozdaglar], [2009]). Multiple effective distributed optimisers can be applied to solve this problem, including the _DIGing_ algorithm ([Nedic et al.], [2017]).

Let ![[Pasted image 20230801112028.png]] _α_ be the learning rate, and _G_([_N_ ]_,_ _E_ ) be the topology of the network in the lst iteration; the DIGing algorithm designs the gradient updates for each agent _i_ as
![[Pasted image 20230801112119.png]]

Intuitively, Eq. ([49]) implies that if all agents aim to reach a consensus on _θ_, they must incorporate a weighted combination of their neighbours’ estimates into their own gradient updates. However, due to the usage of neural networks, the agents may not reach an exact consensus. [Zhang et al.] ([2018b]) also studied the finite-sample bound in a high-probability sense that quantifies the generalisation error of the proposed neural FQI algorithm.

The idea of reaching consensus can be directly applied to solving Eq. ([43]) via policygradient methods. [Zhang et al.] ([2018c]) proposed an actor-critic algorithm in which the global Q-function is approximated individually by each agent. On the basis of Eq. ([15]), the critic of _Qi,_**_π_**_θ_ (_s,_ **_a_**) is modelled by another neural network parameterised by $ω_i$, i.e., _Qi_(_·_,_ _·_; _ωi_), and the parameter $ω_i$ is updated as
![[Pasted image 20230801112149.png]]

where ![[Pasted image 20230801112206.png]]
is the TD error.Similar to The same group of authors later extended this approach to cover the continuous-action space in which a deterministic policy gradient method of Eq. ([16]) is applied ([Zhang] [et al.], [2018a]). Moreover, ([Zhang et al.], [2018c]) and ([Zhang et al.], [2018a]) applied a linear function approximation to achieve an almost sure convergence guarantee. Following this thread, [Suttle et al.] ([2019]) and [Zhang and Zavlanos] ([2019]) extended the actor-critic method to an off-policy setting, rendering more data-efficient MARL algorithms.

#### 6.4 Stochastic Potential Games

The potential game (PG) first appeared in [Monderer and Shapley] ([1996]). The physical meaning of Eq. ([21]) is that if any agent changes its policy unilaterally, the changes in reward will be represented on the potential function shared by all agents. A PG is guaranteed to have a pure-strategy NE – a desirable property that does not generally hold in normal-form games. Many efforts have since been dedicated to finding the NE of (static) PGs ([L˜a et al.], [2016]), among which fictitious play ([Berger], [2007]) and generalised weakened fictitious play ([Leslie and Collins], [2006]) are probably the most common solutions.

Generally, stochastic PGs (SPGs)[29] can be regarded as the “single-agent component” of a multi-agent stochastic game ([Candogan et al.], [2011]) since all agents’ interests in SPGs are described by a single potential function. However, the analysis of SPGs is exceptionally sparse. [Zazo et al.] ([2015]) studied an SPG with deterministic transition dynamics in which agents consider only _open-loop_ _policies_[30]. In fact, generalising a PG to the stochastic setting is further complicated because agents must now execute policies that depend on the state and consider the actions of other players. In this setting, [Gonz´alez-S´anchez and Hern´andez-Lerma] ([2013]) investigated a type of SPG in which they derive a sufficient condition for NE, but it requires each agent’s reward function to be a concave function of the state and the transition function to be invertible. [Macua et al.] ([2018]) studied a general form of SPG where a _closed-loop_ NE can be found. Although they demonstrated the equivalence between solving the closed-loop NE and solving a single-agent optimal control problem, the agents’ policies must depend only on disjoint subsets of components of the state. Notably, both [Gonz´alez-S´anchez and Herna´ndez-] [Lerma] ([2013]) and [Macua et al.] ([2018]) proposed centralised methods; optimisation over the joint action space surely results in a combinatorial complexity when solving the SPGs. In addition, they do not consider an RL setting in which the system is _a priori_ unknown.

The work of [Mguni] ([2020]) is probably the most comprehensive treatment of SPGs in a model-free setting. Similar to [Macua et al.] ([2018]), the authors revealed that the NE of the PG in pure strategies could be found by solving a dual-form MDP, but they reached the conclusion without the disjoint state assumption: the transition dynamics and potential function must be known. Specifically, they provided an algorithm to estimate the potential function based on the reward samples. To avoid combinatorial explosion, they also proposed a distributed policy-gradient method based on generalised weakened fictitious play ([Leslie and Collins], [2006]) that has linear-time complexity.

(29As with team games, stochastic PG is also called dynamic PG or Markov PG.
30Open loop means that agentsactions are a function of time only. By contrast, close-loop policies take into account the state. In deterministic systems, these policies can be optimal and coincide in value. For a stochastic system, an open-loop strategy is unlikely to be optimal since it cannot adapt to state transitions.)

Recently, [Mazumdar and Ratliff] ([2018]) studied the dynamics of gradient-based learn-ing on potential games. They found that in a general superclass of potential games named _Morse-Smale games_ ([Hirsch], [2012]), the limit sets of competitive gradient-based learning with stochastic updates are attractors almost surely, and those attractors are either local Nash equilibria or non-Nash locally asymptotically stable equilibria but not saddle points.

# 7 Learning in Zero-Sum Games

Zero-sum games represent a competitive relationship among players in a game. Solving three-player zero-sum games is believed to be _PPAD_-hard ([Daskalakis and Papadim-] [itriou], [2005]). In the two-player case, the NE (_π_1_,_∗_, π_2_,_∗_) is essentially a saddle point E_π_1_,π_2_,_∗_[_R_] _≤_ E_π_1_,_∗_,π_2_,_∗_ [_R_] _≤_ E_π_1_,_∗_,π_2[_R_]_,_ _∀_π_1_,_ _π_2, and can be formulated as an LP problem in Eq. ([51]).

![[Pasted image 20230801112440.png]]

Eq. ([51]) is considered from the min-player’s perspective. One can also derive a dualform LP from the max-player’s perspective. In discrete games, the minimax theorem ([Von Neumann and Morgenstern], [1945]) is a simple consequence of the strong duality theorem of LP[31] ([Matousek and G¨artner], [2007]),
![[Pasted image 20230801112459.png]]

which suggests the fact that whether the min player acts first or the max player acts first does not matter. However, the minimax theorem does not hold in general for multiplayer zero-sum continuous games in which the reward function is nonconvex-nonconcave. In fact, a barrier to tractability exists for multi-player zero-sum games and two-player zero-sum games with continuous states and actions.

(31Solving zero-sum games is equivalent to solving a LP; Dantzig (1951) also proved the correctness of the other direction, that is, any LP can be reduced to a zero-sum game, though some degenerate solutions need careful treatments (Adler, 2013).)

#### 7.1 Discrete State-Action Games
Similar to single-agent MDP, value-based methods aim to find an optimal value function, which in the context of zero-sum SGs, corresponds to the minimax NE of the game. In two-player zero-sum SGs with discrete states and actions, we know _V_ 1_,π_1_,π_2= $−_V$ 2_,π_1_,π_2 , and by the minimax theorem ([Von Neumann and Morgenstern], [1945]), the optimal value function is _V_ = max_π_2 min_π_1 _V_ = min_π_1 max_π_2 _V_ . In each stage game defined by _Q_1 = _−Q_2, the optimal value can be solved by a matrix zero-sum game through a linear program in Eq. ([51]). [Shapley] ([1953]) introduced the first value-iteration method, written as
![[Pasted image 20230801112558.png]]

and proved **H**Shapley is a contraction mapping (in the sense of the infinity norm) in solving two-player zero-sum SGs. In other words, assuming the transitional dynamics and reward function are known, the value-iteration method will generate a sequence of value functions _{_Vt_}_t_≥_0 that asymptotically converges to the fixed point _V_ _∗_, and the corresponding policies will converge to the NE policies **_π_**_∗_ = (_π_1_,_∗_,_ _π_2_,_∗_).

In contrast to Shapley’s model-based value-iteration method, [Littman] ([1994]) proposed a model-free Q-learning method – Minimax-Q – that extends the classic Q-learning algorithm defined in Eq. ([13]) to solve zero-sum SGs. Specifically, in Minimax-Q, Eq. ([14]) can be equivalently written as
![[Pasted image 20230801112615.png]]

The Q-learning update rule of Minimax-Q is exactly the same as that in Eq. ([13]). Minimax-Q can be considered an approximation algorithm for computing the fixed point _Q_∗_ of the Bellman operator of Eq. ([20]) through stochastic sampling. Importantly, it assumes no knowledge about the environment. [Szepesv´ari and Littman] ([1999]) showed that under similar assumptions to those for Q-learning ([Watkins and Dayan], [1992]), the Bellman operator of Minimax-Q is a contraction mapping operator, and the stochastic updates made by Minimax-Q eventually lead to a unique fixed point that corresponds to the NE value. In addition to the tabular-form Q-function in Minimax-Q, various Qfunction approximators have been developed. For example, [Lagoudakis and Parr] ([2003]) studied the factorised linear architectures for Q-function representation. [Yang et al.] ([2019c]) adopted deep neural networks and derived a rigorous finite-sample error bound. [Zhang et al.] ([2018b]) also derived a finite-sample bound for linear function approximators in the competitive M-MDPs.

#### 7.2 Continuous State-Action Games

Recently, the challenge of training generative adversarial networks (GANs) ([Goodfellow] [et al.], [2014a]) has ignited tremendous research interest in understanding policy gradient methods in two-player continuous games, specifically, games with a continuous stationaction space and nonconvex-nonconcave loss landscape. In GANs, two neural network parameterised models – the generator G and the discriminator D – play a zero-sum game. In this game, the generator attempts to generate data that “look” authentic such that the discriminator cannot tell the difference from the true data; on the other hand, the discriminator tries not to be deceived by the generator. The loss function in this scenario is written as
![[Pasted image 20230801112657.png]]
where _θ_G and _θ_D represent neural networks parameters and _z_ is a random signal, serving as the input to the generator. In searching for the NE, one naive approach is to update both _θ_G and _θ_D by simultaneously implementing the gradient-descent-ascent (GDA) updates with the same step size in Eq. ([55]). This approach is equivalent to a MARL algorithm in which both agents are applying policy-gradient methods. With trivial adjustments to the step size ([Bowling], [2005]; [Bowling and Veloso], [2002]; [Zhang and Lesser], [2010]), GDA methods can work effectively in two-player two-action (thus convex-concave) games. However, in the nonconvex-nonconcave case, where the minimax theorem no longer holds, GDA methods are notoriously flawed from three aspects. First, GDA algorithms may not converge at all ([Balduzzi et al.], [2018a]; [Daskalakis and Panageas], [2018]; [Mertikopoulos et al.],[2018]), resulting in limited cycles[32] in which even the time average[33] does not coincide with NE ([Mazumdar et al.], [2019a]). Second, there exist undesired stable stationary points for the GDA algorithms that are not local optima of the game ([Adolphs et al.], [2019]; [Mazum-] [dar et al.], [2019a]). Third, there exist games whose equilibria are not the attractors of GDA methods at all ([Mazumdar et al.], [2019a]). These problems are partly caused by the intransitive dynamics (e.g., a typical intransitive game is rock-paper-scissors game) that are inherent in zero-sum games ([Balduzzi et al.], [2018a]; [Omidshafiei et al.], [2020]) and the fact that each agent may have a non-smooth objective function. In fact, even in simple linear-quadratic games, the reward function cannot satisfy the smoothness condition[34] globally, and the games are surprisingly not convex either ([Fazel et al.], [2018]; [Mazumdar] [et al.], [2019a]; [Zhang et al.], [2019c]).

Three mainstream approaches have been followed to develop algorithms that have at least a local convergence guarantee. One natural idea is to make the inner loop solvable at a reasonably high level and then focus on a simpler type of game. In other words, the algorithm tries to find a stationary point of the function _Φ_(_·_) := max_θ_D_∈_R_d_ _f_ _·_, θ_D , instead of Eq. ([55]). For example, by considering games with a nonconvex and (strongly) concave loss landscape, [Kong and Monteiro] ([2019]); [Lin et al.] ([2019]); [Lu et al.] ([2020a]); [Nouiehed et al.] ([2019]); [Rafique et al.] ([2018]); [Thekumparampil et al.] ([2019]) presented an affirmative answer that GDA methods can converge to a stationary point in the outer loop of optimising _Φ_(_·_) := max_θ_D_∈_R_d_ _f_ _·_, θ_D . Based on this understanding, they developed various GDA variants that apply the “best response” in the inner loop while maintaining an inexact gradient descent in the outer loop. We refer to [Lin et al.] ([2019]) [Table 1] for a detailed summary of the time complexity of the above methods.

(32Limited cycle is a terminology in the study of dynamical systems, which describes oscillatory systems. In game theory, an example of limit cycles in the strategy space can be found in Rock-Paper-Scissor game.
33In two-player two-action games, Singh et al. (2000) showed that the time average payos would converge to a NE value if their policies do not.
34A dierentiable function is said to be smooth if the gradients of the function are continuous.)

The second mainstream idea is to shift the equilibrium of interest from the NE, which is induced by simultaneous gradient updates, to the Stackelberg equilibrium, which is a solution concept in leader-follower (i.e., alternating update) games. [Jin et al.] ([2019]) introduced the concept of the local Stackelberg equilibrium, named _local_ _minimax_, based on which he established the connection to GDA methods by showing that all stable limit points of GDA are exactly local minimax points. [Fiez et al.] ([2019]) also built connections between the NE and Stackelberg equilibrium by formulating the conditions under which attracting points of GDA dynamics are Stackelberg equilibria in zero-sum games. When the loss function is bilinear, theoretical evidence was found that alternating updates converge faster than simultaneous GDA methods ([Zhang and Yu], [2019]).

The third mainstream idea is to analyse the loss landscape from a game-theoretic perspective and design corresponding algorithms that mitigate oscillatory behaviour. Compared to the previous two mainstream ideas, which helped generate more theoretical insights than applicable algorithms, works within this stream demonstrate strong empirical improvements in training GANs. [Mescheder et al.] ([2017]) investigated the game Hessian and identified that issues on the eigenvalues trigger the limited cycles. As a result, they proposed a new type of update rule based on consensus optimisation, together with a convergence guarantee to a local NE in smooth two-player zero-sum games. [Adolphs et al.] ([2019]) leveraged the curvature information of the loss landscape to propose algorithms in which all stable limit points are guaranteed to be local NEs. Similarly, [Mazumdar et al.] ([2019b]) took advantage of the differential structure of the game and constructed an algorithm for which the local NEs are the only attracting fixed points. In addition, [Daskalakis et al.] ([2017]); [Mertikopoulos et al.] ([2018]) addressed the issue of limit cycling behaviour in training GANs by proposing the technique of _optimistic mirror descent (OMD)_. OMD achieves the last-iterate convergent guarantee in bilinear convexconcave games. Specifically, at each time step, OMD adjusts the gradient of that time step by considering the opponent policy at the next time step. Let _Mt_+1 be the predictor of the next iteration gradient[35]; we can write OMD as follows.

![[Pasted image 20230801112859.png]]

In fact, the pivotal idea of opponent prediction in OMD, developed in the optimisation domain, resembles the idea of approximate policy prediction in the MARL domain ([Foerster et al.], [2018a]; [Zhang and Lesser], [2010]).

(35In practice, it is usually set as the last iteration gradient.)

Thus far, the most promising results are probably those of [Bu et al.] ([2019]) and [Zhang] [et al.] ([2019c]), which reported the first results in solving zero-sum LQ games with a global convergence guarantee. Specifically, [Zhang et al.] ([2019c]) developed the solution through projected nested-gradient methods, while [Bu et al.] ([2019]) solved the problem through a projection-free Stackelberg leadership model. Both of the models achieve a sublinear rate for convergence.

#### 7.3 Extensive-Form Games

As briefly introduced in Section [3.4], zero-sum EFG with imperfect information can be efficiently solved via LP in sequence form representations ([Koller and Megiddo], [1992], [1996]). However, these approaches are limited to solving only small-scale problems (e.g., games with _O_(107) information states). In fact, considerable additional effort is needed to address real-world games (e.g., limit Texas hold’em, which has _O_(1018) game states); to name a few, Monte Carlo Tree Search (MCTS) techniques[36] ([Browne et al.], [2012]; [Cowling] [et al.], [2012]; [Silver et al.], [2016]), isomorphic abstraction techniques ([Billings et al.], [2003]; [Gilpin and Sandholm], [2006]), and iterative (policy) gradient-based approaches ([Gilpin] [et al.], [2007]; [Gordon], [2007]; [Zinkevich], [2003]).

A central idea of iterative policy gradient-based methods is minimising regret[37]. A learning rule achieves no-regret, also called _Hannan consistency_ in game theoretical terms ([Hannan], [1957]), if, intuitively speaking, against any set of opponents it yields a payoff that is no less than the payoff the learning agent could have obtained by playing any one of its pure strategies in hindsight. Recall the reward function under a given policy **_π_** = (_πi, π_−_i_) in Eq. ([27]); the (average) regret of player _i_ is defined by:
![[Pasted image 20230801112949.png]]

A no-regret algorithm satisfies Reg_i_→_ 0 as _T_ _→ ∞_ with probability 1. When Eq. ([57]) equals zero, all agents are acting with their best response to others, which essentially forms a NE. Therefore, one can regard regret as a type of “distance” to NE. As one would expect, the single-agent Q-learning procedure can be shown to be Hannan consistent in a stochastic game against opponents playing stationary policies ([Shoham and Leyton-] [Brown], [2008]) [Chapter 7] since the optimal Q-function guarantees the best response. In contrast, the Minimax-Q algorithm in Eq. ([54]) is not Hannan consistent because if the opponent plays a sub-optimal strategy, Minimax-Q is unable to exploit the opponent due to the over-conservativeness in terms of over-estimating its opponents.

An important result about regret states is that in a zero-sum game at time _T_ , if both players’ average regret is less than _E_, then their average strategy constitutes a 2_E_-NE of the game ([Zinkevich et al.], [2008], Theorem 2). In general-sum games, the average strategy of the _E_-regret algorithm will reach an _E_-_coarse_ _correlated_ _equilibrium_ of the game ([Michael], [2020], Theorem 6.3.1). This result essentially implies that regret-minimising algorithms (or, algorithms with Hannan consistency) applied in a self-play manner can be used as a general technique to approximate the NE of zero-sum games. Building upon this finding, two families of methods are developed, namely, fictitious play types of methods ([Berger], [2007]) and counterfactual regret minimisation ([Zinkevich et al.], [2008]), which lay the theoretical foundations for modern techniques to solve real-world games.

#### 7.3.1 Variations of Fictitious Play

Fictitious play (FP) ([Berger], [2007]) is one of the oldest learning procedures in game theory that is provably convergent for zero-sum games, potential games, and two-player n-action games with generic payoffs. In FP, each player maintains a belief about the empirical mean of the opponents’ average policy, based on which the player selects the best response. With the best response defined in Eq. ([17]), we can write the FP updates as
![[Pasted image 20230801143711.png]]

In the FP scheme, each agent is oblivious to the other agents’ reward; however, they need full access to their own payoff matrix in the stage game. In the continuous case with an infinitesimal learning rate of 1_/t_ _→_ 0, Eq. ([58]) is equivalent to _d_**_π_**_t_/dt_ ∈ **Br** **_π_**_t_ _−_ **_π_**_t_ in which **Br**(**_π_**_t_) = **Br**(_π_t_−_1)_,_ _,_ **Br**(_π_t_−_N_ ) . [Viossat and Zapechelnyuk] ([2013]) proved that continuous FP leads to no regret and is thus Hannan consistent. If the empirical distribution of each $π_i$ converges in FP, then it converges to a NE[38].

(38Note that the convergence in Nash strategy does not necessarily mean the agents will receive the expected payovalue at NE. In the example of Rock-Paper-Scissor games, agentsactions are still miscorrelated after convergence, ipping between one of the three strategies, though their average policies do converge to (131313).)

Although standard discrete-time FP is not Hannan consistent ([Cesa-Bianchi and Lu-] [gosi], [2006], Exercise 3.8), various extensions have been proposed that guarantee such a property; see a full list summarised in [Hart] ([2013]) [Section 10.9]. Smooth FP ([Fudenberg] [and Kreps], [1993]; [Fudenberg and Levine], [1995]) is a stochastic variant of FP (thus also called stochastic FP) that considers a smooth _E_-best response in which the probability of each action is a softmax function of that action’s utility/reward against the historical frequency of the opponents’ play. In smooth FP, each player’s strategy is a genuinemixed strategy. Let _Ri_(_ai_ _, π_t_−_i_) be the expected reward of player _i_’s action $a_i$ ∈ A_i_ under opponents’ strategy _π_−_i_; the probability of playing $a_i$ in the best response is written as
![[Pasted image 20230801143832.png]]

[Bena¨ım and Faure] ([2013]) verified the Hannan consistency of the smooth best response with the smoothing parameter _λ_ being time dependent and vanishing asymptotically. In potential games, smooth FP is known to converge to a neighbourhood of the set of NE ([Hofbauer and Sandholm], [2002]). Recently, [Swenson and Poor] ([2019]) showed a generic result that in almost all _N_ _×_2 potential games, smooth FP converges to the neighbourhood of a pure-strategy NE with a probability of one.

In fact, “smoothing” the cumulative payoffs before computing the best response is crucial to designing learning procedures that achieve Hannan consistency ([Kaniovski and] [Young], [1995]). One way to achieve such smoothness is through stochastic smoothing or adding perturbations[39]. For example, the smooth best response in Eq. ([59]) is a closedform solution if one perturbs the cumulative reward by an additional entropy function, that is,
![[Pasted image 20230801143921.png]]

Apart from smooth FP, another way to add perturbation is the _sampled FP_ in which during each round, the player samples historical time points using a randomised sampling scheme, and plays the best response to the other players’ moves, restricted to the set of sampled time points. Sampled FP is shown to be Hannan consistent when used with Bernoulli sampling ([Li and Tewari], [2018]).

(39The physical meaning of perturbing the cumulative payois to consider the incomplete information about what the opponent has been playing, variability in their payos, and unexplained trembles.)

Among the many extensions of FP, the most important is probably _generalised weak-_ened_ _FP (GWFP)_ ([Leslie and Collins], [2006]), which releases the standard FP by allowing both approximate best response and perturbed average strategy updates. Specifically, if we write the _E_-best response of player _i_ as
![[Pasted image 20230801144015.png]]then the GWFP updating steps change from Eq. ([58]) to
![[Pasted image 20230801144034.png]]

![[Pasted image 20230801144052.png]]
GWFP is an important extension of FP in that it provides two key components for bridging game theoretic ideas with RL techniques. With the approximate best response (highlighted in blue, also named as the “weakened” term), this approach allows one to adopt a model-free RL algorithm, such as deep Q-learning, to compute the best response. Moreover, the perturbation term (highlighted in red, also named as the “generalised” term) enables one to incorporate policy exploration; if one applies an entropy term as the perturbation in addition to the best response (in which the smooth FP in Eq. ([60]) is also recovered), the scheme of maximum-entropy RL methods ([Haarnoja et al.], [2018]) is recovered. In fact, the generalised term also accounts for the perturbation that comes from the fact the beliefs are not updated towards the exact mixed strategy _π_−_i_ but instead towards the observed actions ([Benaım and Hirsch], [1999]). As a direct application, [Perolat et al.] ([2018]) implemented the GWFP process through an actor-critic framework ([Konda and Tsitsiklis], [2000]) in the MARL setting.

Brown’s original version of FP ([Berger], [2007]) describes alternating updates by players; yet, the modern usage of FP involves players updating their beliefs simultaneously ([Berger], [2007]). In fact, [Heinrich et al.] ([2015]) only recently proposed the first FP algorithm for EFG using the sequence-form representation. The extensive-form FP is essentially an adaptation of GWFP from NFG to EFG based on the insight that a mixture of normal-form strategies can be implemented by a weighted combination of behavioural strategies that have the same realisation plan (recall Section [3.3.2]). Specifically, let _π_ and _β_ be two behavioural strategies, _Π_ and _B_ be the two realisation-equivalent mixed strategies[40], and _α_ ∈ R+; then, for each information state _S_, we have
![[Pasted image 20230801144124.png]]

where $σ_S$ is the sequence leading to _S_, _µπ/β_(_σS_) is the realisation probability of $σ_S$ under a given policy, and _π_˜(_S_) defines a new behaviour that is realisation equivalent to the mixed strategy (1 _−_ _α_)_Π_ + $α_B$. The extensive-form FP essentially iterates between Eq. ([61]), which computes the _E_-best response, and Eq. ([63]), which updates the old behavioural strategy with a step size of _α_. Note that these two steps must iterate over all information states of the game in each iteration. Similar to the normal-form FP in Eq. ([58]), extensive-form FP generates a sequence of _{_**_π_**_t_}_t_≥_1 that provably converges to the NE of a zero-sum game under self-play if the step size _α_ goes to zero asymptotically. As a further enhancement, [Heinrich and Silver] ([2016]) implemented neural fictitious selfplay (NFSP), in which the best response step is computed by deep Q-learning ([Mnih] [et al.], [2015]) and the policy mixture step is computed through supervised learning. NFSP requires the storage of large replay buffers of past experiences; [Lockhart et al.] ([2019]) removes this requirement by obtaining the policy mixture for each player through an independent policy-gradient step against the respective best-responding opponent. All these amendments help make extensive-form FP applicable to real-world games with large-scale information states.

#### 7.3.2 Counterfactual Regret Minimisation

Another family of methods achieve Hannan consistency by directly minimising the regret, in particular, a special kind of regret named counterfactual regret (CFR) ([Zinkevich] [et al.], [2008]). Unlike FP methods, which are developed from the stochastic approximation perspective and generally have asymptotic convergence guarantees, CFR methods are established on the framework of online learning and online convex optimisation ([Shalev-] [Shwartz et al.], [2011]), which makes analysing the speed of convergence, i.e., the regret bound, to the NE possible.

(40Recall that in games with perfect recall, Kuhns theorem (Kuhn, 1950a) suggests that the behavioural strategy and mixed strategies are equivalent in terms of the realisation probability of dierent outcomes.)

The key insight from CFR methods is that in order to minimise the total regret in Eq.([57]) to approximate the NE, it suffices to minimise the _immediate counterfactual regret_ at the level of each information state. Mathematically, [Zinkevich et al.] ([2008]) [Theorem 3] shows that the sum of the immediate counterfactual regret over all encountered information states provides an upper bound for the total regret in Eq. ([57]), i.e.,
![[Pasted image 20230801144220.png]]

To fully describe Reg_i_(_S_), we need two additional notations. Let _µ_**_π_**(**_σ_**_S_ _→_ **_σ_**_T_ ) de-note, given agents’ behavioural policies **_π_**, the realisation probability of going from the sequence **_σ_**_S_[41], which leads to the information state _S_ ∈ S_i_ to its extended sequence **_σ_**_T_ , which continues from _S_ and reaches the terminal state _T_ . Let _v_ˆ_i_(**_π_**_,_ _S_) be the _c_ounter-_ _factual value function_, i.e., the expected reward of agent _i_ in non-terminal information state _S_, which is written as
![[Pasted image 20230801144304.png]]

Note that in Eq. ([65]), the contribution from player _i_ in realising **_σ_**_s_ is excluded; we treat whatever action current player _i_ needs to reach state _s_ as having a probability of one, that is, _µπ_i_(**_σ_** ) = 1. The motivation is that now one can make the value function _v_ˆ_i_ **_π_**_,_ _S_ “counterfactual” simply by writing the consequence of player _i_ not playing action _a_ in the information state _S_ as _v_ˆ_i_(**_π_**_|_S_→_a_,_ _S_) _−_ _v_ˆ_i_(**_π_**_,_ _S_) , in which **_π_**_|_S_→_a_ is a joint strategy profile identical to **_π_**, except player _i_ always chooses action _a_ when information state _S_ is encountered. Finally, based on Eq. ([65]), the immediate counterfactual regret can be expressed as
![[Pasted image 20230801144344.png]]

Note that the _T_ in Eq. ([65]) is different from that in Eq. ([66]).

(41Recall that for games of perfect recall, the sequence that leads to the information state, including all the choice nodes within that information state, is unique.)

Since minimising the immediate counterfactual regret minimises the overall regret, we can find an approximate NE by choosing a specific behavioural policy _πi_(_S_) that minimises Eq. ([66]). To this end, one can apply Blackwell’s approachability theorem ([Blackwell et al.], [1956]) to minimise the regret independently on each information set, also known as _regret_ _matching_ ([Hart and Mas-Colell], [2001]). As we are most concerned with positive regret, denoted by _l·J_+, we have _∀_S_ ∈ S_i_,_ _∀_a_ ∈ _χ_(_S_), the strategy of player _i_ at time _T_ + 1 as Eq. ([67]).
![[Pasted image 20230801144432.png]]

In the standard CFR algorithm, for each information set, Eq. ([67]) is used to compute action probabilities in proportion to the positive cumulative regrets. In addition to regret matching, another online learning tool that minimises regret is _Hedge_ ([Freund] [and Schapire], [1997]; [Littlestone and Warmuth], [1994]), in which an exponentially weighted function is used to derive a new strategy, which is
![[Pasted image 20230801144450.png]]

In computing Eq. ([68]), Hedge needs access to the full information of the reward values for all actions, including those that are not selected. _EXP3_ ([Auer et al.], [1995]) extended the Hedge algorithm for a _partial information game_ in which the player knows only the reward of the the chosen action (i.e., a bandit version) and has to estimate the loss of the actions that it does not select. [Brown et al.] ([2017]) augmented the Hedge algorithm with a tree-pruning technique based on dynamic thresholding. [Gordon] ([2007]) developed _Lagrangian_ _hedging_, which unifies no-regret algorithms, including both regret matching and Hedge, through a class of potential functions. We recommend [Cesa-Bianchi and] [Lugosi] ([2006]) for a comprehensive overview of no-regret algorithms.

(42According to Zinkevich (2003), any online convex optimisation problem can be made to incur RegT = ( T).)

No-regret algorithms, under the framework of online learning, offer a natural way to study the regret bound (i.e., how fast the regret decays with time). For example, CFR and its variants ensure a counterfactual regret bound of _O_(_√_T_ )[42], as a result of Eq. ([64]), the convergence rate for the total regret is upper bounded by _O_(_√_T_ _· |_S_|_), which is linear in the number of information states. In other words, the average policy of applying ![](file:///C:/Users/Yuming/AppData/Local/Temp/msohtmlclip1/01/clip_image001.gif)CFR-type methods in a two-player zero-sum EFG generates an _O_(_|_S_|_/_√_T_ )-approximate NE after _T_ steps through self-play[43].

Compared with the LP approach (recall Eq. ([33])), which is applicable only for smallscale EFGs, the standard CFR method can be applied to limit Texas hold’em with as many as 1012 states. CFR+, the fastest implementation of CFR, can solve games with up to 1014 states ([Tammelin et al.], [2015]). However, CFR methods still have a bottleneck in that computing Eq. ([65]) requires a traversal of the entire game tree to the terminal nodes in each iteration. Pruning the sub-optimal paths in the game tree is a natural solution ([Brown et al.], [2017]; [Brown and Sandholm], [2015], [2017]). Many CFR variants have been developed to improve computational efficiency further. [Lanctot et al.] ([2009]) integrated Monte Carlo sampling with CFR (MCCFR) to significantly reduce the per iteration time cost of CFR by traversing a smaller sampled portion of the tree. [Burch et al.] ([2012]) improved MCCFR by sampling only a subset of a player’s actions, which provides even faster convergence rate in games that contain many player actions. [Gibson et al.] ([2012]); [Schmid et al.] ([2019]) investigated the sampling variance and proposed MCCFR variants with a variance reduction module. [Johanson et al.] ([2012b]) introduced a more accurate MCCFR sampler by considering the set of outcomes from the chance node, rather than sampling only one outcome, as in all previous methods. Apart from Monte Carlo methods, function approximation methods have also been introduced ([Jin et al.], [2018]; [Waugh et al.], [2014]). The idea of these methods is to predict regret directly, and the no-regret algorithm then uses these predictions in place of the true regret to define a sequence of policies. To this end, the application of deep neural networks has led to great success ([Brown et al.], [2019]).

Interestingly, there exists a hidden equivalence between model-free policy-based/actor [2] －\||critic MARL methods and the CFR algorithm ([Jin et al.], [2018]; [Srinivasan et al.], [2018]). In particular, if we consider the counterfactual value function in Eq. ([65]) to be explicitly dependent on the action _a_ that player _i_ chooses at state _S_, in which we have _v_ˆ_i_(**_π_**_,_ _S_) = _π_i_(_S_,_ _a_)_q_ˆ_i_(**_π_**_,_ _S_,_ _a_), then it is shown in [Srinivasan et al.] ([2018]) [Section 3.2] that the Q-function in standard MARL _Q_i,_**_π_**(_s,_ **a**) = E_s_1_∼_P_,_**a**_∼_**_π_** _t__γ_t_R_i_(_s,_ **a**_,_ _s_I_)_|_s,_ **a** differs

(43The self-play assumption can in fact be released. Johanson et al. (2012a) shows that in two-player zero-sum games, as long as both agents minimise their regret, not necessarily through the same algorithm, their time-average policies will converge to NE with the same regret bound O T). An example is to let a CFR player play against a best-response opponent.)

from _q_ˆ_i_(**_π_**_,_ _S_,_ _a_) in CFR only by a constant of the probability of reaching _S_, that is,
![[Pasted image 20230801144755.png]]

Subtracting a value function on both sides of Eq. ([69]) leads to the fact that the counterfactual regret of Reg_i_ (_S, a_) in Eq. ([66]) differs from the advantage function in MARL, i.e., _Qi,_**_π_**(_s,_ _ai,_ _a_−_i_) _−_ _V i,_**_π_**(_s,_ _a_−_i_), only by a constant of the realisation probability. As a result, the multi-agent actor-critic algorithm ([Foerster et al.], [2018b]) can be formulated as a special type of CFR method, thus sharing a similar convergence guarantee and regret bound in two-player zero-sum games. The equivalence has also been found by ([Hennes et al.], [2019]), where the CFR method with Hedge can be written as a particular actor-critic method that computes the policy gradient through replicator dynamics.

#### 7.4 Online Markov Decision Processes

A common situation in which online learning techniques are applied is in stateless games, where the learning agent faces an identical decision problem in each trial (e.g., playing a multi-arm bandit in the casino). However, real-world decision problems often occur in a dynamic and changing environment. Such an environment is commonly captured by a state variable which, when incorporated into online learning, leads to an online MDP. Online MDP ([Auer et al.], [2009]; [Even-Dar et al.], [2009]; [Yu et al.], [2009]), also called adversarial MDP[44], focuses on the problem in which the reward and transition dynamics can change over time, i.e., they are non-stationary and time-dependent.

In contrast to an ordinary stochastic game, the opponent/adversary in an online MDP is not necessarily rational or even self-optimising. The aim of studying online MDP is to provide the agent with policies that perform well against every possible opponent (including but not limited to adversarial opponents), and the objective of the learning agent is to minimise its average loss during the learning process. Quantitatively, the loss is measured by how worse off the agent is compared to the best stationary policy in retrospect. The _expected_ _regret_ is thus different from Eq. ([57]) (unless in repeated games) and is written as
![[Pasted image 20230801144904.png]]
where E_π_ denotes the expectation over the sequence of (_s∗t, a∗t_) induced by the stationary policy _π_. Note that the reward function sequence and the transition kernel sequence are given by the adversary, and they are not influenced by the retrospective sequence (_s∗t, a∗t_).

(44The word adversarialis inherited from the online learning literature, i.e., stochastic bandit vs adversarial bandit (Auer et al., 2002). Adversary means there exists a virtual adversary (or, nature) who has complete control over the reward function and transition dynamics, and the adversary does not necessarily maintain a fully competitive relationship with the learning agent.)

The goal is to find a no-regret algorithm that can satisfy Reg_T_ _→_ 0 as _T_ _→ ∞_ with probability 1. A sufficient condition that ensures the existence of no-regret algorithms for online MDPs is the _oblivious_ assumption – both the reward functions and transition kernels are fixed in advance, although they are unknown to the learning agent. This scenario is in contrast to the stateless setting in which no-regret is achievable, even if the opponent is allowed to be _adaptive/non-oblivious_: they can choose the reward function and transition kernels in accordance to (_s_0_,_ _a_0_,_ _, st_) from the learning agent. In short, [Mannor and Shimkin] ([2003]); [Yu et al.] ([2009]) demonstrated that in order to achieve sub-linear regret, it is essential that the changing rewards are chosen obliviously. Furthermore, [Yadkori et al.] ([2013]) showed with the example of an online shortest path problem that there does not exist a polynomial-time solution (in terms of the size of the state-action space) where both the reward functions and transition dynamics are adversarially chosen, even if the adversary is _oblivious_ (i.e., it cannot adapt to the other agent’s historical actions). Most recently, [Cheung et al.] ([2020]); [Ortner et al.] ([2020]) investigated online MDPs where the transitional dynamics are allowed to change slowly (i.e., the total variation does not exceed a specific budget). Therefore, the majority of existing no-regret algorithms for online MDP focus on an oblivious adversary for the reward function only. The nuances of different algorithms lie in whether the transitional kernel is assumed to be known to the learning agent and whether the feedback reward that the agent receives is in the full-information setting or in the bandit setting (i.e., one can only observe the reward of a taken action).

Two design principles can lead to no-regret algorithms that solve online MDPs with an oblivious adversary controlling the reward function. One is to leverage the localglobal regret decomposition result ([Even-Dar et al.], [2005], [2009]) [Lemma 5.4], which demonstrates that one can in fact achieve no regret globally by running a local regretminimisation algorithm at each state; a similar result is observed for the CFR algorithm described in Eq. ([66]). Let _µ_∗_(_·_) denote the state occupancy induced by policy _π_∗_; we then obtain the decomposition result by
![[Pasted image 20230801144948.png]]

Under full knowledge of the transition function and full-information feedback about the reward, [Even-Dar et al.] ([2009]) proposed the famous _MDP-Expert (MDP-E)_ algorithm, which adopts _Hedge_ ([Freund and Schapire], [1997]) as the regret minimiser and achieves _O_( _τ_ 3_T_ ln _|_A_|_) regret, where _τ_ is the bound on the mixing time of MDP [45]. For comparison, the theoretical lower bound for regret in a fixed MDP (i.e., no adversary perturbs the reward function) is _Ω_( _|_S_||_A_|_T_ )[46] ([Auer et al.], [2009]). Interestingly, [Neu et al.] ([2017]) showed that there in fact exists an equivalence between TRPO methods ([Schulman et al.], [2015]) and MDP-E methods. Under bandit feedback, [Neu et al.] ([2010]) analysed _MDPEXP3_, which achieves a regret bound of _O_( _τ_ 3_T_ _|_A_|_ log _|_A_|_/β_), where _β_ is a lower bound on the probability of reaching a certain state under a given policy. Later, [Neu et al.] ([2014]) removed the dependency on _β_ and achieved _O_(_√_T_ log _T_ ) regret. One major advantage of local-global design principle is that it can work seamlessly with function approximation methods ([Bertsekas and Tsitsiklis], [1996]). For example, [Yu et al.] ([2009]) eliminated the requirement of knowing the transition kernel by incorporating Q-learning methods; their proposed _Q-follow the perturbed leader (Q-FPL)_ method achieved _O_(_T_ 2_/_3) regret.[Abbasi-Yadkori et al.] ([2019]) proposed _POLITEX_, which adopted a least square policy evaluation (LSPE) with linear function approximation and achieved _O_(_T_ 3_/_4 +_E_0_T_ ) regret, in which _E_0 is the worst-case approximation error, and [Cai et al.] ([2019a]) used the same LSPE method. However, the proposed _OPPO_ algorithm achieves _O_(_√_T_ ) regret.

Apart from the local-global decomposition principle, another design principle is to formulate the regret minimisation problem as an online linear optimisation (OLO) problem and then apply gradient-descent type methods. Specifically, since the regret in Eq._(71)_ can be further written as the inner product of Reg_T_ run the gradient descent method by _Tt_=1_(_µ_∗ −_ _µ_t_, Rt_分_, one can run the gradient descent method by
![[Pasted image 20230801145133.png]]

where _U_ = _µ ∈ ∆_S_×_A : _a µ_(_s, a_) = _s1,a1 P_ (_s|sI, aI_)_µ_(_sI, aI_) is the set of all valid stationary distributions[47], where _D_ denotes a certain form of divergence and the policy can be extracted by _πt_+1(_a|s_) = _µt_+1(_s, a_)_/µ_(_s_). One significant advantage of this type of method is that it can flexibly handle different model constraints and extensions. If one uses Bregman divergence as _D_, then online mirror descent is recovered ([Nemirovsky and Yudin], [1983]) and is guaranteed to achieve a nearly optimal regret for OLO problems ([Srebro] [et al.], [2011]). [Zimin and Neu] ([2013]) and [Dick et al.] ([2014]) adopted a relative entropy for _D_; the subsequent _online relative entropy policy search (O-REPS)_ algorithm achieves an _O_( _τ T_ log(_|_S_||_A_|_)) regret in the full-information setting and an _O_( _T |_S_||_A_|_ log(_|_S_||_A_|_)) regret in the bandit setting. For comparison, the aforementioned MDP-E algorithm achieves _O_( _τ_ 3_T_ ln _|_A_|_) and _O_( _τ_ 3_T |_A_|_ log _|_A_|/β_), respectively. When the transition dynamics are unknown to the agent, [Rosenberg and Mansour] ([2019]) extended O-REPS by incorporating the classic idea of _optimism in the face of uncertainty_ in [Auer et al.] ([2009]), and the induced _UC-O-REPS_ algorithm achieved _O_(_|_S_|_✓_|_A_|_T_ ) regret.

#### 7.5 Turn-Based Stochastic Games

An important class of games that lie in the middle of SG and EFG is the two-player zerosum turn-based SG (2-TBSG). In TBSG, the state space is split between two agents, S = S1 _∪_ S2_,_ S1 _∩_ S2 = _∅_, and in every time step, the game is in exactly one of the states, either S1 or S2. Two players alternate taking turns to make decisions, and each state is controlled[48] by only one of the players $π_i$ : S_i →_ A_i, i_ = 1_,_ 2. The state then transitions into the next state with probability _P_ : S_i ×_ A_i →_ S_j, i, j_ = 1_,_ 2. Given a joint policy **_π_** = (_π_1_, π_2), the first player seeks to maximise the value function _V **π**_(_s_) = E _∞t_=0_γtR st, π_(_st_) _|s_0 = _s_ , while the second player seeks to minimise it, and the saddle point is the NE of the game.

(47In the online MDP literature, it is generally assumed that every policy reaches its stationary distri bution immediately; see the policy mixing time assumption in Yu et al. (2009) [Assumption 2.1]. 48Note that since the game is turned based, the Nash policies are deterministic.)

Research on 2-TBSG leads to many important finite-sample bounds, i.e., how many samples one would need before reaching the NE at a given precision, for understanding multi-agent learning algorithms. [Hansen et al.] ([2013]) extended [Ye] ([2005], [2010])’s result from single-agent MDP to 2-TBSG and proved that the strongly polynomial time complexity of policy iteration algorithms also holds in the context of 2-TBSG if the payoff matrix is fully accessible. In the RL setting, in which the transition model is unknown, [Sidford et al.] ([2018], [2020]) provided a near-optimal Q-learning algorithm that computes an _E_-optimal strategy with high-probability given _O_ (1 _− γ_)_−_3_E−_2 samples from the transition function for each state-action pair. This result of polynomial-time sample complexity is remarkable since it was believed to hold for only single-agent MDPs. Recently, [Jia et al.] ([2019]) showed that if the transition model can be embedded in some state-action feature space, i.e., _∃ψk_(_sI_) such that _P_ (_sI|s, a_) = 2_K φk_(_s, a_)_ψk_(_sI_)_, ∀sI ∈_ S_,_ (_s, a_) ∈ S_×_A, then the sample complexity of the two-player Q-learning algorithm towards finding an _E_-NE is only linear to the number of features _O_ _K/_(_E_2(1 _−_ _γ_)4) .

![[Pasted image 20230801145405.png]]

All the above works focus on the offline domain, where they assume that there exists an _oracle_ that can unconditionally provide state-action transition samples. [Wei et al.] ([2017]) studied an online setting in an averaged-reward two-player SG. They achieved a polynomial sample-complexity bound if the opponent plays an optimistic best response, and a sublinear regret round against an arbitrary opponent.

#### 7.6 Open-Ended Meta-Games

In solving real-world zero-sum games, such as GO or StarCraft, since the number of atomic pure strategy can be prohibitively large, one feasible approach instead is to focus on _meta-games_. A meta-game is constructed by simulating games that cover combinations of “high-level” policies in the policy space (e.g., “bluff” in Poker or “rushing” in StarCraft), with entries corresponding to the players’ empirical payoffs under a certain joint “high-level” policy profile; therefore, meta-game analysis is often called as _empirical game-theoretic analysis (EGTA)_ ([Tuyls et al.], [2018]; [Wellman], [2006]). Analysing metagames is a practical approach to tackling games that have huge pure-strategy space, since the number of “high-level” policies is usually far smaller than the number of pure strategies. For example, the number of tactics in StarCraft is at hundreds, compared to the vast raw action space of approximately 108 possibilities ([Vinyals et al.], [2017]). Traditional game-theoretical concepts such as NE can still be computed on meta-games, but in a much more scalable manner; this is because the number of “higher-level” strategies in the meta-game is usually far smaller than the number of atomic actions of the underlying game. Furthermore, it has been shown that an _E_-NE of the meta-game is in fact a 2_E_-NE of the underlying game ([Tuyls et al.], [2018]). Meta-games are often **open-ended** because in general there exists an infinite number of policies to play a real-world game, and, as new strategies will be discovered and added to agents’ strategy sets during training, the dimension of the meta-game payoff table will also be expanded. If one writes the game evaluation engine as _φ_ : S1 _×_ S2 _→_ R such that if _S_1 ∈ S1 beats _S_2 ∈ S2, we have _φ_(_S_1_,_ _S_2) _>_ 0, and _φ <_ 0_, φ_ = 0 refers to losses and ties, then the meta-game payoff can be represented by **M** = _φ_(_S_1_,_ _S_2) : (_S_1_,_ _S_2) ∈ S1 _×_ S2 . The sets of S1 and S2 can be regarded as, for example, two populations of deep neural networks (DNNs) and each _S_1_,_ _S_2 is a DNN with independent weights. In such a context, the goal of learning in meta-games is to find S_i_ and policy **_π_**_i_ ∈ _∆_(S_i_) such that the _exploitability_ can be minimised, which is,
![[Pasted image 20230801145536.png]]

![[Pasted image 20230801145455.png]]
**Table 3:** Variations of Different Meta-Game Solvers

A general solver for open-ended meta-games is the _policy space response oracle (PSRO)_ ([Lanctot et al.], [2017]). Inspired by the _double oracle_ algorithm ([McMahan et al.], [2003]), which leverages the _Benders’ decomposition_ ([Benders], [1962]) on solving large-scale linear programming for two-player zero-sum games, PSRO is a direct extension of double oracle ([McMahan et al.], [2003]) by incorporating an RL subroutine as an approximate best response. Specifically, one can write PSRO and its variations in Algorithm [1], which essentially involves an iterative two-step process of solving for the meta-policy first (e.g., Nash over the meta-game), and then based on the meta-policy, finding a new betterperforming policy, against the opponent’s current meta-policy, to augment the existing population. The meta-policy solver, denoted as _S_(_·_), computes a joint meta-policy profile **_π_** based on the current payoff **M** where different solution concepts can be adopted (e.g., NE). Finding a new policy is equivalent to solving a single-player optimisation problem given opponents’ policy sets S_−_i_ and meta-policies **_π_**_−_i_, which are fixed and known. One can regard a new policy as given by an _Oracle_, denoted by _O_. In two-player zero-sum cases, an oracle represents _O_1(**_π_**2) = _{_S_1 : _S_2_∈_S2 **_π_**2(_S_2) _·_ _φ_(_S_1_,_ _S_2) _>_ 0_}_. Generally, Oracles can be implemented through optimisation subroutines such as RL algorithms. Finally, after a new policy is found, the payoff table **M** is expanded, and the missing entries are filled by running new game simulations. The above two-step process loops over each player at each iteration, and it terminates if no new policies can be found for any players.

Algorithm [1] is a general framework, with appropriate choices of meta-game solver _S_ and Oracle _O_, it can represent solvers for different types of meta-games. We summarise variations of meta-game solvers in Table [3]. For example, it is trivial to see that FP/GWFP is recovered when _S_ = UNIFORM(_·_) and _O_i_ = **Br**_i_(_·_)_/_**Br**_i_(_·_). The double oracle ([McMahan et al.], [2003]) and PSRO methods ([Lanctot et al.], [2017]) refer to the cases when the meta-solver computes NE. On solving symmetric zero-sum games (i.e., S1 = S2, and _φ_(_S_1_,_ _S_2) = _−_φ_(_S_2_,_ _S_1)_,_ _∀_S_1_,_ _S_2 ∈ S1), [Balduzzi et al.] ([2019]) proposed the _rectified best response_ to promote behavioural diversity, written as
![[Pasted image 20230801145618.png]]

Through rectifying only the positive values on _φ_(_S_1_,_ _S_2) in Eq. ([74]), player 1 is encouraged to amplify its strengths and ignore its weaknesses in finding a new policy when it plays with the NE of player 2 during training; this turns out to be a critical component to tackle zero-sum games with strong non-transitive dynamics[49].

Double oracle and PSRO methods can only solve zero-sum games. When it comes to multi-player general-sum games, a new solution concept named _α_-Rank ([Omidshafiei] [et al.], [2019]) can be used to replace the intractable NE. The idea of _α_-Rank is built on the _response_ _graph_ of a game. On the response graph, each joint pure-strategy profile isa node, and a directed edge points from node _σ_ ∈ S to node _S_ ∈ S if 1) _σ_ and _S_ differ

in only one single player’s strategy, and 2) that deviating player, denoted by _i_, benefits from deviating from _S_ to _σ_ such that **M**_i_(_σ_) _>_ **M**_i_(_S_). The _sink strongly-connected components (SSCC)_ nodes on the response graph that have only incoming edges but no outgoing edges are of great interest. To find those SSCC nodes, _α_-Rank constructs a random walk along the directed response graph, which can be equivalently described by a Markov chain, with the transition probability matrix **_C_** being:
![[Pasted image 20230801145710.png]]

_η_ = ( _i∈N_(_|Si| −_ 1))_−_1, _m ∈_ N_, α >_ 0 are three constants. Large _α_ ensures the Markov chain is irreducible, and thus guarantees the existence and uniqueness of the _α_-Rank solution, which is the resulting unique stationary distribution **_π_** of the Markov chain, **_C_**_丁**π**_ = **_π_**. The probability mass of each joint strategy in **_π_** can be interpreted as the longevity of that strategy during an evolution process ([Omidshafiei et al.], [2019]). The main advantage of _α_-Rank is that it is unique and its solution is _P_ -complete even on multi-player general-sum games. _αα_-Rank developed by [Yang et al.] ([2019a]) computes _α_Rank based on stochastic gradient methods such that there is no need to store the whole transition matrix in Eq. ([75]) before getting the final output of **_π_**, this is particularly important when meta-games are prohibitively large in real-world domains.

When PSRO adopts _α_-Rank as the meta-solver, it is found that a simple best response fails to converge to the SSCC of a response graph before termination ([Muller et al.], [2019]). To suit _α_-Rank, [Muller et al.] ([2019]) later proposed _preference-based_ _best_ _response_ _oracle_, written as
![[Pasted image 20230801145855.png]]
and the combination of _α_-Rank with **PBr**(_·_) in Eq. ([76]) is called _α_-PSRO. Due to the tractability of _α_-Rank on general-sum games, the _α_-PSRO is credited as a generalised training approach for multi-agent learning.

(49Any symmetric zero-sum games consist of both transitive and non-transitive components (Balduzzi et al., 2019). A game is transitive if the can be represented by a monotonic rating function f such that performance on the game is the dierence in ratings: (S1S2) = f(S1) f(S2), and it is non transitive if satises meaning that winning against some strategies will be counterbalanced with losses against other strategies in the population.)

# 8 Learning in General-Sum Games

Solving general-sum SGs entails an entirely different level of difficulty than solving team games or zero-sum games. In a static two-player normal-form game, finding the NE is known to be _PPAD_-complete ([Chen and Deng], [2006]).

#### 8.1 Solutions by Mathematical Programming

To solve a two-player general-sum discounted stochastic game with discrete states and discrete actions, [Filar and Vrieze] ([2012]) [Chapter 3.8] formulated the problem as a nonlinear programme; the matrix form is written as follows:

$$
\begin{aligned}
\mathrm{min}_{\mathbf{V},} \begin{aligned}f(\mathbf{V},\boldsymbol{\pi})&=\sum_{i=1}^2\mathbf{1}_{|\mathbb{S}|}^T\left|V^i-\left(\mathbf{R}^i(\boldsymbol{\pi})+\gamma\cdot\mathbf{P}(\pi)V^i\right)\right|\end{aligned} \\
&\text{(a) }\pi^2(s)^T\bigg|\mathbf{R}^1(s)+\gamma\cdot\sum_{s^{\prime}}\mathbf{P}(s'|s)V^1(s')\bigg|\le V^1(s)\mathbf{1}_{|\mathbb{A}^1|}^T,\quad\forall s\in\mathbb{S} \\
\text{s.t.}& \begin{aligned}\text{(b) }\left\lfloor\mathbf{R}^2(s)+\gamma\cdot\sum_{s'}\mathbf{P}(s'|s)V^2(s')\right\rfloor\pi^1(s)\leq V^2(s)\mathbf{1}_{|\mathbb{A}^2|},\quad\forall s\in\mathbb{S}\end{aligned} & (77) \\
&\begin{aligned}(\mathrm{c})\pi^{1}\left(s\right)\geq\mathbf{0},\quad\pi^{1}(s)^{T}\mathbf{1}_{\left|\mathbb{A}^{1}\right|}=1,\quad\forall s\in\mathbb{S}\end{aligned} \\
&\begin{aligned}(\mathrm{d})\pi^2(s)\geq\mathbf{0},\quad\pi^2(s)^T\mathbf{1}_{|\mathbb{A}^2|}=1,\quad\forall s\in\mathbb{S}\end{aligned}
\end{aligned}****
$$$

![[Pasted image 20230801145943.png]]

where
- **V** = _(_V i_ : _i_ = 1_,_ 2_分_ is the vector of agents’ values over all states, _V i_ = _(_V i_(_s_) : _s_ ∈ S_分_is the value vector for the _i_-th agent.
- _π_ = _(_π_i_ : _i_ = 1_,_ 2_分_ and _π_i_ = _(_π_i_(_s_) : _s_ ∈ S_分_, where _π_i_(_s_) = _(_π_i_(_a_|_s_) : _a_ ∈ A_i_分_ is the vector representing the stochastic policy in state _s_ ∈ S for the _i_-th agent.
- **R**_i_(_s_) = [_Ri_ (_s, a_1_, a_2) : _a_1 ∈ A1_,_ _a_2 ∈ A2] is the reward matrix for the _i_th agent in state _s_ ∈ S. The rows correspond to the actions of the second agent, and the columns correspond to those of the first agent. With a slight abuse of notation, we use **R**_i_(**_π_**) = **R**_i_ (_(_π_1_,_ _π_2_分_) = _π_2(_s_)_T_ **R**_i_(_s_)_π_1(_s_) : _s_ ∈ S to represent the expected reward vector over all states under joint policy **_π_**.
- **P**(_s_I_|_s_) = [_P_ (_s_I_|_s,_ **_a_**) : **_a_** = _(_a_1_,_ _a_2_分_,_ _a_1 ∈ A1_,_ _a_2 ∈ A2] is a matrix representing the probability of transitioning from the current state _s_ ∈ S to the next state _s_I_ ∈ S. The rows represent the actions of the second agent, and the columns represent those of the first agent. With a slight abuse of notation, we use **P**(**_π_**) = **P** (_(_π_1_,_ _π_2 _分_ ) = _π_2(_s_)_T_ **P**(_s_I_|_s_)_π_1(_s_) : _s_ ∈ S_, s_I_ ∈ S to represent the expected transition probability over all state pairs under joint policy **_π_**.
This is a nonlinear programme because the inequality constraints in the optimisation problem are quadratic in **V** and **_π_**. The objective function in Eq. ([77]) aims to minimise the TD error for a given policy **_π_** over all states, similar to the policy evaluation step in the traditional policy iteration method, and the constraints of (_a_) and (_b_) in Eq. ([77]) act as the policy improvement step, which satisfies the equation when the optimal value function is achieved. Finally, constraints (_c_) and (_d_) ensure the policy is properly defined. Although the NE is proved to exist in general-sum SGs in the form of stationary strategies, solving Eq. ([77]) in the two-player case is notoriously challenging. First, Eq. ([77]) has a non-convex feasible region; second, only the global optimum[50] of Eq.([77]) corresponds to the NE of SGs, while the common gradient-descent type of methods can only guarantee convergence to a local minimum. Apart from the efforts by [Filar and] [Vrieze] ([2012]), [Breton et al.] ([1986]) [Chapter 4] developed a formulation that has nonlinear objectives but linear constraints. Furthermore, [Dermed and Isbell] ([2009]) formulated the NE solution as multi-objective linear program. [Herings and Peeters] ([2010]); [Herings et al.] ([2004]) proposed an algorithm in which a _homotopic path_ between the equilibrium points of _N_ independent MDPs and the _N_ -player SG is traced numerically. This approach yields a Nash equilibrium point of the stochastic game of interest. However, all these methods are tractable only in small-size SGs with at most tens of states and only two players.

(50Note that in the zero-sum case, every local optimum is global.)

#### 8.2 Solutions by Value-Based Methods

A series of value-based methods have been proposed to address general-sum SGs. A majority of these methods adopt classic Q-learning ([Watkins and Dayan], [1992]) as a centralised controller, with the differences being what solution concept the central Qlearner should apply to guide the agents to converge in each iteration. For example, the Nash-Q learner in Eqs. ([19] & [20]) applies NE as the solution concept, the correlatedQ learner adopts correlated equilibrium ([Greenwald et al.], [2003]), and the friend-or-foe learner considers both cooperative (see Eq. ([35])) and competitive equilibrium (see Eq. ([54])) ([Littman], [2001a]). Although many algorithms come with convergence guarantees, the corresponding assumptions are often overly restrictive to be applicable in general. When Nash-Q learning was first proposed ([Hu et al.], [1998]), it required the NE of the SG be unique such that the convergence property could hold. Though strong, this assumption was still noted by [Bowling] ([2000]) to be insufficient to justify the convergence of the Nash-Q algorithm. Later, [Hu and Wellman] ([2003]) corrected her convergence proof by tightening the assumption even further; the uniqueness of the NE must hold for every single stage game encountered during state transitions. Years later, a strikingly negative result by [Zinkevich et al.] ([2006]) concluded that the entire class of value-iteration methods could be excluded from consideration for computing stationary equilibria, including both NE and correlated equilibrium, in general-sum SGs. Unlike those in single-agent RL, the Q values in the multi-agent case are inherently defective for reconstructing the equilibrium policy.

#### 8.3 Solutions by Two-Timescale Analysis

In addition to the centralised Q-learning approach, decentralised Q-learning algorithms have recently received considerable attention because of their potential for scalability. Although independent learners have been accused of having convergence issues ([Tan], [1993]), decentralised methods have made substantial progress with the help of two-timescale stochastic analysis ([Borkar], [1997]) and its application in RL ([Borkar], [2002]).

Two-timescale stochastic analysis is a set of tools certifying that, in a system with two coupled stochastic processes that evolve at different speeds, if the fast process converges to a unique limit point for any particular fixed value of the slow process, we can, quantitatively, analyse the asymptotic behaviour of the algorithm as if the fast process is always fully calibrated to the current value of the slow process ([Borkar], [1997]). As a direct application, [Leslie et al.] ([2003]); [Leslie and Collins] ([2005]) noted that independent Q-learners with agent-dependent learning rates could break the symmetry that leads to the non-convergent limited cycles; as a result, they can converge almost surely to the NE in two-player collaboration games, two-player zero-sum games, and multi-player matching pennies. Similarly, [Prasad et al.] ([2015]) introduced a two-timescale update rule that ensures the training dynamics reach a stationary local NE in general-sum SGs if the critic learns faster than the actor. Later, [Perkins et al.] ([2015]) proposed a distributed actor-critic algorithm that enjoys provable convergence in solving static potential games with continuous actions. Similarly, [Arslan and Yu¨ksel] ([2016]) developed a two-timescale variant of Q-learning that is guaranteed to converge to an equilibrium in SGs with weakly acyclic characteristics, which generalises potential games. Other applications include developing two-timescale update rules for training GANs ([Heusel et al.], [2017]) and developing a two-timescale algorithm with guaranteed asymptotic convergence to the Stackelberg equilibrium in general-sum Stackelberg games.

#### 8.4 Solutions by Policy-Based Methods

Convergence to NE via direct policy search has been extensively studied; however, early results were limited mainly by stateless two-player two-action games ([Abdallah and] [Lesser], [2008]; [Bowling], [2005]; [Bowling and Veloso], [2002]; [Conitzer and Sandholm], [2007]; [Singh et al.], [2000]; [Zhang and Lesser], [2010]). Recently, GAN training has posed a new challenge, thereby rekindling interest in understanding the policy gradient dynamics of continuous games ([Heusel et al.], [2017]; [Mescheder et al.], [2018], [2017]; [Nagarajan and Kolter], [2017]).

Analysing gradient-based algorithms through dynamic systems ([Shub], [2013]) is a natural approach to yield more significant insights into convergence behaviour. However, a fundamental difference is observed when one attempts to apply the same analysis from the single-agent case to the multi-agent case because the combined dynamics of gradient-based learning schemes in multi-agent games do not necessarily correspond to a proper _gradient flow_ – a critical premise for almost sure convergence to a local minimum. In fact, the difficulty of solving general-sum continuous games is exacerbated by the usage of deep networks with stochastic gradient descent. In this context, a key equilibrium concept of interest is the _local_ $N_E$ ([Ratliff et al.], [2013]) or _differential NE_ ([Ratliff et al.], [2014]), defined as follows.

![[Pasted image 20230801150529.png]]

(51This approach is similar in ideology to the work by Candogan et al. (2011), where they leverage the combinatorial Hodge decomposition to decompose any multi-player normal-form game into a potential game plus a harmonic game. However, their equivalence is an open question.)

A recent result by [Mazumdar and Ratliff] ([2018]) suggested that gradient-based algorithms can almost surely avoid a subset of local NE in general-sum games; even worse, there exist non-Nash stationary points. As a tentative treatment, [Balduzzi et al.] ([2018a]) applied _Helmholtz decomposition_[51] to decompose the game Hessian **H**(**_w_**) into a potential part plus a Hamiltonian part. Based on the decomposition, they designed a gradient-based method to address each part and combined them into _symplectic gradient adjustment (GDA)_, which is able to find all local NE for zero-sum games and a subset of local NE for general-sum games. More recently, [Chasnov et al.] ([2019]) separately considered the cases of 1) agents with oracle access to the exact gradient **_ξ_**(**w**) and 2) agents with only an unbiased estimator for **_ξ_**(**w**). In the first case, they provided asymptotic and finite-time convergence rates for the gradient-based learning process to reach the differential NE. In the second case, they derived concentration bounds guaranteeing with high probability that agents will converge to a neighbourhood of a stable local NE in finite time. In the same framework, [Fiez et al.] ([2019]) studied Stackelberg games in which agents take turns to conduct the gradient update rather than acting simultaneously and established the connection under which the equilibrium points of simultaneous gradient descent are Stackelberg equilibria in zero-sum games. [Mertikopoulos and Zhou] ([2019]) investigated the local convergence of no-regret learning and found local NE is attracting under gradient play if and only if a NE satisfies a property known as _variational stability_. This idea is inspired by the seminal notion of _evolutionary stability_ observed in animal populations ([Smith and Price], [1973]).

Finally, it is worth highlighting that the above theoretical analysis of the performance of gradient-based methods on stateless continuous games cannot be taken for granted in SGs. The main reason is that the assumption on the differentiability of the loss function required in continuous games may not hold in general-sum SGs. As clearly noted by [Fazel et al.] ([2018]); [Mazumdar et al.] ([2019a]); [Zhang et al.] ([2019c]), even in the extreme setting of linear-quadratic games, the value functions are not guaranteed to be globally smooth (w.r.t. each agent’s policy parameter).

# 9. Learning in Games when _N_ _→_ +_∞_

As detailed in Section [4], designing learning algorithms in a multi-agent system with _N_ _»_ 2 is a challenging task. One major reason is that the solution concept, such as Nash equilibrium, is difficult to compute in general due to the curse of dimensionality of the multi-agent problem itself. However, if one considers a continuum of agents with _N_ _→_ +_∞_, then the learning problem becomes surprisingly tractable. The intuition is that one can effectively transform a many-body interaction problem into a two-body interaction problem (i.e., agent vs the population mean) via mean-field approximation.

(52An Ising model is a model used to study magnetic phase transitions under different system temperatures. In a 2D Ising model, one can imagine the magnetic spins are laid out on a lattice, and each spin can have one of two directions, either up or down. When the system temperature is high, the direction of the spins is chaotic, and when the temperature is low, the directions of the spins tend to be aligned. Without the mean-field approximation, computing the probability of the spin direction is a combinatorial hard problem; for example, in a 5 5 2D lattice, there are 225 possible spin configurations. A successful approach to solving the Ising model is to observe the phase change under different temperatures and compare it against the ground truth.)

The idea of mean-field approximation, which considers the behaviour of large numbers of particles where individual particles have a negligible impact on the system, originated from physics. Important applications include solving Ising models[52] ([Kadanoff],[2009]; [Weiss], [1907]), or more recently, understanding the learning dynamics of overparameterised deep neural networks ([Hu et al.], [2019]; [Lu et al.], [2020b]; [Sirignano and] [Spiliopoulos], [2020]; [Song et al.], [2018]). In the game theory and MARL context, mean-field approximation essentially enables one to think of the interactions between every possible permutation of agents as an interaction between each agent itself and the aggregated mean effect of the population of the other agents, such that the _N_ -player game (_N_ _→_ +_∞_) turns into a “two”-player game. Moreover, under _the law of large numbers_ and _the theory of propagation of chaos_ ([G¨artner], [1988]; [McKean], [1967]; [Sznitman], [1991]), the aggregated version of the optimisation problem in Eq. ([80]) asymptotically approximates the original _N_ -player game.

The assumption in the mean-field regime that each agent responds only to the mean effect of the population may appear rather limited initially; however, for many realworld applications, agents often cannot access the information of all other agents but can instead know the global information about the population. For example, in highfrequency trading in finance ([Cardaliaguet and Lehalle], [2018]; [Lehalle and Mouzouni], [2019]), each trader cannot know every other trader’s position in the market, although they have access to the aggregated order book from the exchange. Another example is real-time bidding for online advertisements ([Guo et al.], [2019]; [Iyer et al.], [2014]), in which participants can only observe, for example, the second-best prize that wins the auction but not the individual bids from other participants.

There is a subtlety associated with types of games in which one applies the mean-field theory. If one applies the mean-field type theory in non-cooperative[53] games, in which agents act independently to maximise their own individual reward, and the solution concept is NE, then the scenario is usually referred to as a _mean-field game (MFG)_ ([Gu´eant et al.], [2011]; [Huang et al.], [2006]; [Jovanovic and Rosenthal], [1988]; [Lasry and] [Lions], [2007]). If one applies mean-field theory in cooperative games in which there exists a central controller to control all agents cooperatively to reach some Pareto optima, then the situation is usually referred to as _mean-field control (MFC)_ ([Andersson and] [Djehiche], [2011]; [Bensoussan et al.], [2013]), or _McKean-Vlasov dynamics (MKV)_ control. If one applies the mean-field approximation to solve a standard SG through MARL, specifically, to factorise each agent’s reward function or the joint-Q function, such that they depend only on the agent’s local state and the mean action of others, then it is called _mean-field MARL (MF-MARL)_ ([Subramanian et al.], [2020]; [Yang et al.], [2018b]; [Zhou et al.], [2019]).

![[Pasted image 20230801150933.png]]
**Figure 11:** Relations of mean-field learning algorithms in games with large _N_.

Despite the difference in the applicable game types, technically, the differences among MFG/MFC/MF-MARL can be elaborated from the perspective of the order in which the equilibrium is learned (optimised) and the limit as _N_ _→_ +_∞_ is taken ([Carmona] [et al.], [2013]). MFG learns the equilibrium of the game first and then takes the limit as _N_ _→_ +_∞_, while MFC takes the limit first and optimises the equilibrium later. MFMARL is somewhat in between. The mean-field in MF-MARL refers to the empirical average of the states and/or actions of a _finite_ population; _N_ does not have to reach infinity, though the approximation converges asymptotically to the original game when _N_ is large. This result is in contrast to the mean-field in MFG and MFC, which is essentially a probability distribution of states and/or actions of an _infinite_ population (i.e., the Mckean-Vlasov dynamics). Before providing more details, we summarise the relationships of MFG, MFC, and MF-MARL in Figure [11]. Readers are recommended to revisit their differences after finishing reading the below subsections.

(53Note that the word “non-cooperative” does not mean agents cannot collaborate to complete a task, it means agents cannot collude to form a coalition: they have to behave independently.)

#### 9.1 Non-cooperative Mean-Field Game

MFGs have been widely studied in different domains, including physics, economics, and stochastic control ([Carmona et al.], [2018]; [Gu´eant et al.], [2011]). An intuitive example to quickly illustrate the idea of MFG is the problem of _when_ _do_es_ _the_ _me_eting_ _start_ ([Gu´eant] [et al.], [2011]). For a meeting in the real world, people often schedule a calendar time _t_ in advance, and the actual start time _T_ depends on when the majority of participants (e.g., 90%) arrive. Each participant plans to arrive at $τ_i$, and the actual arrival time, _τ_˜_i_ = $τ_i$ + _σ_i_E_i_, is often influenced by some uncontrolled factors _σ_i_E_i_,_ _E_i_ _∼_ _N_ (0_,_ 1), such as weather or traffic. Assuming all players are rational, they do not want to be later than either _t_ or _T_ ; moreover, they do not want to arrive too early and have to wait. The cost function of each individual can be written as _c_i_(_t,_ _T_,_ _τ_˜_i_) = E _α_l_τ_˜_i_ _−_ _t_J_+ + _β_l_τ_˜_i_ _−_ _T_ _J_+ + _γ_l_T_ _−_ _τ_˜_i_J_+ _,_ where $α_,$ _β,_ _γ_ are constants. The key question to ask is when is the best time for an agent to arrive, as a result, when will the meeting actually start, i.e., what is _T_ ?

The challenge of the above problem lies in the coupled relationship between _T_ and _τi_; that is, in order to compute _T_ , we need to know $τ_i$, which is based on _T_ itself. Therefore, solving the time _T_ is essentially equivalent to finding the fixed point, if it exists, of the stochastic process that generates _T_ . In fact, _T_ can be effectively computed through a two-step iterative process, and we denote as _Γ_ 1 and _Γ_ 2. At _Γ_ 1, given the current[54] value of _T_ , each agent solves their optimal arrival time $τ_i$ by minimising their cost _R_i_(_t,_ _T_,_ _τ_˜_i_). At _Γ_ 2, agents calibrate the new estimate of _T_ based on all _τ_i_ values that were computed in _Γ_ 1. _Γ_ 1 and _Γ_ 2 continue iterating until _T_ converges to a fixed point, i.e., _Γ_ 2 _◦_ _Γ_ 1(_T_ _∗_) = _T_ _∗_. The key insight is that the interaction with other agents is captured simply by the mean-field quantity. Since the meeting starts only when 90% of the people arrive, if one considers a continuum of players with _N_ _→_ +_∞_, _T_ becomes the 90_th_ quantile of a distribution, and each agent can easily find the best response. This result contrasts to the cases of a finite number of players, in which the ordered statistic is intractable, especially when _N_ is large (but still finite).

(54At time step 0, it can be a random guess. Since the fixed point exists, the final convergence result is irrelevant to the initial guess.)

Approximating an _N_ -player SG by letting _N_ _→_ +_∞_ and letting each player choose an optimal strategy in response to the population’s macroscopic information (i.e., the mean field), though analytically friendly, is not cost-free. In fact, MFG makes two major assumptions: 1) the impact of each player’s action on the outcome is infinitesimal,resulting in all agents being identical, interchangeable, and indistinguishable; 2) each player maintains _weak_ _interactions_ with others only through a mean field, denoted by _Li ∈ ∆|_S_||_A_|_, which is essentially a population state-action joint distribution
![[Pasted image 20230801151149.png]]

where $s_j$ and $a_j$ player _j_’s local state[55] and local action. Therefore, for SGs that do not share the homogeneity assumption[56] and weak interaction assumption, MFG is not an effective approximation. Furthermore, since agents have no identity in MFG, one can choose a representative agent (the agent index is thus omitted) and write the formulation[57] of the MFG as
![[Pasted image 20230801151216.png]]

Each agent applies a local policy[58] $π_t$ : S _→_ _∆_(A), which assumes the population state is not observable. Note that both the reward function and the transition dynamics depend on the sequence of the mean-field terms _{_L_t_}_∞_t_=0. From each agent’s perspective, the MDP is time-varying and is determined by all other agents.

The solution concept in MFG is a variant of the (Markov perfect) NE named the meanfield equilibrium, which is a pair of _{_π_t_∗_,_ _L_∗_t__}_t_≥_0 that satisfies two conditions: 1) for fixed _L_∗_ = _{_L_∗_t__}_, _π_∗_ = _{_π_t_∗_}_ is the optimal policy, that is, _V_ (_s,_ _π_∗_,_ _L_∗_) _≥_ _V_ (_s,_ _π, L_∗_)_,_ _∀_π_,_ _s_; 2) _L_∗_ matches with the generated mean field when agents follow _π_∗_. The two-step iteration process in the meeting start-time example applied in MFG is then expressed as _Γ_ 1(_Lt_) = _π_t_∗_ and _Γ_ 2(_Lt, πt∗_) = _Lt_+1, and it terminates when _Γ_ 2 _◦ Γ_ 1(_L_) = _L_ = $L_∗$. Mean-field equilibrium is essentially a fixed point of MFG, its existence for discrete-time[59] discounted MFGs has been verified by [Saldi et al.] ([2018]) in the infinite-population limit _N_ _→_+_∞_ and also in the partially observable setting ([Saldi et al.], [2019]). However, these works consider the case where the mean field in MFG includes only the population state. Recently, [Guo et al.] ([2019]) demonstrated the existence of NE in MFG, taking into account both the population states and actions distributions. In addition, they proved that if _Γ_ 1 and _Γ_ 2 meet _small_ _parameter_ _conditions_ ([Huang et al.], [2006]), then the NE is unique in the sense of _L_∗_. In terms of uniqueness, a common result is based on assuming monotonic cost functions ([Lasry and Lions], [2007]). In general, MFGs admit multiple equilibria ([Nutz] [et al.], [2020]); the reachability of multiple equilibria is studied when the cost functions are anti-monotonic ([Cecchin et al.], [2019]) or quadratic ([Delarue and Tchuendom], [2020]).

Based on the two-step fixed-point iteration in MFGs, various model-free RL algorithms have been proposed for learning the NE. The idea is that in the step _Γ_ 1, one can approximate the optimal $π_t$ given $L_t$ through single-agent RL algorithms[60] such as (deep) Q-learning ([Anahtarcı et al.], [2019]; [Anahtarci et al.], [2020]; [Guo et al.], [2019]), (deep) policygradient methods ([Elie et al.], [2020]; [Guo et al.], [2020]; [Subramanian and Mahajan], [2019]; [uz Zaman et al.], [2020]), and actor-critic methods ([Fu et al.], [2019]; [Yang et al.], [2019b]). Then, in step _Γ_ 2, one can compute the forward _Lt_+1 by sampling the new $π_t$ directly or via fictitious play ([Cardaliaguet and Hadikhanloo], [2017]; [Elie et al.], [2019]; [Hadikhan-] [loo and Silva], [2019]). A surprisingly good result is that the sample complexity of both value-based and policy-based learning methods for MFG in fact shares the same order of magnitude as those of single-agent RL algorithms ([Guo et al.], [2020]). However, one major subtlety of these learning algorithms for MFGs is how to obtain stable samples for _Lt_+1. For example, [Guo et al.] ([2020]) discovered that applying a softmax policy for each agent and projecting the mean-field quantity on an _E_-net with finite cover help to significantly stabilise the forward propagation of _Lt_+1.

(55Note that in mean-field learning in games, the state is not assumed to be global. This is different from Dec-POMDP, in which there exists an observation function that maps the global state to the local observation for each agent.
56In fact, the homogeneity in MFG can be relaxed to allow agents to have (finite) different types ([Lacker and Zariphopoulou], [2019]), though within each type, agents must be homogeneous.
57MFG is more commonly formulated in a continuous-time setting in the domain of optimal control, where it is typically composed by a backward _Hamilton-Jacobi-Bellman equation_ (e.g., the Bellman equation in RL is its discrete-time counterpart) that describes the optimal control problem of an individual agent and a forward _Fokker-Planck equation_ that describes the dynamics of the aggregate distribution (i.e., the mean field) of the population.
58A general non-local policy _π_(_s, L_) : S _∆|_S_||_A_|_ _∆_(A) is also valid for MFG, and it makes the learning easier by assuming _L_ is fully observable.
59The existence of equilibrium in continuous-time MFGs is widely studied in the area of stochastic control ([Cardaliaguet et al.], [2015]; [Carmona and Delarue], [2013]; [Carmona et al.], [2016], [2015b]; [Fischer] [et al.], [2017]; [Huang et al.], [2006]; [Lacker], [2015], [2018]; [Lasry and Lions], [2007]), though it may be of less interest to RL researchers.
60Since agents in MFG are homogeneous, if the representative agent reaches convergence, then the joint policy is the NE. Additionally, given _L_t_, the MDP to the representative agent is stationary.)

#### 9.2 Cooperative Mean-Field Control

MFC maintains the same homogeneity assumption and weak interaction assumption as MFG. However, unlike MFG, in which each agent behaves independently, there is a central controller that coordinates all agents’ behaviours in the context of MFC. In cooperative multi-agent learning, assuming each agent observes only a local state, the central controller maximises the aggregated accumulative reward:
![[Pasted image 20230801151426.png]]

Solving Eq. ([80]) is a combinatorial problem. Clearly, the sample complexity of applying the Q-learning algorithm grows exponentially in _N_ ([Even-Dar and Mansour], [2003]). To avoid the curse of dimensionality in _N_ , MFC ([Carmona et al.], [2018]; [Gu et al.], [2019]) pushes _N_ _→_ +_∞_, and under the law of large numbers and the theory of propagation of chaos ([G¨artner], [1988]; [McKean], [1967]; [Sznitman], [1991]), the optimisation problem in Eq. ([80]), in the view of a representative agent, can be equivalently written as
![[Pasted image 20230801151454.png]]
in which (_µt,_ _αt_) is the respective state and action marginal distribution of the mean-field quantity, _µ_t_(_·_) = lim_N_→_+_∞_ 2_N_1(_si_ = _·_)_/N_ , _αt_(_·_) = 2_s_∈_S_µt_(_s_) _·_ $π_t$ (_s, µt_) (_·_), and _R_˜= lim_N_→_+_∞_ _i__R_i_/_N_._ The MFC approach is attractive not only because the dimension of MFC is independent of _N_ , but also because MFC has shown to approximate the original cooperative game in terms of both game values and optimal strategies ([Lacker], [2017]; [Motte and Pham], [2019]).

Although the MFC formulation in Eq. ([81]) appears similar to the MFG formulation in Eq. ([79]), their underlying physical meaning is fundamentally different. As is illustrated in Figure [11], the difference is which operation is performed first: learning the equilibrium of the _N_ -player game or taking the limit as _N_ _→_ +_∞_. In the fixed-point iteration of MFG, one first assumes $L_t$ is given and then lets the (infinite) number of agents find the best response to $L_t$, while in MFC, one assumes an infinite number of agents to avoid the curse of dimensionality in cooperative MARL and then finds the optimal policy for each agent from a central controller perspective. In addition, compared to mean-field NE in MFG, the solution concept of the central controller in MFC is the Pareto optimum[61], an equilibrium point where no individual can be better off without making others worse off. Finally, other differences between MFG and MFC can be found in [Carmona et al.] ([2013]). In MFC, since the marginal distribution of states serves as an input in the agent’s policy and is no longer assumed to be known in each iteration (in contrast to MFG), the dynamic programming principle no longer holds in MFC due to its non-Markovian nature ([Andersson and Djehiche], [2011]; [Buckdahn et al.], [2011]; [Carmona et al.], [2015a]). That is, MFC problems are inherently time-inconsistent. A counter-example of the failure of standard Q-learning in MFC can be found in [Gu et al.] ([2019]). One solution is to learn MFC by adding common noise to the underlying dynamics such that all existing theory on learning MDP with stochastic dynamics can be applied, such as Q-learning ([Carmona et al.], [2019b]). In the special class of linear-quadratic MFCs, [Carmona et al.] ([2019a]) studied the policy-gradient method and its convergence, and [Luo et al.] ([2019]) explored an actor-critic algorithm. However, this approach of adding common noise still suffers from high sample complexity and weak empirical performance ([Gu et al.], [2019]). Importantly, applying dynamic programming in this setting lacks rigorous verifications, leaving aside the measurability issues and the existence of a stationary optimal policy.

Another way to address the time inconsistency in MFCs is to consider an **enlarged** state-action space ([Djete et al.], [2019]; [Gu et al.], [2019]; [Lauriere and Pironneau], [2014]; [Pham and Wei], [2016], [2017], [2018]). This technique is also called “lift up”, which essentially means to lift up the state space and the action space into their corresponding probability measure spaces in which dynamic programming principles hold. For example, [Gu et al.] ([2019]); [Motte and Pham] ([2019]) proposed to lift the finite state-action space S and A to a compact state-action space embedded in Euclidean space denoted by _C_ := _∆_(S) _× H_ and _H_ := _h_ : S _→_ _∆_(A) , and the optimal Q-function associated with the MFC problem in Eq. ([81]) is
![[Pasted image 20230801151643.png]]

(61The Pareto optimum is a subset of NE.)

The physical meaning of _H_ is the set of all possible local policies _h_ : S _→_ _∆_(A) over all different states. Note that after lift up, the mean-field term $µ_t$ in $π_t$ of Eq. ([81]) no longer exists as an input to _h_. Although the support of each _h_ is _|_∆_(A)_|_|_S_|_, it proves to be the minimum space under which the Bellman equation can hold. The Bellman equation for _Q_C_ : _C_ _→_ R is
![[Pasted image 20230801151810.png]]

where _R_ and _Φ_ are the reward function and transition dynamics written as
![[Pasted image 20230801151828.png]]

with _α_(_µ, h_)(_·_) := _µ_(_s_) _·_ _h_(_s_)(_·_) representing the marginal distribution of the meanfield quantity in action. The optimal value function is _V_ _∗_(_µ_) = max_h_∈H_ _Q_C_ _µ, h_ . Since both _µ_ and _h_ are probability distributions, the difficulty of learning MFC then changes to how to deal with continuous state and continuous action inputs to _Q_C_(_µ,_ _h_), which is still an open research question. [Gu et al.] ([2020]) tried to discretise the lifted space _C_ through _E_-net and then adopted the kernel regression on top of the discretisation; impressively, the sample complexity of the induced Q-learning algorithm is independent of the number of agents _N_ .

#### 9.3 Mean-Field MARL

The scalability issue of multi-agent learning in non-cooperative general-sum games can also be alleviated by applying the mean-field approximation directly to each agent’s Qfunction ([Subramanian et al.], [2020]; [Yang et al.], [2018b]; [Zhou et al.], [2019]). In fact, [Yang] [et al.] ([2018b]) was the first to combine mean-field theory with the MARL algorithm. The idea is to first factorise the Q-function using only the local pairwise interactions between agents (see Eq. ([86])) and then apply the mean-field approximation; specifically, one can write the neighbouring agent’s action _a_k_ as the sum of the mean action _a_¯_j_ and a fluctuation term _δ_a_j_,k_, i.e., _a_k_ = _a_¯_j_ + _δ_a_j_,k_,_ _a_¯_j_ = 1 2_a_k_, in which _N_ (_j_) is the set of neighbouring agents of the learning agent _j_ with its size being $N_j$ = $|_N$ _j_ _|_. With the above two processes, we can reach the mean-field Q-function _Q_j_(_s,_ _a_j_,_ _a_¯_j_) that approximates Qj(s, a) as follows
![[Pasted image 20230801151941.png]]

The second term in Eq. ([88]) is zero by definition, and the third term can be bounded if the Q-function is smooth, and it is neglected on purpose. The mean-field action _a_¯_j_ can be interpreted as the empirical distribution of the actions taken by agent _j_’s neighbours. However, unlike the mean-field quantity in MFG or MFC, this quantity does not have to assume an infinite population of agents, which is more friendly for many real-world tasks, although a large _N_ can reduce the approximation error between _a_k_ and _a_¯_j_ due to the law of large numbers. In addition, the mean-field term in MF-MARL does not include the state distribution, unlike MFG or MFC.

Based on the mean-field Q-function, one can write the Q-learning update as
![[Pasted image 20230801152000.png]]

The mean action _a_¯_j_ depends on _a_j_,_ _j_ ∈ _N_ (_j_), which itself depends on the mean action. The chicken-and-egg problem is essentially the time inconsistency that also occurs in MFC. To avoid coupling between _a_j_ and _a_¯_j_, [Yang et al.] ([2018b]) proposed a filtration such that in each stage game _{_**_Q_**_t_}_, the mean action _a_¯_j_ is computed first using each agents’ current policies, i.e., _a_¯_j_ = 1 2_a_k_,_ _a_k_ _∼_ _π_k_, and then given _a_¯_j_ , each agent finds the best response by
![[Pasted image 20230801152035.png]]

For large _β_, the Boltzmann policy in Eq. ([91]) proves to be a contraction mapping, which means the optimal action _a_j_ is unique given _a_¯_j_ ; therefore, the chicken-and-egg problem is resolved[62].

(62Coincidentally, the techniques of fixing the mean-field term first and adopting the Boltzmann policy for each agent were discovered by [Guo et al.] ([2019]) in learning MFGs at the same time.)

MF-Q can be regarded as a modification of the Nash-Q learning algorithm ([Hu] [and Wellman], [2003]), with the solution concept changed from NE to mean-field NE (see the definition in MFG). As a result, under the same conditions, which include the strong assumption that there exists a unique NE at every stage game encountered, **H**MF**_Q_**(_s,_ **_a_**) = E_s_1_∼_p_ **_R_**(_s,_ **_a_**) + _γ_**_v_**MF (_s_I_) proves to be a contraction operator. Furthermore, the asymptotic convergence of the MF-Q learning update in Eq. ([90]) has also been established.

Considering only pairwise interactions in MF-Q may appear rather limited. However, it has been noted that the pairwise approximation of the agent and its neighbours, while significantly reducing the complexity of the interactions among agents, can still preserve global interactions between any pair of agents ([Blume], [1993]). In fact, such an approach is widely adopted in other machine learning domains, for example, factorisation machines ([Rendle], [2010]) and learning to rank ([Cao et al.], [2007]). Based on MF-Q, [Li et al.] ([2019a]) solved the real-world taxi order dispatching task for Uber China and demonstrated strong empirical performance against humans. [Subramanian and Mahajan] ([2019]) extended MF-Q to include multiple types of agents and applied the method to a large-scale predator-prey simulation scenario. [Ganapathi Subramanian et al.] ([2020]) further relaxed the assumption that agents have access to exact cumulative metrics regarding the mean-field behaviour of the system, and proposed partially observable MF-Q that maintains a distribution to model the uncertainty regarding the mean field of the system.

# 10. Future Directions of Interest

**MARL Theory.** In contrast to the remarkable empirical success of MARL methods, developing theoretical understandings of MARL techniques are very much under-explored in the literature. Although many early works have been conducted on understanding the convergence property and the finite-sample bound of single-agent RL algorithms ([Bert-] [sekas and Tsitsiklis], [1996]), extending those results into multi-agent, even many-agent, settings seem to be non-trivial. Furthermore, it has become a common practice nowadays to use DNNs to represent value functions in RL and multi-agent RL. In fact, many recent remarkable successes of multi-agent RL benefit from the success of deep learning techniques ([Baker et al.], [2019b]; [Pachocki et al.], [2018]; [Vinyals et al.], [2019b]). Therefore, there are pressing needs to develop theories that could explain and offer insights into the effectiveness of deep MARL methods. Overall, I believe there is an approximate ten-year gap between the theoretical developments of single-agent RL and multi-agent RL algorithms. Learning the lessons from single-agent RL theories and extending them into multi-agent settings, especially understanding the incurred difficulty due to involving multiple agents, and then generalising the theoretical results to include DNNs could probably act as a practical road map in developing MARL theories. Along this thread, I recommend the work of [Zhang et al.] ([2019b]) for a comprehensive summary of existing MARL algorithms that come with theoretical convergence guarantee.

**Safe and Robust MARL.** Although RL provides a general framework for optimal decision making, it has to incorporate certain types of constraints when RL models are truly to be deployed in the real-world environment. I believe it is critical to firstly account for MARL with robustness and safety constraints; one direct example is on autonomous driving. At a very high level, robustness refers to the property that an algorithm can generalise and maintain robust performance in settings that are different from the training environment ([Abdullah et al.], [2019]; [Morimoto and Doya], [2005]). And safety refers to the property that an algorithm can only act in a pre-defined safety zone with minimum times of violations even during training time ([Garcıa and Fern´andez], [2015]). In fact, the community is still at the early stage of developing theoretical frameworks to encompass either robust or safe constraint in single-agent settings. In the multi-agent setting, the problem could only become more challenging because the solution now requires to take into account the coupling effect between agents, especially those agents that have conflict interests ([Li et al.], [2019b]). In addition to opponents, one should also consider robustness towards the uncertainty of environmental dynamics ([Zhang et al.], [2020]), which in turn will change the behaviours of opponents and pose a more significant challenge.

**Model-Based MARL.** Most of the algorithms I have introduced in this monograph are _model-free_, in the sense that the RL agent does not need to know how the environment works and it can learn how to behave optimally through purely interacting with the environment. In the classic control domain, _model-based_ approaches have been extensively studied in which the learning agent will first build an explicit state-space “model” to understand how the environment works in terms of state-transition dynamics and reward function, and then learn from the “model”. The benefit of model-based algorithms lies in the fact that they often require much fewer data samples from the environment ([Deisenroth and Rasmussen], [2011]). The MARL community has initially come up with model-based approaches, for example the famous R-MAX algorithm ([Brafman] [and Tennenholtz], [2002]), nearly two decades ago. Surprisingly, the developments along the model-based thread halted ever since. Given the impressive results that model-based approaches have demonstrated on single-agent RL tasks ([Hafner et al.], [2019a],[b]; [Schrit-] [twieser et al.], [2020]), model-based MARL approaches deserves more attention from the community.

**Multi-Agent Meta-RL.** Throughout this monograph, I have introduced many MARL applications; each task needs a bespoke MARL model to solve. A natural question to ask is whether we can use one model that can generalise across multiple tasks. For example, [Terry et al.] ([2020]) has put together almost one hundred MARL tasks, including Atari, robotics, and various kinds of board games and pokers into a Gym API. An ambitious goal is to develop algorithms that can solve all of the tasks in one or a few shots. This requires multi-agent meta-RL techniques. Meta-learning aims to train a generalised model on a variety of learning tasks, such that it can solve new learning tasks with few or without additional training samples. Fortunately, [Finn et al.] ([2017]) has proposed a general meta-learning framework – MAML – that is compatible with any model trained with gradient-descent based methods. Although MAML works well on supervised learning tasks, developing meta-RL algorithms seems to be highly non-trivial ([Rothfuss et al.], [2018]), and introducing the meta-learning framework on top of MARL is even an uncharted territory. I expect multi-agent meta-RL to be a challenging yet fruitful research topic, since making a group of agents master multiple games necessarily requires agents to automatically discover their identities and roles when playing different games; this itself is a hot research idea. Besides, the meta-learner in the outer loop would need to figure out how to compute the gradients with respect to the entire inner-loop subroutine, which must be a MARL algorithm such as multi-agent policy gradient method or meanfield Q-learning, and, this would probably lead to exciting enhancements to the existing meta-learning framework.