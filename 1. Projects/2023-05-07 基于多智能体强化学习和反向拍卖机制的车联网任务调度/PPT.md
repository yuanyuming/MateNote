

今天，我很高兴能与大家分享移动边缘计算系统（MEC）在繁忙城市环境中的应用。在车联网领域，现代车辆不仅需要处理越来越多的数据，还要在有限的计算能力和电池容量下执行复杂或对延迟敏感的任务。这些服务涵盖了语音识别、自然语言处理、计算机视觉、机器学习、增强现实等多个方面。每项服务都有其独特的资源需求和连接限制，这对车联网中的任务调度提出了新的挑战。

通过MEC，车辆可以将任务卸载到边缘服务器，从而有效扩展其资源能力，同时减少核心网络的流量需求。想象一下，我们的车辆装备了车载无线连接单元，使得它们能够与城市中的边缘服务器进行实时通信。这些服务器分布在城市的关键位置，拥有强大的计算能力和稳定的能源供应，为了更好地服务于我们的车辆和用户。

在这个系统中，车辆会评估自身的信道连接状态，以确定最佳的任务卸载点。它们向边缘服务器发送任务请求，而服务器则会根据自身的状态和任务的特征，向车辆返回一个报价。车辆在收到所有报价后，会根据预算做出最合适的任务卸载决策。

然而，在移动边缘计算的世界里，车辆与边缘服务器之间的网络连接可能会不稳定，服务器资源也可能受限。这就提出了一个关键问题：如何高效地进行任务卸载和资源分配？我们的研究主要关注如何根据任务需求和预算选择适当的执行计划，以提高用户满意度、边缘服务器的收入和资源的利用率。

因此，进行合理的任务调度，优化服务质量，保证资源的有效利用，对于我们来说至关重要。我们的研究主要关注如何根据任务需求和预算选择适当的执行计划。我们的目标是将任务从车辆卸载到最合适的节点，以提高用户满意度、边缘服务器的收入和资源的利用率。


---

我将向大家介绍在车联网中任务调度的研究现状

在车联网中任务调度领域，我们面临着一个挑战：如何在信息不完全的情况下，有效地进行任务卸载和资源分配。

传统的数学规划和启发式算法受到了限制，而基于随机优化的分布式方法和深度强化学习技术，正致力于适应实时环境变化和解决多目标优化问题。

在车联网任务卸载问题上，现有的的研究大多局限于通信方向的网络信道分配，主要考虑功耗和信道速率。

同时，在拍卖机制的应用上，现有的研究主要关注车辆行驶过程中产生的数据资源的拍卖，并不涉及到如何高效地分配边缘服务器的资源。

此外，我们还在研究多智能体强化学习模型。在现有的车联网任务卸载问题中，强化学习模型往往只能观察到全局状态，或者部署相同的模型，这并不符合真正的多智能体系统的要求。我们的目标是开发出能够在这样的系统中有效工作的模型。

我们的研究通过优化任务调度来实现长期负载均衡和多机问题的解决

---


我们的目标是实现一种**新颖的任务调度方法**。这种方法结合了多智能体强化学习与反向拍卖，能够动态地匹配资源，并通过激励机制来优化性能和成本。

我们采用了自主学习策略，使车辆能够与边缘服务器间高效地匹配资源，适应环境变化，从而提升任务执行的效率。

在资源分配中，通过反向拍卖机制，确保了资源分配的公平性和效率，促进了边缘服务器之间的协同工作。

实现系统在复杂环境中卓越的适应性与稳健性。能够在不确定性中做出准确的任务调度决策，增强了系统的长期稳定性。

最后，我们通过仿真实验验证了这种方法的有效性和优越性。

---


首先，我向大家介绍**反向拍卖**。在拍卖过程中，买方不是设置价格，而是发布招标公告，邀请供应商来竞标。这是一个新颖的拍卖方式，因为在反向拍卖中，是供应商来确定价格。

当我们需要采购一些比较标准化的商品或服务，比如原材料、资源或劳动力。买方会向卖方明确规定商品或服务的规格和要求，然后供应商会提交他们的报价，包括价格和交货条件。买方会评估这些报价，选择最合适的获胜者，并与之签订合同。

这样的采购方式可以帮助买方**降低成本**,**提高采购效率**和**促进供应商之间的竞争**，从而推动市场的健康发展。


---


接着，我将向大家介绍强化学习——一种机器学习范式，它让智能体通过与环境的互动来学习如何做出最佳决策。在我们的研究中，我们利用深度强化学习来训练智能体，使其能够在车联网环境中有效地做出决策。

**强化学习**的核心在于智能体能够观察环境状态，执行动作，并从环境中获得奖励。这些奖励信号帮助智能体学习如何在不同的状态下采取最佳动作，以最大化长期回报。

**深度强化学习**，这是一种结合了深度学习技术的强化学习方法。我们使用神经网络来近似值函数或策略函数，使智能体能够处理更复杂的问题。

---

将强化学习扩展到多个智能体在共享环境中相互作用和学习就是**多智能体强化学习**。主要的难点在于如何控制智能体之间的竞争与合作。在本次研究中, 我们实现了独立学习和集中式训练分布式执行两种范式，使智能体能够在车联网任务调度中更灵活和适应性地做出决策。




---



接下来，我将向大家展示我们基于Python的强化学习仿真环境的设计, 来验证我们的车联网任务调度方法。这个仿真环境模拟了车辆任务调度的整个过程，并提供了一系列评估指标来衡量方法的有效性。

在这个环境中，车辆、服务器和环境生成器协同工作，模拟了任务卸载的动态过程。

车辆在这个环境中扮演任务发起者的角色，它们根据自身的资源需求、连接限制和预算，选择合适的服务器进行任务卸载。服务器则根据自己的资源状况和成本，对任务卸载请求进行报价，并执行被接受的任务。环境生成器负责生成任务和服务器的参数，为用户提供了一个丰富的参数设置，以探索不同的任务卸载场景和策略。

我们的仿真环境使用顺序回合制环境的接口，能够处理任何多智能体强化学习算法。每个智能体代表一个边缘服务器，它们接收车辆的任务卸载请求，并利用自身资源完成任务卸载，从而获取收益。



---


今天，我要向大家介绍我们在车联网中设计的一种新颖的任务调度方法。在这个方法中，我们将车辆视为买方，它们根据自身的需求发起任务卸载请求。

在特定时刻，车辆会评估自己的连接状态，并向那些满足连接限制的边缘服务器发起任务的卸载请求。它们将任务属性发送到满足连接约束的服务器集合中，以请求报价。

而卖方，在我们的系统中，指的是车联网中的边缘服务器。这些服务器接收来自车辆的任务卸载请求，并利用其资源来完成任务，从而获得收益。每个服务器的特征被定义为一对值，其中资源向量和成本描述了服务器的能力和运营成本。

当边缘服务器接收到报价请求后，它们会决定是否接受任务卸载。如果接受，服务器将根据当前时刻的可用资源和任务特征，通过强化学习策略来对任务进行报价。

我们的算法，是基于反向拍卖的车联网任务调度算法的核心。它不仅优化了资源的使用，还确保了任务调度的效率和公平性。

通过这种方法，我们能够在车联网中实现更智能、更高效的资源分配和任务调度。我们相信，这将为未来的智能交通系统带来革命性的变化。

---


我们利用**PPO算法**来训练部署在边缘服务器上的策略网络。这种算法通过限制策略的变化幅度，提高了学习过程的稳定性和准确性。我们的模型结构包括长短期记忆层和全连接层，以更好地处理时间序列数据。

向大家介绍两种强化学习算法：**IPPO（独立PPO）** 和**MAPPO**。

首先，让我们来看看**IPPO**，即独立的Proximal Policy Optimization。它在多智能体环境中对每个智能体独立地应用PPO算法。每个智能体都有自己的观察和动作空间，并且它们的策略是独立更新的。这意味着每个智能体都在尝试优化自己的目标，而不考虑其他智能体的策略或行为。这种方法的优点是简单易行，因为它不需要智能体之间的通信或协调。

接下来是**MAPPO**，即Multi-Agent PPO。与IPPO不同，MAPPO是一种集中式训练但分布式执行的算法。在MAPPO中，尽管每个智能体在执行时仍然独立行动，但在训练阶段，所有智能体的信息会被集中起来，以学习一个共享的策略。这种方法允许智能体在训练过程中考虑其他智能体的行为，从而可能学习到更复杂的协作或竞争策略。

那么，IPPO和MAPPO之间的主要区别是什么？简而言之，**IPPO强调独立性**，每个智能体都在自己的世界里学习，而**MAPPO强调协作**，智能体在训练时共享信息，学习如何在一个多智能体的环境中共存和互动。


--




我们使用**RLlib**这个强大的库来实现和管理训练过程。RLlib能够并行地从环境中采集数据，将这些数据汇总到一个训练批次中，然后根据选定的算法和损失函数计算梯度，并更新模型参数。这个过程会一直重复，直到达到我们设定的终止条件，比如最大迭代次数、最大训练时间或者最佳性能。

在训练过程中，我们的系统会不断地收集数据，这些数据包括观察到的状态、采取的动作、获得的奖励，以及所谓的结束标志，这些都是智能体在模拟环境中经历的不同回合的记录。每当PPO智能体为一个任务报价时，它都会得到一个即时奖励，而下一个状态也会依赖于前一个状态，因为CPU和内存的可用性会在每次任务卸载后更新。最终，我们期望PPO智能体能够学习到任务的需求和优先级，并为所有任务报价，以最大化自身的收入和车辆的效用。

RLlib的工作流程是这样的：首先创建一组rollout workers，这些workers并行地从环境中采集数据，并使用一个或多个策略与环境交互。然后，将所有workers生成的数据汇总到一个训练批次中，接着根据选择的算法和损失函数计算梯度，并更新模型参数。这个过程会不断重复，直到满足我们的终止条件。


---

为了更真实地模拟车联网场景，我们构建了一个包含多辆车和多个边缘服务器的公路仿真环境。车辆会评估服务器信道，并向符合条件的边缘服务器发出任务卸载请求。任务请求的数量遵循泊松分布，其中λ代表单位时间内的任务到达率。任务类型包括存储密集型、计算密集型和存储计算密集型，时长分为长任务和短任务。

我们的任务生成机制考虑了任务数量、资源需求和可卸载的服务器列表。任务数量在每个时间步内服从泊松分布，而资源需求则采用双峰分布，以更准确地模拟现实世界中的任务特征。

通过这个仿真环境，我们能够验证基于多智能体强化学习和反向拍卖机制的车联网任务调度方法的有效性。我们相信，这种方法将为车联网的发展带来新的视角和可能性。

在实验中，我们选择了Azure的专用主机作为边缘服务器的代表，提供了不同规格和价格的虚拟机选项。我们选取了三种不同规格的专用主机SKU，分别对应于大型、中型和小型的边缘服务器，以适应不同的任务需求。

此外，我们采用了两种不同的强化学习智能体模型：**PPO Agent**和**PPO LSTM Agent**。这两种模型都基于PPO算法，但在网络结构上有所不同。PPO Agent使用了多层感知机（MLP），而PPO LSTM Agent则在此基础上增加了LSTM层，以增强模型处理时间序列数据的能力。

---

我们考虑了四种不同的报价策略，它们分别是：

- **Fixed策略**：这是一种基础策略，服务器根据市场价格，结合任务的资源需求和持续时间来确定报价。
- **Random策略**：在这个策略中，服务器仍以市场价格为基准，但会在一定范围内随机选择一个系数来进行报价，增加了报价的不确定性。
- **PPO策略**：这是一种更先进的策略，服务器使用强化学习算法PPO，根据自身的资源状况和任务特征，通过多层感知机（MLP）来动态地确定报价。
- **PPO-LSTM策略**：这个策略在PPO的基础上进一步发展，服务器在多层感知机前增加了长短期记忆网络（LSTM）层，这使得服务器能够捕获时间序列信息，并结合MLP进行更精准的动态报价。


---

为了全面评估这些策略，我们从多个维度进行了考量，具体包括：

- **负载均衡（Load Balance）**：这一指标衡量的是所有服务器之间资源利用率的一致性。我们通过计算资源占用量与资源总量之比的标准差来评估负载均衡。负载均衡的值越低，意味着服务器资源的利用更加均匀。
- **服务器收益（Server Earning）**：这一指标反映了边缘服务器在任务调度过程中获得的总收益。我们通过计算服务器在整个时间步内的总收入减去其维护成本来评估服务器的收益。
- **车辆的效用（Vehicle Utility）**：这一指标表示车辆在任务调度过程中获得的总效用。我们通过计算车辆在每个时间步内支付的费用与其对任务的预算之间的差额来评估车辆的效用。
- **任务的完成率（Complete Rate）**：这一指标衡量的是所有提交的任务中成功完成的比例。我们通过计算在规定时间内完成并成功卸载的任务数量与总提交任务数的比值来评估任务的完成率。

---


我们评估了**PPO**和**PPO-LSTM**——它们在训练多个智能体以最大化服务器收益方面的收敛性能。

在我们的实验中，对这两种智能体进行了长达448,000次迭代的训练，并在每40,000次迭代后进行了策略评估。我们计算了这些策略在10次测试运行中的平均奖励，以此来衡量它们的表现。

我们可以看到，虽然**PPO-LSTM策略**的收敛速度稍慢于**PPO策略**，但它在最终收敛后获得的奖励更高，且在训练过程的后期波动更小。这表明PPO-LSTM策略在长期训练中更为稳定，能够更好地捕捉时序信息，做出更合理的决策。

具体来说，PPO-LSTM策略的平均奖励值为1.72×107，比PPO策略的1.68×107高出约2.4%。更重要的是，PPO-LSTM策略的奖励值波动更小，其标准差为1.12×105，比PPO策略的2.65×105低出约57.7%。


---


我们通过仿真实验与其他基准策略进行了对比分析。我们考察了任务到达数目为10、20和40的三种不同场景，并利用五个随机种子进行了十次独立的仿真实验。我们从负载均衡、服务器收益、车辆效用和任务完成率这四个维度对策略进行了评估。

我们的实验结果显示，PPO-LTSM策略在多个指标上表现优异。例如，在任务到达数为20的场景下，PPO-LTSM策略在负载均衡、服务器收益、车辆效用和任务完成率方面的表现均超过其他策略。这表明PPO-LTSM策略能够在不同的任务负载条件下保持良好的性能。

在横向比较中，我们发现PPO-LTSM策略在长期报酬方面表现最优，而Fixed策略虽然在服务器收益上表现良好，但在其他指标上表现较差，这可能影响用户满意度。Random策略在所有指标上几乎是最差的，而PPO策略虽然在服务器收益上略逊于Fixed策略，但在其他指标上表现更好。

纵向比较显示，随着任务到达数的增加，服务器的收益和利用率普遍上升，但任务完成率却呈下降趋势。这可能是因为服务器过载而无法接收更多任务。在任务到达数为10的情况下，由于任务数相对较少，各策略在负载均衡和任务完成率上表现相似。当任务到达数增至20时，PPO-LSTM策略在各项指标上均超越其他策略，显示出更好的适应性和灵活性。在任务到达数为40的大规模任务情境下，PPO-LSTM策略继续保持领先，实现了更好的负载均衡、车辆效用和任务完成率。

总体而言，PPO-LTSM策略在不同任务到达数下展现了更好的适应性和性能。它通过学习最大化长期报酬，更好地平衡系统各项指标，为不同任务到达数下的系统性能提供均衡的优化。我们的研究表明，PPO-LTSM策略能够利用LSTM捕获时序信息，做出更合理的决策，实现最有效的负载均衡。

---


接下来,我们将探讨两种强化学习策略——**MAPPO**和**PPO-LSTM**在训练中的收敛性能。

在我们的实验中，我们进行了240,000次迭代的训练，并对两种策略的奖励进行了评估。我们选择服务器的总体收益作为奖励函数，这直观地反映了任务调度的效果。结果表明，**MAPPO策略**的收敛速度更快，奖励也更高。这体现了引入的全局价值函数在稳定强化学习训练过程中的重要作用。

进一步的数值分析显示，MAPPO策略在训练过程中最后10%的迭代中，平均奖励值达到了1.81×107，比PPO-LSTM策略的1.72×107高出约5.0%。此外，MAPPO策略的奖励波动更小，其标准差仅为6.2×104，比PPO-LSTM策略的1.12×105低约44.6%，显示出更好的稳定性。

这些结果说明，MAPPO策略通过全局价值函数的引导，使得边缘服务器的智能体能够在训练过程中学习到其他智能体的信息和环境的信息，从而取得更好的长期收益。

---


在我们的实验中，MAPPO策略在负载均衡、服务器收益、车辆效用和任务完成率等关键性能指标上，相较于PPO-LSTM策略表现出色。以任务到达数为20为例，MAPPO策略的表现在所有对比策略中最为突出，其负载均衡为0.21±0.005，服务器收益为3.09±0.00008，车辆效用为1.66±0.033，任务完成率为0.94±0.009。这些结果表明，MAPPO策略不仅提供了更高的奖励，而且具有更低的波动性，从而保证了更好的稳定性。

随着任务到达数的增加，我们观察到服务器的收益和利用率普遍上升，但任务完成率却呈现下降趋势。这可能是由于服务器过载导致无法接收更多任务。在任务到达数为10的情况下，MAPPO策略在车辆效用和任务完成率方面展现出更好的适应性和灵活性。当任务到达数增至20时，MAPPO策略成为所有策略中的佼佼者。而在任务到达数为40的大规模任务情境下，MAPPO策略继续保持领先，实现了更好的负载均衡、车辆效用和任务完成率。

总体来说，无论任务到达数的多少，MAPPO策略都展现了出色的适应性和性能，相较于之前的PPO-LTSM策略，它表现得更加优越。我们的研究表明，MAPPO策略通过引入全局价值函数，能够观测到全局信息，包括其他智能体的信息和环境的信息，从而在训练过程中实现更稳定的学习，达到更好的长期收益。

---

**总结：** 我们的研究成功地提出了一种结合多智能体强化学习和反向拍卖机制的方法，这一方法显著提高了车联网中任务的完成率和车辆的效用。
•提出了基于多智能体强化学习和反向拍卖的车联网任务调度方法，提高了任务完成率和车辆效用。
•通过对比实验，选择了PPO-LSTM算法和MAPPO算法，在学习和决策质量上表现出色。
•实验结果显示，MAPPO算法在车联网模拟环境中具有明显优势。

**展望：** 尽管我们的研究取得了进展，但仍有改进的空间。

•未来的工作将重点解决通信可靠性、信道增益不确定性以及参与者合作性等问题，以提高任务调度的可靠性、高效性和公平性。
•我们将继续深入探索和优化PPO-LSTM算法和MAPPO算法，以进一步提升系统性能，并推动车联网任务调度方法的深入研究和应用。

我们期望通过本次为车联网任务调度领域的发展做出一定的贡献。


