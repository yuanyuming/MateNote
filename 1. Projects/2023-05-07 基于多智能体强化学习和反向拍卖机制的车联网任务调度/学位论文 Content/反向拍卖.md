---
UID: 20240224122341 
tags: 
source: 
cssclass: 
created: "2024-02-24 12:23"
updated: "2024-02-24 15:22"
---
### 基于反向拍卖的车联网任务卸载方法

任务卸载涉及到多个参与者之间的资源分配问题，即如何将车辆的任务分配给合适的边缘服务器，并使得各方都能获得最大化的效用。为了解决这一问题，本文借鉴了经济学中的反向拍卖（Reverse Auction）理论，提出了一种基于反向拍卖的任务卸载方法（Reverse Auction-based Task Offloading, RATO）。反向拍卖是一种一买方多卖方的拍卖形式，适用于买方市场（Buyer’s Market）的交易活动。在反向拍卖中，买方提出自己的需求和预算，卖方根据自身的成本和利润，竞争性地给出自己的报价。最终，买方选择一个或多个报价最低且满足需求的卖方进行交易。反向拍卖可以有效降低买方的采购成本，提高卖方之间的竞争力，实现资源的优化分配。

本文将分别介绍本模型中反向拍卖的角色和过程。

**买方**是车联网中有任务卸载需求，并愿意为其需求支付的车辆所产生的一次请求$\mathbf b_j$。在某一时刻 $t$，车辆自身评估连接状态并向满足其连接限制的边缘服务器发起任务 $\theta_j$的卸载请求, 并将其任务属性 $\mathbf{f}_j$ 发送到满足连接约束的服务器集合$\mathcal S_j$中以请求报价。

**卖方**是车联网中的边缘服务器。边缘服务器可以接收车辆的任务卸载请求，并利用其自身资源来完成任务卸载和获得收益。卖家服务器 $\mathbf s_i$ 的特征可以被定义为 $\mathbf{s}_i = (\mathbf{r}_i, c_i)$，其中资源向量 $\mathbf{r}_i$ 和成本 $c_i$。服务器在接收到报价请求后, 确定能否接受任务卸载, 如果可以接受则根据当前时刻对可用资源$\mathbf r_{s_i,t}$和任务特征$\mathbf f_j$的观测, 通过强化学习学习到的策略, 对任务进行报价.

为简化问题,本文做出以下基本假设：

- 假设时间是分时的，任务只能在一个时间段提交，并且任务时长为整数个时间段。
- 车辆可以根据自身状态，评估在拍卖和卸载的时间内的连接情况，得到任务的边缘服务器限制条件。
- 在卸载市场中存在信息缺失：
  - 卖方的特征和维护成本对买方不可见。增加拍卖的竞争性，提高买方的效用。
  - 没有卖方知道其他参与者的出价，也没有卖方相互勾结。
  - 买方的预算不能公开给卖方。
- 拍卖过程中，一次仅有一个任务参与，在任务拍卖结束后，进入本时隙的下一个任务的拍卖。防止卖方串通抬价。
- 一个买方只能把他的任务卸载给一个卖方，但一个卖方可以同时为多个买方提供服务。
- 任务卸载的结果对于车辆而言，其效用是一致的。

在基于云计算的车联网场景中，我们考虑了 $m$ 个卖家 $\mathcal S$ 和 $n$ 个买家 $\mathcal B$ 的情况。每个买家代表一个车辆，它通过自身的通信单元与边缘服务器进行通信，并且根据与边缘服务器的通信状况生成相应的限制条件，以便在需要进行任务卸载时选择合适的卖家。每个卖家代表一个边缘服务器，它可以通过有线链接与其他边缘服务器和互联网相连，以返回卸载结果。同时，边缘服务器也可以通过有线连接返回任务执行结果和传输与车辆之间的数据。算法1是反向拍卖过程的具体描述.

算法1：基于反向拍卖的任务卸载方法

输入：车辆集合$\mathcal B$，服务器集合$\mathcal S$，时间槽长度$\epsilon$

输出：任务卸载结果$\mathcal R$

1. 初始化：对于每个服务器$\mathbf s_i \in \mathcal S$，初始化其资源向量$\mathbf r_i$，成本$c_i$，报价策略$\pi_i$；初始化卸载任务集合$\mathcal U=\{\mathcal U_1,\dots,\mathcal U_T\}$。
2. 对于每个时间槽$t=1,2,\dots,T$，执行以下步骤：
   1. 对于服务器集群内的每一个服务器$\mathbf s_i \in \mathcal S$, 更新自己的资源使用情况和结算上一个时间槽的收益$p_{t-1,i}-c_i$
   2. 从$\mathcal U_t$中的按照时间顺序选择买家车辆请求$\mathbf b_j$，则执行以下步骤：
      1. 车辆评估自身的连接状态，并生成任务的服务器限制向量$\mathbf \sigma_j$。
      2. 车辆向满足限制条件的服务器集合$\mathcal S_j = \{\mathbf s_i | \forall \mathbf s_i \in \mathcal S, \sigma_{j,i} = 1\}$发送任务卸载请求$\mathbf f_j$。
      3. 对于每个服务器$\mathbf s_i \in \mathcal S_j$，如果其可用资源$r_{s_i}(t)$能够满足任务的资源需求$r_{\theta_j}(t)$，则执行以下步骤：
         1. 服务器根据自身在$[t,t+\epsilon]$时间段的资源使用情况$\mathbf r_{s_i,t}$和任务特征$\mathbf f_j$，以及报价策略$\pi_i$，计算出自己对任务的报价$q_{j,i}$。
         2. 服务器将报价$q_{j,i}$返回给车辆。
      4. 车辆收集所有服务器的报价$\mathcal Q_j = \{q_{j,i} | \forall \mathbf s_i \in \mathcal S_j\}$，并将其非降序排列。
      5. 车辆选择报价最低的服务器作为胜者$\mathbf s_k =\mathop{\arg\min}\limits_{\mathbf s_i}\ q_{j,i}$，并检查其报价$p_j = q_{j,k}$是否满足预算条件$p_j \leq \psi_j$。如果不满足，则本次拍卖失败，任务卸载失败；如果满足，则本次拍卖成功，任务卸载成功，并执行以下步骤：
         1. 车辆将任务发送到胜出者服务器$\mathbf s_k$并支付$p_j$，等待执行结果返回。
         2. 服务器$\mathbf s_k$获得收益, 为任务分配资源并执行任务卸载，更新自身的资源使用情况$\mathbf r_{s_i,t}$
      6. 更新任务卸载结果$\mathcal R$
3. 当到达总时间步$T$时间后, 卸载结束

### PPO算法

在上文中我们已经介绍了MDP建模和环境建模，本节我们将介绍一种基于策略梯度的强化学习算法——PPO算法[1]。PPO算法的目标是在与环境交互采样数据后，使用随机梯度上升优化一个“替代”目标函数，从而改进策略。PPO算法的核心思想是利用按照策略产生的执行轨迹来估计梯度，并使用经验回报或优势函数作为梯度的方向。PPO算法可以处理具有连续状态和动作空间的强化学习环境。

PPO算法的目标函数是由优势值进行缩放的新策略和旧策略之间的比率，即：

$$
J_{\theta}=E_{t}\left[\frac{\pi_{\theta}(a_{t}\mid s_{t})}{\pi_{{\theta_{\mathrm{old}}}}(a_{_t}\mid s_{t})}A^{\mathrm{GAE}(\gamma, \lambda)}_{t}\right]
$$

其中$\pi_{{\theta_{\mathrm{old}}}}$表示更新前的旧策略。这个目标与交叉熵方法有相同的原理：重要性采样。为了限制更新，PPO算法使用了裁剪后的目标：

$$
J_{\theta}^{\mathrm{clip}}=\mathbb{E}_{t}\left[\min(r_{t}(\theta)A^{\mathrm{GAE}(\gamma, \lambda)}_{t},\mathrm{clip}(r_{t}(\theta),1-\varepsilon,1+\varepsilon)A^{\mathrm{GAE}(\gamma, \lambda)}_{t})\right]
$$

此目标将新旧策略之间的比率限制在$[1–ε, 1+ε]$区间内，因此通过更改$ε$，我们可以限制更新的大小。这种裁剪方法可以避免使用KL散度惩罚或自适应更新，从而简化了算法。

PPO算法有两种主流的实现方式，一种是使用KL散度作为惩罚项来限制策略更新的幅度，另一种是使用剪裁函数来限制新旧策略之间的比率。PPO算法还使用了一种更一般的优势函数估计方式，即GAE（Generalized Advantage Estimation）。GAE是由John Schulman等人在2015年的论文[2]中提出的。GAE的基本思想是利用TD($\lambda$)方法来平衡偏差和方差，同时减少优势函数的方差。GAE的定义如下：

$$
A^{\mathrm{GAE}(\gamma, \lambda)}_{t}=\sum_{l=0}^{\infty}(\gamma \lambda)^{l} \delta_{t+l}
$$

其中$\delta_t$是TD误差，即$\delta_t = R_t + \gamma V_\theta(S_{t+1}) - V_\theta(S_t)$。可以看出，GAE是对TD误差的加权累加，其中权重随着$l$的增加而指数衰减。当$\lambda=0$时，GAE退化为一步TD误差；当$\lambda=1$时，GAE退化为MC估计。因此，通过调节$\lambda$的值，我们可以在偏差和方差之间找到一个平衡点。GAE还有一个重要的性质是，它可以消除优势函数的常数偏移量。


PPO算法从环境中获取较长的样本序列，然后用于执行多个训练周期。PPO算法在实现简易性、样本复杂度和调整简易性之间取得了平衡，在每一步都试图计算一个最小化代价函数的更新，同时保证与前一策略的偏离相对较小。PPO算法在各种强化学习任务上表现出了优异的性能。

为了解决第三章提出的车联网环境中的任务卸载问题，我们采用以上描述的PPO算法。这种算法可以根据任务的需求和优先级，直接学习最优策略，而不需要像Q-Learning等基于值函数的算法那样估计状态-动作值。

我们使用多智能体强化学习的框架，让每个边缘服务器都部署一个PPO代理，根据自己的状态和动作来更新自己的策略。这样，不同的服务器之间的策略可以根据任务需求和市场竞争进行自适应调整，从而提高自身的收入和车辆的效用。同时，多智能体强化学习也可以处理多个智能体之间的交互和博弈，使得每个服务器能够考虑其他服务器的行为和影响。为了简化模型，我们假设服务器之间没有额外的通信，而且只进行一次博弈。

接下来，我们将介绍我们的强化学习仿真环境的具体设计, 展示我们的实验结果和分析，比较PPO算法与其他基准算法的性能和效率。



[1]: [Proximal Policy Optimization Algorithms]
[2]: [High-Dimensional Continuous Control Using Generalized Advantage Estimation]

- for iteration=1,2,... do
  - for actor=1,2,... do
    - 在环境中运行策略 $\pi_{\theta_{old}}$ , 进行 $T$ 个时间步
    - 计算优势估计 $\hat{A}_1, \dots, \hat{A}_T$
  - end for
  - 优化代理 $L$ 关于 $\theta$, 通过 $K$ epochs 和 minibatch size $M\le NT$
  - $\theta_{old}\gets \theta$
- end for

### 环境建模

为了实现本论文中多智能体强化学习的环境建模, 我们使用了Pettingzoo最为我们实现自定义环境的接口. Pettingzoo是一个用于表示一般的多智能体强化学习（MARL）问题的Python库，它包括了多种参考环境、有用的工具和创建自定义环境的方法。Pettingzoo支持Agent Environment Cycle（ACE）模式，它是一种适用于顺序的回合制环境的接口，能够处理任何MARL可以考虑的游戏。在ACE模式下，每个智能体都有自己的观察空间和动作空间，而且只有一个智能体可以在每个时间步骤中执行动作。用户可以使用以下方法来与ACE模式的环境进行交互：每一个step开始时使用env.agent_iter()的到当前需要执行动作的agent, 通过env.last()返回最后一个智能体的观察、奖励、终止、截断和信息, 利用返回的信息生成action, 通过env.step(action)执行一个动作并更新环境状态,直到所有的智能体都完成了他们的回合或者游戏结束。

为了利用多智能体强化学习求解这个优化问题，我们建立了如下多智能体强化学习模型：

1. 初始化任务生成模型参数，边缘服务器集群 S。
2. 对于每个时间槽 t，执行以下操作, 直到满足停止条件：
   1. 使用任务生成模型生成在本时间槽中需要卸载的所有任务$T_t$.
   2. 集群中每一个服务器更新资源使用情况, 向车辆返回结束的任务的执行结果, 结算执行完毕的任务获得奖励和维护费用.
   3. 从$T_t$中的选择一个没有参加过拍卖任务$task_i$, 执行如下操作, 直至全部任务都进行拍卖:
      1. 将$task_i$相关信息发送到满足限制的服务器集合$a_i$.
      2. 对于每一个在$a_i$中的边缘服务器$S_j$,执行如下操作,直到所有服务器完成报价.
         1. $S_j$收到任务请求后，根据自己的观测，得到动作, 将报价返回拍卖商.
      3. 拍卖商收到所有服务器的报价后，执行反向拍卖过程，并向车辆通知卸载结果和支付费用。
      4. 车辆收到拍卖结果，检查是否满足其预算要求
         1. 如果满足预算需求, 则同意卸载，并在任务结束后进行支付
            1. 将车辆任务发送到边缘服务器
            2. 边缘服务为任务预留相关资源并开始执行, 更新资源的使用情况.
         2. 如果不满足，则不进行任务卸载,不进行支付.
   4. 当$T_t$中所有任务都进行拍卖后, 进入下一个时间槽$t+1$
3. 输出最终的环境状态，并结束环境。

#### 任务生成模型

完成对任务的建模后, 为模拟真实的任务调度场景, 接下来需要对任务的生成进行建模. 任务的生成一共由三部分组成, 一个时间步内任务的数量, 每个任务对资源的需求, 任务可卸载的服务器列表.

- 一个时间步的任务数量, 本文将其简化为符合泊松分布的随机变量, 每一回合从分布中抽取一个正整数作为该时间步的任务数量.
- 每个任务对资源的需求, 我们将任务的资源需求和执行时间都取自由两个正态分布组合而成的双峰分布, 因为通过分析现实的计算机集群工作负载现实世界中卸载的任务以小任务或大任务居多, 如果以简单的正态分布或者指数分布(负指数分布)可能无法准确建模, 另外选择这样的分布也是为了检验神经网络能否拟合复杂分布.
- 任务可卸载的服务器列表, 由于车联网中的车辆存在动态性, 位置存在随机性, 因此我们将车辆与边缘服务器之间的连接抽象成一个可连接服务器的集合. 由于服务器的位置是固定的, 因此其可连接的车辆范围也是固定的. 为在模型中体现车联网的连接性质, 在生成执行服务器限制时, 我们首先生成一个连接范围限制, 然后从连接范围限制的服务器集合中随机抽取若干服务器作为服务器限制集合. 这时的任务可卸载约束仅有连接所限制,属于服务器的无线设备自发无目的的向其范围内车辆发送的连接检测过程,这时服务器仍然未知任务的具体特征, 因此执行约束不在任务生成模型的建模之中.

因此本次研究系统模型中的任务生成过程如下.

- 设定一个时间步的长度为 $\Delta t$，比如 $\Delta t=1$ 秒或者 $\Delta t=0.1$ 秒；
- 在每个时间步 $n$ 内，根据泊松分布 $Poisson(\lambda)$ 生成一个正整数 $m$，表示这个时间步内有 $m$ 个任务被车辆提交到边缘服务器，其中 $\lambda$ 是泊松分布的参数，表示单位时间内任务到达的平均速率；
- 对于每个任务 $task_i$，根据双峰分布 $Bimodal(\mu_1,\sigma_1,\mu_2,\sigma_2,w)$ 生成它的资源需求和执行时间，表示这个任务需要多少计算资源和存储资源以及执行多长时间，其中 $\mu_1,\sigma_1,\mu_2,\sigma_2$ 是两个正态分布的均值和标准差，$w$ 是两个正态分布的权重；
- 根据车辆的位置和边缘服务器的位置和连接情况，生成一个可连接服务器的集合 $a_i$，表示这个任务可以被发送到哪些边缘服务器；
- 根据车辆的偏好或者其他因素，生成一个预算 $b_i$，表示这个任务的最高支付费用；
- 这样，我们就得到了一个完整的任务 $task_i=(n_i,p_i,r_i,d_i,t_i,a_i,b_i,f_i)$；
- 将这个时间步内生成的所有任务组成一个任务集合 $T_n=\{task_k,task_{k+1},...,task_{k+m}\}$，表示这个时间步内有 $m$ 个任务被车辆提交到边缘服务器。
- 重复这个过程，直到生成所有任务或达到停止条件。




为了解决第三章提出的车联网环境中的任务卸载问题，我们采用以上描述的PPO算法。这种算法可以处理具有连续状态和动作空间的RL环境，而不需要像Q-Learning等基于值函数的算法那样估计状态-动作值，而是直接学习最优策略。

在我们的车辆任务卸载场景中，我们自定义的RL环境将提供与真实车联网场景相似的任务特征，包括任务的类型、大小、持续时间等。此外，边缘服务器的资源也会被考虑，因此CPU和内存的可用性也将作为状态空间的一部分被使用和更新。

我们使用多智能体强化学习的框架，让每个边缘服务器都部署一个PPO代理，根据自己的状态和动作来更新自己的策略。这样，不同的服务器之间的策略可以根据任务需求和市场竞争进行自适应调整，从而提高自身的收入和车辆的效用。同时，多智能体强化学习也可以处理多个智能体之间的交互和博弈，使得每个服务器能够考虑其他服务器的行为和影响。为了简化模型，我们假设服务器之间没有额外的通信，而且只进行一次博弈。

我们使用RLlib来实现和管理训练过程，它可以并行地从环境中采集数据，汇总到一个训练批次中，然后根据选择的算法和损失函数计算梯度，并更新模型参数。我们重复这个过程，直到满足终止条件，如达到最大迭代次数、最大训练时间或最佳性能等。在训练过程中，RL模拟反馈回路不断地收集数据，包括观察到的状态、采取的动作、获得的奖励和所谓的结束标志，表示代理在模拟中经历的不同的回合。每次PPO代理采取一个动作（报价一个任务），都会得到一个即时奖励。下一个状态也将依赖于前一个状态，因为CPU和内存的可用性将在每次任务卸载后进行更新。最终，PPO代理应该能够学习任务的需求和优先级，并完成所有任务的报价，以最大化自身的收入和车辆的效用。