---
aliases: 多智能体强化学习Training Algorithm, '[多智能体强化学习Training Algorithm, "[多智能体强化学习Training Algorithm, ''[多智能体强化学习Training Algorithm, \"多智能体强化学习Training Algorithm, ''''多智能体强化学习Training Algorithm''''\"]'']"]'
UID: 20240224093555 
tags: 
source: 
cssclass: 
created: "2024-02-24 09:35"
updated: "2024-02-24 16:48"
---

随着智能交通技术的快速发展，车联网作为一种新兴的网络模式，正逐渐受到广泛关注。在车联网环境下，如何有效地利用系统资源，提高任务执行效率，成为一个亟待解决的问题。本文旨在最大化系统资源的利用率，提高车联网中任务卸载的整体执行效率，使车辆能够通过互联网访问各种智能服务。由于车辆的计算能力和能量有限，任务卸载可以通过利用边缘服务器来提高复杂和耗时任务的性能和效率。然而，任务卸载也涉及到多个车辆和服务器之间的权衡和协调，这对任务和资源分配提出了重大挑战。为了解决这个问题，我们提出了一种将多智能体强化学习和反向拍卖机制相结合的新方法，使车辆和服务器能够学习和优化任务卸载的竞标策略，实现公平高效的结果。我们采用近端策略优化（PPO）算法和长短期记忆（LSTM）网络来训练多智能体强化学习模型。在我们基于 Python 的开源模拟环境 VehicleJobScheduling 中进行了广泛的实验，该环境真实地模拟了车联网场景，并提供了多种评估指标。研究表明，PPO+LTSM 策略利用 LSTM 捕捉时间信息，能够做出更理性的决策，实现最有效的负载均衡。
**关键词**：车联网；任务卸载；多智能体强化学习；反向拍卖；PPO 算法

With the rapid development of intelligent transportation technology, vehicle networking as a new network mode is gradually receiving widespread attention. In the context of vehicle networking, how to effectively utilize system resources and improve task execution efficiency has become an issue that needs to be resolved urgently. This paper aims to maximize the utilization of system resources and improve the overall execution efficiency of task offloading in vehicle networking, enabling vehicles to access various intelligent services through the Internet. Due to the limited computing power and energy of vehicles, task offloading can improve the performance and efficiency of complex and time-consuming tasks by leveraging edge servers. However, task offloading also involves the trade-off and coordination among multiple vehicles and servers, which poses a significant challenge for task and resource allocation. To address this issue, we propose a novel approach that combines multi-agent reinforcement learning and reverse auction mechanisms, enabling vehicles and servers to learn and optimize their bidding strategies for task offloading, achieving a fair and efficient outcome. We employ the proximal policy optimization (PPO) algorithm with long short-term memory (LSTM) networks to train the multi-agent reinforcement learning model. Extensive experiments are conducted in our open-source Python-based simulation environment, VehicleJobScheduling, which realistically models the vehicle networking scenario and provides multiple evaluation metrics. The study reveals that the PPO+LTSM strategy utilizes LSTM to capture temporal information, enabling more rational decision-making and achieving the most effective load balance.

**Key words**：vehicular networks; task offloading; resource utilization; multi-agent reinforcement learning; reverse auction; PPO algorithm

## 研究背景及意义


随着物联网（IoT）和信息通信技术（ICT）的快速发展，现代车辆正在经历一场技术革命。车联网（IoV）是智能交通系统（ITS）的重要组成部分，它通过车辆之间以及车辆与基础设施之间的信息交换和协作服务，为驾驶员提供实时交通流信息，以提高道路安全、交通效率和驾驶体验。随着技术的进步，车辆可以在有限的计算能力和电池容量下执行更多计算密集型和数据密集型任务，但很难满足复杂或对延迟敏感的任务需求。[^1] 为了解决上述问题，移动边缘计算（MEC）作为车联网中任务调度的一种新方法，被提出让车辆将部分或全部任务卸载到部署在路边的多个 MEC 服务器上执行。在边缘计算的支持下，车辆可以将大部分任务卸载到边缘服务器上执行，有效扩展了其资源能力，同时减少了核心网络的流量需求。通过这种方式，车辆可以提高驾驶的安全性、效率和智能化。

车联网（Internet of Vehicles, IoV）构成了一个由移动车辆和固定基础设施组成的自组织网络，旨在为车辆提供智能、安全和高效的交通服务。随着车辆技术的进步，如自动驾驶、视频分析和个人助手等计算密集型应用的普及，车辆可以执行越来越多计算密集型和数据密集型的任务，包括但不限于导航、视频流传输以及车载娱乐等。然而，随着这些任务需求的增加，车辆的计算能力和电池容量逐渐变得有限，难以满足复杂或对延迟敏感的任务需求。为解决这一问题，移动边缘计算（MEC）作为一种新的计算范式应运而生。通过在道路边缘部署多个MEC服务器，它为车辆提供低延迟、高带宽的计算资源和服务。这项技术使车辆能够将部分或全部任务卸载到MEC服务器上执行，从而降低了自身的能耗和时间开销，为车联网系统带来更安全、更高效和更智能的驾驶体验。

车联网（IoV）中的任务调度是一项重要且具有挑战性的工作。它需要将计算密集型服务分配给车载单元（OBU）或边缘服务器，以提高车辆性能和用户体验。这些服务涵盖语音识别、自然语言处理、计算机视觉、机器学习、增强现实等多个方面，具有不同的类型、大小、优先级、延迟、能耗等特征和需求。然而，随着服务需求的增加，车辆的计算能力和电池容量逐渐变得有限，难以满足复杂或对延迟敏感的服务需求。为解决这一问题，移动边缘计算（MEC）作为一种新的计算范式应运而生。它通过在道路边缘部署多个MEC服务器，为车辆提供低延迟、高带宽的计算资源和服务。

## 国内外研究现状

本文主要研究的问题是车联网（IoV）中的工作流调度问题，即根据车辆的计算任务需求和移动特性，在移动边缘计算（MEC）环境中有效地分配和调度车辆的计算任务，以达到优化系统性能和用户体验的目的。车联网工作流调度涉及到多个方面的问题，如任务卸载策略、资源分配算法、激励机制设计、时延分析等。近年来，随着车联网技术和移动边缘计算技术的发展，车联网工作流调度问题受到了国内外学者的广泛关注，并取得了一些研究成果。为了对本课题有一个全面的了解，我们首先回顾了国内外相关领域的研究现状，主要包括以下几个方面：传统资源调度和随机优化的分布式资源调度方法,强化学习的车联网资源调度,强化学习移动边缘计算资源调度,强化学习服务器资源调度,车联网中的拍卖机制,强化学习定价策略。

现有文献中所采用的方法、技术、策略等主要包括以下几类：

- 基于数学规划或优化模型的方法，例如线性规划、非线性规划、整数规划、混合整数规划等。这类方法通常需要建立精确或近似的目标函数和约束条件，并利用求解器或启发式算法来求解最优解或次优解。这类方法具有理论上的严谨性和可解释性，但是也存在一些局限性，例如对问题假设过于理想化或简化，忽略了实际环境中存在的不确定性和随机性；求解过程复杂耗时，难以适应动态变化的场景；缺乏自适应能力和泛化能力等。
- 基于机器学习或人工智能的方法，例如监督学习、无监督学习、强化学习等。这类方法通常不需要建立精确或近似的目标函数和约束条件，而是通过数据驱动或交互式学习来获取知识并指导行为。这类方法具有一定程度上克服了数学规划或优化模型方法存在局限性方面 的优势 ，例如可以处理不确定性和随机性；可以快速响应动态变化；可以自适应地更新策略并提高泛化能力等。
- 基于博弈论或拍卖理论的方法。这类方法通常需要考虑用户之间存在着利益冲突或竞争关系，并利用博弈论或拍卖理论中相关概念和工具来分析用户之间的策略选择和均衡状态。这类方法具有一定程度上解决了多用户多任务多服务器场景下的资源分配和激励机制设计问题方面的优势，例如可以保证用户之间的公平性、效率性、真实性等 。但是，这类方法也存在一些局限性，例如对用户行为假设过于理想化或简化，忽略了用户之间的合作或信任关系；求解过程依赖于完全信息或共同知识，难以适应信息不对称或不完备的情况；缺乏自适应能力和泛化能力等。

(1).传统资源调度和随机优化的分布式资源调度方法

资源调度是指在有限的资源条件下，根据任务的需求和优先级，合理地分配和利用资源，以达到最优或次优的目标。资源调度问题在各种计算系统中都广泛存在，例如云计算、边缘计算、车联网等。传统的资源调度方法通常基于确定性或随机性的模型，通过数学规划、启发式算法、元启发式算法等技术来求解。传统资源调度方法通常基于集中式的优化模型，需要预先知道系统的参数和状态信息，然而在边缘云环境中，这些信息往往是不完全或不准确的，导致资源调度效率低下。为了解决这一问题，近年来出现了一些基于随机优化的分布式资源调度方法，它们可以根据实时的反馈信息动态地调整资源分配策略。目前已有一些针对边缘云计算场景的服务迁移和负载调度方法被提出，例如：Wang等人（2017）提出了一个联合计算卸载、资源分配和内容缓存的优化框架，并设计了一个基于ADMM的分布式算法来提高具有移动边缘计算的无线蜂窝网络的收益[4]。Rahul等人（2015）通过解耦原始MDP并采用李雅普诺夫优化技术，提出了一种高效、鲁棒、自适应且无需统计知识的边缘云服务迁移和负载调度算法[5]。Ibrahim等人（2019）提出了一种考虑车辆移动性和任务延迟要求的边缘计算工作负载调度方法，并通过拉格朗日松弛技术求解[6]。Zhang 等人（2022）考虑了多跳传输导向的车联网云动态工作流调度问题，提出了一种基于人工蜂群算法和贪心策略相结合的动态工作流调度方法[7]。这些方法在一些特定场景下表现出了较好的效果，但也面临着一些挑战，例如如何保证算法的收敛性和稳定性，如何平衡探索和利用之间的权衡，如何减少通信开销和计算复杂度等。

在分布式资源调度问题中，基于随机优化的方法是一种常见的解决方案。这类方法利用随机性来处理不确定性，通过迭代更新可行解来逼近最优解。然而，基于随机优化的方法也存在一些局限性，例如收敛速度慢、计算开销大、对参数敏感等。因此，近年来出现了一种新的方法，即深度强化学习。深度强化学习是一种结合了深度神经网络和强化学习的技术，可以训练神经网络来快速准确地解决多目标优化问题[8] 。相比于基于随机优化的方法，深度强化学习具有以下优势：一是能够自动学习复杂的策略函数，无需人为设计或调整参数；二是能够利用大量数据和高效算法来提高学习效率和质量；三是能够适应动态变化的环境和目标，并实现在线决策和反馈。

(2).强化学习的车联网资源调度

强化学习是一种基于试错学习和奖励反馈的机器学习方法，它可以让智能体在与环境交互的过程中自主地学习最优或近似最优的策略。强化学习具有以下几个特点：无需事先知道系统模型和参数；能够处理部分可观测和非马尔可夫性的环境；能够适应动态变化和不确定性的环境；能够实现长期目标和多目标之间的平衡。由于这些特点，强化学习被认为是一种适合于解决车联网资源调度问题的方法。强化学习的车联网资源调度是一种利用智能体的自主学习能力，根据车辆的状态、环境的变化和奖励信号，动态优化车辆之间和基础设施之间的通信资源分配的方法。强化学习的车联网资源调度可以提高车联网的性能，如降低时延、增加吞吐量、节省能耗等。强化学习的车联网资源调度面临着多个挑战，如高维状态空间、部分可观测性、非平稳性、多目标优化等。为了解决这些挑战，一些研究者提出了基于深度神经网络、多智能体协作、知识驱动等技术的强化学习算法，并在不同的场景中进行了验证和应用，如频谱共享、信道选择、功率控制、数据传输等。例如,Liu 等人（2019）设计了两种强化学习方法来优化计算卸载和资源分配问题[9]。Liu等人（2020）提出了一种基于优先级和价值函数的任务调度算法，考虑了任务之间的依赖性[10]。Song等人（2023）提出了一种基于潜在博弈理论和联邦深度强化学习的多异构服务器边缘计算卸载方法，能够有效地满足任务车辆用户的定制化服务需求[11]。Pang等人（2020）提出了一种考虑位置隐私保护的车联网计算资源协同调度策略，利用卡尔曼滤波预测车辆间距离，采用双重深度 Q 网络优化总成本[1]。然而，强化学习的车联网资源调度问题也存在一些挑战和难点，例如如何设计合适的状态空间、动作空间和奖励函数，如何处理高维度和连续性的状态和动作，如何实现多智能体之间的协调和博弈，如何提高学习效率和泛化能力等。

(3).强化学习移动边缘计算资源调度

强化学习是一种基于试错学习和奖励反馈的机器学习方法，它可以让智能体在与环境交互的过程中自主地学习最优或近似最优的策略。强化学习具有以下几个特点：无需事先知道系统模型和参数；能够处理部分可观测和非马尔可夫性的环境；能够适应动态变化和不确定性的环境；能够实现长期目标和多目标之间的平衡。由于这些特点，强化学习被认为是一种适合于解决车联网资源调度问题的方法。强化学习的车联网资源调度是一种利用智能体的自主学习能力，根据车辆的状态、环境的变化和奖励信号，动态优化车辆之间和基础设施之间的通信资源分配的方法。强化学习的车联网资源调度可以提高车联网的性能，如降低时延、增加吞吐量、节省能耗等。强化学习的车联网资源调度面临着多个挑战，如高维状态空间、部分可观测性、非平稳性、多目标优化等。为了解决这些挑战，一些研究者提出了基于深度神经网络、多智能体协作、知识驱动等技术的强化学习算法，并在不同的场景中进行了验证和应用，如频谱共享、信道选择、功率控制、数据传输等。例如,Liu 等人（2019）设计了两种强化学习方法来优化计算卸载和资源分配问题[9]。Liu等人（2020）提出了一种基于优先级和价值函数的任务调度算法，考虑了任务之间的依赖性[10]。Song等人（2023）提出了一种基于潜在博弈理论和联邦深度强化学习的多异构服务器边缘计算卸载方法，能够有效地满足任务车辆用户的定制化服务需求[11]。Pang等人（2020）提出了一种考虑位置隐私保护的车联网计算资源协同调度策略，利用卡尔曼滤波预测车辆间距离，采用双重深度 Q 网络优化总成本[1]。然而，强化学习的车联网资源调度问题也存在一些挑战和难点，例如如何设计合适的状态空间、动作空间和奖励函数，如何处理高维度和连续性的状态和动作，如何实现多智能体之间的协调和博弈，如何提高学习效率和泛化能力等。
\
移动边缘计算是一种将云计算的功能和服务延伸到网络边缘的技术，它可以为移动用户提供低延迟、高带宽、高可靠性的计算资源和服务。移动边缘计算中的资源调度问题是指如何在有限的计算资源和网络资源下，根据用户的任务需求和服务质量要求，合理地分配和调度任务在本地执行或卸载到边缘服务器或云服务器上执行。这是一个具有多约束、多目标、动态性和不确定性的优化问题，传统的优化方法难以有效地解决。因此，一些学者尝试使用强化学习来解决移动边缘计算中的资源调度问题，并取得了一定的成果。强化学习是一种基于智能体与环境交互学习最优行为策略的机器学习方法，可以适应不确定性和动态性，并且不需要先验知识或模型。近年来出现了许多基于强化学习的MEC资源调度方法，接下来对其中部分进行介绍。

例如，Zheng 等人（2022）使用深度 Q 网络（DQN）或双重深度 Q 网络（DDQN）来训练移动边缘服务器上的智能体，使其能够根据自身状态和环境状态选择最优或次优的任务卸载策略[12]；Chen J等人（2021）使用深度确定性策略梯度算法（DDPG）来训练连续动作空间下的智能体，使其能够输出最优或次优的任务卸载比例或执行速度[13]；Chen M等利用异步优势行动者-评论者算法(A3C），提出了一个基于软件定义网络和信息中心网络的动态资源分配方案，以提高车辆网络的服务质量[14];Peng等使用多智能体强化学习（MARL）来训练多个边缘服务器之间的智能体，使其能够协作地进行车辆关联和资源分配决策[15]；Peng等人（2022）提出了一种基于深度强化学习和有向无环图的依赖任务卸载策略，能够在多用户多服务器边缘计算环境中，灵活地选择合适的卸载目标服务器，并有效地减少服务延迟和终端能耗[16]。然而，强化学习在移动边缘计算中的资源调度问题仍然面临着一些问题和挑战，例如如何处理状态空间和动作空间的爆炸性增长，如何平衡探索和利用之间的权衡，如何解决多智能体之间的非平稳性和通信开销等。

(4).强化学习服务器资源调度

服务器资源调度是指在数据中心或云平台中，根据用户或应用程序的需求和服务质量要求，合理地分配和调度服务器上的计算资源、存储资源、网络资源等。这是一个涉及到数据中心或云平台的性能、效率、成本、节能等方面的重要问题。传统的服务器资源调度方法通常基于静态规划或启发式算法等技术来求解，但这些方法往往依赖于精确的系统模型和参数，难以适应复杂多变的环境。因此，一些学者利用强化学习来解决服务器资源调度问题，并取得了一些进展。强化学习服务器资源调度（Reinforcement Learning Server Resource Scheduling）是一种利用强化学习方法来优化服务器资源分配和任务执行的技术。它可以应对服务器状态的动态变化、多用户多任务的复杂需求、网络切片和边缘计算等新型网络架构的挑战，以及可再生能源和机器学习服务等新兴应用场景的特点。强化学习服务器资源调度的目标是在保证服务质量和满足约束条件的前提下，最大化系统效率和性能，最小化系统成本和延迟。强化学习服务器资源调度涉及多个层次和方面，包括资源配置、任务卸载、任务调度、并行度配置、协商策略等。近年来，有许多研究者提出了基于深度强化学习（Deep Reinforcement Learning）的服务器资源调度方法，并在不同的场景和数据集上进行了实验验证。

例如，有些学者使用强化学习来优化虚拟机（VM）或容器（Container）在物理机上的放置策略，以提高资源利用率和节约能耗[17–19]，其中Cheng 等人（2018）提出了一个基于深度强化学习的资源配置和任务调度系统，能够有效地降低云服务提供商的数据中心能耗和电费成本，并具有高效、可扩展、自适应和快速收敛等特点[17]，Zhao 等（2021）展示了如何利用深度强化学习在混合云环境中实现自适应的多目标任务调度，以最大化可再生能源的利用率和满足截止日期约束[19]；有些学者使用强化学习来优化服务器上的任务调度策略，以提高任务的执行效率和服务质量[20–25]，例如，Wu 等人（2020）针对服务器状态动态变化导致的资源分配不均衡问题，提出了一种基于深度强化学习和马尔可夫决策过程相结合的任务调度方法[23]，Rjoub等人（2021）提出了四种基于深度学习和强化学习的调度方法，并用真实数据集进行了实验比较。实验结果表明，深度强化学习结合长短期记忆网络（DRL-LSTM）的方法在减少任务执行成本和延迟方面优于其他三种方法[25]。然而，强化学习在服务器资源调度问题中也存在一些问题和挑战，例如如何处理大规模的状态空间和动作空间，如何处理部分可观测和非马尔可夫性的环境，如何处理多目标和多约束的优化问题，如何提高算法的收敛速度和稳定性等。

(5).车联网中的拍卖机制

拍卖机制是一种经济学中常用的激励机制，它可以实现资源或服务的有效分配和价格的公平确定。在车联网中，拍卖机制可以应用于多种场景，例如车辆之间或车辆与基础设施之间的资源或服务交易，例如频谱、缓存、计算、数据等。近年来，许多学者研究了车联网中的拍卖机制，并设计了各种拍卖模型和算法。例如，有些学者使用密封双向拍卖（SDBA）来实现车辆之间的频谱共享和交易；有些学者使用组合双向拍卖（CDBA）来实现车辆之间的缓存共享和交易；有些学者使用反向拍卖（RAM）来实现车辆与边缘服务器之间的计算任务卸载和交易；有些学者使用多属性拍卖（MAA）来实现车辆与数据提供者之间的数据获取和交易。然而，车联网中的拍卖机制也面临着一些问题和挑战，例如如何保证拍卖机制的真实性、有效性、个体合理性、社会福利最大化等性质，如何处理参与者的自私或恶意行为，如何减少拍卖过程中的计算复杂度和通信开销等。

车联网是指通过无线通信技术，实现车辆、道路、交通设施等的信息交互和资源共享的网络。车联网可以提高道路安全，优化交通管理，提升驾驶体验，促进智能出行等。在车联网中，拍卖机制是一种有效的资源分配和价格发现的方法，可以解决供需不平衡、竞争不公平、信息不对称等问题。拍卖机制可以应用于车联网中的多种场景，例如路侧单元（RSU）的接入控制、频谱资源的分配、数据服务的交易等。接下来将对部分研究进行介绍。Vishalatchi等人（2017）介绍了云计算中的虚拟机调度问题，以及一种基于拍卖机制的禁忌搜索算法来解决它。这可以作为一个基础和背景，让读者了解云计算中的资源分配问题和拍卖机制的作用。Ding等人（2016）从云计算转向网格计算，介绍了网格计算无线网络中的动态资源分配问题，以及一种具有预测能力和多属性特征的新颖反向在线拍卖算法来解决它。这可以展示拍卖机制在另一种计算网络中的适用性和创新性。Liwang等人（2019）从网格计算转向车联网，介绍了车联网中存在的计算卸载、资源共享和用户自私等问题，以及一种基于VCG原理的反向拍卖机制来优化计算卸载决策并满足经济属性。这可以展示拍卖机制在更具挑战性和前沿性的领域中的应用和效果。Zhang等人（2022）进一步考虑了公共区块链网络对车联网资源分配问题的影响和支持，提出了一种利用反向拍卖模型和VCG机制激励车辆记录驾驶数据，并使用边缘计算节点支持区块链技术的真实拍卖机制。这可以展示拍卖机制在结合其他先进技术时能够产生更高效、安全、可信等优点。

(6).强化学习定价策略

定价策略是指在市场交易中，根据供需关系、成本收益分析、竞争对手行为等因素，确定商品或服务的价格水平和变化规律。定价策略是一种重要的市场营销手段，它可以影响消费者的购买意愿和行为，从而影响供应商的收入和利润。在车联网中，定价策略可以应用于多种场景，例如车辆之间或车辆与基础设施之间的资源或服务交易，例如频谱、缓存、计算、数据等。近年来，许多学者研究了车联网中的定价策略，并设计了各种定价模型和算法。例如，有些学者使用基于需求函数的定价策略来实现车辆之间的频谱共享和交易；有些学者使用基于成本函数的定价策略来实现车辆之间的缓存共享和交易；有些学者使用基于效用函数的定价策略来实现车辆与边缘服务器之间的计算任务卸载和交易；有些学者使用基于博弈论的定价策略来实现车辆与数据提供者之间的数据获取和交易。然而，车联网中的定价策略也存在一些问题和挑战，例如如何根据市场环境和用户行为动态地调整价格，如何平衡供应商和消费者之间的利益，如何处理多方参与者之间的竞争和合作等。

强化学习定价策略是一种利用强化学习技术来优化定价决策的方法，它可以在不依赖用户响应函数的情况下，通过不断地探索和学习找到最优的定价策略，从而实现需求响应、能耗调度、竞争优势、成本效率等目标。强化学习定价策略在金融量化、电子市场、云计算等领域有着广泛的应用和研究。接下来将对使用强化学习进行定价的部分研究进行介绍。

Kutschinski等人（2003）研究了电子市场中多智能体强化学习定价策略，并提出了一个分布式代理平台来模拟和评估不同竞争环境下的定价效果。随后，Kim等人（2016）将强化学习技术应用到一个不确定微网环境中，解决了动态定价和能耗调度问题。Ghasemkhani等人（2018）提出了一种不依赖用户响应函数的定价算法，通过强化学习找到最优定价策略，实现需求响应目标。Krasheninnikova 等人（2019）将强化学习算法扩展到保险领域，使用马尔可夫决策过程来解决保险续费价格调整的多目标优化问题。Islam等人（2022）提出了一种基于深度强化学习的Spark作业调度算法，考虑了多个SLA目标，利用了云VM定价模型，动态调整了资源配置，用于解决云计算中的虚拟机（VM）调度问题。

综上所述，国内外相关领域的研究现状表明，车联网中的资源调度问题是一个具有重要意义和挑战性的课题，它涉及到多种技术和方法，例如传统优化、随机优化、强化学习、拍卖机制、定价策略等。这些技术和方法在一定程度上可以解决资源调度问题，但也存在一些不足或缺陷，需要进一步的研究和改进。因此，在本文中，我们将结合反向拍卖机制和多智能体深度强化学习，提出一种新颖的车联网工作流调度方法（RAM-DRL），并对其进行理论分析和仿真验证。下面将介绍本文研究的主要内容。

## 本文主要研究内容

本文针对车联网中的任务调度问题，提出了一种结合了多智能体强化学习和逆向拍卖机制的创新方法。该方法的目标是通过动态分配计算密集型服务的执行计划，实现服务、车辆和边缘服务器之间的资源匹配，同时通过激励机制促进车辆和边缘服务器之间的合作，以提高任务调度的效率和性能，降低总成本。具体而言，本文采用了多智能体强化学习算法，使车辆能够根据本地和全局信息自主学习和更新其任务调度策略，从而充分利用系统资源，提高执行效果。同时，引入了逆向拍卖机制，通过资源分配和价格协商，实现了车辆和边缘服务器之间的有效合作。这一机制旨在平衡竞争与合作，提高系统效率和公平性，确保资源的合理分配。本文将该方法命名为基于多智能体深度强化学习（MADRL）的车联网任务调度方法。

MADRL方法利用神经网络和马尔可夫决策过程（MDP）模型，训练每个边缘服务器上的策略网络，使其能够智能计算接受任务的长期奖励，并根据最大化奖励的原则向用户出价。通过结合深度强化学习和逆向拍卖机制，MADRL方法能够在复杂、动态和不确定的多智能体环境中，实现系统的适应性、鲁棒性、可扩展性和分布性。因此，MADRL方法能够适应复杂、动态和不确定的多智能体环境，提高系统的性能和效率。

此外，本文采用了逆拍卖机制，该机制在车联网中的应用场景丰富多样，包括资源或服务交易，如频谱、缓存、计算和数据等。在车联网中，逆拍卖机制不仅能够激励车辆用户共享资源或执行任务，还可应用于社会福利任务的分配，如交通管理、环境监测、数据收集等。通过逆拍卖，平台能够寻找愿意以最低价格提供服务或执行任务的车辆用户或服务器，并与其签署标准合同，从而实现成本节省。本文的后续章节将详细介绍本文提出的方法的设计和实现，并通过仿真实验验证其有效性和优越性。

本文的主要贡献如下：

- 将反向拍卖应用于车联网中的任务调度问题，实现了分布式、自适应、激励兼容的任务调度。
- 设计了一个基于 PPO+LSTM 的报价策略，利用 LSTM 的记忆能力，捕捉任务调度的时序特征和长期依赖，提高报价策略的性能和效果。
- 开发了一个基于 Python 的开源仿真环境 —— VehicleJobScheduling，模拟了车辆任务调度的过程，并提供了一些常用的评估指标。
- 通过仿真实验，验证了本文方法的有效性和优越性，以及强化学习和反向拍卖机制的优势和适用性，并与其他基准方法进行了对比分析。

## 本文组织结构

本文的结构安排如下：


**IoV和MEC中的工作流调度**

车联网（IoV）是一种新兴的技术，它为车辆提供了感知、通信和计算功能，通过车辆之间、车辆与路测设施之间以及车辆与云端之间的信息交换和协同服务，实现更安全、高效和智能的驾驶。然而，车辆本身的计算能力和电池容量有限，无法满足一些复杂或者对延迟敏感的任务需求，例如自动驾驶、视频分析等。移动边缘计算（MEC）是一种新型的计算范式，它通过在道路边缘部署多个MEC服务器，为车辆提供近距离的低延迟高带宽计算资源和服务。IoV和MEC是两种相辅相成的技术。MEC可以让车辆把任务卸载到边缘服务器上，节约自己的能量和时间。这样，IoV产生的大量数据和多样化的服务需求就能得到更好的满足。MEC不仅为IoV提供了低时延、高带宽、高可靠性等网络质量，还利用边缘节点的存储和计算资源进行数据处理、分析和优化，实现数据就近处理、就近消费。

在智能交通系统等领域中，IoV和MEC有着广泛而深刻的应用场景和价值。MEC节点可以为车辆提供多种服务，包括感知、计算和通信等。例如，MEC节点可以根据车辆和道路的实时状态，进行车路协同调度，以提高交通效率和安全性；MEC节点还可以管理停车场的空位，并为车辆提供停车导航和支付服务，以节省时间和费用；此外，MEC节点也可以提供丰富的车载娱乐内容，并根据用户喜好进行个性化推荐，以增加乐趣和舒适度。

工作流调度（Workflow Scheduling）是指在一定的约束条件下，根据某种优化目标，为一组相关的任务分配合适的执行资源的过程。工作流调度的目标是在满足用户需求和资源限制的前提下，优化工作流调度的执行效果，如执行时间、执行成本、能耗、可靠性等。工作流调度需要考虑多种约束条件，如任务之间的依赖关系、资源之间的异构性、资源的可用性和动态性等。工作流调度的评价指标是根据不同的优化目标而定，常见的评价指标有完成所有任务所需的最短时间（Make Span）、完成所有任务所需的总费用（Cost）、完成所有任务所需的总能耗（Energy）、完成所有任务成功率（Reliability）等 。

在IoV（Internet of Vehicles）和MEC（Mobile Edge Computing）环境下，工作流调度面临着更多的特殊性和难点。首先，由于车辆具有高速移动性和动态变化性，如何预测车辆与 MEC 服务器之间的通信质量并根据实时状态进行任务卸载决策是一个难点。其次，由于不同车辆可能有不同的任务类型、优先级、截止时间等约束条件，如何在保证服务质量（QoS）的同时平衡各个 MEC 服务器之间的负载并最大化系统效用是一个关键问题。第三，在多用户多任务多服务器场景下，如何设计一种合理且公平的激励机制来鼓励用户之间相互合作并避免自私或恶意行为也是一个重要问题。本文的目标是提出一种基于反向拍卖机制和深度强化学习相结合的车联网工作流调度方法（RAM-DRL），该方法可以有效地解决上述问题，并在保证收益最大化的前提下，尽可能保证系统可靠性。

**反向拍卖**

普通的拍卖方式是由卖方发起，买方出价最高的获得商品或服务。与正向拍卖相反的是反向拍卖（Reverse Auction Mechanism），它是一种拍卖机制，其中卖方是多个，而买方是一个。在反向拍卖中，买方会提出一个需求，并邀请卖方报出自己的价格。然后，买方会根据自己的目标和预算，从报价中选择一个或多个合适的卖方，并支付给他们相应的价格。反向拍卖可以分为不同的类型，根据竞标者数量、竞标规则、竞标信息等因素进行分类。常见的反向拍卖类型有：排名反向拍卖，日本反向拍卖，荷兰反向拍卖，和开放式反向拍卖。在排名反向拍卖中，每个卖方只能看到自己的排名和最低报价，而不能看到其他卖方的报价。在日本反向拍卖中，买方会提出一个初始报价，并逐渐降低，直到只剩下一个或多个愿意接受的卖方。在荷兰反向拍卖中，买方会提出一个需求清单和一个预算价格，并邀请多个卖方参与竞标，最后可能选择一个或多个中标者。在开放式反向拍卖中，每个卖方都能看到所有的报价和排名，并根据市场情况调整自己的报价。反向拍卖的性质是基于市场竞争和价格信号来确定商品或服务的价值，而不是由政府或其他机构来设定。反向拍卖的优势是可以降低采购成本、提高采购效率、增加透明度和公平性、促进创新和质量提升等。

反向拍卖在激励车辆用户共享资源或执行任务方面也有重要的作用和效果。例如，当一个平台需要为其用户提供某种服务时，可以通过反向拍卖来寻找愿意以最低价格提供该服务的车辆用户，并与之签订标准合同。这样可以使平台节省成本，同时也可以激励车辆用户利用闲置资源或空闲时间来参与竞标，从而获得额外收入。另一方面，反向拍卖也可以用于分配任务给车辆用户，例如交通管理、环境监测、数据收集等。平台可以根据任务需求和预算，在反向拍卖中发布任务信息，并选择出价最低且符合条件的车辆用户来执行任务。这样可以使平台有效地完成任务目标，同时也可以激励车辆用户参与社会公益活动，并获得相应奖励。在车联网和移动边缘计算环境下，由于车辆具有高速移动性和动态变化性，如何设计一种合理且公平的反向拍卖机制来鼓励用户之间相互合作并避免自私或恶意行为也是一个重要问题。本次论文将利用深度强化学习来训练每个 MEC 服务器上部署的策略网络，使其能够根据用户的任务需求和自身状态计算接受任务所产生的长期收益，并根据收益最大化原则向用户进行报价。

**深度强化学习（DRL）**

深度强化学习（DRL）是一种基于神经网络的强化学习方法，它能够在复杂动态优化问题中表现出优异的性能。DRL的基本概念是通过让智能体与环境进行交互，从而学习到一个最优策略，该策略可以使智能体获得最大的长期收益。当存在多个智能体共享一个环境时，就形成了多智能体强化学习（MARL）的问题。MARL的目标是让每个智能体根据自己的奖励和其他智能体的行为来调整自己的策略，从而实现合作或竞争的目标。MARL的方法可以分为基于值函数的方法，基于策略的方法，以及基于演员-评论家的方法，它们分别利用不同的方式来评估或生成策略。DRL和MARL的模型有多种形式，例如深度 Q 网络（DQN），深度确定性策略梯度（DDPG），异步优势演员-评论家（A3C）等，它们分别适用于不同的问题场景和特点。DRL和MARL的算法主要使用神经网络来近似值函数或策略函数，并采用梯度下降或其他优化技术来更新网络参数。

本次论文利用深度强化学习（DRL）来训练每个MEC服务器上部署的策略网络，使其能够根据用户的任务需求和自身状态计算接受任务所产生的长期收益，并根据收益最大化原则向用户进行报价。相比于其他机器学习方法，DRL可以直接从环境中获取反馈信号，并通过不断地探索和利用来优化自身的行为。DRL不需要预先定义特征或标签，也不需要大量的先验知识或假设，因此更适合处理车联网这样的复杂、动态和不确定的环境。本次论文设计了一个三层神经网络结构来近似每个用户 - 状态 - 行动对应的价值函数，并采用异步优势行动者-评论者算法（A3C）算法来更新网络参数并避免过度估计偏差，提高算法的稳定性和准确性。

在本次研究中，我们利用深度强化学习（DRL）来训练每个部署在 MEC 服务器上的策略网络，使其能够根据用户的任务需求和自身状态，计算接受任务所产生的长期收益，并按照收益最大化原则向用户报价。与其他机器学习方法相比，DRL 可以直接从环境中获取反馈信号，并通过持续探索和利用来优化自身行为。DRL 不需要事先定义特征或标签，也不需要大量的先验知识或假设，因此更适合处理车联网这样复杂、动态和不确定的环境。 具体来说，我们设计了一个由 LTSM（Long Short-term Memory）层和三层神经网络组成的结构，来近似每个用户-状态-行动对应的价值函数。LTSM 层的加入，使得模型能够更好地处理时间序列数据，捕捉到用户任务需求和自身状态中的长期依赖关系。然后，我们采用 PPO（Proximal Policy Optimization）算法来更新网络参数，以避免过度估计偏差，提高算法的稳定性和准确性。 PPO 算法是一种在策略梯度算法基础上的改进算法，它通过在多个训练步骤上对策略进行约束，限制策略的变化幅度，从而有效地避免了策略梯度算法中的高估问题。此外，PPO 算法还引入了一个新的-clip 操作，用于限制策略更新的幅度，进一步增强了算法的稳定性。与传统的策略梯度算法相比，PPO 算法在处理连续控制问题时表现更为出色，并且具有更好的收敛性和鲁棒性。



## 问题陈述


在移动边缘计算中, 车联网是一个非常重要的应用场景，车辆需要在行驶过程中执行不同的计算任务，例如语音识别、路线规划、计算机视觉、机器学习、增强现实等。这些任务对于计算资源和存储资源的需求各不相同，而车辆自身的计算能力和电池容量是有限的。因此，车辆可以选择将部分任务卸载到边缘服务器上进行处理，以节省能耗和提高性能。然而，由于车辆和边缘服务器之间的网络连接是不稳定的，而且边缘服务器的资源也是有限的，因而如何高效地进行任务卸载和资源分配成为一个关键问题。

在本次车联网任务调度的问题建模中, 我们假设一个由$m$个卖家$\mathcal S=\{\mathbf s_1,\dots,\mathbf  s_m\}$和$n$个买家$\mathcal B=\{\mathbf b_1,\dots,\mathbf b_n\}$组成的车联网卸载模型. 卖家为能够提供任务卸载的服务器, 表示为$\mathbf s_i =(\mathbf r_i,c_i)$. 每个卖家 $\mathbf s_i$ 有资源向量 $\mathbf r_i=(r_{j,1},\dots,r_{j,d})$ 和成本 $c_i$，其中, $d$表示服务器总的资源种类, $\mathbf r_{j,k}$表示服务器$j$拥有第$k$种资源的数量，$c_i$ 表示单位时间的维护成本。 买家为需要任务卸载的车辆提交的一次任务请求,表示为 $\mathbf b_j=(\theta_j,\psi_j)$, 其中, $\theta_j=(\mathbf f_j,\mathbf \sigma_j)$表示一个任务,$\mathbf f_j=(l_j,r_j,d_j,\tau_j)$ 是任务的执行特征, $\mathbf \sigma_j=(\sigma_{j,1},\dots,\sigma_{j,m})$ 是任务的执行服务器限制向量表示任务$\theta_j$是否可以在服务器$\mathbf s_i$上执行。 $l_j$表示任务的优先级， $\mathbf r_j=(r_{j,1},\dots,r_{j,d})$表示任务的资源需求, $r_{i,k}$表示任务$i$对第$k$种资源的需求数量； $d_j$表示任务的持续时间； $\tau_j$表示任务的到达时间。$\psi_j$为买家$b_j$对$\theta_j$的预算.

在时间步$t$时, 我们用 $\mathcal U_t​$表示车辆在第$t$个时间步提交的所有任务, 任务序号按照时间非降序排列, 假设$\mathcal U_t$中的任务, 每次按照任务到达顺序取一个任务$\theta_j\in \mathcal U_t$进行拍卖, 则本次拍卖中, 买家为$b_j$表示发起本次任务请求的车辆, 卖家为$\mathcal S_j=  \{\mathbf s_i|\forall \mathbf s_i \in \mathcal S, \sigma_{j,i}=1\}$, 表示能够满足执行服务器限制向量的服务器集合.

拍卖开始时, 买家$\mathbf b_j$将$\theta_j$的特征$\mathbf f_j$发送到参加拍卖的所有卖家$\mathcal S_j$, 卖家根据自身状态和接收的任务特征返回报价$\mathcal Q_j = \{q_{j,i}|\forall \mathbf s_i\in \mathcal S_j\}$, $\mathcal Q_j$表示服务器对$\theta_j$的报价集合,$q_{j,i}$表示卖家服务器$\mathbf s_i$对任务$\theta_j$的报价. 随后产生拍卖结果, 服务器分配向量$\mathbf x_{j}=(x_{j,1},\dots,x_{j,m})$ ,支付 $p_{j}=\min(q_j)$. 若$x_{j,i}=1$表示分配到$\mathbf s_i$, 若$x_{j,i}=0$, 表示未分配到$\mathbf s_i$. 买家支付并开始任务卸载。当$\mathcal U_t$中所有任务都进行过拍卖后, 进入下一个时间步$t+1$, 当所有时间步都完成后，拍卖过程结束。

为了描述任务卸载过程中的资源使用情况，我们需要对任务和服务器的资源在执行时的占用进行建模。我们假设任务$\theta_j$在时间$t$对资源的占用为

$$
r_{\theta_j}(t)=(r_{{\theta_j } , 1}(t),\dots,r_{\theta_j,d} (t))
$$

，其中$r_{\theta_j,k}(t)$表示任务$\theta_i$在时间$t$对第$k$种资源的占用量。

$$
r_{\theta_j,k}(t)=\begin{cases}
r_{j,k}&\text{if }\tau_j \leq t < \tau_j+d_j \\
0,&\text{otherwise}
\end{cases}
$$

类似地，我们假设卖家服务器$\mathbf s_i$在时间$t$的可用资源为$r_{\mathbf s_i}(t)=(r_{{\mathbf s_i},1}(t),\dots,r_{{\mathbf s_i},d}(t))$，其中$r_{{\mathbf s_i},k}(t)$表示服务器$\mathbf s_j$在时间$t$拥有第$k$种资源的剩余量。我们假设任务$\theta_j$在分配到服务器$\mathbf s_i$后，会在$\tau_j$立即开始执行，$\tau_j+d_j$时间内完成，即从$\tau_j$到$\tau_j+d_j-1$的时间段内，任务$i$会占用服务器$k$的资源，而在$\tau_j+d_j$时刻释放资源。因此，我们可以定义对于$\theta_j$的服务状态变量$s_{i,k}(t)$为：

$$
s_{j,i}(t)=\begin{cases}
1, & \text{if }  x_{j,i}=1 \text{ and } \tau_j \leq t < \tau_j+d_j\\
0, & \text{otherwise}
\end{cases}
$$

其中$x_{j,i}$表示任务$\theta_i$是否分配到服务器$\mathbf s_i$。服务状态变量$s_{j,i}(t)$表示任务$\theta i$在时间$t$是否占用服务器$\mathbf s_i$的资源。

根据服务状态变量，我们可以计算任务$\theta i$在时间$t$对服务器$\mathbf s_i$的资源占用量为：$r_{j,i}(t)=s_{j,i}(t)\cdot r_{\theta_j}(t)$

由此，我们可以得到服务器$\mathbf s_i$在时间$t$的可用资源为：$r_{s_i}(t)=\mathbf r_i-\sum_{i=1}^{|\mathcal U_t|} r_{j,i}(t)$
其中$|\mathcal U_t|$表示$t$时间内任务总数，$\mathbf r_i$表示服务器$\mathbf s_i$的总资源量。服务器$j$在时间$t$的可用资源$r_i(t)$表示服务器$j​$在时间$t​$可以接受新的任务卸载请求的资源余量。

服务器$j$在接收到任务卸载时会获得收益, 则边缘服务器$\mathbf s_i$在总时间步$T$到达时的收益为:$R_i = \sum_{t=1}^T (p_{t,i} - c_{i})$
其中, $p_{t,i}=\sum_{i=1}^{|\mathcal U_t|} p_{i} x_{j,i}$ 是第$t$个时间步内得到的总支付，$p_{j}$为任务$\theta_j$的支付，$c_{i}$ 是服务器每回合的维护成本。

我们的目标是，在总时间步$T$到达时，对于所有买家$\mathbf b$，卖家$\mathbf s$，设计服务器的任务报价策略，使得每个边缘服务器能够在满足任务预算和服务器执行限制的前提下，最大化卖家整体收益$R=\sum^{m}_{i=1} R_i$.

基于以上定义，我们可以把车联网中的任务调度问题建模为一个优化问题：

$$

\begin{aligned}

&Objective: \max_{q_j} R= \sum^m_{i=1} \sum_{t=1}^T (\sum_{j=1}^{|\mathcal U_t|} p_{j} x_{j,i}- c_j)\\
&\\

&\text{s.t.}\ \ \begin{aligned}

& C1:\sum_{j=1}^n x_{j,i}\sigma_{j,i} \le 1, \forall i \in \mathbf b, j \in \mathbf s \\
& C2:p_j \sum_{i=1}^m  x_{j,i}\sigma_{j,i} \leq b_j, \forall i \in \mathbf b, j \in \mathbf s \\
& C3: r_{s_i}(t)\ge 0, \forall i \in \mathbf b, s_i \in \mathbf s,  t \in [0, T]\\
\end{aligned}

\end{aligned}
$$

限制条件的含义如下：

- C1:表示每个任务只能卸载到$a_j$中的一个边缘服务器上；
- C2:表示每个卸载任务的最终支付不能超过其预算$b_j$。
- C3:表示每个边缘服务器在任意时间点可用资源$r_k(t)\ge 0$

以上就是本文对车联网资源卸载问题的系统模型和问题建模。在下一章中，我们将详细介绍本文提出的基于多智能体强化学习和反向拍卖的任务调度算法.

## 4. Method of MADRL and Reverse-Auction

为了解决边缘计算中的车联网工作流调度问题，我们提出了一个基于多智能体强化学习和反向拍卖机制的方法。我们首先把这个问题建模成了一个在线和NP-hard问题，然后使用强化学习方法来处理复杂的动态的高维度的决策问题。我们采用多智能体强化学习来实现分布式的资源分配，避免集中式管理的通信开销和信息的不准确性，提高系统的稳定度。我们还使用了一种基于反向拍卖的任务卸载方法，用于解决车联网中的资源分配问题。反向拍卖是一种一买方多卖方的拍卖形式，适用于买方市场的交易活动。在反向拍卖中，买方提出自己的需求和预算，卖方根据自身的成本和利润，竞争性地给出自己的报价。最终，买方选择一个或多个报价最低且满足需求的卖方进行交易。反向拍卖可以有效降低买方的采购成本，提高卖方之间的竞争力，实现资源的优化分配。接下来的内容将分别描述多智能体强化学习建模和算法，以及反向拍卖的角色和过程。

## 多智能体强化学习建模

![[Pasted image 20230902100025.png]]
多智能体强化学习过程

![[Pasted image 20230717162154.png]]
**Figure 1**: Single-agent approaches (a) and (b) in comparison with multi-agent RL (c)

为了解决边缘计算中的车联网工作流调度问题，我们提出了一个基于多智能体强化学习和反向拍卖机制的方法。我们首先将这个问题建模成了一个在线和NP-hard问题，因为车联网的任务卸载可以看作一个有时序的多维背包问题。然后使用强化学习方法来处理复杂的动态的高维度的决策问题。我们采用多智能体强化学习来实现分布式的资源分配，避免集中式管理的通信开销和信息的不准确性，提高系统的稳定度。我们还使用了一种基于反向拍卖的任务卸载方法，用于解决车联网中的资源分配问题。反向拍卖是一种一买方多卖方的拍卖形式，适用于买方市场的交易活动。买方提出自己的需求和任务优先级，卖方根据自身资源使用情况和任务特征给出报价。买方只选择一个卖方进行交易。反向拍卖可以有效降低买方的采购成本，提高卖方之间的竞争力，实现资源的优化分配。

### 多智能体强化学习环境建模

在第三章的车联网任务卸载建模中, 边缘服务器被定义为智能体（agent）, 每个智能体通过与环境交互学习可以最大化自身收益的策略. 因为在多智能体环境中,一个智能体的动作会影响其他智能体的决策. 为了描述这种情况，我们使用部分可观察马尔可夫决策过程（partially observable Markov decision process, POMDP）来建模多智能体强化学习. 由K个智能体组成的POMDP可以被定义为

$$
(\mathbf S,\mathbf O_1,\dots,\mathbf O_K,\mathbf A_1,\dots,\mathbf A_K, \mathbf R_1,\dots,\mathbf R_K, \mathbf P_{ss'}, \mathbf P_{o|s})
$$

其中$\mathbf S$为状态空间（state space），$\mathbf O_i$为第i个智能体的观测空间（observation space），$\mathbf A_i$为第i个智能体的动作空间（action space），$\mathbf R_i$为第i个智能体的奖励函数（reward function），$\mathbf P_{ss'}$为状态转移概率（transition probability），$\mathbf P_{o|s}$为观测生成概率（observation likelihood）。在每一个时间间隙, 边缘服务器作为一个智能体, 观察当前环境生成的观测$o_i\in\mathbf O_i$, 根据策略$\pi_i(a_i|o)$选择一个合适的动作$a_i\in\mathbf A_i$. 然后获得相应的奖励$r_i=\mathbf R_i(s,a_1,\dots,a_K)$. 在本章后续部分，我们将详细介绍如何定义$\mathbf S,\mathbf O,\mathbf A,\mathbf R$以及如何设计多智能体强化学习环境.

Littman, M. L. (1994). Markov games as a framework for multi-agent reinforcement learning. In Machine learning proceedings 1994 (pp. 157-163). Morgan Kaufmann.

#### 状态空间 State Space

在强化学习方法中, 状态空间是指整个系统的描述，包含了系统中所有的信息。本次研究的是多智能体强化学习，因此状态空间应当包含所有服务器集群和任务队列的状态。状态空间可以用一个集合$\mathbf S$表示，即

$$
\mathcal S=\{\mathcal B,\mathcal S, \mathbf r_{\mathcal B}\}
$$

其中$\mathcal B$表示卖家服务器,$\mathcal S$表示买家车辆提交的任务请求, $\mathbf r_{\mathcal B}=\{r_{\mathbf s_1},\dots,r_{\mathbf s_m}\}$表示卖家服务器的资源使用情况, $r_{\mathbf s_i}=(r_{\mathbf s_i}(1),\dots,r_{\mathbf s_i}(T))$。

#### 观察空间 Observation Space

在多智能体强化学习中，观察空间是指一个智能体可以观察到的状态空间的子集。在本次研究中，我们假设智能体之间不存在额外的通信与交互，因此智能体$i$所能观察到的状态空间就是自身的服务器在从当前时间$t$到之后一定时间的资源使用情况$\mathbf r_{\mathbf s_i,t}=(r_{\mathbf s_i}(t),\dots,r_{\mathbf s_i}(t+\epsilon))$和当前等待被智能体报价的任务$\theta_j$的任务执行特征$\mathbf f_j$. 因此，边缘服务器i的观察空间为

$$
O_i=\{r_{\mathbf s_i,t},\mathbf f_j\}
$$

。这意味着每个智能体只能根据自己的服务器负载和当前任务的特征来做出决策，而无法获取其他智能体的状态信息。

#### 动作空间 Action Space

动作空间是指智能体可以向环境执行的动作的集合。在多智能体强化学习中，每个智能体都可以执行动作以影响环境的状态，从而获得奖励。不同的任务可能需要不同类型的动作空间，例如离散的或连续的。在本次研究中，我们考虑了一个多智能体服务器报价任务，其中每个智能体代表一个边缘服务器，需要对收到的任务进行报价，以竞争任务的分配。

我们假设每个智能体的动作空间是一个在$[1,2]$的数值，表示其对任务报价的浮动系数。这意味着每个智能体可以选择任意一个在$[1,2]$区间内的实数作为其报价系数$a_i$。这样可以使得智能体有更大的自由度和灵活性来调整自己的报价策略。我们可以用一个区间$\mathbf A_i$表示第$i$个智能体的动作空间，即

$$
\mathbf A_i=[1,2]
$$

当环境接收到智能体$i$的$a_i$的值后, 则其对当前等待被拍卖的任务$\theta_j$的最终报价为

$$
q_i = a_i \times \mathbf p \cdot \mathbf r_j \times \tau_j
$$

，其中$\mathbf p$是资源的市场平均价格向量, $\mathbf r_j$是资源使用量, $\tau_j$是资源使用时间。

然而，并不是所有的服务器都可以对任意的任务进行报价，因为任务有执行服务器限制$\mathcal S_j \subseteq \mathcal S$，表示只有$\mathcal S_j$中的服务器才能满足任务$j$的需求。因此，在时间步$t$，整个系统的动作空间为

$$
A_t=\{a_1 \sigma_{j,1},\dots,a_m\sigma_{j,m}\}
$$

其中，$a_i\sigma_{j,i}$代表第$i$个服务器对其当前请求报价的任务$j$的报价系数。如果某个服务器不满足任务$j$的执行服务器条件限制，则其对应的报价系数为0，表示无法执行任何动作。当对任务$j$进行报价时, 环境仅收集任务对应满足条件限制的服务器的报价, 而忽略其他服务器的报价。

#### 奖励函数 Reward Function

在多智能体强化学习中，奖励函数定义了智能体的目标和评估标准，描述了智能体在执行某一动作后，从一个状态转移到另一个状态所获得的即时奖励。智能体会根据奖励函数来判断自己的动作对于自己目标的达成是有利还是有害，从而调整自己的行为策略，以最大化自己的累积奖励。本次论文考虑了一个多智能体服务器报价任务，其中每个智能体代表一个边缘服务器，需要对收到的任务进行报价，以竞争任务的分配。边缘服务器的目标是最大化自己的收益，即任务支付与服务器成本的差额。因此，我们设计了如下的奖励函数：

$$
\mathbf R_i = p_{t,i}-c_i
$$

这个奖励函数表示第$i$个边缘服务器在一个时间步内获得的奖励，它由两部分组成：$p_{t,i}$表示边缘服务器在一个时间步内完成的任务支付之和，$c_i$表示边缘服务器在一个时间步内的固定成本。

### 基于反向拍卖的车联网任务卸载方法

任务卸载涉及到多个参与者之间的资源分配问题，即如何将车辆的任务分配给合适的边缘服务器，并使得各方都能获得最大化的效用。为了解决这一问题，本文借鉴了经济学中的反向拍卖（Reverse Auction）理论，提出了一种基于反向拍卖的任务卸载方法（Reverse Auction-based Task Offloading, RATO）。反向拍卖是一种一买方多卖方的拍卖形式，适用于买方市场（Buyer’s Market）的交易活动。在反向拍卖中，买方提出自己的需求和预算，卖方根据自身的成本和利润，竞争性地给出自己的报价。最终，买方选择一个或多个报价最低且满足需求的卖方进行交易。反向拍卖可以有效降低买方的采购成本，提高卖方之间的竞争力，实现资源的优化分配。

本文将分别介绍本模型中反向拍卖的角色和过程。

**买方**是车联网中有任务卸载需求，并愿意为其需求支付的车辆所产生的一次请求$\mathbf b_j$。在某一时刻 $t$，车辆自身评估连接状态并向满足其连接限制的边缘服务器发起任务 $\theta_j$的卸载请求, 并将其任务属性 $\mathbf{f}_j$ 发送到满足连接约束的服务器集合$\mathcal S_j$中以请求报价。

**卖方**是车联网中的边缘服务器。边缘服务器可以接收车辆的任务卸载请求，并利用其自身资源来完成任务卸载和获得收益。卖家服务器 $\mathbf s_i$ 的特征可以被定义为 $\mathbf{s}_i = (\mathbf{r}_i, c_i)$，其中资源向量 $\mathbf{r}_i$ 和成本 $c_i$。服务器在接收到报价请求后, 确定能否接受任务卸载, 如果可以接受则根据当前时刻对可用资源$\mathbf r_{s_i,t}$和任务特征$\mathbf f_j$的观测, 通过强化学习学习到的策略, 对任务进行报价.

为简化问题,本文做出以下基本假设：

- 假设时间是分时的，任务只能在一个时间段提交，并且任务时长为整数个时间段。
- 车辆可以根据自身状态，评估在拍卖和卸载的时间内的连接情况，得到任务的边缘服务器限制条件。
- 在卸载市场中存在信息缺失：
  - 卖方的特征和维护成本对买方不可见。增加拍卖的竞争性，提高买方的效用。
  - 没有卖方知道其他参与者的出价，也没有卖方相互勾结。
  - 买方的预算不能公开给卖方。
- 拍卖过程中，一次仅有一个任务参与，在任务拍卖结束后，进入本时隙的下一个任务的拍卖。防止卖方串通抬价。
- 一个买方只能把他的任务卸载给一个卖方，但一个卖方可以同时为多个买方提供服务。
- 任务卸载的结果对于车辆而言，其效用是一致的。

在基于云计算的车联网场景中，我们考虑了 $m$ 个卖家 $\mathcal S$ 和 $n$ 个买家 $\mathcal B$ 的情况。每个买家代表一个车辆，它通过自身的通信单元与边缘服务器进行通信，并且根据与边缘服务器的通信状况生成相应的限制条件，以便在需要进行任务卸载时选择合适的卖家。每个卖家代表一个边缘服务器，它可以通过有线链接与其他边缘服务器和互联网相连，以返回卸载结果。同时，边缘服务器也可以通过有线连接返回任务执行结果和传输与车辆之间的数据。算法1是反向拍卖过程的具体描述.

算法1：基于反向拍卖的任务卸载方法

输入：车辆集合$\mathcal B$，服务器集合$\mathcal S$，时间槽长度$\epsilon$

输出：任务卸载结果$\mathcal R$

1. 初始化：对于每个服务器$\mathbf s_i \in \mathcal S$，初始化其资源向量$\mathbf r_i$，成本$c_i$，报价策略$\pi_i$；初始化卸载任务集合$\mathcal U=\{\mathcal U_1,\dots,\mathcal U_T\}$。
2. 对于每个时间槽$t=1,2,\dots,T$，执行以下步骤：
   1. 对于服务器集群内的每一个服务器$\mathbf s_i \in \mathcal S$, 更新自己的资源使用情况和结算上一个时间槽的收益$p_{t-1,i}-c_i$
   2. 从$\mathcal U_t$中的按照时间顺序选择买家车辆请求$\mathbf b_j$，则执行以下步骤：
      1. 车辆评估自身的连接状态，并生成任务的服务器限制向量$\mathbf \sigma_j$。
      2. 车辆向满足限制条件的服务器集合$\mathcal S_j = \{\mathbf s_i | \forall \mathbf s_i \in \mathcal S, \sigma_{j,i} = 1\}$发送任务卸载请求$\mathbf f_j$。
      3. 对于每个服务器$\mathbf s_i \in \mathcal S_j$，如果其可用资源$r_{s_i}(t)$能够满足任务的资源需求$r_{\theta_j}(t)$，则执行以下步骤：
         1. 服务器根据自身在$[t,t+\epsilon]$时间段的资源使用情况$\mathbf r_{s_i,t}$和任务特征$\mathbf f_j$，以及报价策略$\pi_i$，计算出自己对任务的报价$q_{j,i}$。
         2. 服务器将报价$q_{j,i}$返回给车辆。
      4. 车辆收集所有服务器的报价$\mathcal Q_j = \{q_{j,i} | \forall \mathbf s_i \in \mathcal S_j\}$，并将其非降序排列。
      5. 车辆选择报价最低的服务器作为胜者$\mathbf s_k =\mathop{\arg\min}\limits_{\mathbf s_i}\ q_{j,i}$，并检查其报价$p_j = q_{j,k}$是否满足预算条件$p_j \leq \psi_j$。如果不满足，则本次拍卖失败，任务卸载失败；如果满足，则本次拍卖成功，任务卸载成功，并执行以下步骤：
         1. 车辆将任务发送到胜出者服务器$\mathbf s_k$并支付$p_j$，等待执行结果返回。
         2. 服务器$\mathbf s_k$获得收益, 为任务分配资源并执行任务卸载，更新自身的资源使用情况$\mathbf r_{s_i,t}$
      6. 更新任务卸载结果$\mathcal R$
3. 当到达总时间步$T$时间后, 卸载结束

### PPO算法

在上文中我们已经介绍了MDP建模和环境建模，本节我们将介绍一种基于策略梯度的强化学习算法——PPO算法[1]。PPO算法的目标是在与环境交互采样数据后，使用随机梯度上升优化一个“替代”目标函数，从而改进策略。PPO算法的核心思想是利用按照策略产生的执行轨迹来估计梯度，并使用经验回报或优势函数作为梯度的方向。PPO算法可以处理具有连续状态和动作空间的强化学习环境。

PPO算法的目标函数是由优势值进行缩放的新策略和旧策略之间的比率，即：

$$
J_{\theta}=E_{t}\left[\frac{\pi_{\theta}(a_{t}\mid s_{t})}{\pi_{{\theta_{\mathrm{old}}}}(a_{_t}\mid s_{t})}A^{\mathrm{GAE}(\gamma, \lambda)}_{t}\right]
$$

其中$\pi_{{\theta_{\mathrm{old}}}}$表示更新前的旧策略。这个目标与交叉熵方法有相同的原理：重要性采样。为了限制更新，PPO算法使用了裁剪后的目标：

$$
J_{\theta}^{\mathrm{clip}}=\mathbb{E}_{t}\left[\min(r_{t}(\theta)A^{\mathrm{GAE}(\gamma, \lambda)}_{t},\mathrm{clip}(r_{t}(\theta),1-\varepsilon,1+\varepsilon)A^{\mathrm{GAE}(\gamma, \lambda)}_{t})\right]
$$

此目标将新旧策略之间的比率限制在$[1–ε, 1+ε]$区间内，因此通过更改$ε$，我们可以限制更新的大小。这种裁剪方法可以避免使用KL散度惩罚或自适应更新，从而简化了算法。

PPO算法有两种主流的实现方式，一种是使用KL散度作为惩罚项来限制策略更新的幅度，另一种是使用剪裁函数来限制新旧策略之间的比率。PPO算法还使用了一种更一般的优势函数估计方式，即GAE（Generalized Advantage Estimation）。GAE是由John Schulman等人在2015年的论文[2]中提出的。GAE的基本思想是利用TD($\lambda$)方法来平衡偏差和方差，同时减少优势函数的方差。GAE的定义如下：

$$
A^{\mathrm{GAE}(\gamma, \lambda)}_{t}=\sum_{l=0}^{\infty}(\gamma \lambda)^{l} \delta_{t+l}
$$

其中$\delta_t$是TD误差，即$\delta_t = R_t + \gamma V_\theta(S_{t+1}) - V_\theta(S_t)$。可以看出，GAE是对TD误差的加权累加，其中权重随着$l$的增加而指数衰减。当$\lambda=0$时，GAE退化为一步TD误差；当$\lambda=1$时，GAE退化为MC估计。因此，通过调节$\lambda$的值，我们可以在偏差和方差之间找到一个平衡点。GAE还有一个重要的性质是，它可以消除优势函数的常数偏移量。


PPO算法从环境中获取较长的样本序列，然后用于执行多个训练周期。PPO算法在实现简易性、样本复杂度和调整简易性之间取得了平衡，在每一步都试图计算一个最小化代价函数的更新，同时保证与前一策略的偏离相对较小。PPO算法在各种强化学习任务上表现出了优异的性能。

为了解决第三章提出的车联网环境中的任务卸载问题，我们采用以上描述的PPO算法。这种算法可以根据任务的需求和优先级，直接学习最优策略，而不需要像Q-Learning等基于值函数的算法那样估计状态-动作值。

我们使用多智能体强化学习的框架，让每个边缘服务器都部署一个PPO代理，根据自己的状态和动作来更新自己的策略。这样，不同的服务器之间的策略可以根据任务需求和市场竞争进行自适应调整，从而提高自身的收入和车辆的效用。同时，多智能体强化学习也可以处理多个智能体之间的交互和博弈，使得每个服务器能够考虑其他服务器的行为和影响。为了简化模型，我们假设服务器之间没有额外的通信，而且只进行一次博弈。

接下来，我们将介绍我们的强化学习仿真环境的具体设计, 展示我们的实验结果和分析，比较PPO算法与其他基准算法的性能和效率。



[1]: [Proximal Policy Optimization Algorithms]
[2]: [High-Dimensional Continuous Control Using Generalized Advantage Estimation]

- for iteration=1,2,... do
  - for actor=1,2,... do
    - 在环境中运行策略 $\pi_{\theta_{old}}$ , 进行 $T$ 个时间步
    - 计算优势估计 $\hat{A}_1, \dots, \hat{A}_T$
  - end for
  - 优化代理 $L$ 关于 $\theta$, 通过 $K$ epochs 和 minibatch size $M\le NT$
  - $\theta_{old}\gets \theta$
- end for

### 环境建模

为了实现本论文中多智能体强化学习的环境建模, 我们使用了Pettingzoo最为我们实现自定义环境的接口. Pettingzoo是一个用于表示一般的多智能体强化学习（MARL）问题的Python库，它包括了多种参考环境、有用的工具和创建自定义环境的方法。Pettingzoo支持Agent Environment Cycle（ACE）模式，它是一种适用于顺序的回合制环境的接口，能够处理任何MARL可以考虑的游戏。在ACE模式下，每个智能体都有自己的观察空间和动作空间，而且只有一个智能体可以在每个时间步骤中执行动作。用户可以使用以下方法来与ACE模式的环境进行交互：每一个step开始时使用env.agent_iter()的到当前需要执行动作的agent, 通过env.last()返回最后一个智能体的观察、奖励、终止、截断和信息, 利用返回的信息生成action, 通过env.step(action)执行一个动作并更新环境状态,直到所有的智能体都完成了他们的回合或者游戏结束。

为了利用多智能体强化学习求解这个优化问题，我们建立了如下多智能体强化学习模型：

1. 初始化任务生成模型参数，边缘服务器集群 S。
2. 对于每个时间槽 t，执行以下操作, 直到满足停止条件：
   1. 使用任务生成模型生成在本时间槽中需要卸载的所有任务$T_t$.
   2. 集群中每一个服务器更新资源使用情况, 向车辆返回结束的任务的执行结果, 结算执行完毕的任务获得奖励和维护费用.
   3. 从$T_t$中的选择一个没有参加过拍卖任务$task_i$, 执行如下操作, 直至全部任务都进行拍卖:
      1. 将$task_i$相关信息发送到满足限制的服务器集合$a_i$.
      2. 对于每一个在$a_i$中的边缘服务器$S_j$,执行如下操作,直到所有服务器完成报价.
         1. $S_j$收到任务请求后，根据自己的观测，得到动作, 将报价返回拍卖商.
      3. 拍卖商收到所有服务器的报价后，执行反向拍卖过程，并向车辆通知卸载结果和支付费用。
      4. 车辆收到拍卖结果，检查是否满足其预算要求
         1. 如果满足预算需求, 则同意卸载，并在任务结束后进行支付
            1. 将车辆任务发送到边缘服务器
            2. 边缘服务为任务预留相关资源并开始执行, 更新资源的使用情况.
         2. 如果不满足，则不进行任务卸载,不进行支付.
   4. 当$T_t$中所有任务都进行拍卖后, 进入下一个时间槽$t+1$
3. 输出最终的环境状态，并结束环境。

#### 任务生成模型

完成对任务的建模后, 为模拟真实的任务调度场景, 接下来需要对任务的生成进行建模. 任务的生成一共由三部分组成, 一个时间步内任务的数量, 每个任务对资源的需求, 任务可卸载的服务器列表.

- 一个时间步的任务数量, 本文将其简化为符合泊松分布的随机变量, 每一回合从分布中抽取一个正整数作为该时间步的任务数量.
- 每个任务对资源的需求, 我们将任务的资源需求和执行时间都取自由两个正态分布组合而成的双峰分布, 因为通过分析现实的计算机集群工作负载现实世界中卸载的任务以小任务或大任务居多, 如果以简单的正态分布或者指数分布(负指数分布)可能无法准确建模, 另外选择这样的分布也是为了检验神经网络能否拟合复杂分布.
- 任务可卸载的服务器列表, 由于车联网中的车辆存在动态性, 位置存在随机性, 因此我们将车辆与边缘服务器之间的连接抽象成一个可连接服务器的集合. 由于服务器的位置是固定的, 因此其可连接的车辆范围也是固定的. 为在模型中体现车联网的连接性质, 在生成执行服务器限制时, 我们首先生成一个连接范围限制, 然后从连接范围限制的服务器集合中随机抽取若干服务器作为服务器限制集合. 这时的任务可卸载约束仅有连接所限制,属于服务器的无线设备自发无目的的向其范围内车辆发送的连接检测过程,这时服务器仍然未知任务的具体特征, 因此执行约束不在任务生成模型的建模之中.

因此本次研究系统模型中的任务生成过程如下.

- 设定一个时间步的长度为 $\Delta t$，比如 $\Delta t=1$ 秒或者 $\Delta t=0.1$ 秒；
- 在每个时间步 $n$ 内，根据泊松分布 $Poisson(\lambda)$ 生成一个正整数 $m$，表示这个时间步内有 $m$ 个任务被车辆提交到边缘服务器，其中 $\lambda$ 是泊松分布的参数，表示单位时间内任务到达的平均速率；
- 对于每个任务 $task_i$，根据双峰分布 $Bimodal(\mu_1,\sigma_1,\mu_2,\sigma_2,w)$ 生成它的资源需求和执行时间，表示这个任务需要多少计算资源和存储资源以及执行多长时间，其中 $\mu_1,\sigma_1,\mu_2,\sigma_2$ 是两个正态分布的均值和标准差，$w$ 是两个正态分布的权重；
- 根据车辆的位置和边缘服务器的位置和连接情况，生成一个可连接服务器的集合 $a_i$，表示这个任务可以被发送到哪些边缘服务器；
- 根据车辆的偏好或者其他因素，生成一个预算 $b_i$，表示这个任务的最高支付费用；
- 这样，我们就得到了一个完整的任务 $task_i=(n_i,p_i,r_i,d_i,t_i,a_i,b_i,f_i)$；
- 将这个时间步内生成的所有任务组成一个任务集合 $T_n=\{task_k,task_{k+1},...,task_{k+m}\}$，表示这个时间步内有 $m$ 个任务被车辆提交到边缘服务器。
- 重复这个过程，直到生成所有任务或达到停止条件。




为了解决第三章提出的车联网环境中的任务卸载问题，我们采用以上描述的PPO算法。这种算法可以处理具有连续状态和动作空间的RL环境，而不需要像Q-Learning等基于值函数的算法那样估计状态-动作值，而是直接学习最优策略。

在我们的车辆任务卸载场景中，我们自定义的RL环境将提供与真实车联网场景相似的任务特征，包括任务的类型、大小、持续时间等。此外，边缘服务器的资源也会被考虑，因此CPU和内存的可用性也将作为状态空间的一部分被使用和更新。

我们使用多智能体强化学习的框架，让每个边缘服务器都部署一个PPO代理，根据自己的状态和动作来更新自己的策略。这样，不同的服务器之间的策略可以根据任务需求和市场竞争进行自适应调整，从而提高自身的收入和车辆的效用。同时，多智能体强化学习也可以处理多个智能体之间的交互和博弈，使得每个服务器能够考虑其他服务器的行为和影响。为了简化模型，我们假设服务器之间没有额外的通信，而且只进行一次博弈。

我们使用RLlib来实现和管理训练过程，它可以并行地从环境中采集数据，汇总到一个训练批次中，然后根据选择的算法和损失函数计算梯度，并更新模型参数。我们重复这个过程，直到满足终止条件，如达到最大迭代次数、最大训练时间或最佳性能等。在训练过程中，RL模拟反馈回路不断地收集数据，包括观察到的状态、采取的动作、获得的奖励和所谓的结束标志，表示代理在模拟中经历的不同的回合。每次PPO代理采取一个动作（报价一个任务），都会得到一个即时奖励。下一个状态也将依赖于前一个状态，因为CPU和内存的可用性将在每次任务卸载后进行更新。最终，PPO代理应该能够学习任务的需求和优先级，并完成所有任务的报价，以最大化自身的收入和车辆的效用。



# 多智能体强化学习Training Algorithm

为使用MADRL解决车联网中的任务调度问题, 我们使用了两类基于DRL的算法, 分别为Value-based 和 Policy-based. 为同时测试这两类方法, 我们使用了离散的和连续的两种动作空间. 我们假定环境中的固定参数都相同, 每一个边缘服务器具有一个Agent来根据策略$\pi_i(a_i|s_i)$来确定动作.

![[Pasted image 20230719095250.png]]

深度强化学习的工作流程如下：首先，它创建一组rollout workers，每个worker可以并行地从环境中采集数据，并使用一个或多个策略来与环境交互。每个worker生成的数据包含一系列的动作、奖励、下一个状态和结束标志，这些数据构成了一个episode或rollout。然后，RLlib将所有worker生成的rollout汇总到一个训练批次中，接着根据选择的算法和损失函数计算梯度，并更新模型参数。最后，RLlib重复这个过程，直到满足终止条件，如达到最大迭代次数、最大训练时间或最佳性能等 。

- RLlib使用rollout workers来并行地从环境中采集数据，每个worker可以使用一个或多个策略来与环境交互，产生一系列的动作、奖励、下一个状态和结束标志，这些数据构成了一个episode或rollout。
- RLlib将每个worker产生的rollout汇总到一个训练批次中，然后根据选择的算法和损失函数计算梯度，并更新模型参数。
- RLlib重复这个过程，直到满足终止条件，如达到最大迭代次数、最大训练时间或最佳性能等。



---

**行动空间。** 行动空间是一个智能体可以在环境中执行的动作集合。我们假设每个智能体的行动空间是在范围$[1,2]$内的数值，表示其用于竞标任务的浮点系数。这意味着每个智能体可以选择范围$[1,2]$内的任意实数作为其竞标系数$a_i$。这为智能体提供了更多调整其竞标策略的自由度和灵活性。我们可以使用范围$\mathbf A_i$来表示第$i$个智能体的行动空间，如下所示：$\mathbf A_i=[1,2]$ 当环境从智能体$i$处接收到$a_i$的值时，那么它对于当前等待拍卖的任务$\theta_j$的最终出价为：$q_i = a_i \times \mathbf p \cdot \mathbf r_j \times \tau_j$ 其中$\mathbf p$是资源的市场平均价格向量，$\mathbf r_j$是资源使用情况，$\tau_j$是资源使用时间。然而，并非所有服务器都可以对任何任务进行竞标，因为任务有执行服务器限制$\mathcal S_j \subseteq \mathcal S$，这意味着只有在$\mathcal S_j$中的服务器才能满足任务$j$的要求。因此，在时间步$t$中，整个系统的行动空间是：$A_t=\{a_1\sigma_{j,1},\dots,a_m\sigma_{j,m}\}$ 其中$a_i\sigma_{j,i}$表示第$i$个服务器对当前被请求的任务$\theta_j$的竞标系数。如果服务器不符合任务$\theta_j$的执行服务器条件，则其相应的竞标系数为0，表示它无法采取任何行动。在对$\theta_j$任务进行竞标时，环境仅收集满足任务条件的服务器的竞标，并忽略其他服务器的竞标。

## 实验

计算卸载是一种在车联网中提高车辆性能和节省能耗的技术，它让车辆把部分计算任务卸载到边缘服务器上。计算卸载的目标是最大化车辆的效用，同时保证边缘服务器的收益和用户的体验。为此，边缘服务器需要设计合适的竞价策略，即根据任务的需求、紧急性和自身的资源等约束，决定每次报价的金额。

学习竞价策略涉及复杂的环境建模、不完全信息和动态优化问题，其验证方法选择至关重要。离线验证和在线验证是常见方法，但二者各有优缺点。离线验证基于历史数据进行反事实估计，但结果可能受数据偏差、反事实假设以及环境变化等因素的影响，甚至可能不符合古德哈特定律。相比之下，在线验证通过真实环境中的实验直接测试竞价策略的表现，更可靠，然而成本高、耗时且存在一定风险。

强化学习作为一种通过智能体与环境交互学习最佳行为的方法，在游戏、机器人、推荐系统等领域具有广泛应用前景。尽管其在实践中取得成功，研究社区明确指出其方法不足，并认为可靠的仿真环境是近年的重大进展核心。仿真环境提供接近真实环境但不受其限制和影响的平台，使研究者能快速开发、测试和改进强化学习算法。在某些相关领域，如推荐系统，仿真环境已广泛采用作为有效的评估机制。

针对车联网环境中计算卸载的服务器报价策略的验证，我们依据上述模型开发了相应的强化学习环境。在竞价策略学习过程中，我们利用强化学习辅助边缘服务器根据任务卸载特征和自身约束，动态决定每次出价，以最大化效用或收益。

### 仿真环境的设计和参数设置

为了验证提出的基于多智能体强化学习和反向拍卖机制的车联网任务卸载方法（RATO）的有效性，我们开发了一个名为VehicleJobScheduling的基于Python的开源仿真环境。该仿真环境旨在模拟车辆任务卸载的过程，并提供一系列常用的评估指标。在设计中，我们借鉴了PettingZoo，这是一个广受欢迎的多智能体强化学习框架，其定义了一套标准接口，使得不同算法可以在同一环境中进行比较。

VehicleJobScheduling仿真环境主要由三个组成部分构成，即车辆、服务器和环境生成器。在这个环境中，车辆充当任务的发起者，根据资源需求、连接限制和预算选择适当的服务器进行任务卸载，并支付相应费用。服务器则是任务的执行者，根据资源状况和成本，对收到的任务卸载请求进行报价，并执行被接受的任务。环境生成器负责生成任务和服务器的参数，包括任务的到达率、特征、连接限制，以及服务器的数量、类型、资源向量、成本等。用户可以通过设置这些参数，探索不同的任务卸载场景和策略。

我们的仿真环境支持Agent Environment Cycle（ACE）模式，这是一种适用于顺序回合制环境的接口，能够处理任何多智能体强化学习算法。在ACE模式下，每个智能体代表一个边缘服务器，接收车辆的任务卸载请求，利用自身资源完成任务卸载并获取收益。在每个时间步，有一个或多个车辆向满足其连接限制的服务器集合发送任务卸载请求，并等待服务器返回报价。环境按照连接限制选择一个服务器作为当前回合的行动者，该服务器根据自身资源使用情况、任务特征和报价策略计算任务报价，返回给车辆。当所有满足连接限制的服务器完成报价后，车辆选择一个或多个报价最低、同时满足需求和预算条件的服务器进行交易。交易成功后，车辆将任务发送到胜出者服务器并支付相应金额，等待执行结果返回。服务器获得收益，为任务分配资源并执行任务卸载。如果没有服务器的报价满足车辆的条件，则车辆无法完成任务卸载，无需支付任何金额。

为了模拟车联网场景，我们考虑了一个由多个车辆和多个边缘服务器构成的公路仿真环境。在这个环境中，车辆会对服务器信道进行评估，并向满足信道条件的边缘服务器发送任务卸载请求。我们假设任务请求的数量符合泊松分布，单位时间的任务到达数为20，涵盖了三种任务类型：存储密集型、计算密集型和存储计算密集型，以及两种任务时长：长任务和短任务。任务的持续时间被设定为1到10个时间槽，每个时间槽为1分钟，因此每个任务卸载请求的最大执行时间为10分钟。任务的优先级被设置在0到10之间，而任务对CPU和内存的请求数量最大分别为24和100。任务的连接限制范围为0到3，表示任务可以向满足信道条件的0到3个服务器发送任务卸载请求。

我们对车辆的任务卸载请求进行了抽象，将其表示为任务特征，其中包含四个维度：优先级、资源需求、持续时间以及任务连接限制。任务的优先级在0到10之间取值，并且它影响车辆对任务的预算分配。任务对CPU和内存的请求数量最大分别为24和100。任务的持续时间被设定为1到10个时间槽，每个时间槽为1分钟，因此每个任务卸载请求的最大执行时间为10分钟。任务的连接限制范围为0到3，表示任务可以向满足信道条件的0到3个服务器发送任务卸载请求。

在实验中，我们选择了[Azure的专用主机](https://learn.microsoft.com/zh-cn/azure/virtual-machines/dedicated-hosts)作为边缘服务器，提供多种规格和价格的虚拟机。我们选取了三种不同的专用主机SKU，分别代表大、中、小型的边缘服务器。云服务提供商的成本是按照三年计划的价格计算，而市场价格则是按照Azure美国东部[容器实例](https://azure.microsoft.com/zh-cn/pricing/details/container-instances/)的价格计算。表1列出了我们选用的专用主机的规格和价格。

| Dedicated Host SKUs | vCPUs | RAM       | 3 year plan |
| ------------------- | ----- | --------- | ----------- |
| Mdsv2MedMem-Type1   | 192   | 2,048 GiB | $ 5.81      |
| Easv5-Type1         | 112   | 768 GiB   | $ 3.49      |
| Fsv2-Type2          | 72    | 144 GiB   | $ 2.05      |

表1 选用的专用主机

| 参数        | 取值范围             |
| --------- | ---------------- |
| 任务到达率分布   | 泊松分布             |
| 单位时间任务到达数 | 20               |
| 任务时长      | 1到10个时间槽，每个槽为1分钟 |
| 任务优先级     | 0到10             |
| 任务CPU请求数  | 最大为24            |
| 任务内存请求数   | 最大为100           |
| 任务连接限制范围  | 0到3              |
| 表 环境超参数   |                  |

| Agent Model 类型  | PPO Agent     | PPO LTSM Agent        |
| --------------- | ------------- | --------------------- |
| 网络结构            | 多层感知机（MLP）    | 多层感知机 + 长短时记忆网络（LSTM） |
| 全连接层参数          | [384,384,384] | [384,384,384]         |
| LSTM单元大小        | -             | 256                   |
| 激活函数            | tanh          | tanh                  |
| 表 强化学习 Agent 参数 |               |                       |

在本文中，我们使用了两种不同的强化学习Agent的Model，分别是PPO Agent和PPO LTSM Agent，它们都是基于PPO（Proximal Policy Optimization）算法的，但是在网络结构上有所不同。PPO Agent的Model是一个多层感知机（MLP），它由三个全连接层组成，每层的神经元数量为384，激活函数为tanh。PPO LTSM Agent的Model则是在PPO Agent的Model的基础上增加了一个LSTM层，该层的单元大小为256，也就是每个LSTM单元内部状态向量的维度为256。LSTM层的作用是增加Model的记忆能力，使其能够处理和预测具有时间相关性的数据。我们将这两种Model分别应用于RATO方法，以比较它们在不同场景下的性能表现。

这样的设定使得我们能够在仿真中考虑多样化的车联网场景，涵盖了不同类型和时长的任务，以及车辆与边缘服务器之间的动态交互。接下来，我们将通过对RATO方法在这个仿真环境下的实验进行数值分析，全面评估其在不同场景下的性能表现。我们将特别关注不同服务器报价策略对系统效果的影响，以深入了解提出方法的优势和适用性。

### 深度强化学习代理的收敛性

![[Pasted image 20231128145930.png]]

在本节中，我们分别评估了两种使用PPO强化学习代理的方法, PPO和PPO+LTSM在车联网资源卸载问题中的收敛性能。我们使用了两种基于多智能体强化学习的算法，分别是PPO和PPO+LTSM，来训练多个代理，以最大化服务器在一个回合内的收益。服务器的收益反映了任务卸载的效果，也是代理的奖励函数。我们对代理进行了4480000次迭代的训练，并在每40000次迭代后对算法进行了评估，计算了训练策略在10次测试运行中的平均奖励。图1展示了两种算法在训练过程中的奖励评估情况。表1显示了PPO和PPO+LTSM两种算法在最后448000次迭代中的奖励值的平均值和标准差。

| 算法                                | 平均奖励值       | 标准差         |
| --------------------------------- | ----------- | ----------- |
| PPO                               | $1.68×10^7$ | $2.65×10^5$ |
| PPO+LTSM                          | $1.72×10^7$ | $1.12×10^5$ |
| 表 PPO和PPO+LTSM训练过程中最后10%奖励值方差和平均值 |             |             |

我们可以看到，PPO+LTSM的平均奖励值为$1.72×10^7$，比PPO的$1.68×10^7$高出约**2.4%**。同时，PPO+LTSM的标准差为$1.12×10^5$，比PPO的$2.65×10^5$低出约**57.7%**。这说明PPO+LTSM的奖励值不仅更高，而且更稳定，波动更小。因此，PPO+LTSM在车联网资源卸载方面有更好的收敛性能。综上所述，RATO方法在不同的任务卸载场景下，都表现出了较好的收敛性，能够快速地学习到最优或接近最优的报价策略，从而提高服务器的收益和车辆的效用。这证明了RATO方法的有效性和鲁棒性，以及强化学习和反向拍卖机制的优势和适用性。

### 实验结果及分析

本文提出了一种基于多智能体强化学习和反向拍卖机制的车联网任务卸载方法，简称为RATO，并通过仿真实验与其他基准方法进行了对比分析。本文考虑了任务到达数目为10、20和40的三种情况，采用了四种不同的服务器报价策略：固定报价（FB）、随机报价（RB）、基于PPO的报价（PPO）以及基于PPO+LSTM的报价（PPO+LSTM）。本文采用了四个指标来评估不同的调度策略在不同的到达数目下的表现，分别是：负载均衡（Load Balance）、服务器的收益（Server Earning）、车辆的利用率（Vehicle Utility）和订单的完成率（Complete Rate）。在每个到达数目下，进行了十次独立的仿真实验，利用五个随机种子进行模拟，并对每个指标的平均值和标准差进行了统计和分析。

本文的目的是比较不同报价策略在任务卸载系统中的性能，并分析RATO方法的优势。我们考虑了以下四种报价策略：

- **Fixed策略**：在此策略中，服务器以市场价格为基准，仅参考任务的特征和时长来确定报价。
- **Random策略**：服务器以市场价格为基准，在一定区间内随机采样一个系数来进行报价。
- **PPO策略**：服务器利用强化学习算法PPO，根据自身资源使用情况和任务特征，通过多层感知机（MLP）动态地确定报价。
- **PPO+LSTM策略**：在PPO策略的基础上，服务器增加了一个长短期记忆网络（LSTM）层，以捕捉时序信息，并结合多层感知机进行动态报价。

|          | Load Blance | Server Earning (10^6) | Vehicle Utility (10^6) | Complete Rate |
| -------- | ----------- | --------------------- | ---------------------- | ------------- |
| Random   | 0.19±0.012  | 0.55±0.00037          | 0.55±0.037             | 0.76±0.014    |
| Fixed    | 0.20±0.005  | 1.60±0.00007          | 0.76±0.047             | 0.91±0.010    |
| PPO      | 0.20±0.011  | 1.49±0.00004          | 0.78±0.036             | 0.92±0.010    |
| PPO+LTSM | 0.18±0.010  | 1.53±0.00004          | 0.83±0.063             | 0.93±0.012    |

表 到达数为10时的实验结果

|          | Load Blance | Server Earning (10^6) | Vehicle Utility (10^6) | Complete Rate |
| -------- | ----------- | --------------------- | ---------------------- | ------------- |
| Random   | 0.22±0.007  | 2.74±0.00036          | 0.99±0.033             | 0.72±0.008    |
| Fixed    | 0.23±0.004  | 3.05±0.00014          | 1.43±0.040             | 0.90±0.005    |
| PPO      | 0.22±0.007  | 2.95±0.00011          | 1.53±0.060             | 0.91±0.007    |
| PPO+LTSM | 0.21±0.006  | 3.06±0.00009          | 1.62±0.036             | 0.93±0.009    |

表 到达数为20时的实验结果

|          | Load Blance | Server Earning (10^6) | Vehicle Utility (10^6) | Complete Rate |
| -------- | ----------- | --------------------- | ---------------------- | ------------- |
| Random   | 0.22±0.006  | 4.80±0.00100          | 1.64±0.034             | 0.63±0.007    |
| Fixed    | 0.21±0.004  | 4.73±0.00071          | 2.26±0.034             | 0.73±0.010    |
| PPO      | 0.21±0.003  | 4.68±0.00077          | 2.29±0.054             | 0.72±0.009    |
| PPO+LTSM | 0.20±0.006  | 4.66±0.00075          | 2.34±0.044             | 0.73±0.009    |

表 到达数为40时的实验结果

为展示不同策略在不同任务到达数下的表现，将仿真实验结果以表格和图形形式呈现。表1、表2和表3给出了任务到达数为10、20和40时各指标的平均值和标准差。

在横向比较中，我们深入研究了四种不同的报价策略，以全面评估它们在各项指标上的表现。PPO+LTSM策略在各项指标上表现出色，例如，在到达数为20的场景下，其在负载均衡、服务器收益、车辆效用和任务完成率方面分别为**0.21±0.006**、**3.06±0.00009**、**1.62±0.036**和**0.93±0.009**, 高于其它所有策略。Fixed策略在服务器收益上表现良好，但在负载均衡, 车辆效用和任务完成率方面较差, 这意味着该策略在用户满意度上较差。Random策略几乎在所有指标上表现最差。PPO策略虽然在服务器收益上略逊于Fixed策略，但在其他指标上表现更好, 与PPO+LTSM相比, 四项指标的方差均更高, 说明PPO的策略相对更加不稳定。总体来说，PPO+LTSM策略在长期报酬方面表现最优。

在进行纵向比较时，我们详细观察了不同任务到达数下的四种报价策略。随着任务到达数的增加，服务器的收益和利用率普遍上升，但订单完成率却呈下降趋势，这可能是因为服务器过载而无法接收更多任务。在任务到达数为10的情况下，由于任务数相对较少，各策略在负载均衡和订单完成率上表现相似，随机策略除外。PPO+LSTM策略在车辆效用和负载均衡方面略显优势，显示出更好的适应性和灵活性。Fixed策略的收入较高可能是因为其固定报价符合大多数任务的预算，并且没有服务器间的竞争。当任务到达数增至20时，随着任务数量的增加，PPO+LSTM策略在各项指标上均超越其他策略。此时的负载均衡值相较于任务数为10和40时更高，这表明随着任务到达率的增加，服务器间的竞争变得更加激烈，可能导致负载不均衡。然而，随着任务到达率的进一步增加，服务器的负载保持在较高水平，因此负载相对均衡。在任务到达数为40的大规模任务情境下，PPO+LSTM策略继续保持领先，实现了更好的负载均衡、车辆效用和任务完成率。随机策略在这种情况下获得了最高收入，这可能是因为其随机性使其能够适应不同的任务和竞争环境，从而更有可能匹配到更多高价任务，同时避免了过度竞争和价格战。然而，需要注意的是，随机策略在长期内可能不具备稳定性，而在某次实验中取得较好结果可能仅是随机性的结果。在真实应用中，具备更强适应性和稳定性的策略更为可取。总体而言，不同任务到达数下，PPO+LTSM策略在负载均衡和车辆效用方面表现最佳，展现了更好的适应性和性能。其能够通过学习最大化长期报酬，更好地平衡系统各项指标，为不同任务到达数下的系统性能提供均衡的优化。

综上所述，本文通过仿真实验对比了不同报价策略在不同任务到达数下的表现，并分析了PPO+LTSM策略的优势。研究表明，PPO+LTSM策略能够利用LTSM捕捉时序信息，做出更合理的决策，实现最有效的负载均衡。

本文提出了一种车联网任务卸载方法，它用多智能体强化学习和反向拍卖机制，旨在通过让边缘服务器以合适的价格接收车辆任务，提高任务完成率和车辆效用。，相较于其他方法，本文提出的方法不仅效果更好，而且更为稳定。然而，需要注意的是，本文存在一些局限性，例如未考虑通信可靠性、信道增益的不确定性以及参与者的合作性。未来的工作将着重解决这些问题，以使任务卸载更为可靠、高效和公平。在论文初期的实验中，我对比了几种强化学习方法和网络结构，发现它们存在一些缺陷，影响了学习和决策的质量。最终，我选择了PPO+LSTM策略，因为它能够弥补其他方法的不足，达到了合适的效率和稳定性。未来的工作还包括在更真实的车联网环境中考虑各种影响因素，如车辆的移动性、网络的延迟和带宽等，以提高系统的可靠性。同时，将尝试更多的强化学习算法和模型，并深入研究更多的拍卖机制和规则，如组合拍卖、VCG拍卖等。总体而言，本文提出的基于多智能体强化学习和反向拍卖的车联网任务卸载方法解决了车辆和边缘服务器之间的协作和竞争问题，从而提高了任务的效率和质量。该方法具有创新性和实用性，为车联网的发展和应用提供了新的思路和方案。



