---
aliases: 多智能体强化学习Training Algorithm, '多智能体强化学习Training Algorithm'
UID: 20240223155931
source: null
cssclass: null
created: 2024-02-23 15:59
updated: 2024-02-23 16:24
date updated: 2024-02-23 16:25
---

- 问题陈述
- 系统模型
- 方法
- 实验

## 问题陈述

### MEC 车联网资源卸载架构

MEC 车联网任务卸载是指将车辆上的计算密集或延迟敏感的任务转移到边缘服务器或云服务器上进行处理，以节省车辆的能量消耗和提高任务的执行效率。不同的卸载目标和场景可能需要不同的卸载策略和方法。 车辆与车辆之间的卸载, 可以利用车辆的闲置资源提供服务, 但是任务的卸载主要取决于用户本身的意愿, 其次才是车辆的资源情况, 而一般情况下为了保证用户的体验,我们无法替用户做出决定, 所以车辆与车辆之间的任务卸载不适用于自动调度的场景. 车辆与云服务器之间的任务卸载,利用云服务器强大的计算能力和存储空间,可以处理一些数据量大或计算复杂度高的任务, 但是由于车辆与云服务器之间的延迟较大, 仅适合一些对延迟要求不高的任务,这些任务特征比较明显,其调度可以由车辆自身的调度进行决定. 本次研究的车联网资源卸载模型主要面向车辆与边缘服务之间的资源卸载, 车辆将任务卸载到距离较近的边缘服务器进行计算, 这种方式可以减少任务执行延迟和汽车的能量消耗, 适用于对计算延迟要求较高并且对算力要求较高的任务.例如 自动驾驶,实时控制等应用场景.

由于卸载到边缘服务器所需要实时性较高, 资源需求也较高, 并且服务器的服务意愿也较强, 因此可以使用无人监督的机制来进行资源分配, 提升用户体验和降低服务器管理成本. 为简化本次车联网资源卸载的模型, 本文只考虑车辆向边缘服务器卸载这一种车联网资源卸载范式. 在这种架构中. 车辆通过无线网络与公路边上的边缘服务器进行资源卸载.车辆作为资源请求的发起方, 将通过无线网络与其连接范围内的边缘服务器建立连接, 而边缘服务器固定在道路两旁, 通过评估车辆连接状态情况确定能否接受资源卸载. 当提出卸载请求的车辆收集到边缘服务器连接状态后, 向可接受资源卸载的服务器提交任务请求. 接收到任务请求的边缘服务器根据自身资源利用情况向提出请求的车辆进行报价. 车辆在接受到各个服务器的报价之后决定卸载的边缘服务器和支付的价格. 随后车辆对任务进行卸载, 在任务执行完成后, 车辆向边缘服务器进行支付, 整个车联网资源卸载过程结束.

### 系统模型

为了研究车联网任务调度问题, 我们需要根据上述的MEC车联网资源卸载架构对整个车联网任务调度系统进行建模. 建模内容包含任务生成模型, 边缘服务器集群模型和反向拍卖模型.我们建立了一个任务在每一个时刻数量泊松分布, 资源请求符合双峰分布[^双峰分布]的任务生成模型. 共有$n$ 个在路边并具有无线连接能力的边缘服务器模型和一个反向拍卖的模型. 系统首先由任务生成模型生成这一时间段所需要处理的任务, 任务的数量和资源请求符合相应的分布, 随后根据连接限制条件生成任务执行的服务器限制. 随后系统将生成任务集合的任务派发服务器集群模型, 服务器集群模型将任务请求信息发送到满足服务器限制的边缘服务器. 边缘服务器根据任务相关信息和自身资源利用情况向反向拍卖模型发送自身对任务的报价, 反向拍卖模型在收集服务器返回的报价后决定卸载服务器和支付, 车辆在接受到支付信息后,与自身预算进行比较,如果超出预算,则终止任务卸载,由车辆自身执行任务. 最后, 如果车辆决定向服务器进行任务卸载, 则由边缘服务器执行任务, 任务执行完毕后边缘服务器得到相应的支付.

[^双峰分布]: <https://en.wikipedia.org/wiki/Multimodal_distribution>, 等待补充

#### 任务模型

在车联网资源卸载的过程中，车辆可能向边缘服务器提交各种不同的任务请求，例如计算密集、I/O密集或者两者兼有，不同的任务持续的时间有不同的特征。为了简化模型，我们不再对车辆进行单独建模，因为车辆所提交的任务与车辆本身的性质无关，只与任务本身的属性有关。因此，我们选择对任务的关键信息进行建模。我们用 $task_i=(C_i,A_i)$ 来表示一个任务，其中 $C_i=(p_i,r_i,d_i)$ 是任务的特征，$A_i=(n_i,t_i,a_i,b_i,f_i)$ 是任务的属性。任务的特征和属性的含义如下表所示：

任务的优先级 $p_i$，反映了任务的紧急程度和重要性，它是一个介于0和1之间的实数，越接近1表示越高的优先级；任务的资源需求 $r_i$，表示任务执行所需要的资源，例如计算资源和存储资源，它是一个二元组 $(c_i,s_i)$，其中 $c_i$ 是计算资源需求，$s_i$ 是存储资源需求；任务的持续时间 $d_i$，表示任务从开始到结束所花费的时间；任务的编号 $n_i$，用于区分不同的任务；任务的发起时间 $t_i$，表示任务从车辆发送到边缘服务器的时间点；任务的执行服务器限制 $a_i$，表示任务可以从车辆发送到那些服务器，它是一个服务器的集合，包含了所有能够满足任务连接需求的服务器；任务的预算 $b_i$，表示车辆愿意为任务执行支付的最高费用；最后，任务的最终支付 $f_i$，表示车辆实际为任务执行支付的费用。

我们将所有车辆提交的任务组成一个列表 $\mathcal{T}=\{T_1,T_2,\dots,T_N\}$，其中 $N$ 是总的时间步数, $T_i​$ 是第 i 个时间步中车辆发起的所有任务的集合。为了简化问题，我们假设在每个时间步 $i$ 开始时，只有一个任务参与拍卖，然后根据拍卖结果分配执行服务器。当 $T_i$ 中的所有任务都参与过拍卖后，环境进入下一个时间步 $i+1$，继续进行拍卖, 直到最后一个时间步$N$。我们的目标是设计一个任务报价策略，使得每个边缘服务器能够在满足任务的执行服务器限制 $a_i$ 和预算 $b_i$ 的前提下，最大化每个边缘服务器的收益 $r_s = \sum_{i=1}^N p_{i,s} - c_{i,s}$。其中 $r_s$ 是边缘服务器s的收益，$p_{i,s}$ 是第i个时间步中结束任务的支付，$c_{i,s}$ 是服务器每回合的维护成本。

我们假设每个服务器都可以与其他服务器进行通信，并且可以接受来自外部或内部的任务请求。我们用 $\mathcal{T}=\{T_1,T_2,\dots,T_N\}$ 来表示所有车辆提交的任务列表，其中 $N$ 是总的时间步数。我们假设列表 $\mathcal{T}$ 按照任务发起时间 $t_i$ 的升序排列，即 $t_1 \leq t_2 \leq \dots \leq t_n$。

#### 服务器集群模型

车联网（Vehicular Ad Hoc Network，简称 VANET）是一种由移动车辆和固定基础设施组成的自组织网络，它可以提供各种智能交通服务和应用。车联网中的车辆通常具有有限的计算能力、存储空间和电池寿命，因此在执行一些计算密集、数据密集或时延敏感的任务时，可能无法满足性能和质量要求。为了解决这个问题，一种可行的方法是将车辆的任务卸载到边缘服务器上执行。

边缘服务器（Edge Server）是一种部署在网络边缘的服务器，它可以提供近距离的计算、存储和通信服务，从而降低网络延迟、节省带宽和提高用户体验。边缘服务器在车联网任务卸载中的作用是为车辆提供高效、可靠和安全的任务执行环境，使得车辆可以将一些不适合在本地执行的任务迁移到边缘服务器上，从而节省车辆的资源消耗、提高任务的执行效率和质量、增加车辆的服务收益。

一个服务器集群是由若干个服务器组成的，每个服务器可以执行一些任务，并且有一定的资源限制和收益。服务器集群在车联网任务卸载中的作用是为车辆提供更多的选择和灵活性，使得车辆可以根据自己的需求和偏好，在不同的服务器之间进行任务分配、协调和迁移，从而优化资源利用率、平衡负载压力、增强容错能力。

我们用 $S_k=(A_k,S_k)$ 来表示第 $k$ 个服务器，其中：

- $A_k$：服务器属性（Attribute），是一个元组，表示服务器的固定特征，它包括：
  - 服务器编号 $n_k$，用于唯一标识该服务器
  - 服务器资源 $r_k$，表示该服务器最大资源使用限制
  - 服务器成本 $c_k$，表示服务器维护的成本
- $S_k$：服务器状态（State），是一个元组，表示服务器的动态变化，它包括：
  - 服务器负载 $l_k$，表示该服务器当前的资源使用情况
  - 正在运行的任务集合 $R_k$，表示该服务器正在执行的任务

我们用 $S=\{S_1, S_2, ..., S_n\}$ 来表示一个包含 $n$ 个服务器的集群中所有的服务器。我们用 $C=(S,A)$ 来表示一个集群及其属性，其中：

- $A$：集群属性（Attribute），是一个元组，表示集群的固定特征，它包括：
  - 当前时间 $t$，表示当前的系统时间
  - 资源请求最大限制 $m$，表示集群能够接受的资源请求的最大值
  - 资源请求最大时间限制 $t_m$，表示集群能够接受的资源请求的最大持续时间

### 问题定义

在车联网环境下，车辆决定将任务卸载到边缘服务器，于是向拍卖商（服务提供商）发送任务请求。拍卖商通过获取与车辆相联的基站可以评估当前可以连接到的边缘服务器，边缘服务器可以通过路由的方式访问，只要能达到低延迟就可以。拍卖商将任务请求传递给各个边缘服务器，边缘服务器在接收到请求后根据服务器当前的状态给与一个报价返回拍卖商，拍卖商在接收到所有的报价之后选择报价最低的服务器进行卸载并向用户收取相应报价和向边缘服务器支付其报价。这个过程完成后将会正式开始任务卸载。任务卸载结束后,由卸载车辆向被卸载服务器支付.

- 问题的输入是一个时间段内由需要卸载的车辆产生的任务集合 $\mathcal{T}=\{T_1,T_2,\dots,T_N\}$，其中 $N$ 是总的时间步数，每个时间步 $T_i=\{task_1,task_2,...,task_m\}$ 包含了 $m$ 个任务，每个任务 $task_j=(n_j,p_j,r_j,d_j,t_j,a_j)$ 包含了编号、优先级、资源需求、持续时间、发起时间、执行服务器限制等属性。
- 问题的输出是一个任务卸载方案集合 $\mathcal{P}=\{(S_1,p_1),(S_2,p_2),\dots,(S_N,p_N)\}$，其中每个元组 $(S_i,p_i)$ 表示第 $i$ 个任务被卸载到的边缘服务器 $S_i$ 和车辆支付给拍卖商的价格 $p_i$。
- 问题的目标是在满足每个任务的能耗、延迟和执行服务器限制的前提下，最大化每个边缘服务器的收益 $\sum_{i=1}^n f_i - \sum_{i=1}^n rR_iD_i$。其中 $f_i$ 是任务 $task_i$ 的最终支付，$r$ 是单位资源成本，$R_i$ 是任务 $task_i$ 的实际执行时间，$D_i$ 是任务 $task_i$ 的实际执行服务器。
- 问题的约束是：
  - 车辆只能将任务卸载到一个边缘服务器上；
  - 边缘服务器必须在任务执行时有足够的资源来执行任务；
  - 边缘服务器必须满足任务的执行服务器限制 $a_i$；
  - 任务分配必须在任务发起的时间间隙完成，不允许排队；
  - 车辆必须在任务最终支付 $f_i$ 不超过预算 $b_i$ 的情况下接受拍卖结果。

### 优化目标和约束条件

基于以上定义，我们可以把车联网中的任务调度问题建模为一个优化问题：

$$
\begin{aligned}
& \max \sum_{i=1}^n U_i \\
& \text{s.t.} \\
& C1:\text{每个任务只能在一个设备上执行} \\
& C2:\text{每个设备上执行的任务不能超过其可用资源} \\
& C3:\text{每个卸载任务必须在其所选边缘服务器的覆盖范围内执行} \\
& C4:\text{每个卸载任务的最终支付不能超过其效用}\\
\end{aligned}

$$

其中，$\sum_{i=1}^n U_i$ 是社会福利，表示所有任务的效用之和，C 1 到 C 4 是约束条件。

在移动边缘计算中, 车联网是一个非常重要的应用场景，车辆需要在行驶过程中执行不同的计算任务，例如语音识别、路线规划、计算机视觉、机器学习、增强现实等。这些任务对于计算资源和存储资源的需求各不相同，而车辆自身的计算能力和电池容量是有限的。因此，车辆可以选择将部分任务卸载到边缘服务器上进行处理，以节省能耗和提高性能。然而，由于车辆和边缘服务器之间的网络连接是不稳定的，而且边缘服务器的资源也是有限的，因而如何高效地进行任务卸载和资源分配成为一个关键问题。

在本次车联网任务调度的问题建模中, 我们假设一个由$m$个卖家$\mathcal S=\{\mathbf s_1,\dots,\mathbf  s_m\}$和$n$个买家$\mathcal B=\{\mathbf b_1,\dots,\mathbf b_n\}$组成的车联网卸载模型. 卖家为能够提供任务卸载的服务器, 表示为$\mathbf s_i =(\mathbf r_i,c_i)$. 每个卖家 $\mathbf s_i$ 有资源向量 $\mathbf r_i=(r_{j,1},\dots,r_{j,d})$ 和成本 $c_i$，其中, $d$表示服务器总的资源种类, $\mathbf r_{j,k}$表示服务器$j$拥有第$k$种资源的数量，$c_i$ 表示单位时间的维护成本。 买家为需要任务卸载的车辆提交的一次任务请求,表示为 $\mathbf b_j=(\theta_j,\psi_j)$, 其中, $\theta_j=(\mathbf f_j,\mathbf \sigma_j)$表示一个任务,$\mathbf f_j=(l_j,r_j,d_j,\tau_j)$ 是任务的执行特征, $\mathbf \sigma_j=(\sigma_{j,1},\dots,\sigma_{j,m})$ 是任务的执行服务器限制向量表示任务$\theta_j$是否可以在服务器$\mathbf s_i$上执行。 $l_j$表示任务的优先级， $\mathbf r_j=(r_{j,1},\dots,r_{j,d})$表示任务的资源需求, $r_{i,k}$表示任务$i$对第$k$种资源的需求数量； $d_j$表示任务的持续时间； $\tau_j$表示任务的到达时间。$\psi_j$为买家$b_j$对$\theta_j$的预算.

在时间步$t$时, 我们用 $\mathcal U_t​$表示车辆在第$t$个时间步提交的所有任务, 任务序号按照时间非降序排列, 假设$\mathcal U_t$中的任务, 每次按照任务到达顺序取一个任务$\theta_j\in \mathcal U_t$进行拍卖, 则本次拍卖中, 买家为$b_j$表示发起本次任务请求的车辆, 卖家为$\mathcal S_j=  \{\mathbf s_i|\forall \mathbf s_i \in \mathcal S, \sigma_{j,i}=1\}$, 表示能够满足执行服务器限制向量的服务器集合.

拍卖开始时, 买家$\mathbf b_j$将$\theta_j$的特征$\mathbf f_j$发送到参加拍卖的所有卖家$\mathcal S_j$, 卖家根据自身状态和接收的任务特征返回报价$\mathcal Q_j = \{q_{j,i}|\forall \mathbf s_i\in \mathcal S_j\}$, $\mathcal Q_j$表示服务器对$\theta_j$的报价集合,$q_{j,i}$表示卖家服务器$\mathbf s_i$对任务$\theta_j$的报价. 随后产生拍卖结果, 服务器分配向量$\mathbf x_{j}=(x_{j,1},\dots,x_{j,m})$ ,支付 $p_{j}=\min(q_j)$. 若$x_{j,i}=1$表示分配到$\mathbf s_i$, 若$x_{j,i}=0$, 表示未分配到$\mathbf s_i$. 买家支付并开始任务卸载。当$\mathcal U_t$中所有任务都进行过拍卖后, 进入下一个时间步$t+1$, 当所有时间步都完成后，拍卖过程结束。

为了描述任务卸载过程中的资源使用情况，我们需要对任务和服务器的资源在执行时的占用进行建模。我们假设任务$\theta_j$在时间$t$对资源的占用为

$$
r_{\theta_j}(t)=(r_{{\theta_j } , 1}(t),\dots,r_{\theta_j,d} (t))
$$

，其中$r_{\theta_j,k}(t)$表示任务$\theta_i$在时间$t$对第$k$种资源的占用量。

$$
r_{\theta_j,k}(t)=\begin{cases}
r_{j,k}&\text{if }\tau_j \leq t < \tau_j+d_j \\
0,&\text{otherwise}
\end{cases}
$$

类似地，我们假设卖家服务器$\mathbf s_i$在时间$t$的可用资源为
$r_{\mathbf s_i}(t)=(r_{{\mathbf s_i},1}(t),\dots,r_{{\mathbf s_i},d}(t))$
，其中$r_{{\mathbf s_i},k}(t)$表示服务器$\mathbf s_j$在时间$t$拥有第$k$种资源的剩余量。我们假设任务$\theta_j$在分配到服务器$\mathbf s_i$后，会在$\tau_j$立即开始执行，$\tau_j+d_j$时间内完成，即从$\tau_j$到$\tau_j+d_j-1$的时间段内，任务$i$会占用服务器$k$的资源，而在$\tau_j+d_j$时刻释放资源。因此，我们可以定义对于$\theta_j$的服务状态变量$s_{i,k}(t)$为：

$$
s_{j,i}(t)=\begin{cases}
1, & \text{if }  x_{j,i}=1 \text{ and } \tau_j \leq t < \tau_j+d_j\\
0, & \text{otherwise}
\end{cases}
$$

其中$x_{j,i}$表示任务$\theta_i$是否分配到服务器$\mathbf s_i$。服务状态变量$s_{j,i}(t)$表示任务$\theta i$在时间$t$是否占用服务器$\mathbf s_i$的资源。

根据服务状态变量，我们可以计算任务$\theta i$在时间$t$对服务器$\mathbf s_i$的资源占用量为：
$r_{j,i}(t)=s_{j,i}(t)\cdot r_{\theta_j}(t)$

由此，我们可以得到服务器$\mathbf s_i$在时间$t$的可用资源为：
$r_{s_i}(t)=\mathbf r_i-\sum_{i=1}^{|\mathcal U_t|} r_{j,i}(t)$其中$|\mathcal U_t|$表示$t$时间内任务总数，$\mathbf r_i$表示服务器$\mathbf s_i$的总资源量。服务器$j$在时间$t$的可用资源$r_i(t)$表示服务器$j​$在时间$t​$可以接受新的任务卸载请求的资源余量。

服务器$j$在接收到任务卸载时会获得收益, 则边缘服务器$\mathbf s_i$在总时间步$T$到达时的收益为:
$R_i = \sum_{t=1}^T (p_{t,i} - c_{i})$
其中, $p_{t,i}=\sum_{i=1}^{|\mathcal U_t|} p_{i} x_{j,i}$ 是第$t$个时间步内得到的总支付，$p_{j}$为任务$\theta_j$的支付，$c_{i}$ 是服务器每回合的维护成本。

我们的目标是，在总时间步$T$到达时，对于所有买家$\mathbf b$，卖家$\mathbf s$，设计服务器的任务报价策略，使得每个边缘服务器能够在满足任务预算和服务器执行限制的前提下，最大化卖家整体收益$R=\sum^{m}_{i=1} R_i$.

基于以上定义，我们可以把车联网中的任务调度问题建模为一个优化问题：

$$

\begin{aligned}

&Objective: \max_{q_j} R= \sum^m_{i=1} \sum_{t=1}^T (\sum_{j=1}^{|\mathcal U_t|} p_{j} x_{j,i}- c_j)\\
&\\

&\text{s.t.}\ \ \begin{aligned}

& C1:\sum_{j=1}^n x_{j,i}\sigma_{j,i} \le 1, \forall i \in \mathbf b, j \in \mathbf s \\
& C2:p_j \sum_{i=1}^m  x_{j,i}\sigma_{j,i} \leq b_j, \forall i \in \mathbf b, j \in \mathbf s \\
& C3: r_{s_i}(t)\ge 0, \forall i \in \mathbf b, s_i \in \mathbf s,  t \in [0, T]\\
\end{aligned}

\end{aligned}
$$

限制条件的含义如下：

- C1:表示每个任务只能卸载到$a_j$中的一个边缘服务器上；
- C2:表示每个卸载任务的最终支付不能超过其预算$b_j$。
- C3:表示每个边缘服务器在任意时间点可用资源$r_k(t)\ge 0$

以上就是本文对车联网资源卸载问题的系统模型和问题建模。在下一章中，我们将详细介绍本文提出的基于多智能体强化学习和反向拍卖的任务调度算法.

## 系统模型

### 问题的建模和方法的原理

本文考虑了一个由多个车辆和多个服务器构成的车联网环境，如图 1 所示。车辆是任务的发起者，它们需要根据自己的资源需求、连接限制和预算，选择合适的服务器进行任务卸载，并支付相应的费用。服务器是任务的执行者，它们需要根据自己的资源状况和成本，对收到的任务卸载请求进行报价，并执行被接受的任务。我们假设车辆和服务器之间的通信是无线的，且受到信道条件的影响。我们假设车辆和服务器之间的任务卸载是通过反向拍卖的方式进行的，即车辆向满足其连接限制的服务器集合发送任务卸载请求，并等待服务器返回报价。我们假设车辆和服务器都是自利的，即它们的目标是最大化自己的效用或者收益。
![车联网任务卸载的示意图]
图 1 车联网任务卸载的示意图。车辆和服务器之间的通信是无线的，且受到信道条件的影响。车辆和服务器之间的任务卸载是通过反向拍卖的方式进行的，即车辆向满足其连接限制的服务器集合发送任务卸载请求，并等待服务器返回报价。
为了形式化地描述车联网任务卸载的问题，我们首先定义一些符号和变量。我们用 $N$ 表示车辆的数量，用 $M$ 表示服务器的数量。我们用 $V = {v_1, v_2, ..., v_N}$ 表示车辆的集合，用 $S = {s_1, s_2, ..., s_M}$ 表示服务器的集合。我们用 $T = {t_1, t_2, ..., t_T}$ 表示时间的集合，其中 $T$ 是仿真的总时间槽，每个时间槽的长度为 $\Delta t$。我们用 $R = {r_1, r_2, ..., r_R}$ 表示任务的集合，其中 $R$ 是仿真的总任务数。我们用 $r_i^v$ 表示第 $i$ 个任务的发起者，即车辆 $v_i$。我们用 $r_i^s$ 表示第 $i$ 个任务的执行者，即服务器 $s_i$。我们用 $r_i^t$ 表示第 $i$ 个任务的到达时间，即时间槽 $t_i$。我们用 $r_i^d$ 表示第 $i$ 个任务的持续时间，即时间槽 $d_i$。我们用 $r_i^p$ 表示第 $i$ 个任务的优先级，即一个介于 0 到 10 之间的整数。我们用 $r_i^c$ 表示第 $i$ 个任务的连接限制，即一个介于 0 到 3 之间的整数。我们用 $r_i^b$ 表示第 $i$ 个任务的预算，即车辆 $v_i$ 愿意为任务 $r_i$ 支付的最大金额。我们用 $(s_{j,c},s_{j,m})$ 表示服务器 $s_j$ 的资源向量，即一个二元组 $(s_{j,c},s_{j,m})$，其中 $s_{j,c}$ 表示服务器 $s_j$ 的 CPU 总量，$s_{j,m}$ 表示服务器 $s_j$ 的内存总量。我们用 $(s_{j,c}^u,s_{j,m}^u)$ 表示服务器 $s_j$ 的资源利用率，即一个二元组 $(s_{j,c}^u,s_{j,m}^u)$，其中 $s_{j,c}^u$ 表示服务器 $s_j$ 的 CPU 利用率，$s_{j,m}^u$ 表示服务器 $s_j$ 的内存利用率。我们用 $(s_{j,c}^a,s_{j,m}^a)$ 表示服务器 $s_j$ 的资源可用性，即一个二元组 $(s_{j,c}^a,s_{j,m}^a)$，其中 $s_{j,c}^a$ 表示服务器 $s_j$ 的 CPU 可用量，$s_{j,m}^a$ 表示服务器 $s_j$ 的内存可用量。我们用 $(s_{j,c}^k,s_{j,m}^k)$ 表示服务器 $s_j$ 的成本，即一个二元组 $(s_{j,c}^k,s_{j,m}^k)$，其中 $s_{j,c}^k$ 表示服务器 $s_j$ 的 CPU 单位成本，$s_{j,m}^k$ 表示服务器 $s_j$ 的内存单位成本。我们用 $s_j^h$ 表示服务器 $s_j$ 的收益，即服务器 $s_j$ 从任务卸载中获得的总金额。我们用 $s_j^g$ 表示服务器 $s_j$ 的效用，即服务器 $s_j$ 的收益减去服务器 $s_j$ 的成本。我们用 $v_i^l$ 表示车辆 $v_i$ 的效用，即车辆 $v_i$ 从任务卸载中获得的服务质量。我们用 $v_i^w$ 表示车辆 $v_i$ 的信道条件，即一个介于 0 到 1 之间的实数，表示车辆 $v_i$ 与服务器之间的通信质量。我们用 $(v_{i,x},v_{i,y})$ 表示车辆 $v_i$ 的位置，即一个二元组 $(v_{i,x},v_{i,y})$，表示车辆 $v_i$ 在地图上的坐标。我们用 $(v_{i,x}^y,v_{i,y}^y)$ 表示车辆 $v_i$ 的速度，即一个二元组 $(v_{i,x}^y,v_{i,y}^y)$，表示车辆 $v_i$ 在 x 轴和 y 轴上的速度分量。我们用 $(v_{i,x}^z,v_{i,y}^z)$ 表示车辆 $v_i$ 的加速度，即一个二元组 $(v_{i,x}^z,v_{i,y}^z)$，表示车辆 $v_i$ 在 x 轴和 y 轴上的加速度分量。
基于上述的符号和变量，我们可以将车联网任务卸载的问题建模为一个多智能体强化学习和反向拍卖的问题，如图 2 所示。在该问题中，每个智能体代表一个边缘服务器，它可以接收车辆的任务卸载请求，并利用其自身资源来完成任务卸载和获得收益。每个时间步骤中，有一个或多个车辆向满足其连接限制的服务器集合发送任务卸载请求，并等待服务器返回报价。环境会按照任务请求的连接限制，依次选择一个服务器作为当前回合的行动者，它可以根据自身在当前时间段内的资源使用情况和任务特征，以及报价策略，计算出自己对任务的报价，然后将报价发送给车辆。车辆会根据自己的预算和效用，选择一个报价最低的服务器进行卸载.

![多智能体强化学习和反向拍卖的示意图] 图 2 多智能体强化学习和反向拍卖的示意图。

每个智能体代表一个边缘服务器，它可以接收车辆的任务卸载请求，并利用其自身资源来完成任务卸载和获得收益。每个时间步骤中，有一个或多个车辆向满足其连接限制的服务器集合发送任务卸载请求，并等待服务器返回报价。环境会按照任务请求的连接限制，依次选择一个服务器作为当前回合的行动者，它可以根据自身在当前时间段内的资源使用情况和任务特征，以及报价策略，计算出自己对任务的报价，然后将报价发送给车辆。车辆会根据自己的预算和效用，选择一个报价最低的服务器作为任务的执行者，并与之建立连接。服务器会根据任务的资源需求，分配自己的资源，并执行任务。服务器会根据任务的执行结果，获得相应的收益，并更新自己的报价策略。车辆会根据任务的服务质量，获得相应的效用，并支付相应的费用。

## 方法

本节主要介绍了本文提出的基于多智能体强化学习和反向拍卖机制的车联网任务卸载方法（RATO）的问题建模和方法设计，包括系统模型、优化目标、决策方法和算法流程等。
系统模型本文考虑了一个由N个车辆和M个边缘服务器组成的车联网系统，如图2所示。每个车辆都有一个或多个计算任务需要执行，每个边缘服务器都有一定的计算资源可以提供给车辆进行任务卸载。车辆和边缘服务器之间通过无线通信进行数据传输，如IEEE 802.11p和LTE等。车辆和边缘服务器之间的通信质量取决于车辆的移动性、距离、信道等因素。本文假设车辆和边缘服务器之间的通信是可靠的，即不考虑数据丢包和错误的情况。
本文采用了以下的符号和定义：

- $V={v_1,v_2,...,v_N}$表示车辆的集合，$N$表示车辆的数量。
- $S={s_1,s_2,...,s_M}$表示边缘服务器的集合，$M$表示边缘服务器的数量。
- $T={t_1,t_2,...,t_K}$表示计算任务的集合，$K$表示计算任务的数量。
- $t_i=(p_i,d_i,c_i,m_i,l_i)$表示第$i$个计算任务的属性，其中$p_i$表示任务的优先级，$d_i$表示任务的截止时间，$c_i$表示任务的计算量，$m_i$表示任务的内存需求，$l_i$表示任务的连接限制，即任务可以向满足信道条件的$l_i$个服务器发送任务卸载请求。
- $r_j=(f_j,n_j)$表示第$j$个边缘服务器的资源，其中$f_j$表示服务器的计算能力，$n_j$表示服务器的内存容量。
- $h_{ij}$表示第$i$个车辆和第$j$个边缘服务器之间的信道增益，反映了通信质量的好坏。
- $b_i$表示第$i$个车辆的任务卸载预算，即车辆愿意为任务卸载支付的最大金额。
- $q_{ij}$表示第$j$个边缘服务器对第$i$个车辆的任务卸载请求的报价，即服务器要求车辆为任务卸载支付的金额。
- $x_{ij}$表示第$i$个车辆是否将任务卸载给第$j$个边缘服务器的决策变量，如果是，则$x_{ij}=1$，否则，$x_{ij}=0$。
- $y_{ij}$表示第$j$个边缘服务器是否接受第$i$个车辆的任务卸载请求的决策变量，如果是，则$y_{ij}=1$，否则，$y_{ij}=0$。
- $z_{ij}$表示第$i$个车辆和第$j$个边缘服务器之间是否发生任务卸载的结果变量，如果是，则$z_{ij}=1$，否则，$z_{ij}=0$。显然，$z_{ij}=x_{ij}y_{ij}$。
- $u_i$表示第$i$个车辆的效用，即车辆在任务卸载过程中获得的总效用，包括任务执行的效用和任务卸载的效用。任务执行的效用是指任务执行的效率和质量，如任务完成率、任务时延等。任务卸载的效用是指任务卸载的收入和支出，如任务报价、任务成本等。
- $e_j$表示第$j$个边缘服务器的收益，即服务器在任务卸载过程中获得的总收益，包括任务执行的收益和任务卸载的收益。任务执行的收益是指任务执行的效用和价值，如任务完成率、任务质量等。任务卸载的收益是指任务卸载的收入和支出，如任务报价、任务成本等。

优化目标本文的优化目标是最大化车辆的效用和边缘服务器的收益，同时保证任务的完成率和质量。具体地，本文定义了以下的优化目标函数：

- 车辆的效用最大化：

$\max_{x_{ij}} \sum_{i=1}^N u_i=\sum_{i=1}^N \sum_{j=1}^M z_{ij}(u_{ij}^e-u_{ij}^c)$
其中，$u_{ij}^e$表示第$i$个车辆将任务卸载给第$j$个边缘服务器后的任务执行效用，$u_{ij}^c$表示第$i$个车辆将任务卸载给第$j$个边缘服务器后的任务卸载成本。任务执行效用和任务卸载成本的具体计算方法将在后文给出。

- 边缘服务器的收益最大化：

$\max_{y_{ij}} \sum_{j=1}^M e_j=\sum_{j=1}^M \sum_{i=1}^N z_{ij}(e_{ij}^e+e_{ij}^c)$
其中，$e_{ij}^e$表示第$j$个边缘服务器接受第$i$个车辆的任务卸载请求后的任务执行收益，$e_{ij}^c$表示第$j$个边缘服务器接受第$i$个车辆的任务卸载请求后的任务卸载收入。任务执行收益和任务卸载收入的具体计算方法将在后文给出。

- 任务的完成率和质量保证：

$\sum_{i=1}^N \sum_{j=1}^M z_{ij} \geq \alpha K$
$\sum_{i=1}^N \sum_{j=1}^M z_{ij} t_i \leq \beta \sum_{i=1}^N t_i$
其中，$\alpha$和$\beta$是两个给定的常数，分别表示任务的完成率和任务的平均时延的阈值。任务的完成率是指成功卸载并执行的任务占总任务的比例，任务的平均时延是指所有任务的卸载时延的平均值，卸载时延是指从任务生成到任务完成的总时间。这两个约束条件的目的是为了保证任务卸载的效果和质量，避免任务的丢失和延迟。
决策方法本文采用了基于多智能体强化学习和反向拍卖机制的决策方法，即让每个边缘服务器作为一个智能体，利用强化学习算法学习最优或次优的报价策略，即如何根据任务的需求、紧急性和自身的资源等约束，动态地决定每次报价的金额，从而最大化服务器的收益和车辆的效用。同时，让每个车辆作为一个拍卖者，利用反向拍卖机制选择最优或次优的卸载策略，即如何根据任务的连接限制和预算条件，选择选择满足其连接限制和预算条件的服务器进行任务卸载，从而提高任务的完成率和质量。
本文采用了一种基于PPO（Proximal Policy Optimization）算法的多智能体强化学习方法，来训练边缘服务器的报价策略。PPO是一种基于策略梯度的强化学习算法，它通过优化一个目标函数，来更新策略网络的参数，从而提高策略的性能。PPO的目标函数是一个带有剪裁的比率函数，它可以有效地防止策略的更新幅度过大，导致策略的崩溃或震荡。PPO的目标函数如下：
$L(\theta)=\mathbb{E}{s,a,r}\left[\min\left(\frac{\pi{\theta}(a|s)}{\pi_{\theta_{\text{old}}}(a|s)}A^{\pi_{\theta_{\text{old}}}}(s,a),\text{clip}\left(\frac{\pi_{\theta}(a|s)}{\pi_{\theta_{\text{old}}}(a|s)},1-\epsilon,1+\epsilon\right)A^{\pi_{\theta_{\text{old}}}}(s,a)\right)\right]$
其中，$\theta$表示策略网络的参数，$\theta_{\text{old}}$表示上一次更新后的策略网络的参数，$\pi_{\theta}(a|s)$表示策略网络输出的在状态$s$下采取动作$a$的概率，$A^{\pi_{\theta_{\text{old}}}}(s,a)$表示在状态$s$下采取动作$a$的优势函数，即动作的价值与状态的价值之差，$\epsilon$表示剪裁的阈值，通常取0.1或0.2。
本文将边缘服务器的报价策略建模为一个部分可观测马尔可夫决策过程（Partially Observable Markov Decision Process，POMDP），其中，每个边缘服务器作为一个智能体，需要根据自身的观察和历史信息，做出每次报价的决策。本文定义了以下的POMDP元素：

- 状态空间$S$。状态空间表示边缘服务器的观察空间，即边缘服务器可以观察到的信息，包括自身的资源状况、任务的特征、车辆的预算等。本文将状态空间定义为一个连续的向量空间，即$S=\mathbb{R}^n$，其中$n$表示状态向量的维度。
- 动作空间$A$。动作空间表示边缘服务器的报价空间，即边缘服务器可以对任务卸载请求提出的报价，包括固定报价、随机报价、学习报价等。本文将动作空间定义为一个连续的标量空间，即$A=\mathbb{R}$，其中报价的取值范围为$[0,b_i]$，即不能超过车辆的预算。
- 转移函数$T$。转移函数表示边缘服务器的状态在不同时间步之间的转移概率，即在当前状态和动作下，下一个状态出现的概率，即$T:S\times A\times S\rightarrow [0,1]$。本文假设转移函数是未知的，即边缘服务器无法预测下一个状态的出现，只能通过与环境的交互来学习。
- 奖励函数$R$。奖励函数表示边缘服务器在不同状态和动作下获得的即时奖励，即在当前状态和动作下，边缘服务器的收益，即$R:S\times A\rightarrow \mathbb{R}$。本文将奖励函数定义为边缘服务器的收益，即$R(s,a)=e_j$，其中$e_j$是第$j$个边缘服务器的收益，具体计算方法将在后文给出。
- 策略函数$\pi$。策略函数表示边缘服务器在不同状态下采取不同动作的概率分布，即在当前状态下，每个动作被选择的概率，即$\pi:A\times S\rightarrow [0,1]$。本文将策略函数定义为一个神经网络，即$\pi_{\theta}(a|s)$，其中$\theta$表示神经网络的参数，$a$表示动作，$s$表示状态。本文的目标是通过强化学习算法，优化神经网络的参数，从而得到最优或次优的策略函数。

本文采用了一种基于反向拍卖机制的任务卸载方法，即让每个车辆作为一个拍卖者，利用反向拍卖机制选择最优或次优的卸载策略，即如何根据任务的连接限制和预算条件，选择满足其连接限制和预算条件的服务器进行任务卸载，从而提高任务的完成率和质量。反向拍卖机制是一种常用的资源分配机制，它与正向拍卖机制相反，即由多个卖方（边缘服务器）对一个买方（车辆）的需求进行竞价，买方选择报价最低的卖方进行交易。反向拍卖机制的优点是可以有效地降低买方的成本，提高卖方的竞争力，实现资源的高效分配和利用。反向拍卖机制的过程如下：

- 每个车辆根据自身的任务特征和预算，向满足其连接限制的边缘服务器发送任务卸载请求，即$x_{ij}=1$，其中$j\in L_i$，$L_i$表示第$i$个车辆的连接限制集合，即满足信道条件的服务器集合。
- 每个边缘服务器根据自身的资源状况和报价策略，对收到的任务卸载请求进行报价，即$q_{ij}$，其中$i\in R_j$，$R_j$表示第$j$个边缘服务器的请求集合，即向其发送任务卸载请求的车辆集合。
- 每个车辆根据收到的报价，选择一个或多个报价最低、同时满足需求和预算条件的边缘服务器进行交易，即$y_{ij}=1$，其中$j\in W_i$，$W_i$表示第$i$个车辆的胜出者集合，即被其选择进行任务卸载的服务器集合。
- 交易成功后，车辆将任务发送到胜出者服务器并支付相应金额，等待执行结果返回，即$z_{ij}=1$，其中$j\in W_i$。服务器获得收益，为任务分配资源并执行任务卸载。如果没有服务器的报价满足车辆的条件，则车辆无法完成任务卸载，无需支付任何金额，即$z_{ij}=0$，其中$j\in L_i$。

算法流程本文提出了一种基于多智能体强化学习和反向拍卖机制的车联网任务卸载算法，简称为RATO算法。该算法的流程如算法1所示。该算法主要包括以下几个步骤：

算法1：基于多智能体强化学习和反向拍卖机制的车联网任务卸载算法（RATO）

输入：车辆集合$V$，边缘服务器集合$S$，计算任务集合$T$，边缘服务器的策略网络参数$\theta$，其他相关参数和变量
输出：车辆和边缘服务器的最终的效用和收益，以及任务的完成率和质量等指标
步骤：

1. 初始化车辆和边缘服务器的集合，计算任务的集合，边缘服务器的策略网络的参数，以及其他相关的参数和变量
2. 循环执行以下步骤，直到达到终止条件，如最大迭代次数或最小误差阈值等

- 任务生成。每个车辆根据自身的任务特征和预算，生成一个或多个计算任务，并将其添加到任务集合中
- 任务卸载请求。每个车辆根据自身的任务连接限制，向满足其连接限制的边缘服务器发送任务卸载请求，即$x_{ij}=1$，其中$j\in L_i$
- 任务卸载报价。每个边缘服务器根据自身的资源状况和策略网络的输出，对收到的任务卸载请求进行报价，即$q_{ij}=\pi_{\theta}(a|s)$，其中$i\in R_j$
- 任务卸载选择。每个车辆根据收到的报价，选择一个或多个报价最低、同时满足需求和预算条件的边缘服务器进行交易，即$y_{ij}=1$，其中$j\in W_i$
- 任务卸载执行。交易成功后，车辆将任务发送到胜出者服务器并支付相应金额，等待执行结果返回，即$z_{ij}=1$，其中$j\in W_i$。服务器获得收益，为任务分配资源并执行任务卸载。如果没有服务器的报价满足车辆的条件，则车辆无法完成任务卸载，无需支付任何金额，即$z_{ij}=0$，其中$j\in L_i$
- 状态更新。每个边缘服务器根据任务卸载的结果，更新自身的状态，如资源状况、任务队列等
- 奖励计算。每个边缘服务器根据任务卸载的结果，计算自身的奖励，即收益，如$R(s,a)=e_j$
- 经验存储。每个边缘服务器将自身的经验，即状态、动作、奖励和下一个状态，存储到自身的经验回放缓冲区中，用于后续的策略更新
- 策略更新。每隔一定的时间步，每个边缘服务器从自身的经验回放缓冲区中随机抽取一批经验，利用PPO算法更新自身的策略网络的参数，从而提高自身的策略性能

3. 输出车辆和边缘服务器的最终的效用和收益，以及任务的完成率和质量等指标

本文的第三节问题的建模和方法的设计到此结束。

## 4. Method of MADRL and Reverse-Auction

为了解决边缘计算中的车联网工作流调度问题，我们提出了一个基于多智能体强化学习和反向拍卖机制的方法。我们首先把这个问题建模成了一个在线和NP-hard问题，然后使用强化学习方法来处理复杂的动态的高维度的决策问题。我们采用多智能体强化学习来实现分布式的资源分配，避免集中式管理的通信开销和信息的不准确性，提高系统的稳定度。我们还使用了一种基于反向拍卖的任务卸载方法，用于解决车联网中的资源分配问题。反向拍卖是一种一买方多卖方的拍卖形式，适用于买方市场的交易活动。在反向拍卖中，买方提出自己的需求和预算，卖方根据自身的成本和利润，竞争性地给出自己的报价。最终，买方选择一个或多个报价最低且满足需求的卖方进行交易。反向拍卖可以有效降低买方的采购成本，提高卖方之间的竞争力，实现资源的优化分配。接下来的内容将分别描述多智能体强化学习建模和算法，以及反向拍卖的角色和过程。

## 多智能体强化学习建模

在第三章的车联网任务卸载建模中, 边缘服务器被定义为智能体（agent）, 每个智能体通过与环境交互学习可以最大化自身收益的策略. 因为在多智能体环境中,一个智能体的动作会影响其他智能体的决策. 为了描述这种情况，我们使用扩展的马尔可夫决策过程（extended Markov decision process, eMDP）来建模多智能体强化学习. 由K个智能体组成的eMDP可以被定义为

$$
(\mathcal S,\mathcal A_1,\dots,\mathcal A_K, \mathcal R_1,\dots,\mathcal R_K, \mathcal P_{ss'})
$$

$\mathcal S$

为状态空间（state space），

$\mathcal A_i$为第i个智能体的动作空间（action space），$\mathcal R_i$为第i个智能体的奖励函数（reward function），$\mathcal P_{ss'}$为状态转移概率（transition probability）。在每一个时间间隙, 边缘服务器作为一个智能体, 观察当前环境的状态$s\in\mathcal S$, 根据策略$\pi_i(a_i|s)$选择一个合适的动作$a_i\in\mathcal A_i$. 然后获得相应的奖励$r_i=\mathcal R_i(s,a_1,\dots,a_K)$. 为了使用强化学习方法, 我们需要将问题转换为马尔可夫决策过程（Markov decision process, MDP），它包括状态空间（state space, $\mathcal S$），观测空间（observation space, $\mathcal O$），动作空间（action space, $\mathcal A$），和奖励函数（reward function, $\mathcal R$）.

Littman, M. L. (1994). Markov games as a framework for multi-agent reinforcement learning. In Machine learning proceedings 1994 (pp. 157-163). Morgan Kaufmann.

![[Pasted image 20230902100025.png]]
多智能体强化学习过程

![[Pasted image 20230717162154.png]]
**Figure 1**: Single-agent approaches (a) and (b) in comparison with multi-agent RL (c)

我们将车联网中的任务调度问题建模如下.

- 状态空间
- 观察空间
- 动作空间
- 奖励函数
- 环境建模

### 状态空间 State Space

在强化学习方法中, 状态空间是指整个系统的描述，包含了系统中所有的信息。本次研究的是多智能体强化学习，因此状态空间应当包含所有边缘服务器本身的状态, 边缘服务器i的状态可以表示为$S_i=(L_i,B_i)$， 其中:

- $R_i=(r_{it})_{T\times R}$，现有资源时间槽表示边缘服务器在一定时间内的资源占用情况。其中，$r_{it}$ 表示第 $i$ 个边缘服务器在第 $t$ 个时间段内剩余的第 $r$ 种资源量，$T$ 表示总时间段数，$R$ 表示总资源种类数。这个状态特征可以反映边缘服务器的资源可用性和限制性。
- $B_i=(task_j)$，表示边缘服务器当前等待被报价的任务信息。其中，$task_j$ 表示当前编号为 $j$ 的任务正在向第 $i$ 个边缘服务器请求报价，$task_j=\{p_j, r_j, d_j\}$, 其中$p_j$为任务优先级，$r_j$为任务的资源需求，$d_j$为任务持续时间。

可用 $S=\{S_1,S_2,\dots,S_m\}$ 来表示所有边缘服务器的状态，其中，$S$ 表示整个服务器集群的状态集合，$S_i$ 表示第 $i$ 个边缘服务器的状态。因为整个服务器集群是由多个服务器组成，所有边缘服务器状态的集合就是本次研究的状态空间。

观察空间 Observation Space
观察空间定义了一个智能体可以观察到的状态空间， 本次研究所定义的智能体之间不存在额外的通信与交互，因此每个智能体所能观察到的状态空间就是自身的服务器状态$S_i$。因此，边缘服务器i的观察空间为$O_i=S_i$。==（等待完善）==

### 动作空间 Action Space

动作空间定义了智能体可以向环境执行的动作, 在多智能体强化学习中，每个智能体都可以执行动作以影响环境的状态，我们先定义单个智能体的动作空间。单个智能体的动作空间具体为一个在$a_i \in [1/3,3]$的数值，$a_i$市场价格的浮动系数，用以决定最终的服务器报价。为了使用Value-based和Policy-based的方法, 我们同时建模了discrete space和continuous space的两种action space.

- discrete space
  - 将价格区间等分为若干份
- continuous space
  - $[1/3,3]$ 的连续区间
    action space到action 转换
- discrete space
  - 将价格区间等分为若干份, 每一份对应的是动作空间的一个值
- continuous space
  - $[1/3,3]$ 的连续区间

当环境接收到服务器i的$a_i$的值后, 服务器i最终报价为$bid_i = a_i \times avergeprice \times resource \times duration$。$avergeprice$是资源的平均价格, $resource$是资源使用量, $duration$是资源使用时间
因此,在时间步t，整个系统的动作空间为

$$
A_t=\{a_1, a_2,\dots,a_m\}
$$

其中，$a_i$代表第i个服务器对其当前请求报价的任务的报价的系数. 当对任务进行报价时, 环境仅收集任务对应满足条件限制的服务器的报价, 而忽略其他服务器的报价.

### 奖励函数 Reward Function

奖励函数被用来描述智能体在执行某一动作后,从一个状态转移到另一个状态的奖励. 奖励函数的设置会影响每个智能体的目标，智能体会通过奖励函数判断自己选择的action对于自己目标的达成时正向的还是负向的影响来最大化自己的累积奖励。
在本次论文中,边缘服务器的目标是最大化自己的收益, 边缘服务器会在每回合开始时, 减去自身服务器的成本，并且结算本回合结束的任务获得收益。
因此, 奖励函数可以被写为

$$
r_i = p_i-c_i
$$

其中，$p_i$ 表示本回合的任务支付，$c_i$ 表示服务器成本。

### 环境建模

为了实现本论文中多智能体强化学习的环境建模, 我们使用了Pettingzoo最为我们实现自定义环境的接口. Pettingzoo是一个用于表示一般的多智能体强化学习（MARL）问题的Python库，它包括了多种参考环境、有用的工具和创建自定义环境的方法。Pettingzoo支持Agent Environment Cycle（ACE）模式，它是一种适用于顺序的回合制环境的接口，能够处理任何MARL可以考虑的游戏。在ACE模式下，每个智能体都有自己的观察空间和动作空间，而且只有一个智能体可以在每个时间步骤中执行动作。用户可以使用以下方法来与ACE模式的环境进行交互：每一个step开始时使用env.agent_iter()的到当前需要执行动作的agent, 通过env.last()返回最后一个智能体的观察、奖励、终止、截断和信息, 利用返回的信息生成action, 通过env.step(action)执行一个动作并更新环境状态,直到所有的智能体都完成了他们的回合或者游戏结束。

为了利用多智能体强化学习求解这个优化问题，我们建立了如下多智能体强化学习模型：

1. 初始化任务生成模型参数，边缘服务器集群 S。
2. 对于每个时间槽 t，执行以下操作, 直到满足停止条件：
   1. 使用任务生成模型生成在本时间槽中需要卸载的所有任务$T_t$.
   2. 集群中每一个服务器更新资源使用情况, 向车辆返回结束的任务的执行结果, 结算执行完毕的任务获得奖励和维护费用.
   3. 从$T_t$中的选择一个没有参加过拍卖任务$task_i$, 执行如下操作, 直至全部任务都进行拍卖:
      1. 将$task_i$相关信息发送到满足限制的服务器集合$a_i$.
      2. 对于每一个在$a_i$中的边缘服务器$S_j$,执行如下操作,直到所有服务器完成报价.
         1. $S_j$收到任务请求后，根据自己的观测，得到动作, 将报价返回拍卖商.
      3. 拍卖商收到所有服务器的报价后，执行反向拍卖过程，并向车辆通知卸载结果和支付费用。==可以考虑二价拍卖==
      4. 车辆收到拍卖结果，检查是否满足其预算要求
         1. 如果满足预算需求, 则同意卸载，并在任务结束后进行支付==目前是执行后支付,考虑立即支付, 执行后支付应该会偏向于小任务, 立即支付会偏向于大任务, 未验证==
            1. 将车辆任务发送到边缘服务器
            2. 边缘服务为任务预留相关资源并开始执行, 更新资源的使用情况.
         2. 如果不满足，则不进行任务卸载,不进行支付.
   4. 当$T_t$中所有任务都进行拍卖后, 进入下一个时间槽$t+1$
3. 输出最终的环境状态，并结束环境。

#### 任务生成模型

完成对任务的建模后, 为模拟真实的任务调度场景, 接下来需要对任务的生成进行建模. 任务的生成一共由三部分组成, 一个时间步内任务的数量, 每个任务对资源的需求, 任务可卸载的服务器列表.

- 一个时间步的任务数量, 本文将其简化为符合泊松分布的随机变量, 每一回合从分布中抽取一个正整数作为该时间步的任务数量.
- 每个任务对资源的需求, 我们将任务的资源需求和执行时间都取自由两个正态分布组合而成的双峰分布, 因为通过分析现实的计算机集群工作负载现实世界中卸载的任务以小任务或大任务居多, 如果以简单的正态分布或者指数分布(负指数分布)可能无法准确建模, 另外选择这样的分布也是为了检验神经网络能否拟合复杂分布.==(这里任务分布待讨论)==
- 任务可卸载的服务器列表, 由于车联网中的车辆存在动态性, 位置存在随机性, 因此我们将车辆与边缘服务器之间的连接抽象成一个可连接服务器的集合. 由于服务器的位置是固定的, 因此其可连接的车辆范围也是固定的. 为在模型中体现车联网的连接性质, 在生成执行服务器限制时, 我们首先生成一个连接范围限制, 然后从连接范围限制的服务器集合中随机抽取若干服务器作为服务器限制集合. 这时的任务可卸载约束仅有连接所限制,属于服务器的无线设备自发无目的的向其范围内车辆发送的连接检测过程,这时服务器仍然未知任务的具体特征, 因此执行约束不在任务生成模型的建模之中.

因此本次研究系统模型中的任务生成过程如下.

- 设定一个时间步的长度为 $\Delta t$，比如 $\Delta t=1$ 秒或者 $\Delta t=0.1$ 秒；
- 在每个时间步 $n$ 内，根据泊松分布 $Poisson(\lambda)$ 生成一个正整数 $m$，表示这个时间步内有 $m$ 个任务被车辆提交到边缘服务器，其中 $\lambda$ 是泊松分布的参数，表示单位时间内任务到达的平均速率；
- 对于每个任务 $task_i$，根据双峰分布 $Bimodal(\mu_1,\sigma_1,\mu_2,\sigma_2,w)$ 生成它的资源需求和执行时间，表示这个任务需要多少计算资源和存储资源以及执行多长时间，其中 $\mu_1,\sigma_1,\mu_2,\sigma_2$ 是两个正态分布的均值和标准差，$w$ 是两个正态分布的权重；
- 根据车辆的位置和边缘服务器的位置和连接情况，生成一个可连接服务器的集合 $a_i$，表示这个任务可以被发送到哪些边缘服务器；
- 根据车辆的偏好或者其他因素，生成一个预算 $b_i$，表示这个任务的最高支付费用；
- 这样，我们就得到了一个完整的任务 $task_i=(n_i,p_i,r_i,d_i,t_i,a_i,b_i,f_i)$；
- 将这个时间步内生成的所有任务组成一个任务集合 $T_n=\{task_k,task_{k+1},...,task_{k+m}\}$，表示这个时间步内有 $m$ 个任务被车辆提交到边缘服务器。
- 重复这个过程，直到生成所有任务或达到停止条件。

#### 反向拍卖模型

在本文中，我们使用了一种基于反向拍卖的任务卸载方法，用于解决车联网中的资源分配问题。反向拍卖是一种一买方多卖方的拍卖形式，适用于买方市场的交易活动。在反向拍卖中，买方提出自己的需求和预算，卖方根据自身的成本和利润，竞争性地给出自己的报价。最终，买方选择一个或多个报价最低且满足需求的卖方进行交易。反向拍卖可以有效降低买方的采购成本，提高卖方之间的竞争力，实现资源的优化分配。接下来将分别介绍本模型中反向拍卖的角色和反向拍卖过程.

在本文中，我们使用了一种基于反向拍卖的任务卸载方法，用于解决车联网中的资源分配问题。接下来将分别介绍本模型中反向拍卖的角色和反向拍卖过程.

买家, 任务发起者, 是车联网中有任务卸载需求, 并愿意为其需求支付的车辆. 在某一时刻t, 车辆向满足其连接限制(车辆可以顺利通过无线网络将任务卸载)的边缘服务器发起任务$task_j$, $task_j$根据其本身属性, 生成报价请求$b_j=\{p_j,r_j, d_j\}$. 其中, $p_j$任务优先级，$r_j$任务的资源需求，$d_j$任务持续时间. 报价请求将会由车辆发送到满足连接约束的服务器中以请求报价.

卖家, 服务提供商, 也就是车联网中的边缘服务器. 边缘服务器可以接收车辆的任务卸载请求, 并利用其自身资源来完成任务卸载和获得收益. 服务器i的特征可以被定义为$S_i=(L_i,R_i,C_i)$, 其中$L_k$：服务器负载（Load）,$R_k$：服务器资源（Resource）, $C_k$：服务器成本（Cost）.

服务器报价模型是指服务器根据任务的特征和自身的状况，计算出自己对任务的报价。为通过强化学习完成定价, 我们定义了一个函数，用于计算服务器的报价：

$$
bid_{i,j} = a_i \times p \times r_j \times d_j
$$

其中, $bid_{i,j}$为服务器i对任务j的报价, $a_i$为服务器通过强化学习结合自身观察生成的动作, 作为报价的系数. $p$为市场平均单位时间资源价格向量, $r_j$为任务j对资源的需求量, $d_j$为任务的持续时间. 则$p \times r_j \times d_j$可以理解为市场整体的平均价格.

一个经纪人是一个中间代理，负责主持和指导拍卖过程。经纪人的责任是确保拍卖顺利进行，保持真实性和个体理性，从而最大化买方的效用，同时保护卖方的利益。在车联网中，一个基站或路侧单元可以通过一个拍卖控制器进行资源拍卖。为使服务器之间充分竞争, 报价可信, 防止出现服务器一起抬价和神经网络无法收敛, 我们设定了一个市场平均价格作为参考价格, 具体报价方式为服务器根据任务特征和服务器自身状况, 报价一个参考价格的偏差.

基于云计算的车联网场景，包含若干车辆和n个卖方$s_j∈\{s_1,s_2,...,s_N\}$. 车辆通过自身的通信单元与边缘服务器进行通信, 并且评估车辆与边缘服务器的通信状况, 以在需要进行任务卸载时生成边缘服务器限制条件。边缘服务器可以通过有线链接相互连接,并且与互联网连接以便进行任务卸载环境的搭建和信息交流。基本假设如下。

1. 假设时间是分时的，任务只能在一个时间段提交, 并且任务时长为整数个时间段。
2. 拍卖过程中, 一次仅有一个任务参与, 在任务拍卖结束后, 进行本时隙的下一个任务的拍卖. 如果本时隙没有任务, 则进入下一个时隙.
3. 在卸载市场中存在信息缺失：首先，卖方的特征和维护成本对经纪人和买方不可见；没有卖方知道其他参与者的出价，也没有卖方相互勾结。另外，买方的私人信息（例如，买方的预算）不能公开给卖方。
4. 车辆可以根据自身状态, 评估在拍卖和卸载的时间内的连接情况, 得到任务的边缘服务器限制条件. 任务可以在卸载时保持与边缘服务器的稳定连接.
5. 一个买方只能把他的任务卸载给一个卖方，但一个卖方可以同时为多个买方提供服务。随着时间前进, 任务在到达其持续时间限制时完成, 其占用服务器的资源被释放, 以便服务器继续为其他买方服务.
6. 任务卸载的结果对于车辆而言, 其效用是一致的. 对于用户而言, 服务器的处理过程是透明的, 无论是哪一个服务器接收了卸载对于任务执行的结果是一致的.
7. 拍卖在车辆的行驶过程中自动进行, 经纪人不存在自由意志, 仅仅只是执行拍卖流程.

在设计了拍卖过程的基本假设后,结合服务器的报价过程, 我们可以书写反向拍卖模型. 拍卖商, 作为主持和引导拍卖的代理人, 在本次模型中拍卖商是一个运行在车联网中的程序, 用来完成每次反向拍卖的流程.反向拍卖卸载过程如下：

- 车辆将任务特征发送到满足任务连接限制的服务器, 向服务器请求报价
- 服务器接收到任务请求后, 先检查自身资源能否满足本次任务的资源请求, 如果无法满足则退出拍卖, 不进行报价. 如果可以满足资源请求, 则进入报价流程.
- 服务器根据自身资源使用情况和任务特征, 计算出自身报价.
- 车辆在收到报价后, 检查是否存在满足卸载需求的服务器, 如果不存在, 则本次拍卖结束, 任务卸载失败. 如果存在则继续.
- 根据服务器产生的报价, 进行反向拍卖, 得到胜者和支付.
- 检查支付是否满足预算需求, 如果不满足, 本次拍卖结束, 任务卸载失败.如果满足, 继续.
- 开始卸载到胜者服务器, 将支付设定为拍卖结果的支付.
- 任务执行完成后, 服务器获得支付, 并将结果返回车辆.

反向拍卖模型是指车辆根据服务器的报价，选择合适的服务器进行任务卸载。我们假设车辆只能将任务卸载到一个服务器上，并且要满足任务的能耗、延迟和预算限制。我们定义了一个函数E，用于计算将任务卸载到服务器所消耗的能耗，以及一个函数L，用于计算将任务卸载到服务器所产生的延迟。我们的目标是在满足限制条件的前提下，最小化车辆支付给服务器的总价格。我们用以下数学符号和公式来表示这个问题：

$$
\begin{aligned}
& \min_{x} V.P = \sum_{i=1}^n x_i * Si.p \\
& \text{s.t.} \\
& \sum_{i=1}^n x_i = 1 \\
& x_i \in \{0, 1\} \\
& Si.p \leq T.b \\
& T.e \leq E(T, Si) \\
& T.l \geq L(T, Si) \\
\end{aligned}
$$

其中xi是一个决策变量，表示是否将任务T分配给服务器Si。

## 多智能体强化学习算法

算法的简介, ==测试一下那种效果最好进行介绍, 剩下作为对比==

- dqn
- ddpg
- mappo
- maddpg ==未成功==
- a2c
- sac

为了解决提出的RL环境中的作业调度问题，我们使用了两种基于DRL的算法。第一种是基于Q-Learning的方法，叫做Deep Q-Learning (DQN)。另一种是一种策略梯度算法，叫做REINFORCE。我们选择这些算法是因为它们可以处理具有离散状态和动作空间的RL环境。而且，这两种算法的工作过程也不同，其中DQN优化状态-动作值，而REINFORCE直接更新策略。从Spark作业调度的角度来看，RL环境将提供与我们运行的真实工作负载类似的作业规范。此外，集群资源也相同，因此VM资源可用性也将作为状态空间的一部分被使用和更新。每次DRL代理采取一个动作（放置一个执行器），都会得到一个即时奖励。下一个状态也将依赖于前一个状态，因为VM和作业规范将在每次放置后进行更新。最终，DRL代理应该能够学习资源可用性和需求约束，并完成所有作业的所有执行器的调度，以完成情节并获得情节奖励。

RL模拟反馈回路不断地收集数据，对于单个（单智能体案例）或多个（多智能体案例）策略进行训练，确保策略的权重保持同步。因此，收集的环境数据包含观察到的状态、采取的动作、、获得的奖励和所谓的**结束**标志，表示代理在模拟中经历的不同的回合。

模拟的动作 -> 奖励 -> 下一个状态 -> 训练 -> 重复，直到终止状态，称为一个**回合**，或者在RLlib中，一个**演示**。

- RLlib使用rollout workers来并行地从环境中采集数据，每个worker可以使用一个或多个策略来与环境交互，产生一系列的动作、奖励、下一个状态和结束标志，这些数据构成了一个episode或rollout。
- RLlib将每个worker产生的rollout汇总到一个训练批次中，然后根据选择的算法和损失函数计算梯度，并更新模型参数。
- RLlib重复这个过程，直到满足终止条件，如达到最大迭代次数、最大训练时间或最佳性能等。

#### 5.1.2 Deep Q-Learning (DQN)

Q-学习可以作为动态规划（DP）问题来求解，其中我们可以将_Q_-函数表示为一个二维矩阵，包含每一对$s$和$a$的值。然而，在高维空间中（状态和动作对的总数很大），表格Q-学习的解决方案是不可行的。因此，通常用一个神经网络来训练参数$\theta$，来近似Q-值，即$Q(s,a;\theta) \approx Q^*(s,a)$。这里，每一步$i$需要最小化以下损失函数

$$
{{{ L}_{i}(\theta_{i})=\mathbb{E}_{s,a,r,s^{\prime}\sim\rho(\cdot)}\left[(y_{i}-Q(s,a;\theta_{i}))^{2}\right],}}\tag{16}
$$

其中$y_{i}=r+\gamma\mathrm{max}_{a^{\prime}} Q (s^{\prime}, a^{\prime};\theta_{i-1})$。

这里，$\rho$是从环境中采样的转移$\{s,a,r,s'\}$的分布。$y_i$被称为时间差分（TD）目标，*$y_i$ _ $Q$被称为TD误差。

注意，目标$y_i$是一个变化的目标。在监督学习中，我们有一个固定的目标。因此，我们可以训练一个神经网络，通过减少损失函数，让它在每一步向目标靠近。然而，在强化学习中，由于我们逐渐地学习环境的知识，目标$y_i$总是在改进，对于网络来说它就像一个移动的目标，从而使它不稳定。目标网络具有固定的网络参数，因为它是从之前的迭代中采样的。因此，目标网络的参数用于更新当前网络，以实现稳定的训练。

此外，我们希望我们的输入数据是独立同分布（i.i.d.）的。然而，在同一条轨迹（或者说情节）中，迭代之间是相关的。在训练迭代中，我们更新模型参数，使$Q(s,a)$更接近真实值。这些更新会影响其他估计，并使网络不稳定。因此，可以使用一个循环的*回放缓冲区*来保存环境中的之前的转移（状态、动作、奖励样本）。因此，从回放缓冲区中抽取一个小批量样本来训练深度神经网络，使得数据更加独立和类似于i.i.d.

DQN是一种离线算法，它在从环境中收集数据时使用了不同的策略。原因是如果一直使用正在改进的策略，由于状态-动作空间覆盖不足，算法可能会收敛到次优策略。因此，使用了一个$\epsilon$-贪婪策略，它以概率$1-\epsilon$选择贪婪动作，以概率$\epsilon$选择随机动作，这样它就可以观察到任何未探索过的状态，确保算法不会陷入局部最大值。我们使用回放缓冲区和目标网络的DQN[38]算法在算法1中总结。

![[Pasted image 20230726102735.png]]

### 5.1 REINFORCE Agent

DQN优化状态-动作值，从而间接地优化策略。然而，策略梯度方法直接对策略进行建模和优化。策略通常用一个关于$\theta$的参数化函数表示，写成$\pi_\theta$。相应地，$\pi(a_t|s_t)$是在时刻$t$给定状态$s_t$时选择动作$a_t$的概率。智能体能够获得的奖励的数量取决于这个策略。

在传统的策略梯度算法中，每次迭代收集一批样本，然后使用收集到的样本对策略应用公式(17)中显示的更新。

$$
\nabla_{\theta}\mathrm{E}_{\pi_{\theta}}\left[\sum_{t=0}^{T}\gamma^{t}r_{t}\right]=\mathbb{E}_{\pi_\theta}\left[\sum_{t=0}^{T}\nabla_{\theta}l o g\pi_{\theta}(a_{t}|s_{t})R_{t}\right].\tag{17}
$$

这里，$\gamma$是折扣因子，而$s_t$、$a_t$和$r_t$分别用来表示时刻$t$的状态、动作和奖励。$T$是任意单个回合的长度。$R_t$是折扣累积回报，可以按照公式(18)计算

$$
R_{t}=\sum_{t^{\prime}=t}^{T}\gamma^{t^{\prime}-t}r_{t}.\tag{18}
$$

这里，t'从当前时刻$t$开始，这意味着如果采取当前的动作，我们将得到一个即时奖励$r_t$，它也影响了我们能够累积到回合结束时的奖励量。
公式(17)中显示的期望回报使用了最大似然度，它衡量了观察到的数据的可能性。在强化学习的背景下，它意味着我们可以期望在当前策略下得到当前轨迹的可能性。当似然度与奖励相乘时，如果产生正向奖励，则策略的似然度增加。另一方面，如果给出较少或负向奖励，则策略的似然度减少。总之，模型试图保持效果较好的策略，并倾向于丢弃效果不好的策略。然而，由于公式显示为一个期望值，它不能直接使用。因此，使用一个基于采样的估计器来代替，如公式(19)所示

$$
\nabla_{\theta}J(\theta)\approx\frac{1}{N}\sum_{i=1}^{N}\left(\sum_{t=1}^{T}{\nabla}_{\theta}l o g\pi_{\theta}(a_{i,t}|s_{i,t})\right)\left(\sum_{t=1}^{T}r(s_{i,t},a_{i,t})\right).\tag{19}
$$

这里，$\nabla_\theta J(\theta)$是目标目标函数$J$的策略梯度，用$\theta$参数化。我们还假设在每次迭代中，采样了N条轨迹$( \tau_1,...,\tau_n)$，其中每条轨迹$\tau_i$是一个状态、动作和奖励的列表：$\tau_i = s_i,a_i,r_i$对于时间步$t=0$到$t=T_i$。在这项工作中，我们使用REINFORCE [39]算法，如算法2所示。这个算法通过利用蒙特卡罗采样（通过计算一个完整回合的奖励来学习）来工作。在收集步骤（第2行）之后，算法使用更新的策略梯度和一个学习参数a来更新底层网络（第4行）。注意，在采样轨迹时，使用了E-贪婪策略。

![[Pasted image 20230726103631.png]]

### PPO Algorithm

**策略梯度方法**是一类基于策略的强化学习算法，它们通过对策略参数进行梯度下降来优化一个期望累积折扣奖励的目标函数。策略梯度方法的核心思想是利用按照策略产生的执行轨迹来估计梯度，并使用经验回报或优势函数作为梯度的方向。策略梯度方法可以处理连续的动作空间，并且可以在每个时间步更新策略参数。

策略梯度方法的工作原理是通过计算策略梯度的估计量，并将其应用到随机梯度上升算法中。最常用的梯度估计量的形式如下，其中$\pi_{\theta}$是一个随机策略，$\hat{A}_{t}$是时间步$t$处的优势函数的估计量。

${\hat{g}}={\hat{\mathbb{E}}}_{t}\left[\nabla_{\theta}\log\pi_{\theta}(a_{t}\mid s_{t}){\hat{A}}_{t}\right]\tag1$

这里，期望$\hat{\mathbb{E}}_{t}[\ldots]$表示在一个有限的样本批次上的经验平均，在一个交替采样和优化的算法中。使用自动微分软件的实现方法是通过构造一个目标函数，其梯度是策略梯度估计量；估计量$\hat{g}$是通过对目标函数求导得到的

$$
L^{PG}(\theta)=\hat{\mathbb{E}}_{t}\Big[\log\pi_{\theta}(a_{t}\mid s_{t})\hat{A}_{t}\Big]. \tag2
$$

这个目标函数可以看作是根据[34]给出的期望累积折扣奖励的目标函数的对数似然，其中$Q^{\pi_{\theta}}(s,a)$表示从状态$s$开始，（确定性地）选择动作$a$，然后按照策略$π_θ$行动所获得的期望累积折扣奖励。

虽然使用相同的轨迹对这个损失$L^{PG}$进行多步优化是很有吸引力的，但这样做并没有很好的理由，而且从经验上看，它经常导致破坏性的大规模策略更新。

一个典型的策略梯度方法是REINFORCE算法[35]，它使用了蒙特卡罗方法来估计回报，并且可以直观地理解为强化那些导致更好回报的动作。REINFORCE算法通过梯度下降更新策略参数：

$$
\theta\leftarrow\theta+\alpha\sum_{t}\nabla_{\theta}\log\pi_{\theta}(s_{t},a_{t})v_{t},\tag3
$$

这里，$α$是步长。方程[3]沿着$\nabla_{\theta}\log\pi_{\theta}(s_{t},a_{t})v_{t}$这个方向走一步；这个方向表示如何改变策略参数以增加${π_θ}(s_t,a_t)$（状态$s_t$下动作$a_t$的概率）。步长的大小取决于回报$v_t$有多大。

在我们的设计中，我们使用了PPO算法[32]，它使用了剪裁函数来限制新旧策略之间的比率，并且使用了一种更一般的优势函数估计方式。PPO算法可以进行多次的小批量更新，而不是像标准的策略梯度方法那样每个数据样本只进行一次梯度更新。PPO算法在实现简易性、样本复杂度和调整简易性之间取得了平衡，在各种强化学习任务上表现出了优异的性能。更多细节如下。

**PPO算法**是一种基于策略梯度的强化学习算法，它的目标是在与环境交互采样数据后，使用随机梯度上升优化一个“替代”目标函数，从而改进策略。PPO算法有两种主流的实现方式，一种是使用KL散度作为惩罚项来限制策略更新的幅度，另一种是使用剪裁函数来限制新旧策略之间的比率。PPO算法还使用了一种更一般的优势函数估计方式，并且从环境中获取较长的样本序列，然后用于执行多个训练周期。PPO算法在实现简易性、样本复杂度和调整简易性之间取得了平衡，在各种强化学习任务上表现出了优异的性能。

PPO算法是由OpenAI团队发表在2017年的论文[1]中提出的。它是在TRPO之后很久才提出的一种改进算法。TRPO也是一种基于策略梯度的强化学习算法，它通过最大化一个近似的目标函数来更新策略，同时保证与前一策略的KL散度在一个信任域内。TRPO虽然在连续控制任务上表现良好，但是它不适合共享参数或有辅助损失函数的情况。

PPO算法对经典A2C方法的核心改进是更改用于估算策略梯度的公式。A2C方法是一种基于演员-评论家框架的强化学习算法，它使用一个演员网络来输出动作概率分布，一个评论家网络来输出状态值函数。A2C方法使用所执行动作的对数概率梯度来更新演员网络，即：

$$
\nabla_{\theta}J(\theta)=\mathbb{E}_{t}[\nabla_{\theta}\log\pi_{\theta}(a_{t}\mid s_{t})A_{t}]
$$

其中$A_t$是优势函数，表示动作$a_t$相对于状态$s_t$的平均价值的优势程度。A2C方法每个数据样本只进行一次梯度更新，因此可能导致高方差和低效率。

PPO方法不是使用所执行动作的对数概率梯度，而是使用了一个不同的目标：由优势值进行缩放的新策略和旧策略之间的比率。这个目标可以写为：

$$
J_{\theta}=E_{t}\left[\frac{\pi_{\theta}(a_{t}\mid s_{t})}{\pi_{{\theta_{\mathrm{old}}}}(a_{_t}\mid s_{t})}A^{\mathrm{GAE}(\gamma, \lambda)}_{t}\right]
$$

其中$\pi_{{\theta_{\mathrm{old}}}}$表示更新前的旧策略。这个目标与交叉熵方法有相同的原理：重要性采样。为了限制更新，PPO算法使用了裁剪后的目标：

$$
J_{\theta}^{\mathrm{clip}}=\mathbb{E}_{t}\left[\min(r_{t}(\theta)A^{\mathrm{GAE}(\gamma, \lambda)}_{t},\mathrm{clip}(r_{t}(\theta),1-\varepsilon,1+\varepsilon)A^{\mathrm{GAE}(\gamma, \lambda)}_{t})\right]
$$

此目标将新旧策略之间的比率限制在$[1–ε, 1+ε]$区间内，因此通过更改$ε$，我们可以限制更新的大小。这种裁剪方法可以避免使用KL散度惩罚或自适应更新，从而简化了算法。

PPO算法使用了一种更一般的优势函数估计方式，即GAE（Generalized Advantage Estimation）。GAE是由John Schulman等人在2015年的论文[2]中提出的。GAE的基本思想是利用TD($\lambda$)方法来平衡偏差和方差，同时减少优势函数的方差。GAE的定义如下：

$$
A^{\mathrm{GAE}(\gamma, \lambda)}_{t}=\sum_{l=0}^{\infty}(\gamma \lambda)^{l} \delta_{t+l}
$$

其中$\delta_t$是TD误差，即$\delta_t = R_t + \gamma V_\theta(S_{t+1}) - V_\theta(S_t)$。可以看出，GAE是对TD误差的加权累加，其中权重随着$l$的增加而指数衰减。当$\lambda=0$时，GAE退化为一步TD误差；当$\lambda=1$时，GAE退化为MC估计。因此，通过调节$\lambda$的值，我们可以在偏差和方差之间找到一个平衡点。GAE还有一个重要的性质是，它可以消除优势函数的常数偏移量。

PPO算法从环境中获取较长的样本序列，然后用于执行多个训练周期。PPO算法在实现简易性、样本复杂度和调整简易性之间取得了平衡，在每一步都试图计算一个最小化代价函数的更新，同时保证与前一策略的偏离相对较小。PPO算法在各种强化学习任务上表现出了优异的性能。

[1]: [Proximal Policy Optimization Algorithms]
[2]: [High-Dimensional Continuous Control Using Generalized Advantage Estimation]

---

### PPO算法

在上文中我们已经介绍了MDP建模和环境建模，本节我们将介绍一种基于策略梯度的强化学习算法——PPO算法[1]。PPO算法的目标是在与环境交互采样数据后，使用随机梯度上升优化一个“替代”目标函数，从而改进策略。PPO算法的核心思想是利用按照策略产生的执行轨迹来估计梯度，并使用经验回报或优势函数作为梯度的方向。PPO算法可以处理连续的动作空间，并且可以在每个时间步更新策略参数。

PPO算法的目标函数是由优势值进行缩放的新策略和旧策略之间的比率，即：

$$
J_{\theta}=E_{t}\left[\frac{\pi_{\theta}(a_{t}\mid s_{t})}{\pi_{{\theta_{\mathrm{old}}}}(a_{_t}\mid s_{t})}A^{\mathrm{GAE}(\gamma, \lambda)}_{t}\right]
$$

其中$\pi_{{\theta_{\mathrm{old}}}}$表示更新前的旧策略。这个目标与交叉熵方法有相同的原理：重要性采样。为了限制更新，PPO算法使用了裁剪后的目标：

$$
J_{\theta}^{\mathrm{clip}}=\mathbb{E}_{t}\left[\min(r_{t}(\theta)A^{\mathrm{GAE}(\gamma, \lambda)}_{t},\mathrm{clip}(r_{t}(\theta),1-\varepsilon,1+\varepsilon)A^{\mathrm{GAE}(\gamma, \lambda)}_{t})\right]
$$

此目标将新旧策略之间的比率限制在$[1–ε, 1+ε]$区间内，因此通过更改$ε$，我们可以限制更新的大小。这种裁剪方法可以避免使用KL散度惩罚或自适应更新，从而简化了算法。

PPO算法有两种主流的实现方式，一种是使用KL散度作为惩罚项来限制策略更新的幅度，另一种是使用剪裁函数来限制新旧策略之间的比率。PPO算法还使用了一种更一般的优势函数估计方式，即GAE（Generalized Advantage Estimation）。GAE是由John Schulman等人在2015年的论文[2]中提出的。GAE的基本思想是利用TD($\lambda$)方法来平衡偏差和方差，同时减少优势函数的方差。GAE的定义如下：

$$
A^{\mathrm{GAE}(\gamma, \lambda)}_{t}=\sum_{l=0}^{\infty}(\gamma \lambda)^{l} \delta_{t+l}
$$

其中$\delta_t$是TD误差，即$\delta_t = R_t + \gamma V_\theta(S_{t+1}) - V_\theta(S_t)$。可以看出，GAE是对TD误差的加权累加，其中权重随着$l$的增加而指数衰减。当$\lambda=0$时，GAE退化为一步TD误差；当$\lambda=1$时，GAE退化为MC估计。因此，通过调节$\lambda$的值，我们可以在偏差和方差之间找到一个平衡点。GAE还有一个重要的性质是，它可以消除优势函数的常数偏移量。

PPO算法从环境中获取较长的样本序列，然后用于执行多个训练周期。PPO算法在每一步都试图最小化一个代价函数，同时保证与前一策略的偏离相对较小。PPO算法在实现简易性、样本复杂度和调整简易性之间取得了平衡，在各种强化学习任务上表现出了优异的性能。

**Algorithm 1** PPO, Actor-Critic Style

- for iteration=1,2,... do
  - for actor=1,2,... do
    - Run policy $\pi_{\theta_{old}}$ in environment for T timesteps
    - Compute advantage estimates $\hat{A}_1, \dots, \hat{A}_T$
  - end for
  - Optimize surrogate $L$ wrt $\theta$, with $K$ epochs and minibatch size $M\le NT$
  - $\theta_{old}\gets \theta$
- end for

为了解决第三章提出的车联网环境中的任务卸载问题，我们采用以上描述的PPO算法。这种算法可以处理具有连续状态和动作空间的RL环境，而不需要像Q-Learning等基于值函数的算法那样估计状态-动作值，而是直接学习最优策略。

在我们的车辆任务卸载场景中，我们自定义的RL环境将提供与真实车联网场景相似的任务特征，包括任务的类型、大小、持续时间等。此外，边缘服务器的资源也会被考虑，因此CPU和内存的可用性也将作为状态空间的一部分被使用和更新。

我们使用多智能体强化学习的框架，让每个边缘服务器都部署一个PPO代理，根据自己的状态和动作来更新自己的策略。这样，不同的服务器之间的策略可以根据任务需求和市场竞争进行自适应调整，从而提高自身的收入和车辆的效用。同时，多智能体强化学习也可以处理多个智能体之间的交互和博弈，使得每个服务器能够考虑其他服务器的行为和影响。为了简化模型，我们假设服务器之间没有额外的通信，而且只进行一次博弈。

我们使用RLlib来实现和管理训练过程，它可以并行地从环境中采集数据，汇总到一个训练批次中，然后根据选择的算法和损失函数计算梯度，并更新模型参数。我们重复这个过程，直到满足终止条件，如达到最大迭代次数、最大训练时间或最佳性能等。在训练过程中，RL模拟反馈回路不断地收集数据，包括观察到的状态、采取的动作、获得的奖励和所谓的结束标志，表示代理在模拟中经历的不同的回合。每次PPO代理采取一个动作（报价一个任务），都会得到一个即时奖励。下一个状态也将依赖于前一个状态，因为CPU和内存的可用性将在每次任务卸载后进行更新。最终，PPO代理应该能够学习任务的需求和优先级，并完成所有任务的报价，以最大化自身的收入和车辆的效用。

![[da316cb2d97f69253083b7f353cd11e4_MD5.png]]

# 多智能体强化学习Training Algorithm

为使用MADRL解决车联网中的任务调度问题, 我们使用了两类基于DRL的算法, 分别为Value-based 和 Policy-based. 为同时测试这两类方法, 我们使用了离散的和连续的两种动作空间. 我们假定环境中的固定参数都相同, 每一个边缘服务器具有一个Agent来根据策略$\pi_i(a_i|s_i)$来确定动作.

![[Pasted image 20230719095250.png]]

深度强化学习的工作流程如下：首先，它创建一组rollout workers，每个worker可以并行地从环境中采集数据，并使用一个或多个策略来与环境交互。每个worker生成的数据包含一系列的动作、奖励、下一个状态和结束标志，这些数据构成了一个episode或rollout。然后，RLlib将所有worker生成的rollout汇总到一个训练批次中，接着根据选择的算法和损失函数计算梯度，并更新模型参数。最后，RLlib重复这个过程，直到满足终止条件，如达到最大迭代次数、最大训练时间或最佳性能等 。

- RLlib使用rollout workers来并行地从环境中采集数据，每个worker可以使用一个或多个策略来与环境交互，产生一系列的动作、奖励、下一个状态和结束标志，这些数据构成了一个episode或rollout。
- RLlib将每个worker产生的rollout汇总到一个训练批次中，然后根据选择的算法和损失函数计算梯度，并更新模型参数。
- RLlib重复这个过程，直到满足终止条件，如达到最大迭代次数、最大训练时间或最佳性能等。

使用分布式执行以增加灵活性

为了解决边缘计算中的车联网工作流调度问题，我们提出了一个基于多智能体强化学习和反向拍卖机制的方法。我们首先将这个问题建模成了一个在线和NP-hard问题，因为车联网的任务卸载可以看作一个有时序的多维背包问题。然后使用强化学习方法来处理复杂的动态的高维度的决策问题。我们采用多智能体强化学习来实现分布式的资源分配，避免集中式管理的通信开销和信息的不准确性，提高系统的稳定度。我们还使用了一种基于反向拍卖的任务卸载方法，用于解决车联网中的资源分配问题。反向拍卖是一种一买方多卖方的拍卖形式，适用于买方市场的交易活动。买方提出自己的需求和任务优先级，卖方根据自身资源使用情况和任务特征给出报价。买方只选择一个卖方进行交易。反向拍卖可以有效降低买方的采购成本，提高卖方之间的竞争力，实现资源的优化分配。

### 多智能体强化学习环境建模

在第三章的车联网任务卸载建模中, 边缘服务器被定义为智能体（agent）, 每个智能体通过与环境交互学习可以最大化自身收益的策略. 因为在多智能体环境中,一个智能体的动作会影响其他智能体的决策. 为了描述这种情况，我们使用部分可观察马尔可夫决策过程（partially observable Markov decision process, POMDP）来建模多智能体强化学习. 由K个智能体组成的POMDP可以被定义为

$$
(\mathbf S,\mathbf O_1,\dots,\mathbf O_K,\mathbf A_1,\dots,\mathbf A_K, \mathbf R_1,\dots,\mathbf R_K, \mathbf P_{ss'}, \mathbf P_{o|s})
$$

其中$\mathbf S$为状态空间（state space），$\mathbf O_i$为第i个智能体的观测空间（observation space），$\mathbf A_i$为第i个智能体的动作空间（action space），$\mathbf R_i$为第i个智能体的奖励函数（reward function），$\mathbf P_{ss'}$为状态转移概率（transition probability），$\mathbf P_{o|s}$为观测生成概率（observation likelihood）。在每一个时间间隙, 边缘服务器作为一个智能体, 观察当前环境生成的观测$o_i\in\mathbf O_i$, 根据策略$\pi_i(a_i|o)$选择一个合适的动作$a_i\in\mathbf A_i$. 然后获得相应的奖励$r_i=\mathbf R_i(s,a_1,\dots,a_K)$. 在本章后续部分，我们将详细介绍如何定义$\mathbf S,\mathbf O,\mathbf A,\mathbf R$以及如何设计多智能体强化学习环境.

Littman, M. L. (1994). Markov games as a framework for multi-agent reinforcement learning. In Machine learning proceedings 1994 (pp. 157-163). Morgan Kaufmann.

#### 状态空间 State Space

在强化学习方法中, 状态空间是指整个系统的描述，包含了系统中所有的信息。本次研究的是多智能体强化学习，因此状态空间应当包含所有服务器集群和任务队列的状态。状态空间可以用一个集合$\mathbf S$表示，即

$$
\mathcal S=\{\mathcal B,\mathcal S, \mathbf r_{\mathcal B}\}
$$

其中$\mathcal B$表示卖家服务器,$\mathcal S$表示买家车辆提交的任务请求, $\mathbf r_{\mathcal B}=\{r_{\mathbf s_1},\dots,r_{\mathbf s_m}\}$表示卖家服务器的资源使用情况, $r_{\mathbf s_i}=(r_{\mathbf s_i}(1),\dots,r_{\mathbf s_i}(T))$。

#### 观察空间 Observation Space

在多智能体强化学习中，观察空间是指一个智能体可以观察到的状态空间的子集。在本次研究中，我们假设智能体之间不存在额外的通信与交互，因此智能体$i$所能观察到的状态空间就是自身的服务器在从当前时间$t$到之后一定时间的资源使用情况$\mathbf r_{\mathbf s_i,t}=(r_{\mathbf s_i}(t),\dots,r_{\mathbf s_i}(t+\epsilon))$和当前等待被智能体报价的任务$\theta_j$的任务执行特征$\mathbf f_j$. 因此，边缘服务器i的观察空间为

$$
O_i=\{r_{\mathbf s_i,t},\mathbf f_j\}
$$

。这意味着每个智能体只能根据自己的服务器负载和当前任务的特征来做出决策，而无法获取其他智能体的状态信息。

#### 动作空间 Action Space

动作空间是指智能体可以向环境执行的动作的集合。在多智能体强化学习中，每个智能体都可以执行动作以影响环境的状态，从而获得奖励。不同的任务可能需要不同类型的动作空间，例如离散的或连续的。在本次研究中，我们考虑了一个多智能体服务器报价任务，其中每个智能体代表一个边缘服务器，需要对收到的任务进行报价，以竞争任务的分配。

我们假设每个智能体的动作空间是一个在$[1,2]$的数值，表示其对任务报价的浮动系数。这意味着每个智能体可以选择任意一个在$[1,2]$区间内的实数作为其报价系数$a_i$。这样可以使得智能体有更大的自由度和灵活性来调整自己的报价策略。我们可以用一个区间$\mathbf A_i$表示第$i$个智能体的动作空间，即

$$
\mathbf A_i=[1,2]
$$

当环境接收到智能体$i$的$a_i$的值后, 则其对当前等待被拍卖的任务$\theta_j$的最终报价为

$$
q_i = a_i \times \mathbf p \cdot \mathbf r_j \times \tau_j
$$

，其中$\mathbf p$是资源的市场平均价格向量, $\mathbf r_j$是资源使用量, $\tau_j$是资源使用时间。

然而，并不是所有的服务器都可以对任意的任务进行报价，因为任务有执行服务器限制$\mathcal S_j \subseteq \mathcal S$，表示只有$\mathcal S_j$中的服务器才能满足任务$j$的需求。因此，在时间步$t$，整个系统的动作空间为

$$
A_t=\{a_1 \sigma_{j,1},\dots,a_m\sigma_{j,m}\}
$$

其中，$a_i\sigma_{j,i}$代表第$i$个服务器对其当前请求报价的任务$j$的报价系数。如果某个服务器不满足任务$j$的执行服务器条件限制，则其对应的报价系数为0，表示无法执行任何动作。当对任务$j$进行报价时, 环境仅收集任务对应满足条件限制的服务器的报价, 而忽略其他服务器的报价。

#### 奖励函数 Reward Function

在多智能体强化学习中，奖励函数定义了智能体的目标和评估标准，描述了智能体在执行某一动作后，从一个状态转移到另一个状态所获得的即时奖励。智能体会根据奖励函数来判断自己的动作对于自己目标的达成是有利还是有害，从而调整自己的行为策略，以最大化自己的累积奖励。本次论文考虑了一个多智能体服务器报价任务，其中每个智能体代表一个边缘服务器，需要对收到的任务进行报价，以竞争任务的分配。边缘服务器的目标是最大化自己的收益，即任务支付与服务器成本的差额。因此，我们设计了如下的奖励函数：

$$
\mathbf R_i = p_{t,i}-c_i
$$

这个奖励函数表示第$i$个边缘服务器在一个时间步内获得的奖励，它由两部分组成：$p_{t,i}$表示边缘服务器在一个时间步内完成的任务支付之和，$c_i$表示边缘服务器在一个时间步内的固定成本。

### 基于反向拍卖的车联网任务卸载方法

任务卸载涉及到多个参与者之间的资源分配问题，即如何将车辆的任务分配给合适的边缘服务器，并使得各方都能获得最大化的效用。为了解决这一问题，本文借鉴了经济学中的反向拍卖（Reverse Auction）理论，提出了一种基于反向拍卖的任务卸载方法（Reverse Auction-based Task Offloading, RATO）。反向拍卖是一种一买方多卖方的拍卖形式，适用于买方市场（Buyer’s Market）的交易活动。在反向拍卖中，买方提出自己的需求和预算，卖方根据自身的成本和利润，竞争性地给出自己的报价。最终，买方选择一个或多个报价最低且满足需求的卖方进行交易。反向拍卖可以有效降低买方的采购成本，提高卖方之间的竞争力，实现资源的优化分配。

本文将分别介绍本模型中反向拍卖的角色和过程。

**买方**是车联网中有任务卸载需求，并愿意为其需求支付的车辆所产生的一次请求$\mathbf b_j$。在某一时刻 $t$，车辆自身评估连接状态并向满足其连接限制的边缘服务器发起任务 $\theta_j$的卸载请求, 并将其任务属性 $\mathbf{f}_j$ 发送到满足连接约束的服务器集合$\mathcal S_j$中以请求报价。

**卖方**是车联网中的边缘服务器。边缘服务器可以接收车辆的任务卸载请求，并利用其自身资源来完成任务卸载和获得收益。卖家服务器 $\mathbf s_i$ 的特征可以被定义为 $\mathbf{s}_i = (\mathbf{r}_i, c_i)$，其中资源向量 $\mathbf{r}_i$ 和成本 $c_i$。服务器在接收到报价请求后, 确定能否接受任务卸载, 如果可以接受则根据当前时刻对可用资源$\mathbf r_{s_i,t}$和任务特征$\mathbf f_j$的观测, 通过强化学习学习到的策略, 对任务进行报价.

为简化问题,本文做出以下基本假设：

- 假设时间是分时的，任务只能在一个时间段提交，并且任务时长为整数个时间段。
- 车辆可以根据自身状态，评估在拍卖和卸载的时间内的连接情况，得到任务的边缘服务器限制条件。
- 在卸载市场中存在信息缺失：
  - 卖方的特征和维护成本对买方不可见。增加拍卖的竞争性，提高买方的效用。
  - 没有卖方知道其他参与者的出价，也没有卖方相互勾结。
  - 买方的预算不能公开给卖方。
- 拍卖过程中，一次仅有一个任务参与，在任务拍卖结束后，进入本时隙的下一个任务的拍卖。防止卖方串通抬价。
- 一个买方只能把他的任务卸载给一个卖方，但一个卖方可以同时为多个买方提供服务。
- 任务卸载的结果对于车辆而言，其效用是一致的。

在基于云计算的车联网场景中，我们考虑了 $m$ 个卖家 $\mathcal S$ 和 $n$ 个买家 $\mathcal B$ 的情况。每个买家代表一个车辆，它通过自身的通信单元与边缘服务器进行通信，并且根据与边缘服务器的通信状况生成相应的限制条件，以便在需要进行任务卸载时选择合适的卖家。每个卖家代表一个边缘服务器，它可以通过有线链接与其他边缘服务器和互联网相连，以返回卸载结果。同时，边缘服务器也可以通过有线连接返回任务执行结果和传输与车辆之间的数据。算法1是反向拍卖过程的具体描述.

算法1：基于反向拍卖的任务卸载方法

输入：车辆集合$\mathcal B$，服务器集合$\mathcal S$，时间槽长度$\epsilon$

输出：任务卸载结果$\mathcal R$

1. 初始化：对于每个服务器$\mathbf s_i \in \mathcal S$，初始化其资源向量$\mathbf r_i$，成本$c_i$，报价策略$\pi_i$；初始化卸载任务集合$\mathcal U=\{\mathcal U_1,\dots,\mathcal U_T\}$。
2. 对于每个时间槽$t=1,2,\dots,T$，执行以下步骤：
   1. 对于服务器集群内的每一个服务器$\mathbf s_i \in \mathcal S$, 更新自己的资源使用情况和结算上一个时间槽的收益$p_{t-1,i}-c_i$
   2. 从$\mathcal U_t$中的按照时间顺序选择买家车辆请求$\mathbf b_j$，则执行以下步骤：
      1. 车辆评估自身的连接状态，并生成任务的服务器限制向量$\mathbf \sigma_j$。
      2. 车辆向满足限制条件的服务器集合$\mathcal S_j = \{\mathbf s_i | \forall \mathbf s_i \in \mathcal S, \sigma_{j,i} = 1\}$发送任务卸载请求$\mathbf f_j$。
      3. 对于每个服务器$\mathbf s_i \in \mathcal S_j$，如果其可用资源$r_{s_i}(t)$能够满足任务的资源需求$r_{\theta_j}(t)$，则执行以下步骤：
         1. 服务器根据自身在$[t,t+\epsilon]$时间段的资源使用情况$\mathbf r_{s_i,t}$和任务特征$\mathbf f_j$，以及报价策略$\pi_i$，计算出自己对任务的报价$q_{j,i}$。
         2. 服务器将报价$q_{j,i}$返回给车辆。
      4. 车辆收集所有服务器的报价$\mathcal Q_j = \{q_{j,i} | \forall \mathbf s_i \in \mathcal S_j\}$，并将其非降序排列。
      5. 车辆选择报价最低的服务器作为胜者$\mathbf s_k =\mathop{\arg\min}\limits_{\mathbf s_i}\ q_{j,i}$，并检查其报价$p_j = q_{j,k}$是否满足预算条件$p_j \leq \psi_j$。如果不满足，则本次拍卖失败，任务卸载失败；如果满足，则本次拍卖成功，任务卸载成功，并执行以下步骤：
         1. 车辆将任务发送到胜出者服务器$\mathbf s_k$并支付$p_j$，等待执行结果返回。
         2. 服务器$\mathbf s_k$获得收益, 为任务分配资源并执行任务卸载，更新自身的资源使用情况$\mathbf r_{s_i,t}$
      6. 更新任务卸载结果$\mathcal R$
3. 当到达总时间步$T$时间后, 卸载结束

### PPO算法

在上文中我们已经介绍了MDP建模和环境建模，本节我们将介绍一种基于策略梯度的强化学习算法——PPO算法[1]。PPO算法的目标是在与环境交互采样数据后，使用随机梯度上升优化一个“替代”目标函数，从而改进策略。PPO算法的核心思想是利用按照策略产生的执行轨迹来估计梯度，并使用经验回报或优势函数作为梯度的方向。PPO算法可以处理具有连续状态和动作空间的强化学习环境。

PPO算法的目标函数是由优势值进行缩放的新策略和旧策略之间的比率，即：

$$
J_{\theta}=E_{t}\left[\frac{\pi_{\theta}(a_{t}\mid s_{t})}{\pi_{{\theta_{\mathrm{old}}}}(a_{_t}\mid s_{t})}A^{\mathrm{GAE}(\gamma, \lambda)}_{t}\right]
$$

其中$\pi_{{\theta_{\mathrm{old}}}}$表示更新前的旧策略。这个目标与交叉熵方法有相同的原理：重要性采样。为了限制更新，PPO算法使用了裁剪后的目标：

$$
J_{\theta}^{\mathrm{clip}}=\mathbb{E}_{t}\left[\min(r_{t}(\theta)A^{\mathrm{GAE}(\gamma, \lambda)}_{t},\mathrm{clip}(r_{t}(\theta),1-\varepsilon,1+\varepsilon)A^{\mathrm{GAE}(\gamma, \lambda)}_{t})\right]
$$

此目标将新旧策略之间的比率限制在$[1–ε, 1+ε]$区间内，因此通过更改$ε$，我们可以限制更新的大小。这种裁剪方法可以避免使用KL散度惩罚或自适应更新，从而简化了算法。

PPO算法有两种主流的实现方式，一种是使用KL散度作为惩罚项来限制策略更新的幅度，另一种是使用剪裁函数来限制新旧策略之间的比率。PPO算法还使用了一种更一般的优势函数估计方式，即GAE（Generalized Advantage Estimation）。GAE是由John Schulman等人在2015年的论文[2]中提出的。GAE的基本思想是利用TD($\lambda$)方法来平衡偏差和方差，同时减少优势函数的方差。GAE的定义如下：

$$
A^{\mathrm{GAE}(\gamma, \lambda)}_{t}=\sum_{l=0}^{\infty}(\gamma \lambda)^{l} \delta_{t+l}
$$

其中$\delta_t$是TD误差，即$\delta_t = R_t + \gamma V_\theta(S_{t+1}) - V_\theta(S_t)$。可以看出，GAE是对TD误差的加权累加，其中权重随着$l$的增加而指数衰减。当$\lambda=0$时，GAE退化为一步TD误差；当$\lambda=1$时，GAE退化为MC估计。因此，通过调节$\lambda$的值，我们可以在偏差和方差之间找到一个平衡点。GAE还有一个重要的性质是，它可以消除优势函数的常数偏移量。

为了解决第三章提出的车联网环境中的任务卸载问题，我们采用以上描述的PPO算法。这种算法可以根据任务的需求和优先级，直接学习最优策略，而不需要像Q-Learning等基于值函数的算法那样估计状态-动作值。

我们使用多智能体强化学习的框架，让每个边缘服务器都部署一个PPO代理，根据自己的状态和动作来更新自己的策略。这样，不同的服务器之间的策略可以根据任务需求和市场竞争进行自适应调整，从而提高自身的收入和车辆的效用。同时，多智能体强化学习也可以处理多个智能体之间的交互和博弈，使得每个服务器能够考虑其他服务器的行为和影响。为了简化模型，我们假设服务器之间没有额外的通信，而且只进行一次博弈。

接下来，我们将介绍我们的强化学习仿真环境的具体设计, 展示我们的实验结果和分析，比较PPO算法与其他基准算法的性能和效率。

---

**行动空间。** 行动空间是一个智能体可以在环境中执行的动作集合。我们假设每个智能体的行动空间是在范围$[1,2]$内的数值，表示其用于竞标任务的浮点系数。这意味着每个智能体可以选择范围$[1,2]$内的任意实数作为其竞标系数$a_i$。这为智能体提供了更多调整其竞标策略的自由度和灵活性。我们可以使用范围$\mathbf A_i$来表示第$i$个智能体的行动空间，如下所示：$\mathbf A_i=[1,2]$ 当环境从智能体$i$处接收到$a_i$的值时，那么它对于当前等待拍卖的任务$\theta_j$的最终出价为：$q_i = a_i \times \mathbf p \cdot \mathbf r_j \times \tau_j$ 其中$\mathbf p$是资源的市场平均价格向量，$\mathbf r_j$是资源使用情况，$\tau_j$是资源使用时间。然而，并非所有服务器都可以对任何任务进行竞标，因为任务有执行服务器限制$\mathcal S_j \subseteq \mathcal S$，这意味着只有在$\mathcal S_j$中的服务器才能满足任务$j$的要求。因此，在时间步$t$中，整个系统的行动空间是：$A_t=\{a_1\sigma_{j,1},\dots,a_m\sigma_{j,m}\}$ 其中$a_i\sigma_{j,i}$表示第$i$个服务器对当前被请求的任务$\theta_j$的竞标系数。如果服务器不符合任务$\theta_j$的执行服务器条件，则其相应的竞标系数为0，表示它无法采取任何行动。在对$\theta_j$任务进行竞标时，环境仅收集满足任务条件的服务器的竞标，并忽略其他服务器的竞标。

## 实验

计算卸载是一种在车联网中提高车辆性能和节省能耗的技术，它让车辆把部分计算任务卸载到边缘服务器上。计算卸载的目标是最大化车辆的效用，同时保证边缘服务器的收益和用户的体验。为此，边缘服务器需要设计合适的竞价策略，即根据任务的需求、紧急性和自身的资源等约束，决定每次报价的金额。

学习竞价策略涉及复杂的环境建模、不完全信息和动态优化问题，其验证方法选择至关重要。离线验证和在线验证是常见方法，但二者各有优缺点。离线验证基于历史数据进行反事实估计，但结果可能受数据偏差、反事实假设以及环境变化等因素的影响，甚至可能不符合古德哈特定律。相比之下，在线验证通过真实环境中的实验直接测试竞价策略的表现，更可靠，然而成本高、耗时且存在一定风险。

强化学习作为一种通过智能体与环境交互学习最佳行为的方法，在游戏、机器人、推荐系统等领域具有广泛应用前景。尽管其在实践中取得成功，研究社区明确指出其方法不足，并认为可靠的仿真环境是近年的重大进展核心。仿真环境提供接近真实环境但不受其限制和影响的平台，使研究者能快速开发、测试和改进强化学习算法。在某些相关领域，如推荐系统，仿真环境已广泛采用作为有效的评估机制。

针对车联网环境中计算卸载的服务器报价策略的验证，我们依据上述模型开发了相应的强化学习环境。在竞价策略学习过程中，我们利用强化学习辅助边缘服务器根据任务卸载特征和自身约束，动态决定每次出价，以最大化效用或收益。

### 仿真环境的设计和参数设置

为了验证提出的基于多智能体强化学习和反向拍卖机制的车联网任务卸载方法（RATO）的有效性，我们开发了一个名为VehicleJobScheduling的基于Python的开源仿真环境。该仿真环境旨在模拟车辆任务卸载的过程，并提供一系列常用的评估指标。在设计中，我们借鉴了PettingZoo，这是一个广受欢迎的多智能体强化学习框架，其定义了一套标准接口，使得不同算法可以在同一环境中进行比较。

VehicleJobScheduling仿真环境主要由三个组成部分构成，即车辆、服务器和环境生成器。在这个环境中，车辆充当任务的发起者，根据资源需求、连接限制和预算选择适当的服务器进行任务卸载，并支付相应费用。服务器则是任务的执行者，根据资源状况和成本，对收到的任务卸载请求进行报价，并执行被接受的任务。环境生成器负责生成任务和服务器的参数，包括任务的到达率、特征、连接限制，以及服务器的数量、类型、资源向量、成本等。用户可以通过设置这些参数，探索不同的任务卸载场景和策略。

我们的仿真环境支持Agent Environment Cycle（ACE）模式，这是一种适用于顺序回合制环境的接口，能够处理任何多智能体强化学习算法。在ACE模式下，每个智能体代表一个边缘服务器，接收车辆的任务卸载请求，利用自身资源完成任务卸载并获取收益。在每个时间步，有一个或多个车辆向满足其连接限制的服务器集合发送任务卸载请求，并等待服务器返回报价。环境按照连接限制选择一个服务器作为当前回合的行动者，该服务器根据自身资源使用情况、任务特征和报价策略计算任务报价，返回给车辆。当所有满足连接限制的服务器完成报价后，车辆选择一个或多个报价最低、同时满足需求和预算条件的服务器进行交易。交易成功后，车辆将任务发送到胜出者服务器并支付相应金额，等待执行结果返回。服务器获得收益，为任务分配资源并执行任务卸载。如果没有服务器的报价满足车辆的条件，则车辆无法完成任务卸载，无需支付任何金额。

为了模拟车联网场景，我们考虑了一个由多个车辆和多个边缘服务器构成的公路仿真环境。在这个环境中，车辆会对服务器信道进行评估，并向满足信道条件的边缘服务器发送任务卸载请求。我们假设任务请求的数量符合泊松分布，单位时间的任务到达数为20，涵盖了三种任务类型：存储密集型、计算密集型和存储计算密集型，以及两种任务时长：长任务和短任务。任务的持续时间被设定为1到10个时间槽，每个时间槽为1分钟，因此每个任务卸载请求的最大执行时间为10分钟。任务的优先级被设置在0到10之间，而任务对CPU和内存的请求数量最大分别为24和100。任务的连接限制范围为0到3，表示任务可以向满足信道条件的0到3个服务器发送任务卸载请求。

我们对车辆的任务卸载请求进行了抽象，将其表示为任务特征，其中包含四个维度：优先级、资源需求、持续时间以及任务连接限制。任务的优先级在0到10之间取值，并且它影响车辆对任务的预算分配。任务对CPU和内存的请求数量最大分别为24和100。任务的持续时间被设定为1到10个时间槽，每个时间槽为1分钟，因此每个任务卸载请求的最大执行时间为10分钟。任务的连接限制范围为0到3，表示任务可以向满足信道条件的0到3个服务器发送任务卸载请求。

在实验中，我们选择了[Azure的专用主机](https://learn.microsoft.com/zh-cn/azure/virtual-machines/dedicated-hosts)作为边缘服务器，提供多种规格和价格的虚拟机。我们选取了三种不同的专用主机SKU，分别代表大、中、小型的边缘服务器。云服务提供商的成本是按照三年计划的价格计算，而市场价格则是按照Azure美国东部[容器实例](https://azure.microsoft.com/zh-cn/pricing/details/container-instances/)的价格计算。表1列出了我们选用的专用主机的规格和价格。

| Dedicated Host SKUs | vCPUs | RAM       | 3 year plan |
| ------------------- | ----- | --------- | ----------- |
| Mdsv2MedMem-Type1   | 192   | 2,048 GiB | $ 5.81      |
| Easv5-Type1         | 112   | 768 GiB   | $ 3.49      |
| Fsv2-Type2          | 72    | 144 GiB   | $ 2.05      |

表1 选用的专用主机

| 参数        | 取值范围             |
| --------- | ---------------- |
| 任务到达率分布   | 泊松分布             |
| 单位时间任务到达数 | 20               |
| 任务时长      | 1到10个时间槽，每个槽为1分钟 |
| 任务优先级     | 0到10             |
| 任务CPU请求数  | 最大为24            |
| 任务内存请求数   | 最大为100           |
| 任务连接限制范围  | 0到3              |
| 表 环境超参数   |                  |

| Agent Model 类型  | PPO Agent     | PPO LTSM Agent        |
| --------------- | ------------- | --------------------- |
| 网络结构            | 多层感知机（MLP）    | 多层感知机 + 长短时记忆网络（LSTM） |
| 全连接层参数          | [384,384,384] | [384,384,384]         |
| LSTM单元大小        | -             | 256                   |
| 激活函数            | tanh          | tanh                  |
| 表 强化学习 Agent 参数 |               |                       |

在本文中，我们使用了两种不同的强化学习Agent的Model，分别是PPO Agent和PPO LTSM Agent，它们都是基于PPO（Proximal Policy Optimization）算法的，但是在网络结构上有所不同。PPO Agent的Model是一个多层感知机（MLP），它由三个全连接层组成，每层的神经元数量为384，激活函数为tanh。PPO LTSM Agent的Model则是在PPO Agent的Model的基础上增加了一个LSTM层，该层的单元大小为256，也就是每个LSTM单元内部状态向量的维度为256。LSTM层的作用是增加Model的记忆能力，使其能够处理和预测具有时间相关性的数据。我们将这两种Model分别应用于RATO方法，以比较它们在不同场景下的性能表现。

这样的设定使得我们能够在仿真中考虑多样化的车联网场景，涵盖了不同类型和时长的任务，以及车辆与边缘服务器之间的动态交互。接下来，我们将通过对RATO方法在这个仿真环境下的实验进行数值分析，全面评估其在不同场景下的性能表现。我们将特别关注不同服务器报价策略对系统效果的影响，以深入了解提出方法的优势和适用性。

### 深度强化学习代理的收敛性

![[Pasted image 20231128145930.png]]

在本节中，我们分别评估了两种使用PPO强化学习代理的方法, PPO和PPO+LTSM在车联网资源卸载问题中的收敛性能。我们使用了两种基于多智能体强化学习的算法，分别是PPO和PPO+LTSM，来训练多个代理，以最大化服务器在一个回合内的收益。服务器的收益反映了任务卸载的效果，也是代理的奖励函数。我们对代理进行了4480000次迭代的训练，并在每40000次迭代后对算法进行了评估，计算了训练策略在10次测试运行中的平均奖励。图1展示了两种算法在训练过程中的奖励评估情况。表1显示了PPO和PPO+LTSM两种算法在最后448000次迭代中的奖励值的平均值和标准差。

| 算法                                | 平均奖励值       | 标准差         |
| --------------------------------- | ----------- | ----------- |
| PPO                               | $1.68×10^7$ | $2.65×10^5$ |
| PPO+LTSM                          | $1.72×10^7$ | $1.12×10^5$ |
| 表 PPO和PPO+LTSM训练过程中最后10%奖励值方差和平均值 |             |             |

我们可以看到，PPO+LTSM的平均奖励值为$1.72×10^7$，比PPO的$1.68×10^7$高出约**2.4%**。同时，PPO+LTSM的标准差为$1.12×10^5$，比PPO的$2.65×10^5$低出约**57.7%**。这说明PPO+LTSM的奖励值不仅更高，而且更稳定，波动更小。因此，PPO+LTSM在车联网资源卸载方面有更好的收敛性能。综上所述，RATO方法在不同的任务卸载场景下，都表现出了较好的收敛性，能够快速地学习到最优或接近最优的报价策略，从而提高服务器的收益和车辆的效用。这证明了RATO方法的有效性和鲁棒性，以及强化学习和反向拍卖机制的优势和适用性。

### 实验结果及分析

本文提出了一种基于多智能体强化学习和反向拍卖机制的车联网任务卸载方法，简称为RATO，并通过仿真实验与其他基准方法进行了对比分析。本文考虑了任务到达数目为10、20和40的三种情况，采用了四种不同的服务器报价策略：固定报价（FB）、随机报价（RB）、基于PPO的报价（PPO）以及基于PPO+LSTM的报价（PPO+LSTM）。本文采用了四个指标来评估不同的调度策略在不同的到达数目下的表现，分别是：负载均衡（Load Balance）、服务器的收益（Server Earning）、车辆的利用率（Vehicle Utility）和订单的完成率（Complete Rate）。在每个到达数目下，进行了十次独立的仿真实验，利用五个随机种子进行模拟，并对每个指标的平均值和标准差进行了统计和分析。

本文的目的是比较不同报价策略在任务卸载系统中的性能，并分析RATO方法的优势。我们考虑了以下四种报价策略：

- **Fixed策略**：在此策略中，服务器以市场价格为基准，仅参考任务的特征和时长来确定报价。
- **Random策略**：服务器以市场价格为基准，在一定区间内随机采样一个系数来进行报价。
- **PPO策略**：服务器利用强化学习算法PPO，根据自身资源使用情况和任务特征，通过多层感知机（MLP）动态地确定报价。
- **PPO+LSTM策略**：在PPO策略的基础上，服务器增加了一个长短期记忆网络（LSTM）层，以捕捉时序信息，并结合多层感知机进行动态报价。

|          | Load Blance | Server Earning (10^6) | Vehicle Utility (10^6) | Complete Rate |
| -------- | ----------- | --------------------- | ---------------------- | ------------- |
| Random   | 0.19±0.012  | 0.55±0.00037          | 0.55±0.037             | 0.76±0.014    |
| Fixed    | 0.20±0.005  | 1.60±0.00007          | 0.76±0.047             | 0.91±0.010    |
| PPO      | 0.20±0.011  | 1.49±0.00004          | 0.78±0.036             | 0.92±0.010    |
| PPO+LTSM | 0.18±0.010  | 1.53±0.00004          | 0.83±0.063             | 0.93±0.012    |

表 到达数为10时的实验结果

|          | Load Blance | Server Earning (10^6) | Vehicle Utility (10^6) | Complete Rate |
| -------- | ----------- | --------------------- | ---------------------- | ------------- |
| Random   | 0.22±0.007  | 2.74±0.00036          | 0.99±0.033             | 0.72±0.008    |
| Fixed    | 0.23±0.004  | 3.05±0.00014          | 1.43±0.040             | 0.90±0.005    |
| PPO      | 0.22±0.007  | 2.95±0.00011          | 1.53±0.060             | 0.91±0.007    |
| PPO+LTSM | 0.21±0.006  | 3.06±0.00009          | 1.62±0.036             | 0.93±0.009    |

表 到达数为20时的实验结果

|          | Load Blance | Server Earning (10^6) | Vehicle Utility (10^6) | Complete Rate |
| -------- | ----------- | --------------------- | ---------------------- | ------------- |
| Random   | 0.22±0.006  | 4.80±0.00100          | 1.64±0.034             | 0.63±0.007    |
| Fixed    | 0.21±0.004  | 4.73±0.00071          | 2.26±0.034             | 0.73±0.010    |
| PPO      | 0.21±0.003  | 4.68±0.00077          | 2.29±0.054             | 0.72±0.009    |
| PPO+LTSM | 0.20±0.006  | 4.66±0.00075          | 2.34±0.044             | 0.73±0.009    |

表 到达数为40时的实验结果

为展示不同策略在不同任务到达数下的表现，将仿真实验结果以表格和图形形式呈现。表1、表2和表3给出了任务到达数为10、20和40时各指标的平均值和标准差。

在横向比较中，我们深入研究了四种不同的报价策略，以全面评估它们在各项指标上的表现。PPO+LTSM策略在各项指标上表现出色，例如，在到达数为20的场景下，其在负载均衡、服务器收益、车辆效用和任务完成率方面分别为**0.21±0.006**、**3.06±0.00009**、**1.62±0.036**和**0.93±0.009**, 高于其它所有策略。Fixed策略在服务器收益上表现良好，但在负载均衡, 车辆效用和任务完成率方面较差, 这意味着该策略在用户满意度上较差。Random策略几乎在所有指标上表现最差。PPO策略虽然在服务器收益上略逊于Fixed策略，但在其他指标上表现更好, 与PPO+LTSM相比, 四项指标的方差均更高, 说明PPO的策略相对更加不稳定。总体来说，PPO+LTSM策略在长期报酬方面表现最优。

在进行纵向比较时，我们详细观察了不同任务到达数下的四种报价策略。随着任务到达数的增加，服务器的收益和利用率普遍上升，但订单完成率却呈下降趋势，这可能是因为服务器过载而无法接收更多任务。在任务到达数为10的情况下，由于任务数相对较少，各策略在负载均衡和订单完成率上表现相似，随机策略除外。PPO+LSTM策略在车辆效用和负载均衡方面略显优势，显示出更好的适应性和灵活性。Fixed策略的收入较高可能是因为其固定报价符合大多数任务的预算，并且没有服务器间的竞争。当任务到达数增至20时，随着任务数量的增加，PPO+LSTM策略在各项指标上均超越其他策略。此时的负载均衡值相较于任务数为10和40时更高，这表明随着任务到达率的增加，服务器间的竞争变得更加激烈，可能导致负载不均衡。然而，随着任务到达率的进一步增加，服务器的负载保持在较高水平，因此负载相对均衡。在任务到达数为40的大规模任务情境下，PPO+LSTM策略继续保持领先，实现了更好的负载均衡、车辆效用和任务完成率。随机策略在这种情况下获得了最高收入，这可能是因为其随机性使其能够适应不同的任务和竞争环境，从而更有可能匹配到更多高价任务，同时避免了过度竞争和价格战。然而，需要注意的是，随机策略在长期内可能不具备稳定性，而在某次实验中取得较好结果可能仅是随机性的结果。在真实应用中，具备更强适应性和稳定性的策略更为可取。总体而言，不同任务到达数下，PPO+LTSM策略在负载均衡和车辆效用方面表现最佳，展现了更好的适应性和性能。其能够通过学习最大化长期报酬，更好地平衡系统各项指标，为不同任务到达数下的系统性能提供均衡的优化。

综上所述，本文通过仿真实验对比了不同报价策略在不同任务到达数下的表现，并分析了PPO+LTSM策略的优势。研究表明，PPO+LTSM策略能够利用LTSM捕捉时序信息，做出更合理的决策，实现最有效的负载均衡。

本文旨在分析不同报价策略在不同任务到达数下对各项指标的影响，包括服务器收益、车辆利用率、订单完成率和负载均衡。所考虑的四种策略分别是Fixed、Random、PPO和PPO+LTSM。

---

**各策略概述：**

1. **Fixed策略：**
   - 简单的报价策略，根据任务距离和时长确定报价，不考虑资源使用情况和任务特征。
   - 在服务器收益上表现良好，但在服务器利用率和订单完成率上落后于PPO+LTSM策略。
   - 由于采用固定报价策略，不能动态调整，导致资源分配效率低下。

2. **Random策略：**
   - 随机的报价策略，在给定范围内随机分配订单，不考虑资源使用情况和任务特征。
   - 在所有指标上表现最差，可能因为随机分配导致资源浪费和拥堵。

3. **PPO策略：**
   - 基于强化学习的报价策略，根据资源使用情况和任务特征进行报价，但不考虑时序信息。
   - 在服务器收益上稍逊于Fixed策略，但在车辆效用和订单完成率上优于Fixed策略。
   - 决策不稳定可能是因为未考虑时序信息导致的。

4. **PPO+LTSM策略：**
   - 结合强化学习和长短期记忆的报价策略，利用LTSM捕捉时序信息做出更合理决策。
   - 在所有指标上表现出色，优于或接近其他策略，能更好地平衡系统各项指标。
   - 通过学习最大化长期报酬，提高系统整体性能。

---

**实验结果呈现：**

为展示不同策略在不同任务到达数下的表现，将仿真实验结果以表格和图形形式呈现。表1、表2和表3给出了任务到达数为10、20和40时各指标的平均值和标准差。

---

**横向比较：**

- **PPO+LTSM策略表现出色：**
  - 在所有指标上或接近其他策略，展现更好的平衡性。
  - 利用LTSM捕捉时序信息，做出更合理的决策。
- **Fixed策略相对局限：**
  - 在服务器收益上表现良好，但在车辆效用和订单完成率不如基于强化学习的策略。
  - 资源分配不平衡。
- **Random策略表现最差：**
  - 未考虑资源和任务特征，导致表现不佳。
  - 随机报价可能导致无法满足任务预算和任务分配失败。
- **PPO策略决策不稳定：**
  - 在服务器收益上稍逊于Fixed策略，但在其他指标上优于Fixed策略。
  - 未考虑时序信息可能导致决策不稳定。

综合来看，PPO+LTSM策略能够在任务的选择和分配中，更有效地学习如何最大化长期的报酬，从而提高系统的整体性能。

---

**纵向比较：**

随着任务到达数的增加，观察到服务器收益和利用率上升，但订单完成率下降。在各策略中，PPO+LTSM表现最佳，特别在负载均衡和车辆效用方面。

1. **任务到达数为10时：**
   - 在相对较小的任务数场景下，各策略在负载均衡和订单完成率上表现相近，除了随机策略。
   - PPO+LTSM在车辆效用和负载均衡上稍优，显示出更好的适应性和灵活性。
   - Fixed策略的收入较高，可能因为其固定报价能够符合大多数任务预算，且不存在服务器之间的竞争。
2. **任务到达数为20时：**
   - 随着任务数量增加，PPO+LTSM策略各项指标均超越其他对比策略，尤其在负载均衡和任务完成率上的优势更为显著。
   - Fixed和PPO策略在车辆利用率下降，而Random策略相对稳定。
   - 负载均衡值相较于任务数为10和40时更大。随着任务到达率的增加，服务器间竞争激烈，可能导致负载不均衡。然而，任务到达率继续增加，服务器负载维持在高水平，因此负载相对较为均衡。
3. **任务到达数为40时：**
   - 在大规模任务到达情境下，PPO+LTSM策略继续领先，实现更好的负载均衡、车辆效用和任务完成率。
   - Random策略在这个情景下取得最高收益。这可能是因为随机性导致其适应不同任务和竞争情况，随机性使其更可能匹配到更多高价值的任务，同时避免了过度竞争和价格战。
   - 需要注意的是，Random策略在长期内可能不具备稳定性，而在某一次实验中取得较好的结果可能是由于随机性的影响。在真实应用中，具备更强适应性和稳定性的策略更为可取。

- 随着任务到达数的增加，服务器的收益和利用率通常上升，但任务完成率可能下降，因为服务器过载而无法接收更多的任务。
- 不同策略在任务到达数下的表现：
  - **负载均衡**：各策略能够保持一定程度的负载均衡，避免服务器过载或空闲。PPO+LTSM策略在所有情况下的负载均衡最佳，实现最有效的资源分配。
  - **服务器收益**：PPO+LTSM策略在所有情况下的服务器收益高于其他策略，表明其更倾向于提高服务器收益，而不是负载均衡。
  - **车辆效用**：PPO+LTSM策略在所有情况下的车辆效用最高，为车辆提供最优服务质量，表明其优化目标是提高车辆满意度而非最大化服务器收益。
  - **任务完成率**：不同策略下任务完成率的变化，随着任务到达数增加，任务完成率可能下降，特别是Random策略表现不佳。
- 结论：PPO+LTSM策略是一种平衡系统各项指标、提高系统整体性能的报价策略。其能够捕捉时序信息，实现有效的负载均衡，提高车辆效用，为不同任务到达数下的系统性能提供均衡的优化。

---

**结论与展望：**

- **PPO+LTSM策略的优越性：**
  - 在不同任务到达数目下均表现出色，具有更好的适应性和鲁棒性。
  - LTSM的引入使得模型能够更好地捕捉任务之间的时序关系，提高了决策的准确性。
- **系统整体性能提升：**
  - 通过RATO方法，相较于传统的Fixed和Random策略，整体系统性能有了显著提升。
  - PPO+LTSM策略在各项指标上都达到了较好的平衡。
- **未来展望：**
  - 进一步优化模型，考虑更多复杂的场景和因素，以提高系统在实际应用中的鲁棒性。
  - 探索其他深度强化学习算法和神经网络结构，以进一步提升决策的准确性和稳定性。

通过本文的研究，RATO方法在车联网任务卸载中展现出了良好的性能，为未来智能调度系统的设计和优化提供了有益的经验和启示。

---

**总结：**

综合来看，PPO+LTSM策略在任务选择和分配中更有效地学习如何最大化长期报酬，提高系统整体性能。在不同到达数下，它展现了更好的平衡性和适应性，相比其他策略更具优势。
