---
aliases: 多智能体强化学习Training Algorithm, "[多智能体强化学习Training Algorithm, '[多智能体强化学习Training Algorithm, \"多智能体强化学习Training Algorithm, ''多智能体强化学习Training Algorithm''\"]']"
tags: 
cssclass:
source:
created: "2024-02-23 15:59"
updated: "2024-02-24 16:18"
---


## 问题陈述


在移动边缘计算中, 车联网是一个非常重要的应用场景，车辆需要在行驶过程中执行不同的计算任务，例如语音识别、路线规划、计算机视觉、机器学习、增强现实等。这些任务对于计算资源和存储资源的需求各不相同，而车辆自身的计算能力和电池容量是有限的。因此，车辆可以选择将部分任务卸载到边缘服务器上进行处理，以节省能耗和提高性能。然而，由于车辆和边缘服务器之间的网络连接是不稳定的，而且边缘服务器的资源也是有限的，因而如何高效地进行任务卸载和资源分配成为一个关键问题。

在本次车联网任务调度的问题建模中, 我们假设一个由$m$个卖家$\mathcal S=\{\mathbf s_1,\dots,\mathbf  s_m\}$和$n$个买家$\mathcal B=\{\mathbf b_1,\dots,\mathbf b_n\}$组成的车联网卸载模型. 卖家为能够提供任务卸载的服务器, 表示为$\mathbf s_i =(\mathbf r_i,c_i)$. 每个卖家 $\mathbf s_i$ 有资源向量 $\mathbf r_i=(r_{j,1},\dots,r_{j,d})$ 和成本 $c_i$，其中, $d$表示服务器总的资源种类, $\mathbf r_{j,k}$表示服务器$j$拥有第$k$种资源的数量，$c_i$ 表示单位时间的维护成本。 买家为需要任务卸载的车辆提交的一次任务请求,表示为 $\mathbf b_j=(\theta_j,\psi_j)$, 其中, $\theta_j=(\mathbf f_j,\mathbf \sigma_j)$表示一个任务,$\mathbf f_j=(l_j,r_j,d_j,\tau_j)$ 是任务的执行特征, $\mathbf \sigma_j=(\sigma_{j,1},\dots,\sigma_{j,m})$ 是任务的执行服务器限制向量表示任务$\theta_j$是否可以在服务器$\mathbf s_i$上执行。 $l_j$表示任务的优先级， $\mathbf r_j=(r_{j,1},\dots,r_{j,d})$表示任务的资源需求, $r_{i,k}$表示任务$i$对第$k$种资源的需求数量； $d_j$表示任务的持续时间； $\tau_j$表示任务的到达时间。$\psi_j$为买家$b_j$对$\theta_j$的预算.

在时间步$t$时, 我们用 $\mathcal U_t​$表示车辆在第$t$个时间步提交的所有任务, 任务序号按照时间非降序排列, 假设$\mathcal U_t$中的任务, 每次按照任务到达顺序取一个任务$\theta_j\in \mathcal U_t$进行拍卖, 则本次拍卖中, 买家为$b_j$表示发起本次任务请求的车辆, 卖家为$\mathcal S_j=  \{\mathbf s_i|\forall \mathbf s_i \in \mathcal S, \sigma_{j,i}=1\}$, 表示能够满足执行服务器限制向量的服务器集合.

拍卖开始时, 买家$\mathbf b_j$将$\theta_j$的特征$\mathbf f_j$发送到参加拍卖的所有卖家$\mathcal S_j$, 卖家根据自身状态和接收的任务特征返回报价$\mathcal Q_j = \{q_{j,i}|\forall \mathbf s_i\in \mathcal S_j\}$, $\mathcal Q_j$表示服务器对$\theta_j$的报价集合,$q_{j,i}$表示卖家服务器$\mathbf s_i$对任务$\theta_j$的报价. 随后产生拍卖结果, 服务器分配向量$\mathbf x_{j}=(x_{j,1},\dots,x_{j,m})$ ,支付 $p_{j}=\min(q_j)$. 若$x_{j,i}=1$表示分配到$\mathbf s_i$, 若$x_{j,i}=0$, 表示未分配到$\mathbf s_i$. 买家支付并开始任务卸载。当$\mathcal U_t$中所有任务都进行过拍卖后, 进入下一个时间步$t+1$, 当所有时间步都完成后，拍卖过程结束。

为了描述任务卸载过程中的资源使用情况，我们需要对任务和服务器的资源在执行时的占用进行建模。我们假设任务$\theta_j$在时间$t$对资源的占用为

$$
r_{\theta_j}(t)=(r_{{\theta_j } , 1}(t),\dots,r_{\theta_j,d} (t))
$$

，其中$r_{\theta_j,k}(t)$表示任务$\theta_i$在时间$t$对第$k$种资源的占用量。

$$
r_{\theta_j,k}(t)=\begin{cases}
r_{j,k}&\text{if }\tau_j \leq t < \tau_j+d_j \\
0,&\text{otherwise}
\end{cases}
$$

类似地，我们假设卖家服务器$\mathbf s_i$在时间$t$的可用资源为$r_{\mathbf s_i}(t)=(r_{{\mathbf s_i},1}(t),\dots,r_{{\mathbf s_i},d}(t))$，其中$r_{{\mathbf s_i},k}(t)$表示服务器$\mathbf s_j$在时间$t$拥有第$k$种资源的剩余量。我们假设任务$\theta_j$在分配到服务器$\mathbf s_i$后，会在$\tau_j$立即开始执行，$\tau_j+d_j$时间内完成，即从$\tau_j$到$\tau_j+d_j-1$的时间段内，任务$i$会占用服务器$k$的资源，而在$\tau_j+d_j$时刻释放资源。因此，我们可以定义对于$\theta_j$的服务状态变量$s_{i,k}(t)$为：

$$
s_{j,i}(t)=\begin{cases}
1, & \text{if }  x_{j,i}=1 \text{ and } \tau_j \leq t < \tau_j+d_j\\
0, & \text{otherwise}
\end{cases}
$$

其中$x_{j,i}$表示任务$\theta_i$是否分配到服务器$\mathbf s_i$。服务状态变量$s_{j,i}(t)$表示任务$\theta i$在时间$t$是否占用服务器$\mathbf s_i$的资源。

根据服务状态变量，我们可以计算任务$\theta i$在时间$t$对服务器$\mathbf s_i$的资源占用量为：$r_{j,i}(t)=s_{j,i}(t)\cdot r_{\theta_j}(t)$

由此，我们可以得到服务器$\mathbf s_i$在时间$t$的可用资源为：$r_{s_i}(t)=\mathbf r_i-\sum_{i=1}^{|\mathcal U_t|} r_{j,i}(t)$
其中$|\mathcal U_t|$表示$t$时间内任务总数，$\mathbf r_i$表示服务器$\mathbf s_i$的总资源量。服务器$j$在时间$t$的可用资源$r_i(t)$表示服务器$j​$在时间$t​$可以接受新的任务卸载请求的资源余量。

服务器$j$在接收到任务卸载时会获得收益, 则边缘服务器$\mathbf s_i$在总时间步$T$到达时的收益为:$R_i = \sum_{t=1}^T (p_{t,i} - c_{i})$
其中, $p_{t,i}=\sum_{i=1}^{|\mathcal U_t|} p_{i} x_{j,i}$ 是第$t$个时间步内得到的总支付，$p_{j}$为任务$\theta_j$的支付，$c_{i}$ 是服务器每回合的维护成本。

我们的目标是，在总时间步$T$到达时，对于所有买家$\mathbf b$，卖家$\mathbf s$，设计服务器的任务报价策略，使得每个边缘服务器能够在满足任务预算和服务器执行限制的前提下，最大化卖家整体收益$R=\sum^{m}_{i=1} R_i$.

基于以上定义，我们可以把车联网中的任务调度问题建模为一个优化问题：

$$

\begin{aligned}

&Objective: \max_{q_j} R= \sum^m_{i=1} \sum_{t=1}^T (\sum_{j=1}^{|\mathcal U_t|} p_{j} x_{j,i}- c_j)\\
&\\

&\text{s.t.}\ \ \begin{aligned}

& C1:\sum_{j=1}^n x_{j,i}\sigma_{j,i} \le 1, \forall i \in \mathbf b, j \in \mathbf s \\
& C2:p_j \sum_{i=1}^m  x_{j,i}\sigma_{j,i} \leq b_j, \forall i \in \mathbf b, j \in \mathbf s \\
& C3: r_{s_i}(t)\ge 0, \forall i \in \mathbf b, s_i \in \mathbf s,  t \in [0, T]\\
\end{aligned}

\end{aligned}
$$

限制条件的含义如下：

- C1:表示每个任务只能卸载到$a_j$中的一个边缘服务器上；
- C2:表示每个卸载任务的最终支付不能超过其预算$b_j$。
- C3:表示每个边缘服务器在任意时间点可用资源$r_k(t)\ge 0$

在车联网中，为了有效地处理车辆在行驶过程中的多样化计算任务，采用移动边缘计算进行任务卸载成为一种关键的解决方案。本文通过建立一个车联网卸载模型，涵盖了卖家（边缘服务器）和买家（车辆）之间的拍卖过程，以及任务和服务器在资源占用上的具体建模。问题的目标是在满足任务预算和服务器执行限制的前提下，通过设计任务报价策略使得整体收益最大化。通过多个约束条件，如任务唯一性、预算限制和资源可用性，对问题进行了详细的数学建模。这为后续章节介绍基于多智能体强化学习和反向拍卖的任务调度算法提供了理论基础。

## 4. Method of MADRL and Reverse-Auction

为了解决边缘计算中的车联网工作流调度问题，我们提出了一个基于多智能体强化学习和反向拍卖机制的方法。我们首先把这个问题建模成了一个在线和NP-hard问题，然后使用强化学习方法来处理复杂的动态的高维度的决策问题。我们采用多智能体强化学习来实现分布式的资源分配，避免集中式管理的通信开销和信息的不准确性，提高系统的稳定度。我们还使用了一种基于反向拍卖的任务卸载方法，用于解决车联网中的资源分配问题。反向拍卖是一种一买方多卖方的拍卖形式，适用于买方市场的交易活动。在反向拍卖中，买方提出自己的需求和预算，卖方根据自身的成本和利润，竞争性地给出自己的报价。最终，买方选择一个或多个报价最低且满足需求的卖方进行交易。反向拍卖可以有效降低买方的采购成本，提高卖方之间的竞争力，实现资源的优化分配。接下来的内容将分别描述多智能体强化学习建模和算法，以及反向拍卖的角色和过程。

## 多智能体强化学习建模

![[Pasted image 20230902100025.png]]
多智能体强化学习过程

![[Pasted image 20230717162154.png]]
**Figure 1**: Single-agent approaches (a) and (b) in comparison with multi-agent RL (c)

为了解决边缘计算中的车联网工作流调度问题，我们提出了一个基于多智能体强化学习和反向拍卖机制的方法。我们首先将这个问题建模成了一个在线和NP-hard问题，因为车联网的任务卸载可以看作一个有时序的多维背包问题。然后使用强化学习方法来处理复杂的动态的高维度的决策问题。我们采用多智能体强化学习来实现分布式的资源分配，避免集中式管理的通信开销和信息的不准确性，提高系统的稳定度。我们还使用了一种基于反向拍卖的任务卸载方法，用于解决车联网中的资源分配问题。反向拍卖是一种一买方多卖方的拍卖形式，适用于买方市场的交易活动。买方提出自己的需求和任务优先级，卖方根据自身资源使用情况和任务特征给出报价。买方只选择一个卖方进行交易。反向拍卖可以有效降低买方的采购成本，提高卖方之间的竞争力，实现资源的优化分配。

### 多智能体强化学习环境建模

在第三章的车联网任务卸载建模中, 边缘服务器被定义为智能体（agent）, 每个智能体通过与环境交互学习可以最大化自身收益的策略. 因为在多智能体环境中,一个智能体的动作会影响其他智能体的决策. 为了描述这种情况，我们使用部分可观察马尔可夫决策过程（partially observable Markov decision process, POMDP）来建模多智能体强化学习. 由K个智能体组成的POMDP可以被定义为

$$
(\mathbf S,\mathbf O_1,\dots,\mathbf O_K,\mathbf A_1,\dots,\mathbf A_K, \mathbf R_1,\dots,\mathbf R_K, \mathbf P_{ss'}, \mathbf P_{o|s})
$$

其中$\mathbf S$为状态空间（state space），$\mathbf O_i$为第i个智能体的观测空间（observation space），$\mathbf A_i$为第i个智能体的动作空间（action space），$\mathbf R_i$为第i个智能体的奖励函数（reward function），$\mathbf P_{ss'}$为状态转移概率（transition probability），$\mathbf P_{o|s}$为观测生成概率（observation likelihood）。在每一个时间间隙, 边缘服务器作为一个智能体, 观察当前环境生成的观测$o_i\in\mathbf O_i$, 根据策略$\pi_i(a_i|o)$选择一个合适的动作$a_i\in\mathbf A_i$. 然后获得相应的奖励$r_i=\mathbf R_i(s,a_1,\dots,a_K)$. 在本章后续部分，我们将详细介绍如何定义$\mathbf S,\mathbf O,\mathbf A,\mathbf R$以及如何设计多智能体强化学习环境.

Littman, M. L. (1994). Markov games as a framework for multi-agent reinforcement learning. In Machine learning proceedings 1994 (pp. 157-163). Morgan Kaufmann.

#### 状态空间 State Space

在强化学习方法中, 状态空间是指整个系统的描述，包含了系统中所有的信息。本次研究的是多智能体强化学习，因此状态空间应当包含所有服务器集群和任务队列的状态。状态空间可以用一个集合$\mathbf S$表示，即

$$
\mathcal S=\{\mathcal B,\mathcal S, \mathbf r_{\mathcal B}\}
$$

其中$\mathcal B$表示卖家服务器,$\mathcal S$表示买家车辆提交的任务请求, $\mathbf r_{\mathcal B}=\{r_{\mathbf s_1},\dots,r_{\mathbf s_m}\}$表示卖家服务器的资源使用情况, $r_{\mathbf s_i}=(r_{\mathbf s_i}(1),\dots,r_{\mathbf s_i}(T))$。

#### 观察空间 Observation Space

在多智能体强化学习中，观察空间是指一个智能体可以观察到的状态空间的子集。在本次研究中，我们假设智能体之间不存在额外的通信与交互，因此智能体$i$所能观察到的状态空间就是自身的服务器在从当前时间$t$到之后一定时间的资源使用情况$\mathbf r_{\mathbf s_i,t}=(r_{\mathbf s_i}(t),\dots,r_{\mathbf s_i}(t+\epsilon))$和当前等待被智能体报价的任务$\theta_j$的任务执行特征$\mathbf f_j$. 因此，边缘服务器i的观察空间为

$$
O_i=\{r_{\mathbf s_i,t},\mathbf f_j\}
$$

。这意味着每个智能体只能根据自己的服务器负载和当前任务的特征来做出决策，而无法获取其他智能体的状态信息。

#### 动作空间 Action Space

动作空间是指智能体可以向环境执行的动作的集合。在多智能体强化学习中，每个智能体都可以执行动作以影响环境的状态，从而获得奖励。不同的任务可能需要不同类型的动作空间，例如离散的或连续的。在本次研究中，我们考虑了一个多智能体服务器报价任务，其中每个智能体代表一个边缘服务器，需要对收到的任务进行报价，以竞争任务的分配。

我们假设每个智能体的动作空间是一个在$[1,2]$的数值，表示其对任务报价的浮动系数。这意味着每个智能体可以选择任意一个在$[1,2]$区间内的实数作为其报价系数$a_i$。这样可以使得智能体有更大的自由度和灵活性来调整自己的报价策略。我们可以用一个区间$\mathbf A_i$表示第$i$个智能体的动作空间，即

$$
\mathbf A_i=[1,2]
$$

当环境接收到智能体$i$的$a_i$的值后, 则其对当前等待被拍卖的任务$\theta_j$的最终报价为

$$
q_i = a_i \times \mathbf p \cdot \mathbf r_j \times \tau_j
$$

，其中$\mathbf p$是资源的市场平均价格向量, $\mathbf r_j$是资源使用量, $\tau_j$是资源使用时间。

然而，并不是所有的服务器都可以对任意的任务进行报价，因为任务有执行服务器限制$\mathcal S_j \subseteq \mathcal S$，表示只有$\mathcal S_j$中的服务器才能满足任务$j$的需求。因此，在时间步$t$，整个系统的动作空间为

$$
A_t=\{a_1 \sigma_{j,1},\dots,a_m\sigma_{j,m}\}
$$

其中，$a_i\sigma_{j,i}$代表第$i$个服务器对其当前请求报价的任务$j$的报价系数。如果某个服务器不满足任务$j$的执行服务器条件限制，则其对应的报价系数为0，表示无法执行任何动作。当对任务$j$进行报价时, 环境仅收集任务对应满足条件限制的服务器的报价, 而忽略其他服务器的报价。

#### 奖励函数 Reward Function

在多智能体强化学习中，奖励函数定义了智能体的目标和评估标准，描述了智能体在执行某一动作后，从一个状态转移到另一个状态所获得的即时奖励。智能体会根据奖励函数来判断自己的动作对于自己目标的达成是有利还是有害，从而调整自己的行为策略，以最大化自己的累积奖励。本次论文考虑了一个多智能体服务器报价任务，其中每个智能体代表一个边缘服务器，需要对收到的任务进行报价，以竞争任务的分配。边缘服务器的目标是最大化自己的收益，即任务支付与服务器成本的差额。因此，我们设计了如下的奖励函数：

$$
\mathbf R_i = p_{t,i}-c_i
$$

这个奖励函数表示第$i$个边缘服务器在一个时间步内获得的奖励，它由两部分组成：$p_{t,i}$表示边缘服务器在一个时间步内完成的任务支付之和，$c_i$表示边缘服务器在一个时间步内的固定成本。

### 基于反向拍卖的车联网任务卸载方法

任务卸载涉及到多个参与者之间的资源分配问题，即如何将车辆的任务分配给合适的边缘服务器，并使得各方都能获得最大化的效用。为了解决这一问题，本文借鉴了经济学中的反向拍卖（Reverse Auction）理论，提出了一种基于反向拍卖的任务卸载方法（Reverse Auction-based Task Offloading, RATO）。反向拍卖是一种一买方多卖方的拍卖形式，适用于买方市场（Buyer’s Market）的交易活动。在反向拍卖中，买方提出自己的需求和预算，卖方根据自身的成本和利润，竞争性地给出自己的报价。最终，买方选择一个或多个报价最低且满足需求的卖方进行交易。反向拍卖可以有效降低买方的采购成本，提高卖方之间的竞争力，实现资源的优化分配。

本文将分别介绍本模型中反向拍卖的角色和过程。

**买方**是车联网中有任务卸载需求，并愿意为其需求支付的车辆所产生的一次请求$\mathbf b_j$。在某一时刻 $t$，车辆自身评估连接状态并向满足其连接限制的边缘服务器发起任务 $\theta_j$的卸载请求, 并将其任务属性 $\mathbf{f}_j$ 发送到满足连接约束的服务器集合$\mathcal S_j$中以请求报价。

**卖方**是车联网中的边缘服务器。边缘服务器可以接收车辆的任务卸载请求，并利用其自身资源来完成任务卸载和获得收益。卖家服务器 $\mathbf s_i$ 的特征可以被定义为 $\mathbf{s}_i = (\mathbf{r}_i, c_i)$，其中资源向量 $\mathbf{r}_i$ 和成本 $c_i$。服务器在接收到报价请求后, 确定能否接受任务卸载, 如果可以接受则根据当前时刻对可用资源$\mathbf r_{s_i,t}$和任务特征$\mathbf f_j$的观测, 通过强化学习学习到的策略, 对任务进行报价.

为简化问题,本文做出以下基本假设：

- 假设时间是分时的，任务只能在一个时间段提交，并且任务时长为整数个时间段。
- 车辆可以根据自身状态，评估在拍卖和卸载的时间内的连接情况，得到任务的边缘服务器限制条件。
- 在卸载市场中存在信息缺失：
  - 卖方的特征和维护成本对买方不可见。增加拍卖的竞争性，提高买方的效用。
  - 没有卖方知道其他参与者的出价，也没有卖方相互勾结。
  - 买方的预算不能公开给卖方。
- 拍卖过程中，一次仅有一个任务参与，在任务拍卖结束后，进入本时隙的下一个任务的拍卖。防止卖方串通抬价。
- 一个买方只能把他的任务卸载给一个卖方，但一个卖方可以同时为多个买方提供服务。
- 任务卸载的结果对于车辆而言，其效用是一致的。

在基于云计算的车联网场景中，我们考虑了 $m$ 个卖家 $\mathcal S$ 和 $n$ 个买家 $\mathcal B$ 的情况。每个买家代表一个车辆，它通过自身的通信单元与边缘服务器进行通信，并且根据与边缘服务器的通信状况生成相应的限制条件，以便在需要进行任务卸载时选择合适的卖家。每个卖家代表一个边缘服务器，它可以通过有线链接与其他边缘服务器和互联网相连，以返回卸载结果。同时，边缘服务器也可以通过有线连接返回任务执行结果和传输与车辆之间的数据。算法1是反向拍卖过程的具体描述.

算法1：基于反向拍卖的任务卸载方法

输入：车辆集合$\mathcal B$，服务器集合$\mathcal S$，时间槽长度$\epsilon$

输出：任务卸载结果$\mathcal R$

1. 初始化：对于每个服务器$\mathbf s_i \in \mathcal S$，初始化其资源向量$\mathbf r_i$，成本$c_i$，报价策略$\pi_i$；初始化卸载任务集合$\mathcal U=\{\mathcal U_1,\dots,\mathcal U_T\}$。
2. 对于每个时间槽$t=1,2,\dots,T$，执行以下步骤：
   1. 对于服务器集群内的每一个服务器$\mathbf s_i \in \mathcal S$, 更新自己的资源使用情况和结算上一个时间槽的收益$p_{t-1,i}-c_i$
   2. 从$\mathcal U_t$中的按照时间顺序选择买家车辆请求$\mathbf b_j$，则执行以下步骤：
      1. 车辆评估自身的连接状态，并生成任务的服务器限制向量$\mathbf \sigma_j$。
      2. 车辆向满足限制条件的服务器集合$\mathcal S_j = \{\mathbf s_i | \forall \mathbf s_i \in \mathcal S, \sigma_{j,i} = 1\}$发送任务卸载请求$\mathbf f_j$。
      3. 对于每个服务器$\mathbf s_i \in \mathcal S_j$，如果其可用资源$r_{s_i}(t)$能够满足任务的资源需求$r_{\theta_j}(t)$，则执行以下步骤：
         1. 服务器根据自身在$[t,t+\epsilon]$时间段的资源使用情况$\mathbf r_{s_i,t}$和任务特征$\mathbf f_j$，以及报价策略$\pi_i$，计算出自己对任务的报价$q_{j,i}$。
         2. 服务器将报价$q_{j,i}$返回给车辆。
      4. 车辆收集所有服务器的报价$\mathcal Q_j = \{q_{j,i} | \forall \mathbf s_i \in \mathcal S_j\}$，并将其非降序排列。
      5. 车辆选择报价最低的服务器作为胜者$\mathbf s_k =\mathop{\arg\min}\limits_{\mathbf s_i}\ q_{j,i}$，并检查其报价$p_j = q_{j,k}$是否满足预算条件$p_j \leq \psi_j$。如果不满足，则本次拍卖失败，任务卸载失败；如果满足，则本次拍卖成功，任务卸载成功，并执行以下步骤：
         1. 车辆将任务发送到胜出者服务器$\mathbf s_k$并支付$p_j$，等待执行结果返回。
         2. 服务器$\mathbf s_k$获得收益, 为任务分配资源并执行任务卸载，更新自身的资源使用情况$\mathbf r_{s_i,t}$
      6. 更新任务卸载结果$\mathcal R$
3. 当到达总时间步$T$时间后, 卸载结束

### PPO算法

在上文中我们已经介绍了MDP建模和环境建模，本节我们将介绍一种基于策略梯度的强化学习算法——PPO算法[1]。PPO算法的目标是在与环境交互采样数据后，使用随机梯度上升优化一个“替代”目标函数，从而改进策略。PPO算法的核心思想是利用按照策略产生的执行轨迹来估计梯度，并使用经验回报或优势函数作为梯度的方向。PPO算法可以处理具有连续状态和动作空间的强化学习环境。

PPO算法的目标函数是由优势值进行缩放的新策略和旧策略之间的比率，即：

$$
J_{\theta}=E_{t}\left[\frac{\pi_{\theta}(a_{t}\mid s_{t})}{\pi_{{\theta_{\mathrm{old}}}}(a_{_t}\mid s_{t})}A^{\mathrm{GAE}(\gamma, \lambda)}_{t}\right]
$$

其中$\pi_{{\theta_{\mathrm{old}}}}$表示更新前的旧策略。这个目标与交叉熵方法有相同的原理：重要性采样。为了限制更新，PPO算法使用了裁剪后的目标：

$$
J_{\theta}^{\mathrm{clip}}=\mathbb{E}_{t}\left[\min(r_{t}(\theta)A^{\mathrm{GAE}(\gamma, \lambda)}_{t},\mathrm{clip}(r_{t}(\theta),1-\varepsilon,1+\varepsilon)A^{\mathrm{GAE}(\gamma, \lambda)}_{t})\right]
$$

此目标将新旧策略之间的比率限制在$[1–ε, 1+ε]$区间内，因此通过更改$ε$，我们可以限制更新的大小。这种裁剪方法可以避免使用KL散度惩罚或自适应更新，从而简化了算法。

PPO算法有两种主流的实现方式，一种是使用KL散度作为惩罚项来限制策略更新的幅度，另一种是使用剪裁函数来限制新旧策略之间的比率。PPO算法还使用了一种更一般的优势函数估计方式，即GAE（Generalized Advantage Estimation）。GAE是由John Schulman等人在2015年的论文[2]中提出的。GAE的基本思想是利用TD($\lambda$)方法来平衡偏差和方差，同时减少优势函数的方差。GAE的定义如下：

$$
A^{\mathrm{GAE}(\gamma, \lambda)}_{t}=\sum_{l=0}^{\infty}(\gamma \lambda)^{l} \delta_{t+l}
$$

其中$\delta_t$是TD误差，即$\delta_t = R_t + \gamma V_\theta(S_{t+1}) - V_\theta(S_t)$。可以看出，GAE是对TD误差的加权累加，其中权重随着$l$的增加而指数衰减。当$\lambda=0$时，GAE退化为一步TD误差；当$\lambda=1$时，GAE退化为MC估计。因此，通过调节$\lambda$的值，我们可以在偏差和方差之间找到一个平衡点。GAE还有一个重要的性质是，它可以消除优势函数的常数偏移量。


PPO算法从环境中获取较长的样本序列，然后用于执行多个训练周期。PPO算法在实现简易性、样本复杂度和调整简易性之间取得了平衡，在每一步都试图计算一个最小化代价函数的更新，同时保证与前一策略的偏离相对较小。PPO算法在各种强化学习任务上表现出了优异的性能。

为了解决第三章提出的车联网环境中的任务卸载问题，我们采用以上描述的PPO算法。这种算法可以根据任务的需求和优先级，直接学习最优策略，而不需要像Q-Learning等基于值函数的算法那样估计状态-动作值。

我们使用多智能体强化学习的框架，让每个边缘服务器都部署一个PPO代理，根据自己的状态和动作来更新自己的策略。这样，不同的服务器之间的策略可以根据任务需求和市场竞争进行自适应调整，从而提高自身的收入和车辆的效用。同时，多智能体强化学习也可以处理多个智能体之间的交互和博弈，使得每个服务器能够考虑其他服务器的行为和影响。为了简化模型，我们假设服务器之间没有额外的通信，而且只进行一次博弈。

接下来，我们将介绍我们的强化学习仿真环境的具体设计, 展示我们的实验结果和分析，比较PPO算法与其他基准算法的性能和效率。



[1]: [Proximal Policy Optimization Algorithms]
[2]: [High-Dimensional Continuous Control Using Generalized Advantage Estimation]

- for iteration=1,2,... do
  - for actor=1,2,... do
    - 在环境中运行策略 $\pi_{\theta_{old}}$ , 进行 $T$ 个时间步
    - 计算优势估计 $\hat{A}_1, \dots, \hat{A}_T$
  - end for
  - 优化代理 $L$ 关于 $\theta$, 通过 $K$ epochs 和 minibatch size $M\le NT$
  - $\theta_{old}\gets \theta$
- end for

### 环境建模

为了实现本论文中多智能体强化学习的环境建模, 我们使用了Pettingzoo最为我们实现自定义环境的接口. Pettingzoo是一个用于表示一般的多智能体强化学习（MARL）问题的Python库，它包括了多种参考环境、有用的工具和创建自定义环境的方法。Pettingzoo支持Agent Environment Cycle（ACE）模式，它是一种适用于顺序的回合制环境的接口，能够处理任何MARL可以考虑的游戏。在ACE模式下，每个智能体都有自己的观察空间和动作空间，而且只有一个智能体可以在每个时间步骤中执行动作。用户可以使用以下方法来与ACE模式的环境进行交互：每一个step开始时使用env.agent_iter()的到当前需要执行动作的agent, 通过env.last()返回最后一个智能体的观察、奖励、终止、截断和信息, 利用返回的信息生成action, 通过env.step(action)执行一个动作并更新环境状态,直到所有的智能体都完成了他们的回合或者游戏结束。

为了利用多智能体强化学习求解这个优化问题，我们建立了如下多智能体强化学习模型：

1. 初始化任务生成模型参数，边缘服务器集群 S。
2. 对于每个时间槽 t，执行以下操作, 直到满足停止条件：
   1. 使用任务生成模型生成在本时间槽中需要卸载的所有任务$T_t$.
   2. 集群中每一个服务器更新资源使用情况, 向车辆返回结束的任务的执行结果, 结算执行完毕的任务获得奖励和维护费用.
   3. 从$T_t$中的选择一个没有参加过拍卖任务$task_i$, 执行如下操作, 直至全部任务都进行拍卖:
      1. 将$task_i$相关信息发送到满足限制的服务器集合$a_i$.
      2. 对于每一个在$a_i$中的边缘服务器$S_j$,执行如下操作,直到所有服务器完成报价.
         1. $S_j$收到任务请求后，根据自己的观测，得到动作, 将报价返回拍卖商.
      3. 拍卖商收到所有服务器的报价后，执行反向拍卖过程，并向车辆通知卸载结果和支付费用。
      4. 车辆收到拍卖结果，检查是否满足其预算要求
         1. 如果满足预算需求, 则同意卸载，并在任务结束后进行支付
            1. 将车辆任务发送到边缘服务器
            2. 边缘服务为任务预留相关资源并开始执行, 更新资源的使用情况.
         2. 如果不满足，则不进行任务卸载,不进行支付.
   4. 当$T_t$中所有任务都进行拍卖后, 进入下一个时间槽$t+1$
3. 输出最终的环境状态，并结束环境。

#### 任务生成模型

完成对任务的建模后, 为模拟真实的任务调度场景, 接下来需要对任务的生成进行建模. 任务的生成一共由三部分组成, 一个时间步内任务的数量, 每个任务对资源的需求, 任务可卸载的服务器列表.

- 一个时间步的任务数量, 本文将其简化为符合泊松分布的随机变量, 每一回合从分布中抽取一个正整数作为该时间步的任务数量.
- 每个任务对资源的需求, 我们将任务的资源需求和执行时间都取自由两个正态分布组合而成的双峰分布, 因为通过分析现实的计算机集群工作负载现实世界中卸载的任务以小任务或大任务居多, 如果以简单的正态分布或者指数分布(负指数分布)可能无法准确建模, 另外选择这样的分布也是为了检验神经网络能否拟合复杂分布.
- 任务可卸载的服务器列表, 由于车联网中的车辆存在动态性, 位置存在随机性, 因此我们将车辆与边缘服务器之间的连接抽象成一个可连接服务器的集合. 由于服务器的位置是固定的, 因此其可连接的车辆范围也是固定的. 为在模型中体现车联网的连接性质, 在生成执行服务器限制时, 我们首先生成一个连接范围限制, 然后从连接范围限制的服务器集合中随机抽取若干服务器作为服务器限制集合. 这时的任务可卸载约束仅有连接所限制,属于服务器的无线设备自发无目的的向其范围内车辆发送的连接检测过程,这时服务器仍然未知任务的具体特征, 因此执行约束不在任务生成模型的建模之中.

因此本次研究系统模型中的任务生成过程如下.

- 设定一个时间步的长度为 $\Delta t$，比如 $\Delta t=1$ 秒或者 $\Delta t=0.1$ 秒；
- 在每个时间步 $n$ 内，根据泊松分布 $Poisson(\lambda)$ 生成一个正整数 $m$，表示这个时间步内有 $m$ 个任务被车辆提交到边缘服务器，其中 $\lambda$ 是泊松分布的参数，表示单位时间内任务到达的平均速率；
- 对于每个任务 $task_i$，根据双峰分布 $Bimodal(\mu_1,\sigma_1,\mu_2,\sigma_2,w)$ 生成它的资源需求和执行时间，表示这个任务需要多少计算资源和存储资源以及执行多长时间，其中 $\mu_1,\sigma_1,\mu_2,\sigma_2$ 是两个正态分布的均值和标准差，$w$ 是两个正态分布的权重；
- 根据车辆的位置和边缘服务器的位置和连接情况，生成一个可连接服务器的集合 $a_i$，表示这个任务可以被发送到哪些边缘服务器；
- 根据车辆的偏好或者其他因素，生成一个预算 $b_i$，表示这个任务的最高支付费用；
- 这样，我们就得到了一个完整的任务 $task_i=(n_i,p_i,r_i,d_i,t_i,a_i,b_i,f_i)$；
- 将这个时间步内生成的所有任务组成一个任务集合 $T_n=\{task_k,task_{k+1},...,task_{k+m}\}$，表示这个时间步内有 $m$ 个任务被车辆提交到边缘服务器。
- 重复这个过程，直到生成所有任务或达到停止条件。




为了解决第三章提出的车联网环境中的任务卸载问题，我们采用以上描述的PPO算法。这种算法可以处理具有连续状态和动作空间的RL环境，而不需要像Q-Learning等基于值函数的算法那样估计状态-动作值，而是直接学习最优策略。

在我们的车辆任务卸载场景中，我们自定义的RL环境将提供与真实车联网场景相似的任务特征，包括任务的类型、大小、持续时间等。此外，边缘服务器的资源也会被考虑，因此CPU和内存的可用性也将作为状态空间的一部分被使用和更新。

我们使用多智能体强化学习的框架，让每个边缘服务器都部署一个PPO代理，根据自己的状态和动作来更新自己的策略。这样，不同的服务器之间的策略可以根据任务需求和市场竞争进行自适应调整，从而提高自身的收入和车辆的效用。同时，多智能体强化学习也可以处理多个智能体之间的交互和博弈，使得每个服务器能够考虑其他服务器的行为和影响。为了简化模型，我们假设服务器之间没有额外的通信，而且只进行一次博弈。

我们使用RLlib来实现和管理训练过程，它可以并行地从环境中采集数据，汇总到一个训练批次中，然后根据选择的算法和损失函数计算梯度，并更新模型参数。我们重复这个过程，直到满足终止条件，如达到最大迭代次数、最大训练时间或最佳性能等。在训练过程中，RL模拟反馈回路不断地收集数据，包括观察到的状态、采取的动作、获得的奖励和所谓的结束标志，表示代理在模拟中经历的不同的回合。每次PPO代理采取一个动作（报价一个任务），都会得到一个即时奖励。下一个状态也将依赖于前一个状态，因为CPU和内存的可用性将在每次任务卸载后进行更新。最终，PPO代理应该能够学习任务的需求和优先级，并完成所有任务的报价，以最大化自身的收入和车辆的效用。



# 多智能体强化学习Training Algorithm

为使用MADRL解决车联网中的任务调度问题, 我们使用了两类基于DRL的算法, 分别为Value-based 和 Policy-based. 为同时测试这两类方法, 我们使用了离散的和连续的两种动作空间. 我们假定环境中的固定参数都相同, 每一个边缘服务器具有一个Agent来根据策略$\pi_i(a_i|s_i)$来确定动作.

![[Pasted image 20230719095250.png]]

深度强化学习的工作流程如下：首先，它创建一组rollout workers，每个worker可以并行地从环境中采集数据，并使用一个或多个策略来与环境交互。每个worker生成的数据包含一系列的动作、奖励、下一个状态和结束标志，这些数据构成了一个episode或rollout。然后，RLlib将所有worker生成的rollout汇总到一个训练批次中，接着根据选择的算法和损失函数计算梯度，并更新模型参数。最后，RLlib重复这个过程，直到满足终止条件，如达到最大迭代次数、最大训练时间或最佳性能等 。

- RLlib使用rollout workers来并行地从环境中采集数据，每个worker可以使用一个或多个策略来与环境交互，产生一系列的动作、奖励、下一个状态和结束标志，这些数据构成了一个episode或rollout。
- RLlib将每个worker产生的rollout汇总到一个训练批次中，然后根据选择的算法和损失函数计算梯度，并更新模型参数。
- RLlib重复这个过程，直到满足终止条件，如达到最大迭代次数、最大训练时间或最佳性能等。



---

**行动空间。** 行动空间是一个智能体可以在环境中执行的动作集合。我们假设每个智能体的行动空间是在范围$[1,2]$内的数值，表示其用于竞标任务的浮点系数。这意味着每个智能体可以选择范围$[1,2]$内的任意实数作为其竞标系数$a_i$。这为智能体提供了更多调整其竞标策略的自由度和灵活性。我们可以使用范围$\mathbf A_i$来表示第$i$个智能体的行动空间，如下所示：$\mathbf A_i=[1,2]$ 当环境从智能体$i$处接收到$a_i$的值时，那么它对于当前等待拍卖的任务$\theta_j$的最终出价为：$q_i = a_i \times \mathbf p \cdot \mathbf r_j \times \tau_j$ 其中$\mathbf p$是资源的市场平均价格向量，$\mathbf r_j$是资源使用情况，$\tau_j$是资源使用时间。然而，并非所有服务器都可以对任何任务进行竞标，因为任务有执行服务器限制$\mathcal S_j \subseteq \mathcal S$，这意味着只有在$\mathcal S_j$中的服务器才能满足任务$j$的要求。因此，在时间步$t$中，整个系统的行动空间是：$A_t=\{a_1\sigma_{j,1},\dots,a_m\sigma_{j,m}\}$ 其中$a_i\sigma_{j,i}$表示第$i$个服务器对当前被请求的任务$\theta_j$的竞标系数。如果服务器不符合任务$\theta_j$的执行服务器条件，则其相应的竞标系数为0，表示它无法采取任何行动。在对$\theta_j$任务进行竞标时，环境仅收集满足任务条件的服务器的竞标，并忽略其他服务器的竞标。

## 实验

计算卸载是一种在车联网中提高车辆性能和节省能耗的技术，它让车辆把部分计算任务卸载到边缘服务器上。计算卸载的目标是最大化车辆的效用，同时保证边缘服务器的收益和用户的体验。为此，边缘服务器需要设计合适的竞价策略，即根据任务的需求、紧急性和自身的资源等约束，决定每次报价的金额。

学习竞价策略涉及复杂的环境建模、不完全信息和动态优化问题，其验证方法选择至关重要。离线验证和在线验证是常见方法，但二者各有优缺点。离线验证基于历史数据进行反事实估计，但结果可能受数据偏差、反事实假设以及环境变化等因素的影响，甚至可能不符合古德哈特定律。相比之下，在线验证通过真实环境中的实验直接测试竞价策略的表现，更可靠，然而成本高、耗时且存在一定风险。

强化学习作为一种通过智能体与环境交互学习最佳行为的方法，在游戏、机器人、推荐系统等领域具有广泛应用前景。尽管其在实践中取得成功，研究社区明确指出其方法不足，并认为可靠的仿真环境是近年的重大进展核心。仿真环境提供接近真实环境但不受其限制和影响的平台，使研究者能快速开发、测试和改进强化学习算法。在某些相关领域，如推荐系统，仿真环境已广泛采用作为有效的评估机制。

针对车联网环境中计算卸载的服务器报价策略的验证，我们依据上述模型开发了相应的强化学习环境。在竞价策略学习过程中，我们利用强化学习辅助边缘服务器根据任务卸载特征和自身约束，动态决定每次出价，以最大化效用或收益。

### 仿真环境的设计和参数设置

为了验证提出的基于多智能体强化学习和反向拍卖机制的车联网任务卸载方法（RATO）的有效性，我们开发了一个名为VehicleJobScheduling的基于Python的开源仿真环境。该仿真环境旨在模拟车辆任务卸载的过程，并提供一系列常用的评估指标。在设计中，我们借鉴了PettingZoo，这是一个广受欢迎的多智能体强化学习框架，其定义了一套标准接口，使得不同算法可以在同一环境中进行比较。

VehicleJobScheduling仿真环境主要由三个组成部分构成，即车辆、服务器和环境生成器。在这个环境中，车辆充当任务的发起者，根据资源需求、连接限制和预算选择适当的服务器进行任务卸载，并支付相应费用。服务器则是任务的执行者，根据资源状况和成本，对收到的任务卸载请求进行报价，并执行被接受的任务。环境生成器负责生成任务和服务器的参数，包括任务的到达率、特征、连接限制，以及服务器的数量、类型、资源向量、成本等。用户可以通过设置这些参数，探索不同的任务卸载场景和策略。

我们的仿真环境支持Agent Environment Cycle（ACE）模式，这是一种适用于顺序回合制环境的接口，能够处理任何多智能体强化学习算法。在ACE模式下，每个智能体代表一个边缘服务器，接收车辆的任务卸载请求，利用自身资源完成任务卸载并获取收益。在每个时间步，有一个或多个车辆向满足其连接限制的服务器集合发送任务卸载请求，并等待服务器返回报价。环境按照连接限制选择一个服务器作为当前回合的行动者，该服务器根据自身资源使用情况、任务特征和报价策略计算任务报价，返回给车辆。当所有满足连接限制的服务器完成报价后，车辆选择一个或多个报价最低、同时满足需求和预算条件的服务器进行交易。交易成功后，车辆将任务发送到胜出者服务器并支付相应金额，等待执行结果返回。服务器获得收益，为任务分配资源并执行任务卸载。如果没有服务器的报价满足车辆的条件，则车辆无法完成任务卸载，无需支付任何金额。

为了模拟车联网场景，我们考虑了一个由多个车辆和多个边缘服务器构成的公路仿真环境。在这个环境中，车辆会对服务器信道进行评估，并向满足信道条件的边缘服务器发送任务卸载请求。我们假设任务请求的数量符合泊松分布，单位时间的任务到达数为20，涵盖了三种任务类型：存储密集型、计算密集型和存储计算密集型，以及两种任务时长：长任务和短任务。任务的持续时间被设定为1到10个时间槽，每个时间槽为1分钟，因此每个任务卸载请求的最大执行时间为10分钟。任务的优先级被设置在0到10之间，而任务对CPU和内存的请求数量最大分别为24和100。任务的连接限制范围为0到3，表示任务可以向满足信道条件的0到3个服务器发送任务卸载请求。

我们对车辆的任务卸载请求进行了抽象，将其表示为任务特征，其中包含四个维度：优先级、资源需求、持续时间以及任务连接限制。任务的优先级在0到10之间取值，并且它影响车辆对任务的预算分配。任务对CPU和内存的请求数量最大分别为24和100。任务的持续时间被设定为1到10个时间槽，每个时间槽为1分钟，因此每个任务卸载请求的最大执行时间为10分钟。任务的连接限制范围为0到3，表示任务可以向满足信道条件的0到3个服务器发送任务卸载请求。

在实验中，我们选择了[Azure的专用主机](https://learn.microsoft.com/zh-cn/azure/virtual-machines/dedicated-hosts)作为边缘服务器，提供多种规格和价格的虚拟机。我们选取了三种不同的专用主机SKU，分别代表大、中、小型的边缘服务器。云服务提供商的成本是按照三年计划的价格计算，而市场价格则是按照Azure美国东部[容器实例](https://azure.microsoft.com/zh-cn/pricing/details/container-instances/)的价格计算。表1列出了我们选用的专用主机的规格和价格。

| Dedicated Host SKUs | vCPUs | RAM       | 3 year plan |
| ------------------- | ----- | --------- | ----------- |
| Mdsv2MedMem-Type1   | 192   | 2,048 GiB | $ 5.81      |
| Easv5-Type1         | 112   | 768 GiB   | $ 3.49      |
| Fsv2-Type2          | 72    | 144 GiB   | $ 2.05      |

表1 选用的专用主机

| 参数        | 取值范围             |
| --------- | ---------------- |
| 任务到达率分布   | 泊松分布             |
| 单位时间任务到达数 | 20               |
| 任务时长      | 1到10个时间槽，每个槽为1分钟 |
| 任务优先级     | 0到10             |
| 任务CPU请求数  | 最大为24            |
| 任务内存请求数   | 最大为100           |
| 任务连接限制范围  | 0到3              |
| 表 环境超参数   |                  |

| Agent Model 类型  | PPO Agent     | PPO LTSM Agent        |
| --------------- | ------------- | --------------------- |
| 网络结构            | 多层感知机（MLP）    | 多层感知机 + 长短时记忆网络（LSTM） |
| 全连接层参数          | [384,384,384] | [384,384,384]         |
| LSTM单元大小        | -             | 256                   |
| 激活函数            | tanh          | tanh                  |
| 表 强化学习 Agent 参数 |               |                       |

在本文中，我们使用了两种不同的强化学习Agent的Model，分别是PPO Agent和PPO LTSM Agent，它们都是基于PPO（Proximal Policy Optimization）算法的，但是在网络结构上有所不同。PPO Agent的Model是一个多层感知机（MLP），它由三个全连接层组成，每层的神经元数量为384，激活函数为tanh。PPO LTSM Agent的Model则是在PPO Agent的Model的基础上增加了一个LSTM层，该层的单元大小为256，也就是每个LSTM单元内部状态向量的维度为256。LSTM层的作用是增加Model的记忆能力，使其能够处理和预测具有时间相关性的数据。我们将这两种Model分别应用于RATO方法，以比较它们在不同场景下的性能表现。

这样的设定使得我们能够在仿真中考虑多样化的车联网场景，涵盖了不同类型和时长的任务，以及车辆与边缘服务器之间的动态交互。接下来，我们将通过对RATO方法在这个仿真环境下的实验进行数值分析，全面评估其在不同场景下的性能表现。我们将特别关注不同服务器报价策略对系统效果的影响，以深入了解提出方法的优势和适用性。

### 深度强化学习代理的收敛性

![[Pasted image 20231128145930.png]]

在本节中，我们分别评估了两种使用PPO强化学习代理的方法, PPO和PPO+LTSM在车联网资源卸载问题中的收敛性能。我们使用了两种基于多智能体强化学习的算法，分别是PPO和PPO+LTSM，来训练多个代理，以最大化服务器在一个回合内的收益。服务器的收益反映了任务卸载的效果，也是代理的奖励函数。我们对代理进行了4480000次迭代的训练，并在每40000次迭代后对算法进行了评估，计算了训练策略在10次测试运行中的平均奖励。图1展示了两种算法在训练过程中的奖励评估情况。表1显示了PPO和PPO+LTSM两种算法在最后448000次迭代中的奖励值的平均值和标准差。

| 算法                                | 平均奖励值       | 标准差         |
| --------------------------------- | ----------- | ----------- |
| PPO                               | $1.68×10^7$ | $2.65×10^5$ |
| PPO+LTSM                          | $1.72×10^7$ | $1.12×10^5$ |
| 表 PPO和PPO+LTSM训练过程中最后10%奖励值方差和平均值 |             |             |

我们可以看到，PPO+LTSM的平均奖励值为$1.72×10^7$，比PPO的$1.68×10^7$高出约**2.4%**。同时，PPO+LTSM的标准差为$1.12×10^5$，比PPO的$2.65×10^5$低出约**57.7%**。这说明PPO+LTSM的奖励值不仅更高，而且更稳定，波动更小。因此，PPO+LTSM在车联网资源卸载方面有更好的收敛性能。综上所述，RATO方法在不同的任务卸载场景下，都表现出了较好的收敛性，能够快速地学习到最优或接近最优的报价策略，从而提高服务器的收益和车辆的效用。这证明了RATO方法的有效性和鲁棒性，以及强化学习和反向拍卖机制的优势和适用性。

### 实验结果及分析

本文提出了一种基于多智能体强化学习和反向拍卖机制的车联网任务卸载方法，简称为RATO，并通过仿真实验与其他基准方法进行了对比分析。本文考虑了任务到达数目为10、20和40的三种情况，采用了四种不同的服务器报价策略：固定报价（FB）、随机报价（RB）、基于PPO的报价（PPO）以及基于PPO+LSTM的报价（PPO+LSTM）。本文采用了四个指标来评估不同的调度策略在不同的到达数目下的表现，分别是：负载均衡（Load Balance）、服务器的收益（Server Earning）、车辆的利用率（Vehicle Utility）和订单的完成率（Complete Rate）。在每个到达数目下，进行了十次独立的仿真实验，利用五个随机种子进行模拟，并对每个指标的平均值和标准差进行了统计和分析。

本文的目的是比较不同报价策略在任务卸载系统中的性能，并分析RATO方法的优势。我们考虑了以下四种报价策略：

- **Fixed策略**：在此策略中，服务器以市场价格为基准，仅参考任务的特征和时长来确定报价。
- **Random策略**：服务器以市场价格为基准，在一定区间内随机采样一个系数来进行报价。
- **PPO策略**：服务器利用强化学习算法PPO，根据自身资源使用情况和任务特征，通过多层感知机（MLP）动态地确定报价。
- **PPO+LSTM策略**：在PPO策略的基础上，服务器增加了一个长短期记忆网络（LSTM）层，以捕捉时序信息，并结合多层感知机进行动态报价。

|          | Load Blance | Server Earning (10^6) | Vehicle Utility (10^6) | Complete Rate |
| -------- | ----------- | --------------------- | ---------------------- | ------------- |
| Random   | 0.19±0.012  | 0.55±0.00037          | 0.55±0.037             | 0.76±0.014    |
| Fixed    | 0.20±0.005  | 1.60±0.00007          | 0.76±0.047             | 0.91±0.010    |
| PPO      | 0.20±0.011  | 1.49±0.00004          | 0.78±0.036             | 0.92±0.010    |
| PPO+LTSM | 0.18±0.010  | 1.53±0.00004          | 0.83±0.063             | 0.93±0.012    |

表 到达数为10时的实验结果

|          | Load Blance | Server Earning (10^6) | Vehicle Utility (10^6) | Complete Rate |
| -------- | ----------- | --------------------- | ---------------------- | ------------- |
| Random   | 0.22±0.007  | 2.74±0.00036          | 0.99±0.033             | 0.72±0.008    |
| Fixed    | 0.23±0.004  | 3.05±0.00014          | 1.43±0.040             | 0.90±0.005    |
| PPO      | 0.22±0.007  | 2.95±0.00011          | 1.53±0.060             | 0.91±0.007    |
| PPO+LTSM | 0.21±0.006  | 3.06±0.00009          | 1.62±0.036             | 0.93±0.009    |

表 到达数为20时的实验结果

|          | Load Blance | Server Earning (10^6) | Vehicle Utility (10^6) | Complete Rate |
| -------- | ----------- | --------------------- | ---------------------- | ------------- |
| Random   | 0.22±0.006  | 4.80±0.00100          | 1.64±0.034             | 0.63±0.007    |
| Fixed    | 0.21±0.004  | 4.73±0.00071          | 2.26±0.034             | 0.73±0.010    |
| PPO      | 0.21±0.003  | 4.68±0.00077          | 2.29±0.054             | 0.72±0.009    |
| PPO+LTSM | 0.20±0.006  | 4.66±0.00075          | 2.34±0.044             | 0.73±0.009    |

表 到达数为40时的实验结果

为展示不同策略在不同任务到达数下的表现，将仿真实验结果以表格和图形形式呈现。表1、表2和表3给出了任务到达数为10、20和40时各指标的平均值和标准差。

在横向比较中，我们深入研究了四种不同的报价策略，以全面评估它们在各项指标上的表现。PPO+LTSM策略在各项指标上表现出色，例如，在到达数为20的场景下，其在负载均衡、服务器收益、车辆效用和任务完成率方面分别为**0.21±0.006**、**3.06±0.00009**、**1.62±0.036**和**0.93±0.009**, 高于其它所有策略。Fixed策略在服务器收益上表现良好，但在负载均衡, 车辆效用和任务完成率方面较差, 这意味着该策略在用户满意度上较差。Random策略几乎在所有指标上表现最差。PPO策略虽然在服务器收益上略逊于Fixed策略，但在其他指标上表现更好, 与PPO+LTSM相比, 四项指标的方差均更高, 说明PPO的策略相对更加不稳定。总体来说，PPO+LTSM策略在长期报酬方面表现最优。

在进行纵向比较时，我们详细观察了不同任务到达数下的四种报价策略。随着任务到达数的增加，服务器的收益和利用率普遍上升，但订单完成率却呈下降趋势，这可能是因为服务器过载而无法接收更多任务。在任务到达数为10的情况下，由于任务数相对较少，各策略在负载均衡和订单完成率上表现相似，随机策略除外。PPO+LSTM策略在车辆效用和负载均衡方面略显优势，显示出更好的适应性和灵活性。Fixed策略的收入较高可能是因为其固定报价符合大多数任务的预算，并且没有服务器间的竞争。当任务到达数增至20时，随着任务数量的增加，PPO+LSTM策略在各项指标上均超越其他策略。此时的负载均衡值相较于任务数为10和40时更高，这表明随着任务到达率的增加，服务器间的竞争变得更加激烈，可能导致负载不均衡。然而，随着任务到达率的进一步增加，服务器的负载保持在较高水平，因此负载相对均衡。在任务到达数为40的大规模任务情境下，PPO+LSTM策略继续保持领先，实现了更好的负载均衡、车辆效用和任务完成率。随机策略在这种情况下获得了最高收入，这可能是因为其随机性使其能够适应不同的任务和竞争环境，从而更有可能匹配到更多高价任务，同时避免了过度竞争和价格战。然而，需要注意的是，随机策略在长期内可能不具备稳定性，而在某次实验中取得较好结果可能仅是随机性的结果。在真实应用中，具备更强适应性和稳定性的策略更为可取。总体而言，不同任务到达数下，PPO+LTSM策略在负载均衡和车辆效用方面表现最佳，展现了更好的适应性和性能。其能够通过学习最大化长期报酬，更好地平衡系统各项指标，为不同任务到达数下的系统性能提供均衡的优化。

综上所述，本文通过仿真实验对比了不同报价策略在不同任务到达数下的表现，并分析了PPO+LTSM策略的优势。研究表明，PPO+LTSM策略能够利用LTSM捕捉时序信息，做出更合理的决策，实现最有效的负载均衡。
