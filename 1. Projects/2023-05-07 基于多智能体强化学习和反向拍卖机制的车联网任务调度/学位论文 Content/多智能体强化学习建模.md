---
UID: 20240224121405 
tags: 
source: 
cssclass: 
created: "2024-02-24 12:13"
updated: "2024-02-24 12:22"
---
为了解决边缘计算中的车联网工作流调度问题，我们提出了一个基于多智能体强化学习和反向拍卖机制的方法。我们首先将这个问题建模成了一个在线和NP-hard问题，因为车联网的任务卸载可以看作一个有时序的多维背包问题。然后使用强化学习方法来处理复杂的动态的高维度的决策问题。我们采用多智能体强化学习来实现分布式的资源分配，避免集中式管理的通信开销和信息的不准确性，提高系统的稳定度。我们还使用了一种基于反向拍卖的任务卸载方法，用于解决车联网中的资源分配问题。反向拍卖是一种一买方多卖方的拍卖形式，适用于买方市场的交易活动。买方提出自己的需求和任务优先级，卖方根据自身资源使用情况和任务特征给出报价。买方只选择一个卖方进行交易。反向拍卖可以有效降低买方的采购成本，提高卖方之间的竞争力，实现资源的优化分配。

### 多智能体强化学习环境建模

在第三章的车联网任务卸载建模中, 边缘服务器被定义为智能体（agent）, 每个智能体通过与环境交互学习可以最大化自身收益的策略. 因为在多智能体环境中,一个智能体的动作会影响其他智能体的决策. 为了描述这种情况，我们使用部分可观察马尔可夫决策过程（partially observable Markov decision process, POMDP）来建模多智能体强化学习. 由K个智能体组成的POMDP可以被定义为

$$
(\mathbf S,\mathbf O_1,\dots,\mathbf O_K,\mathbf A_1,\dots,\mathbf A_K, \mathbf R_1,\dots,\mathbf R_K, \mathbf P_{ss'}, \mathbf P_{o|s})
$$

其中$\mathbf S$为状态空间（state space），$\mathbf O_i$为第i个智能体的观测空间（observation space），$\mathbf A_i$为第i个智能体的动作空间（action space），$\mathbf R_i$为第i个智能体的奖励函数（reward function），$\mathbf P_{ss'}$为状态转移概率（transition probability），$\mathbf P_{o|s}$为观测生成概率（observation likelihood）。在每一个时间间隙, 边缘服务器作为一个智能体, 观察当前环境生成的观测$o_i\in\mathbf O_i$, 根据策略$\pi_i(a_i|o)$选择一个合适的动作$a_i\in\mathbf A_i$. 然后获得相应的奖励$r_i=\mathbf R_i(s,a_1,\dots,a_K)$. 在本章后续部分，我们将详细介绍如何定义$\mathbf S,\mathbf O,\mathbf A,\mathbf R$以及如何设计多智能体强化学习环境.

Littman, M. L. (1994). Markov games as a framework for multi-agent reinforcement learning. In Machine learning proceedings 1994 (pp. 157-163). Morgan Kaufmann.

#### 状态空间 State Space

在强化学习方法中, 状态空间是指整个系统的描述，包含了系统中所有的信息。本次研究的是多智能体强化学习，因此状态空间应当包含所有服务器集群和任务队列的状态。状态空间可以用一个集合$\mathbf S$表示，即

$$
\mathcal S=\{\mathcal B,\mathcal S, \mathbf r_{\mathcal B}\}
$$

其中$\mathcal B$表示卖家服务器,$\mathcal S$表示买家车辆提交的任务请求, $\mathbf r_{\mathcal B}=\{r_{\mathbf s_1},\dots,r_{\mathbf s_m}\}$表示卖家服务器的资源使用情况, $r_{\mathbf s_i}=(r_{\mathbf s_i}(1),\dots,r_{\mathbf s_i}(T))$。

#### 观察空间 Observation Space

在多智能体强化学习中，观察空间是指一个智能体可以观察到的状态空间的子集。在本次研究中，我们假设智能体之间不存在额外的通信与交互，因此智能体$i$所能观察到的状态空间就是自身的服务器在从当前时间$t$到之后一定时间的资源使用情况$\mathbf r_{\mathbf s_i,t}=(r_{\mathbf s_i}(t),\dots,r_{\mathbf s_i}(t+\epsilon))$和当前等待被智能体报价的任务$\theta_j$的任务执行特征$\mathbf f_j$. 因此，边缘服务器i的观察空间为

$$
O_i=\{r_{\mathbf s_i,t},\mathbf f_j\}
$$

。这意味着每个智能体只能根据自己的服务器负载和当前任务的特征来做出决策，而无法获取其他智能体的状态信息。

#### 动作空间 Action Space

动作空间是指智能体可以向环境执行的动作的集合。在多智能体强化学习中，每个智能体都可以执行动作以影响环境的状态，从而获得奖励。不同的任务可能需要不同类型的动作空间，例如离散的或连续的。在本次研究中，我们考虑了一个多智能体服务器报价任务，其中每个智能体代表一个边缘服务器，需要对收到的任务进行报价，以竞争任务的分配。

我们假设每个智能体的动作空间是一个在$[1,2]$的数值，表示其对任务报价的浮动系数。这意味着每个智能体可以选择任意一个在$[1,2]$区间内的实数作为其报价系数$a_i$。这样可以使得智能体有更大的自由度和灵活性来调整自己的报价策略。我们可以用一个区间$\mathbf A_i$表示第$i$个智能体的动作空间，即

$$
\mathbf A_i=[1,2]
$$

当环境接收到智能体$i$的$a_i$的值后, 则其对当前等待被拍卖的任务$\theta_j$的最终报价为

$$
q_i = a_i \times \mathbf p \cdot \mathbf r_j \times \tau_j
$$

，其中$\mathbf p$是资源的市场平均价格向量, $\mathbf r_j$是资源使用量, $\tau_j$是资源使用时间。

然而，并不是所有的服务器都可以对任意的任务进行报价，因为任务有执行服务器限制$\mathcal S_j \subseteq \mathcal S$，表示只有$\mathcal S_j$中的服务器才能满足任务$j$的需求。因此，在时间步$t$，整个系统的动作空间为

$$
A_t=\{a_1 \sigma_{j,1},\dots,a_m\sigma_{j,m}\}
$$

其中，$a_i\sigma_{j,i}$代表第$i$个服务器对其当前请求报价的任务$j$的报价系数。如果某个服务器不满足任务$j$的执行服务器条件限制，则其对应的报价系数为0，表示无法执行任何动作。当对任务$j$进行报价时, 环境仅收集任务对应满足条件限制的服务器的报价, 而忽略其他服务器的报价。

#### 奖励函数 Reward Function

在多智能体强化学习中，奖励函数定义了智能体的目标和评估标准，描述了智能体在执行某一动作后，从一个状态转移到另一个状态所获得的即时奖励。智能体会根据奖励函数来判断自己的动作对于自己目标的达成是有利还是有害，从而调整自己的行为策略，以最大化自己的累积奖励。本次论文考虑了一个多智能体服务器报价任务，其中每个智能体代表一个边缘服务器，需要对收到的任务进行报价，以竞争任务的分配。边缘服务器的目标是最大化自己的收益，即任务支付与服务器成本的差额。因此，我们设计了如下的奖励函数：

$$
\mathbf R_i = p_{t,i}-c_i
$$

这个奖励函数表示第$i$个边缘服务器在一个时间步内获得的奖励，它由两部分组成：$p_{t,i}$表示边缘服务器在一个时间步内完成的任务支付之和，$c_i$表示边缘服务器在一个时间步内的固定成本。
