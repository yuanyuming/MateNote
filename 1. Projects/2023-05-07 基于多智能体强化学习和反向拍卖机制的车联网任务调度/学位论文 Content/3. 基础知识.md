---
tags: 
UID: 20240223155921
source: null
cssclass: null
created: "2024-02-23 15:59"
updated: "2024-02-24 08:20"
date updated: 2024-02-24 08:17
---

**IoV和MEC中的工作流调度**

车联网（IoV）是一种新兴的技术，它为车辆提供了感知、通信和计算功能，通过车辆之间、车辆与路测设施之间以及车辆与云端之间的信息交换和协同服务，实现更安全、高效和智能的驾驶。然而，车辆本身的计算能力和电池容量有限，无法满足一些复杂或者对延迟敏感的任务需求，例如自动驾驶、视频分析等。移动边缘计算（MEC）是一种新型的计算范式，它通过在道路边缘部署多个MEC服务器，为车辆提供近距离的低延迟高带宽计算资源和服务。IoV和MEC是两种相辅相成的技术。MEC可以让车辆把任务卸载到边缘服务器上，节约自己的能量和时间。这样，IoV产生的大量数据和多样化的服务需求就能得到更好的满足。MEC不仅为IoV提供了低时延、高带宽、高可靠性等网络质量，还利用边缘节点的存储和计算资源进行数据处理、分析和优化，实现数据就近处理、就近消费。

在智能交通系统等领域中，IoV和MEC有着广泛而深刻的应用场景和价值。MEC节点可以为车辆提供多种服务，包括感知、计算和通信等。例如，MEC节点可以根据车辆和道路的实时状态，进行车路协同调度，以提高交通效率和安全性；MEC节点还可以管理停车场的空位，并为车辆提供停车导航和支付服务，以节省时间和费用；此外，MEC节点也可以提供丰富的车载娱乐内容，并根据用户喜好进行个性化推荐，以增加乐趣和舒适度。

工作流调度（Workflow Scheduling）是指在一定的约束条件下，根据某种优化目标，为一组相关的任务分配合适的执行资源的过程。工作流调度的目标是在满足用户需求和资源限制的前提下，优化工作流调度的执行效果，如执行时间、执行成本、能耗、可靠性等。工作流调度需要考虑多种约束条件，如任务之间的依赖关系、资源之间的异构性、资源的可用性和动态性等。工作流调度的评价指标是根据不同的优化目标而定，常见的评价指标有完成所有任务所需的最短时间（Make Span）、完成所有任务所需的总费用（Cost）、完成所有任务所需的总能耗（Energy）、完成所有任务成功率（Reliability）等 。

在IoV（Internet of Vehicles）和MEC（Mobile Edge Computing）环境下，工作流调度面临着更多的特殊性和难点。首先，由于车辆具有高速移动性和动态变化性，如何预测车辆与 MEC 服务器之间的通信质量并根据实时状态进行任务卸载决策是一个难点。其次，由于不同车辆可能有不同的任务类型、优先级、截止时间等约束条件，如何在保证服务质量（QoS）的同时平衡各个 MEC 服务器之间的负载并最大化系统效用是一个关键问题。第三，在多用户多任务多服务器场景下，如何设计一种合理且公平的激励机制来鼓励用户之间相互合作并避免自私或恶意行为也是一个重要问题。本文的目标是提出一种基于反向拍卖机制和深度强化学习相结合的车联网工作流调度方法（RAM-DRL），该方法可以有效地解决上述问题，并在保证收益最大化的前提下，尽可能保证系统可靠性。

**反向拍卖**

普通的拍卖方式是由卖方发起，买方出价最高的获得商品或服务。与正向拍卖相反的是反向拍卖（Reverse Auction Mechanism），它是一种拍卖机制，其中卖方是多个，而买方是一个。在反向拍卖中，买方会提出一个需求，并邀请卖方报出自己的价格。然后，买方会根据自己的目标和预算，从报价中选择一个或多个合适的卖方，并支付给他们相应的价格。反向拍卖可以分为不同的类型，根据竞标者数量、竞标规则、竞标信息等因素进行分类。常见的反向拍卖类型有：排名反向拍卖，日本反向拍卖，荷兰反向拍卖，和开放式反向拍卖。在排名反向拍卖中，每个卖方只能看到自己的排名和最低报价，而不能看到其他卖方的报价。在日本反向拍卖中，买方会提出一个初始报价，并逐渐降低，直到只剩下一个或多个愿意接受的卖方。在荷兰反向拍卖中，买方会提出一个需求清单和一个预算价格，并邀请多个卖方参与竞标，最后可能选择一个或多个中标者。在开放式反向拍卖中，每个卖方都能看到所有的报价和排名，并根据市场情况调整自己的报价。反向拍卖的性质是基于市场竞争和价格信号来确定商品或服务的价值，而不是由政府或其他机构来设定。反向拍卖的优势是可以降低采购成本、提高采购效率、增加透明度和公平性、促进创新和质量提升等。

反向拍卖在激励车辆用户共享资源或执行任务方面也有重要的作用和效果。例如，当一个平台需要为其用户提供某种服务时，可以通过反向拍卖来寻找愿意以最低价格提供该服务的车辆用户，并与之签订标准合同。这样可以使平台节省成本，同时也可以激励车辆用户利用闲置资源或空闲时间来参与竞标，从而获得额外收入。另一方面，反向拍卖也可以用于分配任务给车辆用户，例如交通管理、环境监测、数据收集等。平台可以根据任务需求和预算，在反向拍卖中发布任务信息，并选择出价最低且符合条件的车辆用户来执行任务。这样可以使平台有效地完成任务目标，同时也可以激励车辆用户参与社会公益活动，并获得相应奖励。在车联网和移动边缘计算环境下，由于车辆具有高速移动性和动态变化性，如何设计一种合理且公平的反向拍卖机制来鼓励用户之间相互合作并避免自私或恶意行为也是一个重要问题。本次论文将利用深度强化学习来训练每个 MEC 服务器上部署的策略网络，使其能够根据用户的任务需求和自身状态计算接受任务所产生的长期收益，并根据收益最大化原则向用户进行报价。

**深度强化学习（DRL）**

深度强化学习（DRL）是一种基于神经网络的强化学习方法，它能够在复杂动态优化问题中表现出优异的性能。DRL的基本概念是通过让智能体与环境进行交互，从而学习到一个最优策略，该策略可以使智能体获得最大的长期收益。当存在多个智能体共享一个环境时，就形成了多智能体强化学习（MARL）的问题。MARL的目标是让每个智能体根据自己的奖励和其他智能体的行为来调整自己的策略，从而实现合作或竞争的目标。MARL的方法可以分为基于值函数的方法，基于策略的方法，以及基于演员-评论家的方法，它们分别利用不同的方式来评估或生成策略。DRL和MARL的模型有多种形式，例如深度 Q 网络（DQN），深度确定性策略梯度（DDPG），异步优势演员-评论家（A3C）等，它们分别适用于不同的问题场景和特点。DRL和MARL的算法主要使用神经网络来近似值函数或策略函数，并采用梯度下降或其他优化技术来更新网络参数。

本次论文利用深度强化学习（DRL）来训练每个MEC服务器上部署的策略网络，使其能够根据用户的任务需求和自身状态计算接受任务所产生的长期收益，并根据收益最大化原则向用户进行报价。相比于其他机器学习方法，DRL可以直接从环境中获取反馈信号，并通过不断地探索和利用来优化自身的行为。DRL不需要预先定义特征或标签，也不需要大量的先验知识或假设，因此更适合处理车联网这样的复杂、动态和不确定的环境。本次论文设计了一个三层神经网络结构来近似每个用户 - 状态 - 行动对应的价值函数，并采用异步优势行动者-评论者算法（A3C）算法来更新网络参数并避免过度估计偏差，提高算法的稳定性和准确性。

在本次研究中，我们利用深度强化学习（DRL）来训练每个部署在 MEC 服务器上的策略网络，使其能够根据用户的任务需求和自身状态，计算接受任务所产生的长期收益，并按照收益最大化原则向用户报价。与其他机器学习方法相比，DRL 可以直接从环境中获取反馈信号，并通过持续探索和利用来优化自身行为。DRL 不需要事先定义特征或标签，也不需要大量的先验知识或假设，因此更适合处理车联网这样复杂、动态和不确定的环境。 具体来说，我们设计了一个由 LTSM（Long Short-term Memory）层和三层神经网络组成的结构，来近似每个用户-状态-行动对应的价值函数。LTSM 层的加入，使得模型能够更好地处理时间序列数据，捕捉到用户任务需求和自身状态中的长期依赖关系。然后，我们采用 PPO（Proximal Policy Optimization）算法来更新网络参数，以避免过度估计偏差，提高算法的稳定性和准确性。 PPO 算法是一种在策略梯度算法基础上的改进算法，它通过在多个训练步骤上对策略进行约束，限制策略的变化幅度，从而有效地避免了策略梯度算法中的高估问题。此外，PPO 算法还引入了一个新的-clip 操作，用于限制策略更新的幅度，进一步增强了算法的稳定性。与传统的策略梯度算法相比，PPO 算法在处理连续控制问题时表现更为出色，并且具有更好的收敛性和鲁棒性。
