---
UID: 20240225010723
aliases: null
source: null
cssclass: null
created: 2024-02-25T00:00:00.000Z
date updated: 2024-02-25 01:07
---

在本节中，我们分别评估了两种使用PPO强化学习代理的方法, PPO和PPO+LTSM在车联网资源卸载问题中的收敛性能。我们使用了两种基于多智能体强化学习的算法，分别是PPO和PPO+LTSM，来训练多个代理，以最大化服务器在一个回合内的收益。服务器的收益反映了任务卸载的效果，也是代理的奖励函数。我们对代理进行了4480000次迭代的训练，并在每40000次迭代后对算法进行了评估，计算了训练策略在10次测试运行中的平均奖励。图1展示了两种算法在训练过程中的奖励评估情况。表1显示了PPO和PPO+LTSM两种算法在最后448000次迭代中的奖励值的平均值和标准差。

| 算法                                | 平均奖励值       | 标准差         |
| --------------------------------- | ----------- | ----------- |
| PPO                               | $1.68×10^7$ | $2.65×10^5$ |
| PPO+LTSM                          | $1.72×10^7$ | $1.12×10^5$ |
| 表 PPO和PPO+LTSM训练过程中最后10%奖励值方差和平均值 |             |             |

我们可以看到，PPO+LTSM的平均奖励值为$1.72×10^7$，比PPO的$1.68×10^7$高出约**2.4%**。同时，PPO+LTSM的标准差为$1.12×10^5$，比PPO的$2.65×10^5$低出约**57.7%**。这说明PPO+LTSM的奖励值不仅更高，而且更稳定，波动更小。因此，PPO+LTSM在车联网资源卸载方面有更好的收敛性能。综上所述，RATO方法在不同的任务卸载场景下，都表现出了较好的收敛性，能够快速地学习到最优或接近最优的报价策略，从而提高服务器的收益和车辆的效用。这证明了RATO方法的有效性和鲁棒性，以及强化学习和反向拍卖机制的优势和适用性。
