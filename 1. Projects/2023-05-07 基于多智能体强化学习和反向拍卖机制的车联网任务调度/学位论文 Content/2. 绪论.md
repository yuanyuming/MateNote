---
tags: 
UID: 20240223155744
source: null
cssclass: null
created: "2024-02-23 15:57"
updated: "2024-02-24 08:13"
date updated: 2024-02-23 16:00
---


- 研究背景及意义
- 国内外研究现状
- 本文主要研究内容
- 本文组织结构

## 研究背景及意义


随着物联网（IoT）和信息通信技术（ICT）的快速发展，现代车辆正在经历一场技术革命。车联网（IoV）是智能交通系统（ITS）的重要组成部分，它通过车辆之间以及车辆与基础设施之间的信息交换和协作服务，为驾驶员提供实时交通流信息，以提高道路安全、交通效率和驾驶体验。随着技术的进步，车辆可以在有限的计算能力和电池容量下执行更多计算密集型和数据密集型任务，但很难满足复杂或对延迟敏感的任务需求。[^1] 为了解决上述问题，移动边缘计算（MEC）作为车联网中任务调度的一种新方法，被提出让车辆将部分或全部任务卸载到部署在路边的多个 MEC 服务器上执行。在边缘计算的支持下，车辆可以将大部分任务卸载到边缘服务器上执行，有效扩展了其资源能力，同时减少了核心网络的流量需求。通过这种方式，车辆可以提高驾驶的安全性、效率和智能化。

车联网（Internet of Vehicles, IoV）构成了一个由移动车辆和固定基础设施组成的自组织网络，旨在为车辆提供智能、安全和高效的交通服务。随着车辆技术的进步，如自动驾驶、视频分析和个人助手等计算密集型应用的普及，车辆可以执行越来越多计算密集型和数据密集型的任务，包括但不限于导航、视频流传输以及车载娱乐等。然而，随着这些任务需求的增加，车辆的计算能力和电池容量逐渐变得有限，难以满足复杂或对延迟敏感的任务需求。为解决这一问题，移动边缘计算（MEC）作为一种新的计算范式应运而生。通过在道路边缘部署多个MEC服务器，它为车辆提供低延迟、高带宽的计算资源和服务。这项技术使车辆能够将部分或全部任务卸载到MEC服务器上执行，从而降低了自身的能耗和时间开销，为车联网系统带来更安全、更高效和更智能的驾驶体验。

车联网（IoV）中的任务调度是一项重要且具有挑战性的工作。它需要将计算密集型服务分配给车载单元（OBU）或边缘服务器，以提高车辆性能和用户体验。这些服务涵盖语音识别、自然语言处理、计算机视觉、机器学习、增强现实等多个方面，具有不同的类型、大小、优先级、延迟、能耗等特征和需求。然而，随着服务需求的增加，车辆的计算能力和电池容量逐渐变得有限，难以满足复杂或对延迟敏感的服务需求。为解决这一问题，移动边缘计算（MEC）作为一种新的计算范式应运而生。它通过在道路边缘部署多个MEC服务器，为车辆提供低延迟、高带宽的计算资源和服务。

## 国内外研究现状

本文主要研究的问题是车联网（IoV）中的工作流调度问题，即根据车辆的计算任务需求和移动特性，在移动边缘计算（MEC）环境中有效地分配和调度车辆的计算任务，以达到优化系统性能和用户体验的目的。车联网工作流调度涉及到多个方面的问题，如任务卸载策略、资源分配算法、激励机制设计、时延分析等。近年来，随着车联网技术和移动边缘计算技术的发展，车联网工作流调度问题受到了国内外学者的广泛关注，并取得了一些研究成果。为了对本课题有一个全面的了解，我们首先回顾了国内外相关领域的研究现状，主要包括以下几个方面：传统资源调度和随机优化的分布式资源调度方法,强化学习的车联网资源调度,强化学习移动边缘计算资源调度,强化学习服务器资源调度,车联网中的拍卖机制,强化学习定价策略。

现有文献中所采用的方法、技术、策略等主要包括以下几类：

- 基于数学规划或优化模型的方法，例如线性规划、非线性规划、整数规划、混合整数规划等。这类方法通常需要建立精确或近似的目标函数和约束条件，并利用求解器或启发式算法来求解最优解或次优解。这类方法具有理论上的严谨性和可解释性，但是也存在一些局限性，例如对问题假设过于理想化或简化，忽略了实际环境中存在的不确定性和随机性；求解过程复杂耗时，难以适应动态变化的场景；缺乏自适应能力和泛化能力等。
- 基于机器学习或人工智能的方法，例如监督学习、无监督学习、强化学习等。这类方法通常不需要建立精确或近似的目标函数和约束条件，而是通过数据驱动或交互式学习来获取知识并指导行为。这类方法具有一定程度上克服了数学规划或优化模型方法存在局限性方面 的优势 ，例如可以处理不确定性和随机性；可以快速响应动态变化；可以自适应地更新策略并提高泛化能力等。
- 基于博弈论或拍卖理论的方法。这类方法通常需要考虑用户之间存在着利益冲突或竞争关系，并利用博弈论或拍卖理论中相关概念和工具来分析用户之间的策略选择和均衡状态。这类方法具有一定程度上解决了多用户多任务多服务器场景下的资源分配和激励机制设计问题方面的优势，例如可以保证用户之间的公平性、效率性、真实性等 。但是，这类方法也存在一些局限性，例如对用户行为假设过于理想化或简化，忽略了用户之间的合作或信任关系；求解过程依赖于完全信息或共同知识，难以适应信息不对称或不完备的情况；缺乏自适应能力和泛化能力等。

(1).传统资源调度和随机优化的分布式资源调度方法

资源调度是指在有限的资源条件下，根据任务的需求和优先级，合理地分配和利用资源，以达到最优或次优的目标。资源调度问题在各种计算系统中都广泛存在，例如云计算、边缘计算、车联网等。传统的资源调度方法通常基于确定性或随机性的模型，通过数学规划、启发式算法、元启发式算法等技术来求解。传统资源调度方法通常基于集中式的优化模型，需要预先知道系统的参数和状态信息，然而在边缘云环境中，这些信息往往是不完全或不准确的，导致资源调度效率低下。为了解决这一问题，近年来出现了一些基于随机优化的分布式资源调度方法，它们可以根据实时的反馈信息动态地调整资源分配策略。目前已有一些针对边缘云计算场景的服务迁移和负载调度方法被提出，例如：Wang等人（2017）提出了一个联合计算卸载、资源分配和内容缓存的优化框架，并设计了一个基于ADMM的分布式算法来提高具有移动边缘计算的无线蜂窝网络的收益[4]。Rahul等人（2015）通过解耦原始MDP并采用李雅普诺夫优化技术，提出了一种高效、鲁棒、自适应且无需统计知识的边缘云服务迁移和负载调度算法[5]。Ibrahim等人（2019）提出了一种考虑车辆移动性和任务延迟要求的边缘计算工作负载调度方法，并通过拉格朗日松弛技术求解[6]。Zhang 等人（2022）考虑了多跳传输导向的车联网云动态工作流调度问题，提出了一种基于人工蜂群算法和贪心策略相结合的动态工作流调度方法[7]。这些方法在一些特定场景下表现出了较好的效果，但也面临着一些挑战，例如如何保证算法的收敛性和稳定性，如何平衡探索和利用之间的权衡，如何减少通信开销和计算复杂度等。

在分布式资源调度问题中，基于随机优化的方法是一种常见的解决方案。这类方法利用随机性来处理不确定性，通过迭代更新可行解来逼近最优解。然而，基于随机优化的方法也存在一些局限性，例如收敛速度慢、计算开销大、对参数敏感等。因此，近年来出现了一种新的方法，即深度强化学习。深度强化学习是一种结合了深度神经网络和强化学习的技术，可以训练神经网络来快速准确地解决多目标优化问题[8] 。相比于基于随机优化的方法，深度强化学习具有以下优势：一是能够自动学习复杂的策略函数，无需人为设计或调整参数；二是能够利用大量数据和高效算法来提高学习效率和质量；三是能够适应动态变化的环境和目标，并实现在线决策和反馈。

(2).强化学习的车联网资源调度

强化学习是一种基于试错学习和奖励反馈的机器学习方法，它可以让智能体在与环境交互的过程中自主地学习最优或近似最优的策略。强化学习具有以下几个特点：无需事先知道系统模型和参数；能够处理部分可观测和非马尔可夫性的环境；能够适应动态变化和不确定性的环境；能够实现长期目标和多目标之间的平衡。由于这些特点，强化学习被认为是一种适合于解决车联网资源调度问题的方法。强化学习的车联网资源调度是一种利用智能体的自主学习能力，根据车辆的状态、环境的变化和奖励信号，动态优化车辆之间和基础设施之间的通信资源分配的方法。强化学习的车联网资源调度可以提高车联网的性能，如降低时延、增加吞吐量、节省能耗等。强化学习的车联网资源调度面临着多个挑战，如高维状态空间、部分可观测性、非平稳性、多目标优化等。为了解决这些挑战，一些研究者提出了基于深度神经网络、多智能体协作、知识驱动等技术的强化学习算法，并在不同的场景中进行了验证和应用，如频谱共享、信道选择、功率控制、数据传输等。例如,Liu 等人（2019）设计了两种强化学习方法来优化计算卸载和资源分配问题[9]。Liu等人（2020）提出了一种基于优先级和价值函数的任务调度算法，考虑了任务之间的依赖性[10]。Song等人（2023）提出了一种基于潜在博弈理论和联邦深度强化学习的多异构服务器边缘计算卸载方法，能够有效地满足任务车辆用户的定制化服务需求[11]。Pang等人（2020）提出了一种考虑位置隐私保护的车联网计算资源协同调度策略，利用卡尔曼滤波预测车辆间距离，采用双重深度 Q 网络优化总成本[1]。然而，强化学习的车联网资源调度问题也存在一些挑战和难点，例如如何设计合适的状态空间、动作空间和奖励函数，如何处理高维度和连续性的状态和动作，如何实现多智能体之间的协调和博弈，如何提高学习效率和泛化能力等。

(3).强化学习移动边缘计算资源调度

移动边缘计算是一种将云计算的功能和服务延伸到网络边缘的技术，它可以为移动用户提供低延迟、高带宽、高可靠性的计算资源和服务。移动边缘计算中的资源调度问题是指如何在有限的计算资源和网络资源下，根据用户的任务需求和服务质量要求，合理地分配和调度任务在本地执行或卸载到边缘服务器或云服务器上执行。这是一个具有多约束、多目标、动态性和不确定性的优化问题，传统的优化方法难以有效地解决。因此，一些学者尝试使用强化学习来解决移动边缘计算中的资源调度问题，并取得了一定的成果。强化学习是一种基于智能体与环境交互学习最优行为策略的机器学习方法，可以适应不确定性和动态性，并且不需要先验知识或模型。近年来出现了许多基于强化学习的MEC资源调度方法，接下来对其中部分进行介绍。

例如，Zheng 等人（2022）使用深度 Q 网络（DQN）或双重深度 Q 网络（DDQN）来训练移动边缘服务器上的智能体，使其能够根据自身状态和环境状态选择最优或次优的任务卸载策略[12]；Chen J等人（2021）使用深度确定性策略梯度算法（DDPG）来训练连续动作空间下的智能体，使其能够输出最优或次优的任务卸载比例或执行速度[13]；Chen M等利用异步优势行动者-评论者算法(A3C），提出了一个基于软件定义网络和信息中心网络的动态资源分配方案，以提高车辆网络的服务质量[14];Peng等使用多智能体强化学习（MARL）来训练多个边缘服务器之间的智能体，使其能够协作地进行车辆关联和资源分配决策[15]；Peng等人（2022）提出了一种基于深度强化学习和有向无环图的依赖任务卸载策略，能够在多用户多服务器边缘计算环境中，灵活地选择合适的卸载目标服务器，并有效地减少服务延迟和终端能耗[16]。然而，强化学习在移动边缘计算中的资源调度问题仍然面临着一些问题和挑战，例如如何处理状态空间和动作空间的爆炸性增长，如何平衡探索和利用之间的权衡，如何解决多智能体之间的非平稳性和通信开销等。

(4).强化学习服务器资源调度

服务器资源调度是指在数据中心或云平台中，根据用户或应用程序的需求和服务质量要求，合理地分配和调度服务器上的计算资源、存储资源、网络资源等。这是一个涉及到数据中心或云平台的性能、效率、成本、节能等方面的重要问题。传统的服务器资源调度方法通常基于静态规划或启发式算法等技术来求解，但这些方法往往依赖于精确的系统模型和参数，难以适应复杂多变的环境。因此，一些学者利用强化学习来解决服务器资源调度问题，并取得了一些进展。强化学习服务器资源调度（Reinforcement Learning Server Resource Scheduling）是一种利用强化学习方法来优化服务器资源分配和任务执行的技术。它可以应对服务器状态的动态变化、多用户多任务的复杂需求、网络切片和边缘计算等新型网络架构的挑战，以及可再生能源和机器学习服务等新兴应用场景的特点。强化学习服务器资源调度的目标是在保证服务质量和满足约束条件的前提下，最大化系统效率和性能，最小化系统成本和延迟。强化学习服务器资源调度涉及多个层次和方面，包括资源配置、任务卸载、任务调度、并行度配置、协商策略等。近年来，有许多研究者提出了基于深度强化学习（Deep Reinforcement Learning）的服务器资源调度方法，并在不同的场景和数据集上进行了实验验证。

例如，有些学者使用强化学习来优化虚拟机（VM）或容器（Container）在物理机上的放置策略，以提高资源利用率和节约能耗[17–19]，其中Cheng 等人（2018）提出了一个基于深度强化学习的资源配置和任务调度系统，能够有效地降低云服务提供商的数据中心能耗和电费成本，并具有高效、可扩展、自适应和快速收敛等特点[17]，Zhao 等（2021）展示了如何利用深度强化学习在混合云环境中实现自适应的多目标任务调度，以最大化可再生能源的利用率和满足截止日期约束[19]；有些学者使用强化学习来优化服务器上的任务调度策略，以提高任务的执行效率和服务质量[20–25]，例如，Wu 等人（2020）针对服务器状态动态变化导致的资源分配不均衡问题，提出了一种基于深度强化学习和马尔可夫决策过程相结合的任务调度方法[23]，Rjoub等人（2021）提出了四种基于深度学习和强化学习的调度方法，并用真实数据集进行了实验比较。实验结果表明，深度强化学习结合长短期记忆网络（DRL-LSTM）的方法在减少任务执行成本和延迟方面优于其他三种方法[25]。然而，强化学习在服务器资源调度问题中也存在一些问题和挑战，例如如何处理大规模的状态空间和动作空间，如何处理部分可观测和非马尔可夫性的环境，如何处理多目标和多约束的优化问题，如何提高算法的收敛速度和稳定性等。

(5).车联网中的拍卖机制

拍卖机制是一种经济学中常用的激励机制，它可以实现资源或服务的有效分配和价格的公平确定。在车联网中，拍卖机制可以应用于多种场景，例如车辆之间或车辆与基础设施之间的资源或服务交易，例如频谱、缓存、计算、数据等。近年来，许多学者研究了车联网中的拍卖机制，并设计了各种拍卖模型和算法。例如，有些学者使用密封双向拍卖（SDBA）来实现车辆之间的频谱共享和交易；有些学者使用组合双向拍卖（CDBA）来实现车辆之间的缓存共享和交易；有些学者使用反向拍卖（RAM）来实现车辆与边缘服务器之间的计算任务卸载和交易；有些学者使用多属性拍卖（MAA）来实现车辆与数据提供者之间的数据获取和交易。然而，车联网中的拍卖机制也面临着一些问题和挑战，例如如何保证拍卖机制的真实性、有效性、个体合理性、社会福利最大化等性质，如何处理参与者的自私或恶意行为，如何减少拍卖过程中的计算复杂度和通信开销等。

车联网是指通过无线通信技术，实现车辆、道路、交通设施等的信息交互和资源共享的网络。车联网可以提高道路安全，优化交通管理，提升驾驶体验，促进智能出行等。在车联网中，拍卖机制是一种有效的资源分配和价格发现的方法，可以解决供需不平衡、竞争不公平、信息不对称等问题。拍卖机制可以应用于车联网中的多种场景，例如路侧单元（RSU）的接入控制、频谱资源的分配、数据服务的交易等。接下来将对部分研究进行介绍。Vishalatchi等人（2017）介绍了云计算中的虚拟机调度问题，以及一种基于拍卖机制的禁忌搜索算法来解决它。这可以作为一个基础和背景，让读者了解云计算中的资源分配问题和拍卖机制的作用。Ding等人（2016）从云计算转向网格计算，介绍了网格计算无线网络中的动态资源分配问题，以及一种具有预测能力和多属性特征的新颖反向在线拍卖算法来解决它。这可以展示拍卖机制在另一种计算网络中的适用性和创新性。Liwang等人（2019）从网格计算转向车联网，介绍了车联网中存在的计算卸载、资源共享和用户自私等问题，以及一种基于VCG原理的反向拍卖机制来优化计算卸载决策并满足经济属性。这可以展示拍卖机制在更具挑战性和前沿性的领域中的应用和效果。Zhang等人（2022）进一步考虑了公共区块链网络对车联网资源分配问题的影响和支持，提出了一种利用反向拍卖模型和VCG机制激励车辆记录驾驶数据，并使用边缘计算节点支持区块链技术的真实拍卖机制。这可以展示拍卖机制在结合其他先进技术时能够产生更高效、安全、可信等优点。

(6).强化学习定价策略

定价策略是指在市场交易中，根据供需关系、成本收益分析、竞争对手行为等因素，确定商品或服务的价格水平和变化规律。定价策略是一种重要的市场营销手段，它可以影响消费者的购买意愿和行为，从而影响供应商的收入和利润。在车联网中，定价策略可以应用于多种场景，例如车辆之间或车辆与基础设施之间的资源或服务交易，例如频谱、缓存、计算、数据等。近年来，许多学者研究了车联网中的定价策略，并设计了各种定价模型和算法。例如，有些学者使用基于需求函数的定价策略来实现车辆之间的频谱共享和交易；有些学者使用基于成本函数的定价策略来实现车辆之间的缓存共享和交易；有些学者使用基于效用函数的定价策略来实现车辆与边缘服务器之间的计算任务卸载和交易；有些学者使用基于博弈论的定价策略来实现车辆与数据提供者之间的数据获取和交易。然而，车联网中的定价策略也存在一些问题和挑战，例如如何根据市场环境和用户行为动态地调整价格，如何平衡供应商和消费者之间的利益，如何处理多方参与者之间的竞争和合作等。

强化学习定价策略是一种利用强化学习技术来优化定价决策的方法，它可以在不依赖用户响应函数的情况下，通过不断地探索和学习找到最优的定价策略，从而实现需求响应、能耗调度、竞争优势、成本效率等目标。强化学习定价策略在金融量化、电子市场、云计算等领域有着广泛的应用和研究。接下来将对使用强化学习进行定价的部分研究进行介绍。

Kutschinski等人（2003）研究了电子市场中多智能体强化学习定价策略，并提出了一个分布式代理平台来模拟和评估不同竞争环境下的定价效果。随后，Kim等人（2016）将强化学习技术应用到一个不确定微网环境中，解决了动态定价和能耗调度问题。Ghasemkhani等人（2018）提出了一种不依赖用户响应函数的定价算法，通过强化学习找到最优定价策略，实现需求响应目标。Krasheninnikova 等人（2019）将强化学习算法扩展到保险领域，使用马尔可夫决策过程来解决保险续费价格调整的多目标优化问题。Islam等人（2022）提出了一种基于深度强化学习的Spark作业调度算法，考虑了多个SLA目标，利用了云VM定价模型，动态调整了资源配置，用于解决云计算中的虚拟机（VM）调度问题。

综上所述，国内外相关领域的研究现状表明，车联网中的资源调度问题是一个具有重要意义和挑战性的课题，它涉及到多种技术和方法，例如传统优化、随机优化、强化学习、拍卖机制、定价策略等。这些技术和方法在一定程度上可以解决资源调度问题，但也存在一些不足或缺陷，需要进一步的研究和改进。因此，在本文中，我们将结合反向拍卖机制和多智能体深度强化学习，提出一种新颖的车联网工作流调度方法（RAM-DRL），并对其进行理论分析和仿真验证。下面将介绍本文研究的主要内容。

## 本文主要研究内容

本文针对车联网中的任务调度问题，提出了一种结合了多智能体强化学习和逆向拍卖机制的创新方法。该方法的目标是通过动态分配计算密集型服务的执行计划，实现服务、车辆和边缘服务器之间的资源匹配，同时通过激励机制促进车辆和边缘服务器之间的合作，以提高任务调度的效率和性能，降低总成本。具体而言，本文采用了多智能体强化学习算法，使车辆能够根据本地和全局信息自主学习和更新其任务调度策略，从而充分利用系统资源，提高执行效果。同时，引入了逆向拍卖机制，通过资源分配和价格协商，实现了车辆和边缘服务器之间的有效合作。这一机制旨在平衡竞争与合作，提高系统效率和公平性，确保资源的合理分配。本文将该方法命名为基于多智能体深度强化学习（MADRL）的车联网任务调度方法。

MADRL方法利用神经网络和马尔可夫决策过程（MDP）模型，训练每个边缘服务器上的策略网络，使其能够智能计算接受任务的长期奖励，并根据最大化奖励的原则向用户出价。通过结合深度强化学习和逆向拍卖机制，MADRL方法能够在复杂、动态和不确定的多智能体环境中，实现系统的适应性、鲁棒性、可扩展性和分布性。因此，MADRL方法能够适应复杂、动态和不确定的多智能体环境，提高系统的性能和效率。

此外，本文采用了逆拍卖机制，该机制在车联网中的应用场景丰富多样，包括资源或服务交易，如频谱、缓存、计算和数据等。在车联网中，逆拍卖机制不仅能够激励车辆用户共享资源或执行任务，还可应用于社会福利任务的分配，如交通管理、环境监测、数据收集等。通过逆拍卖，平台能够寻找愿意以最低价格提供服务或执行任务的车辆用户或服务器，并与其签署标准合同，从而实现成本节省。本文的后续章节将详细介绍本文提出的方法的设计和实现，并通过仿真实验验证其有效性和优越性。

本文的主要贡献如下：

- 将反向拍卖应用于车联网中的任务调度问题，实现了分布式、自适应、激励兼容的任务调度。
- 设计了一个基于 PPO+LSTM 的报价策略，利用 LSTM 的记忆能力，捕捉任务调度的时序特征和长期依赖，提高报价策略的性能和效果。
- 开发了一个基于 Python 的开源仿真环境 —— VehicleJobScheduling，模拟了车辆任务调度的过程，并提供了一些常用的评估指标。
- 通过仿真实验，验证了本文方法的有效性和优越性，以及强化学习和反向拍卖机制的优势和适用性，并与其他基准方法进行了对比分析。

## 本文组织结构

本文的结构安排如下：第二节综述了相关工作，第三节阐述了问题的建模，第四节描述了方法的设计，第五节展示了仿真环境和实验结果，第六节总结了本文的主要结论和未来工作。