---
UID: 20240224164859
source: null
cssclass: null
created: 2024-02-24 16:48
updated: 2024-02-24 21:53
date updated: 2024-02-24 22:24
---

## 问题陈述

在移动边缘计算中，车联网扮演着关键的角色，通过执行各种计算任务如语音识别、路线规划、计算机视觉、机器学习和增强现实，提升车辆的智能化和交互性。为了有效地进行任务卸载和资源分配，车辆可以选择将部分任务卸载到边缘服务器进行处理。然而，由于车辆和边缘服务器之间的网络连接不稳定，以及边缘服务器的资源有限，如何在这种环境中实现任务调度成为一个复杂而具有挑战性的问题。

在本章问题的建模中，我们依然和第三章类似考虑一个由$m$个卖家（边缘服务器）$\mathcal S=\{\mathbf s_1,\dots,\mathbf  s_m\}$和$n$个买家（车辆）$\mathcal B=\{\mathbf b_1,\dots,\mathbf b_n\}$组成的卸载模型。卖家$\mathbf s_i$具有资源向量$\mathbf r_i=(r_{i,1},\dots,r_{i,d})$和成本$c_i$，表示服务器拥有的资源种类和维护成本。买家$\mathbf b_j$提交的任务请求表示为$\mathbf b_j=(\theta_j,\psi_j)$，其中$\theta_j=(\mathbf f_j,\mathbf \sigma_j)$表示任务的执行特征和服务器执行限制向量。在任务到达时，买家发起拍卖，卖家报价，最终形成分配方案和支付，买家支付并开始任务卸载。问题的目标是通过设计服务器的任务报价策略，使得每个边缘服务器在满足任务预算和服务器执行限制的前提下，最大化卖家整体收益$R=\sum^{m}_{i=1} R_i$。

为了解决这一问题，我们引入了基于多智能体强化学习和反向拍卖的任务调度算法，具体而言，我们采用了MAPPO（Multiple Agent Proximal Policy Optimization）算法。与传统的集中式PPO算法不同，我们在第三章中使用了分布式学习分布式部署的方式，每个边缘服务器都独立地执行PPO算法进行学习和决策，服务器之间没有直接的通信。

在本章中，我们将详细介绍MAPPO算法的分布式学习和分布式部署实现细节，并通过实验和性能评估来验证其在车联网任务调度问题上的有效性和可行性。最终，我们将提出一些建议和结论，为未来车联网任务调度的研究和应用提供指导。

## 集中式学习分布式部署的多智能体强化学习算法

在本章的车联网任务调度问题中，我们仍然使用第三章的多智能体马尔可夫过程和反向拍卖过程建模整个系统。系统中的每个边缘服务器都被建模为一个智能体，其状态、动作和奖励构成了一个马尔可夫决策过程。在任务到达时，买家发起反向拍卖，卖家根据任务和自身情况进行报价，最终形成任务分配和支付。

MARL（多智能体强化学习）算法通常分为两种主要框架：集中式学习和分散式学习。在集中式学习中（引用[^6]），系统直接学习单一策略，以生成所有智能体的联合动作。这种方法具有全局性的决策优势，可以协调多个智能体的行为，但也可能在规模扩展时受到限制。相反，在分散学习中（引用[^21]），每个智能体独立地优化自己的奖励，具有较强的灵活性，适应不同的环境和博弈情境。然而，即使在简单的矩阵博弈中，分散学习可能面临不稳定性的问题（引用[^12]）。介于这两种框架之间的是集中训练和分散执行（CTDE）算法。过去的CTDE方法采用了行动者-评论家结构（引用[^22][^11]），学习以全局信息为输入的集中式评论家。另一种CTDE算法是值分解（VD）方法，通过将联合 $q$ 函数表示为智能体局部 $q$ 函数的函数（引用[^32][^27][^31]），在流行的MARL基准测试中取得了最先进的结果（引用[^37][^36]）。值分解方法通过在智能体之间分解值函数，实现一定程度的分散性，同时在训练过程中利用全局信息进行集中式指导。

MAPPO（Multi-agent PPO）是PPO算法在多智能体任务中的一种变体，同样采用actor-critic架构。与单智能体情境的不同之处在于此时critic学习的是一个中心价值函数（centralized value function）。简而言之，这意味着critic能够观测到全局信息（global state），其中包括其他智能体的信息和环境的信息。这种集中式的critic设计使得算法能够更全面地评估各个智能体的行为，增强了对多智能体任务的适应能力。

我们在多智能体环境中实现的 PPO 与单智能体环境中的 PPO 非常相似，它通过学习一个策略 $\pi_\theta$ 和一个值函数 $V_\phi(s)$ 来进行训练；这些函数被表示为两个独立的神经网络。值函数 $V_\phi(s)$ 用于方差减少，并且仅在训练过程中使用；因此，它可以接收额外的全局信息作为输入，这些信息在智能体的局部观察中不存在，使得多智能体领域中的 PPO 可以遵循 CTDE 结构。

MAPPO 训练两个独立的神经网络：参数为 $\theta$ 的行动者网络和参数为 $\phi$ 的值函数网络（称为批评家）。 如果代理是同质的，这些网络可以在所有代理之间共享，但每个代理也可以拥有自己的一对演员和评论家网络。 为了符号方便，我们在这里假设所有代理共享评论家和参与者网络。 具体来说，批评者网络（表示为 $V_\phi$）执行以下映射：$S \rightarrow R$。全局状态可以是特定于代理的或与代理无关的。

actor 网络表示为 $\pi_\theta$，将代理观察 $o^{(k)}_i$ 映射到离散动作空间中动作的分类分布，或者映射到多元高斯分布的均值和标准差向量，从中连续采样动作行动空间。 actor 网络经过训练以最大化下面的目标：

$$
L(\theta) = \left[\frac{1}{Bn}\sum_{i=1}^{B}\sum_{k=1}^{n}\min\left(r_{\theta,i}^{(k)}A_{i}^{(k)}, \operatorname{clip}\left(r_{\theta,i}^{(k)},1-\epsilon,1+\epsilon\right)A_{i}^{(k)}\right)\right]+\sigma\frac{1}{Bn}\sum_{i=1}^{B}\sum_{k=1}^{n}S\left[\pi_{\theta}(o_{i}^{(k)})\right]
$$

其中，$r_{\theta,i}^{(k)}=\frac{\pi_\theta(a_i^{(k)}|o_i^{(k)})}{\pi_{\theta_{old}}(a_i^{(k)}|o_i^{(k)})}.A_i^{(k)}$ 通过GAE方法计算，$S$ 为策略的熵，$\sigma$ 为熵系数超参数。

上面目标的第一部分的目的是最大化优势，使得 actor 朝着批评家指引的方向前进。具体而言，目标函数中的第一项是为了最大化优势，使 actor 的动作更趋向于批评家建议的方向。这有助于提高训练效果，使得智能体更加倾向于采取对当前状态有利的行动。

第二部分的目的是让策略的熵最大化。策略的熵衡量了动作分布的不确定性，通过最大化熵，我们可以尽可能地使输出的动作分布更加分散，避免过于集中。这有助于在探索过程中保持一定的多样性，提高学习的鲁棒性。

而批评家网络的目标则是让估计的未来回报最大化。批评家网络通过学习对当前状态的值函数来估计未来可能获得的回报，从而指导 actor 网络的训练。最大化估计的未来回报有助于智能体在决策过程中更好地评估不同动作的长期影响，从而优化其策略。

评论家网络训练目标是最小化损失函数
$\begin{aligned}L(\phi)=\frac{1}{Bn}\sum_{i=1}^{D}\sum_{k=1}^{n}(\max[(V_{\phi}(s_{i}^{(k)})-\hat{R}_{i})^{2},(\text{clip}(V_{\phi}(s_{i}^{(k)}),V_{\phi_{old}}(s_{i}^{(k)})-\varepsilon,V_{\phi_{old}}(s_{i}^{(k)})+\varepsilon)-\hat{R}_{i})^{2}],\end{aligned}$
其中, $\hat{R}_i$折扣奖励至目标描述计算奖励信号的方式，其中奖励信号在计算过程中按照一定的折扣率递减。这有助于强调未来奖励相对于当前奖励的重要性。在上面的损失函数中，B 指的是批量大小，n 指的是代理的数量。
