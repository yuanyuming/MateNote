---
UID: 20240311214336 
aliases: 
tags: 
source: 
cssclass: 
created: 2024-03-11
---
## 多智能体强化学习简介


当涉及多个智能体在共享环境中交互时，就形成了多智能体强化学习（MARL）的问题。多智能体深度强化学习的目标是让每个智能体根据自己的奖励和其他智能体的行为来调整自己的策略，从而实现合作或竞争。


在多智能体强化学习（MARL）中，根据智能体之间信息共享和决策方式的不同，可以采用不同的学习范式。集中学习范式要求所有智能体共享信息并作为一个整体进行决策，适合智能体数量较少的场景，如多机器人协作任务。独立学习范式允许每个智能体仅根据自身信息进行决策，适用于智能体数量较多的环境，如多智能体游戏。分布式学习范式则介于两者之间，智能体之间可以共享部分信息并协同进行决策，适用于多智能体通信网络。集中式训练分布式执行（CTDE）范式在训练阶段采用集中学习，在执行阶段则转为独立决策，结合了集中学习和分布式学习的优势，适用于复杂任务，如自动驾驶领域。不同范式的选择取决于智能体数量、环境复杂度和任务需求，集中学习适用于简单环境，独立学习适用于复杂环境，而分布式学习和CTDE提供了性能与可扩展性之间的平衡。

Pettingzoo 是一个用于研究多智能体强化学习（MARL）的 Python 库，提供了包括 Atari 游戏、棋类游戏、实时策略游戏和机器人控制等经典和新颖的多智能体环境。它允许使用 Agent Environment Cycle（AEC）模型来表示多智能体环境，该模型中，代理依次行动并在采取行动前接收更新的观测和奖励，环境则在每个代理步骤后更新。AEC 模型的灵活性使其能够处理 MARL 中各种类型的游戏。

在本次研究中，我们分别实现了分布式学习和集中式训练分布式执行（CTDE）两种范式，而不是同时实现这两种范式。我们通过分布式学习范式，使每个智能体能够根据自身的信息独立作出决策。在集中式训练分布式执行（CTDE）范式中，所有智能体在训练阶段共享信息并作为一个整体进行决策，而在执行阶段则转为独立决策。这种方法提高了系统的灵活性和适应性，尤其是在车联网任务调度的强化学习环境中。此外，我们利用Pettingzoo库中的Agent Environment Cycle（AEC）模型来构建我们的自定义强化学习环境。这种灵活的环境模型为我们的研究提供了强大的支持，使我们能够更有效地处理复杂的车联网任务调度问题。
