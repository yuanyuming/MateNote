---
date created: 2023-11-20 15:54
date updated: 2023-11-30 15:51
---

## 实验

计算卸载是一种在车联网中提高车辆性能和节省能耗的技术，它让车辆把部分计算任务卸载到边缘服务器上。计算卸载的目标是最大化车辆的效用，同时保证边缘服务器的收益和用户的体验。为此，边缘服务器需要设计合适的竞价策略，即根据任务的需求、紧急性和自身的资源等约束，决定每次报价的金额。

学习竞价策略涉及复杂的环境建模、不完全信息和动态优化问题，其验证方法选择至关重要。离线验证和在线验证是常见方法，但二者各有优缺点。离线验证基于历史数据进行反事实估计，但结果可能受数据偏差、反事实假设以及环境变化等因素的影响，甚至可能不符合古德哈特定律。相比之下，在线验证通过真实环境中的实验直接测试竞价策略的表现，更可靠，然而成本高、耗时且存在一定风险。

强化学习作为一种通过智能体与环境交互学习最佳行为的方法，在游戏、机器人、推荐系统等领域具有广泛应用前景。尽管其在实践中取得成功，研究社区明确指出其方法不足，并认为可靠的仿真环境是近年的重大进展核心。仿真环境提供接近真实环境但不受其限制和影响的平台，使研究者能快速开发、测试和改进强化学习算法。在某些相关领域，如推荐系统，仿真环境已广泛采用作为有效的评估机制。

针对车联网环境中计算卸载的服务器报价策略的验证，我们依据上述模型开发了相应的强化学习环境。在竞价策略学习过程中，我们利用强化学习辅助边缘服务器根据任务卸载特征和自身约束，动态决定每次出价，以最大化效用或收益。

### 仿真环境的设计和参数设置

为了验证提出的基于多智能体强化学习和反向拍卖机制的车联网任务卸载方法（RATO）的有效性，我们开发了一个名为VehicleJobScheduling的基于Python的开源仿真环境。该仿真环境旨在模拟车辆任务卸载的过程，并提供一系列常用的评估指标。在设计中，我们借鉴了PettingZoo，这是一个广受欢迎的多智能体强化学习框架，其定义了一套标准接口，使得不同算法可以在同一环境中进行比较。

VehicleJobScheduling仿真环境主要由三个组成部分构成，即车辆、服务器和环境生成器。在这个环境中，车辆充当任务的发起者，根据资源需求、连接限制和预算选择适当的服务器进行任务卸载，并支付相应费用。服务器则是任务的执行者，根据资源状况和成本，对收到的任务卸载请求进行报价，并执行被接受的任务。环境生成器负责生成任务和服务器的参数，包括任务的到达率、特征、连接限制，以及服务器的数量、类型、资源向量、成本等。用户可以通过设置这些参数，探索不同的任务卸载场景和策略。

我们的仿真环境支持Agent Environment Cycle（ACE）模式，这是一种适用于顺序回合制环境的接口，能够处理任何多智能体强化学习算法。在ACE模式下，每个智能体代表一个边缘服务器，接收车辆的任务卸载请求，利用自身资源完成任务卸载并获取收益。在每个时间步，有一个或多个车辆向满足其连接限制的服务器集合发送任务卸载请求，并等待服务器返回报价。环境按照连接限制选择一个服务器作为当前回合的行动者，该服务器根据自身资源使用情况、任务特征和报价策略计算任务报价，返回给车辆。当所有满足连接限制的服务器完成报价后，车辆选择一个或多个报价最低、同时满足需求和预算条件的服务器进行交易。交易成功后，车辆将任务发送到胜出者服务器并支付相应金额，等待执行结果返回。服务器获得收益，为任务分配资源并执行任务卸载。如果没有服务器的报价满足车辆的条件，则车辆无法完成任务卸载，无需支付任何金额。

为了模拟车联网场景，我们考虑了一个由多个车辆和多个边缘服务器构成的公路仿真环境。在这个环境中，车辆会对服务器信道进行评估，并向满足信道条件的边缘服务器发送任务卸载请求。我们假设任务请求的数量符合泊松分布，单位时间的任务到达数为20，涵盖了三种任务类型：存储密集型、计算密集型和存储计算密集型，以及两种任务时长：长任务和短任务。任务的持续时间被设定为1到10个时间槽，每个时间槽为1分钟，因此每个任务卸载请求的最大执行时间为10分钟。任务的优先级被设置在0到10之间，而任务对CPU和内存的请求数量最大分别为24和100。任务的连接限制范围为0到3，表示任务可以向满足信道条件的0到3个服务器发送任务卸载请求。

我们对车辆的任务卸载请求进行了抽象，将其表示为任务特征，其中包含四个维度：优先级、资源需求、持续时间以及任务连接限制。任务的优先级在0到10之间取值，并且它影响车辆对任务的预算分配。任务对CPU和内存的请求数量最大分别为24和100。任务的持续时间被设定为1到10个时间槽，每个时间槽为1分钟，因此每个任务卸载请求的最大执行时间为10分钟。任务的连接限制范围为0到3，表示任务可以向满足信道条件的0到3个服务器发送任务卸载请求。

在实验中，我们选择了[Azure的专用主机](https://learn.microsoft.com/zh-cn/azure/virtual-machines/dedicated-hosts)作为边缘服务器，提供多种规格和价格的虚拟机。我们选取了三种不同的专用主机SKU，分别代表大、中、小型的边缘服务器。云服务提供商的成本是按照三年计划的价格计算，而市场价格则是按照Azure美国东部[容器实例](https://azure.microsoft.com/zh-cn/pricing/details/container-instances/)的价格计算。表1列出了我们选用的专用主机的规格和价格。

| Dedicated Host SKUs | vCPUs | RAM       | 3 year plan |
| ------------------- | ----- | --------- | ----------- |
| Mdsv2MedMem-Type1   | 192   | 2,048 GiB | $ 5.81      |
| Easv5-Type1         | 112   | 768 GiB   | $ 3.49      |
| Fsv2-Type2          | 72    | 144 GiB   | $ 2.05      |

表1 选用的专用主机

| 参数        | 取值范围             |
| --------- | ---------------- |
| 任务到达率分布   | 泊松分布             |
| 单位时间任务到达数 | 20               |
| 任务时长      | 1到10个时间槽，每个槽为1分钟 |
| 任务优先级     | 0到10             |
| 任务CPU请求数  | 最大为24            |
| 任务内存请求数   | 最大为100           |
| 任务连接限制范围  | 0到3              |
| 表 环境超参数   |                  |

| Agent Model 类型  | PPO Agent     | PPO LTSM Agent        |
| --------------- | ------------- | --------------------- |
| 网络结构            | 多层感知机（MLP）    | 多层感知机 + 长短时记忆网络（LSTM） |
| 全连接层参数          | [384,384,384] | [384,384,384]         |
| LSTM单元大小        | -             | 256                   |
| 激活函数            | tanh          | tanh                  |
| 表 强化学习 Agent 参数 |               |                       |

在本文中，我们使用了两种不同的强化学习Agent的Model，分别是PPO Agent和PPO LTSM Agent，它们都是基于PPO（Proximal Policy Optimization）算法的，但是在网络结构上有所不同。PPO Agent的Model是一个多层感知机（MLP），它由三个全连接层组成，每层的神经元数量为384，激活函数为tanh。PPO LTSM Agent的Model则是在PPO Agent的Model的基础上增加了一个LSTM层，该层的单元大小为256，也就是每个LSTM单元内部状态向量的维度为256。LSTM层的作用是增加Model的记忆能力，使其能够处理和预测具有时间相关性的数据。我们将这两种Model分别应用于RATO方法，以比较它们在不同场景下的性能表现。

这样的设定使得我们能够在仿真中考虑多样化的车联网场景，涵盖了不同类型和时长的任务，以及车辆与边缘服务器之间的动态交互。接下来，我们将通过对RATO方法在这个仿真环境下的实验进行数值分析，全面评估其在不同场景下的性能表现。我们将特别关注不同服务器报价策略对系统效果的影响，以深入了解提出方法的优势和适用性。

: PettingZoo: Gym for Multi-Agent Reinforcement Learning

### 深度强化学习代理的收敛性

![[Pasted image 20231128145930.png]]

在本节中，我们评估了RATO方法在车联网资源卸载问题中的收敛性能。我们使用了两种基于多智能体强化学习的算法，分别是PPO和PPO+LTSM，来训练多个代理，以最大化服务器在一个回合内的收益。服务器的收益反映了任务卸载的效果，也是代理的奖励函数。我们对代理进行了4480000次迭代的训练，并在每40000次迭代后对算法进行了评估，计算了训练策略在10次测试运行中的平均奖励。图1展示了两种算法在训练过程中的奖励评估情况。表1显示了PPO和PPO+LTSM两种算法在最后448000次迭代中的奖励值的平均值和标准差。我们可以看到，PPO+LTSM的平均奖励值为$1.72×10^7$，比PPO的$1.68×10^7$高出约**2.4%**。同时，PPO+LTSM的标准差为$1.12×10^5$，比PPO的2.65×10^5低出约**57.7%**。这说明PPO+LTSM的奖励值不仅更高，而且更稳定，波动更小。因此，PPO+LTSM在车联网资源卸载方面有更好的收敛性能。综上所述，RATO方法在不同的任务卸载场景下，都表现出了较好的收敛性，能够快速地学习到最优或接近最优的报价策略，从而提高服务器的收益和车辆的效用。这证明了RATO方法的有效性和鲁棒性，以及强化学习和反向拍卖机制的优势和适用性。

| 算法                                | 平均奖励值       | 标准差         |
| --------------------------------- | ----------- | ----------- |
| PPO                               | $1.68×10^7$ | $2.65×10^5$ |
| PPO+LTSM                          | $1.72×10^7$ | $1.12×10^5$ |
表 PPO和PPO+LTSM训练过程中最后10%奖励值方差和平均值

### 实验结果及分析

本文提出了一种基于多智能体强化学习和反向拍卖机制的车联网任务卸载方法，简称为RATO，并通过仿真实验与其他基准方法进行了对比分析。本文考虑了任务到达数目为10、20和40的三种情况，采用了四种不同的服务器报价策略：固定报价（FB）、随机报价（RB）、基于PPO的报价（PPO）以及基于PPO+LSTM的报价（PPO+LSTM）。本文采用了四个指标来评估不同的调度策略在不同的到达数目下的表现，分别是：负载均衡（Load Balance）、服务器的收益（Server Earning）、车辆的利用率（Vehicle Utility）和订单的完成率（Complete Rate）。在每个到达数目下，进行了十次独立的仿真实验，利用五个随机种子进行模拟，并对每个指标的平均值和标准差进行了统计和分析。

本文的目的是比较不同的报价策略在任务卸载系统中的性能，以及分析RATO方法的优势。我们考虑了四种不同的报价策略，分别是：

- **Fixed策略**：服务器根据任务的距离和时长来确定报价，不考虑自身资源使用情况和任务特征。
- **Random策略**：服务器根据自身的成本和市场价格，设定一个报价区间，对每个任务卸载请求都返回一个在该区间内的随机报价。
- **PPO策略**：服务器根据自身的成本和市场价格，以及自身资源使用情况和任务特征，利用强化学习算法PPO来动态地确定报价。
- **PPO+LTSM策略**：服务器根据自身的成本和市场价格，以及自身资源使用情况和任务特征，利用强化学习算法PPO和长短期记忆网络（LSTM）来动态地确定报价，并利用LSTM来捕捉时序信息。

我们在不同的任务卸载场景下，分别对比了RATO方法与基准方法的收敛性。基准方法包括Fixed、Random和PPO三种策略。

|          | Load Blance | Server Earning (10^6) | Vehicle Utility (10^6) | Complete Rate |
| -------- | ----------- | --------------------- | ---------------------- | ------------- |
| Random   | 0.19±0.012  | 0.55±0.00037          | 0.55±0.037             | 0.76±0.014    |
| Fixed    | 0.20±0.005  | 1.60±0.00007          | 0.76±0.047             | 0.91±0.010    |
| PPO      | 0.20±0.011  | 1.49±0.00004          | 0.78±0.036             | 0.92±0.010    |
| PPO+LTSM | 0.18±0.010  | 1.53±0.00004          | 0.83±0.063             | 0.93±0.012    |

表 到达数为10时的实验结果

|          | Load Blance | Server Earning (10^6) | Vehicle Utility (10^6) | Complete Rate |
| -------- | ----------- | --------------------- | ---------------------- | ------------- |
| Random   | 0.22±0.007  | 2.74±0.00036          | 0.99±0.033             | 0.72±0.008    |
| Fixed    | 0.23±0.004  | 3.05±0.00014          | 1.43±0.040             | 0.90±0.005    |
| PPO      | 0.22±0.007  | 2.95±0.00011          | 1.53±0.060             | 0.91±0.007    |
| PPO+LTSM | 0.21±0.006  | 3.06±0.00009          | 1.62±0.036             | 0.93±0.009    |

表 到达数为20时的实验结果

|          | Load Blance | Server Earning (10^6) | Vehicle Utility (10^6) | Complete Rate |
| -------- | ----------- | --------------------- | ---------------------- | ------------- |
| Random   | 0.22±0.006  | 4.80±0.00100          | 1.64±0.034             | 0.63±0.007    |
| Fixed    | 0.21±0.004  | 4.73±0.00071          | 2.26±0.034             | 0.73±0.010    |
| PPO      | 0.21±0.003  | 4.68±0.00077          | 2.29±0.054             | 0.72±0.009    |
| PPO+LTSM | 0.20±0.006  | 4.66±0.00075          | 2.34±0.044             | 0.73±0.009    |

表 到达数为40时的实验结果

本文旨在分析不同报价策略在不同任务到达数下对各项指标的影响，包括服务器收益、车辆利用率、订单完成率和负载均衡。所考虑的四种策略分别是Fixed、Random、PPO和PPO+LTSM。

在各种报价策略中，首先考察了Fixed策略，这是一种简单的报价方式，基于任务的距离和时长确定报价，但未考虑资源使用和任务特征，导致在服务器利用率和订单完成率上的表现相对较差。其次，Random策略采用随机分配订单的方式，未充分考虑资源和任务特征，结果在所有指标上表现最差，可能导致资源浪费和拥堵。随后，我们研究了PPO策略，这是一种基于强化学习的报价方式，尽管在服务器收益上稍逊于Fixed策略，但在车辆效用和订单完成率上却优于Fixed策略，然而，其决策可能不够稳定因为未考虑时序信息。最后，关注了PPO+LTSM策略，结合了强化学习和长短期记忆，通过考虑时序信息实现更为合理的决策，在所有指标上表现出色，能够更好地平衡系统各项指标，通过学习最大化长期报酬提高系统整体性能。这对比分析表明，在任务选择和分配中，PPO+LTSM策略在适应性和性能方面更为突出，尤其是在考虑时序信息、平衡系统各项指标方面的优势。

为展示不同策略在不同任务到达数下的表现，将仿真实验结果以表格和图形形式呈现。表1、表2和表3给出了任务到达数为10、20和40时各指标的平均值和标准差。

在横向比较中，我们深入研究了四种不同的报价策略，以全面评估它们在各项指标上的表现。首先，PPO+LTSM策略在所有指标上都展现出色，要么优于其他策略，要么与其接近，这可以归因于该策略能够充分利用LTSM捕捉时序信息，使其决策更为合理。其次，关注了Fixed策略，尽管在服务器收益上表现良好，但相对于基于强化学习的策略，在车辆效用和订单完成率方面表现相对较差，这暗示着其在资源分配方面存在不平衡。在接下来的比较中，Random策略被发现表现最差，未充分考虑资源和任务特征，导致整体性能不佳。最后，我们考察了PPO策略，尽管在服务器收益上略逊于Fixed策略，但在其他指标上表现优于Fixed策略，然而，其决策的不稳定性可能是由于未考虑时序信息而引起的。总体而言，通过这次横向比较，我们得出结论，PPO+LTSM策略在任务选择和分配中能够更有效地学习如何最大化长期报酬，从而提高系统的整体性能，为选择合适的报价策略提供了明确的指导。

在纵向比较中，我们对不同任务到达数下的四种报价策略进行了详细观察。随着任务到达数的增加，服务器的收益和利用率普遍上升，但订单完成率却呈下降趋势，这可能是由于服务器过载而无法接收更多任务所致。

在任务到达数为10时，相对较小的任务数场景下，各策略在负载均衡和订单完成率上表现相近，除了随机策略。PPO+LTSM在车辆效用和负载均衡方面稍优，显示出更好的适应性和灵活性。Fixed策略的较高收入可能是因为其固定报价能够符合大多数任务预算，且不存在服务器之间的竞争。

当任务到达数为20时，随着任务数量的增加，PPO+LTSM策略各项指标均超越其他对比策略，尤其在负载均衡和任务完成率上的优势更为显著。相比之下，Fixed和PPO策略在车辆利用率方面下降，而Random策略相对较为稳定。此时的负载均衡值相较于任务数为10和40时更大，说明随着任务到达率的增加，服务器间竞争激烈，可能导致负载不均衡。然而，任务到达率继续增加，服务器负载维持在高水平，因此负载相对较为均衡。

在任务到达数为40时，大规模任务到达情境下，PPO+LTSM策略继续领先，实现更好的负载均衡、车辆效用和任务完成率。Random策略在这个情景下取得最高收益，可能是因为随机性导致其适应不同任务和竞争情况，使其更可能匹配到更多高价值的任务，同时避免了过度竞争和价格战。然而，需要注意的是Random策略在长期内可能不具备稳定性，而在某一次实验中取得较好的结果可能是由于随机性的影响。

在真实应用中，具备更强适应性和稳定性的策略更为可取。总体而言，不同任务到达数下，PPO+LTSM策略在负载均衡和车辆效用方面表现最佳，展现了更好的适应性和性能。其能够通过学习最大化长期报酬，更好地平衡系统各项指标，为不同任务到达数下的系统性能提供均衡的优化。

因此，综上所述，我们得出以下结论：PPO+LTSM策略是一种能够在不同的任务到达数下，平衡系统各项指标，提高系统整体性能的报价策略。其能够利用LTSM捕捉时序信息，做出更合理的决策。其优化目标是最大化长期的报酬，而不是仅关注短期负载均衡或最大化服务器收益。该策略实现了最有效的负载均衡，确保服务器资源利用率均匀，进而提高任务卸载的效率。同时，它提供了最优的服务质量，使车辆能够更经济高效地完成任务。因此，我们认为PPO+LTSM策略具有潜在的应用价值，可推广至其他类似的任务分配系统，以提升系统效率和收益。

综上所述，本文通过仿真实验，对比了不同报价策略在不同任务到达数下的表现，分析了RATO方法的优势。实验结果表明，PPO+LTSM策略能够更好地平衡系统的各项指标，提高系统的整体性能，为选择合适的报价策略提供了明确的指导。本文的工作为车联网任务卸载系统的研究提供了一种有效的方法，也为未来的研究提供了一些启示和方向。