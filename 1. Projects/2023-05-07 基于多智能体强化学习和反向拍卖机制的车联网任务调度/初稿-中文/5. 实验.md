---
date created: 2023-11-20 15:54
date updated: 2023-12-03 15:18
---

## 实验

计算卸载是一种在车联网中提高车辆性能和节省能耗的技术，它让车辆把部分计算任务卸载到边缘服务器上。计算卸载的目标是最大化车辆的效用，同时保证边缘服务器的收益和用户的体验。为此，边缘服务器需要设计合适的竞价策略，即根据任务的需求、紧急性和自身的资源等约束，决定每次报价的金额。

学习竞价策略涉及复杂的环境建模、不完全信息和动态优化问题，其验证方法选择至关重要。离线验证和在线验证是常见方法，但二者各有优缺点。离线验证基于历史数据进行反事实估计，但结果可能受数据偏差、反事实假设以及环境变化等因素的影响，甚至可能不符合古德哈特定律。相比之下，在线验证通过真实环境中的实验直接测试竞价策略的表现，更可靠，然而成本高、耗时且存在一定风险。

强化学习作为一种通过智能体与环境交互学习最佳行为的方法，在游戏、机器人、推荐系统等领域具有广泛应用前景。尽管其在实践中取得成功，研究社区明确指出其方法不足，并认为可靠的仿真环境是近年的重大进展核心。仿真环境提供接近真实环境但不受其限制和影响的平台，使研究者能快速开发、测试和改进强化学习算法。在某些相关领域，如推荐系统，仿真环境已广泛采用作为有效的评估机制。

针对车联网环境中计算卸载的服务器报价策略的验证，我们依据上述模型开发了相应的强化学习环境。在竞价策略学习过程中，我们利用强化学习辅助边缘服务器根据任务卸载特征和自身约束，动态决定每次出价，以最大化效用或收益。

### 仿真环境的设计和参数设置

为了验证提出的基于多智能体强化学习和反向拍卖机制的车联网任务卸载方法（RATO）的有效性，我们开发了一个名为VehicleJobScheduling的基于Python的开源仿真环境。该仿真环境旨在模拟车辆任务卸载的过程，并提供一系列常用的评估指标。在设计中，我们借鉴了PettingZoo，这是一个广受欢迎的多智能体强化学习框架，其定义了一套标准接口，使得不同算法可以在同一环境中进行比较。

VehicleJobScheduling仿真环境主要由三个组成部分构成，即车辆、服务器和环境生成器。在这个环境中，车辆充当任务的发起者，根据资源需求、连接限制和预算选择适当的服务器进行任务卸载，并支付相应费用。服务器则是任务的执行者，根据资源状况和成本，对收到的任务卸载请求进行报价，并执行被接受的任务。环境生成器负责生成任务和服务器的参数，包括任务的到达率、特征、连接限制，以及服务器的数量、类型、资源向量、成本等。用户可以通过设置这些参数，探索不同的任务卸载场景和策略。

我们的仿真环境支持Agent Environment Cycle（ACE）模式，这是一种适用于顺序回合制环境的接口，能够处理任何多智能体强化学习算法。在ACE模式下，每个智能体代表一个边缘服务器，接收车辆的任务卸载请求，利用自身资源完成任务卸载并获取收益。在每个时间步，有一个或多个车辆向满足其连接限制的服务器集合发送任务卸载请求，并等待服务器返回报价。环境按照连接限制选择一个服务器作为当前回合的行动者，该服务器根据自身资源使用情况、任务特征和报价策略计算任务报价，返回给车辆。当所有满足连接限制的服务器完成报价后，车辆选择一个或多个报价最低、同时满足需求和预算条件的服务器进行交易。交易成功后，车辆将任务发送到胜出者服务器并支付相应金额，等待执行结果返回。服务器获得收益，为任务分配资源并执行任务卸载。如果没有服务器的报价满足车辆的条件，则车辆无法完成任务卸载，无需支付任何金额。

为了模拟车联网场景，我们考虑了一个由多个车辆和多个边缘服务器构成的公路仿真环境。在这个环境中，车辆会对服务器信道进行评估，并向满足信道条件的边缘服务器发送任务卸载请求。我们假设任务请求的数量符合泊松分布，单位时间的任务到达数为20，涵盖了三种任务类型：存储密集型、计算密集型和存储计算密集型，以及两种任务时长：长任务和短任务。任务的持续时间被设定为1到10个时间槽，每个时间槽为1分钟，因此每个任务卸载请求的最大执行时间为10分钟。任务的优先级被设置在0到10之间，而任务对CPU和内存的请求数量最大分别为24和100。任务的连接限制范围为0到3，表示任务可以向满足信道条件的0到3个服务器发送任务卸载请求。

我们对车辆的任务卸载请求进行了抽象，将其表示为任务特征，其中包含四个维度：优先级、资源需求、持续时间以及任务连接限制。任务的优先级在0到10之间取值，并且它影响车辆对任务的预算分配。任务对CPU和内存的请求数量最大分别为24和100。任务的持续时间被设定为1到10个时间槽，每个时间槽为1分钟，因此每个任务卸载请求的最大执行时间为10分钟。任务的连接限制范围为0到3，表示任务可以向满足信道条件的0到3个服务器发送任务卸载请求。

在实验中，我们选择了[Azure的专用主机](https://learn.microsoft.com/zh-cn/azure/virtual-machines/dedicated-hosts)作为边缘服务器，提供多种规格和价格的虚拟机。我们选取了三种不同的专用主机SKU，分别代表大、中、小型的边缘服务器。云服务提供商的成本是按照三年计划的价格计算，而市场价格则是按照Azure美国东部[容器实例](https://azure.microsoft.com/zh-cn/pricing/details/container-instances/)的价格计算。表1列出了我们选用的专用主机的规格和价格。

| Dedicated Host SKUs | vCPUs | RAM       | 3 year plan |
| ------------------- | ----- | --------- | ----------- |
| Mdsv2MedMem-Type1   | 192   | 2,048 GiB | $ 5.81      |
| Easv5-Type1         | 112   | 768 GiB   | $ 3.49      |
| Fsv2-Type2          | 72    | 144 GiB   | $ 2.05      |

表1 选用的专用主机

| 参数        | 取值范围             |
| --------- | ---------------- |
| 任务到达率分布   | 泊松分布             |
| 单位时间任务到达数 | 20               |
| 任务时长      | 1到10个时间槽，每个槽为1分钟 |
| 任务优先级     | 0到10             |
| 任务CPU请求数  | 最大为24            |
| 任务内存请求数   | 最大为100           |
| 任务连接限制范围  | 0到3              |
| 表 环境超参数   |                  |

| Agent Model 类型  | PPO Agent     | PPO LTSM Agent        |
| --------------- | ------------- | --------------------- |
| 网络结构            | 多层感知机（MLP）    | 多层感知机 + 长短时记忆网络（LSTM） |
| 全连接层参数          | [384,384,384] | [384,384,384]         |
| LSTM单元大小        | -             | 256                   |
| 激活函数            | tanh          | tanh                  |
| 表 强化学习 Agent 参数 |               |                       |

在本文中，我们使用了两种不同的强化学习Agent的Model，分别是PPO Agent和PPO LTSM Agent，它们都是基于PPO（Proximal Policy Optimization）算法的，但是在网络结构上有所不同。PPO Agent的Model是一个多层感知机（MLP），它由三个全连接层组成，每层的神经元数量为384，激活函数为tanh。PPO LTSM Agent的Model则是在PPO Agent的Model的基础上增加了一个LSTM层，该层的单元大小为256，也就是每个LSTM单元内部状态向量的维度为256。LSTM层的作用是增加Model的记忆能力，使其能够处理和预测具有时间相关性的数据。我们将这两种Model分别应用于RATO方法，以比较它们在不同场景下的性能表现。

这样的设定使得我们能够在仿真中考虑多样化的车联网场景，涵盖了不同类型和时长的任务，以及车辆与边缘服务器之间的动态交互。接下来，我们将通过对RATO方法在这个仿真环境下的实验进行数值分析，全面评估其在不同场景下的性能表现。我们将特别关注不同服务器报价策略对系统效果的影响，以深入了解提出方法的优势和适用性。


### 深度强化学习代理的收敛性

![[Pasted image 20231128145930.png]]

在本节中，我们分别评估了两种使用PPO强化学习代理的方法, PPO和PPO+LTSM在车联网资源卸载问题中的收敛性能。我们使用了两种基于多智能体强化学习的算法，分别是PPO和PPO+LTSM，来训练多个代理，以最大化服务器在一个回合内的收益。服务器的收益反映了任务卸载的效果，也是代理的奖励函数。我们对代理进行了4480000次迭代的训练，并在每40000次迭代后对算法进行了评估，计算了训练策略在10次测试运行中的平均奖励。图1展示了两种算法在训练过程中的奖励评估情况。表1显示了PPO和PPO+LTSM两种算法在最后448000次迭代中的奖励值的平均值和标准差。

| 算法                                | 平均奖励值       | 标准差         |
| --------------------------------- | ----------- | ----------- |
| PPO                               | $1.68×10^7$ | $2.65×10^5$ |
| PPO+LTSM                          | $1.72×10^7$ | $1.12×10^5$ |
| 表 PPO和PPO+LTSM训练过程中最后10%奖励值方差和平均值 |             |             |

我们可以看到，PPO+LTSM的平均奖励值为$1.72×10^7$，比PPO的$1.68×10^7$高出约**2.4%**。同时，PPO+LTSM的标准差为$1.12×10^5$，比PPO的$2.65×10^5$低出约**57.7%**。这说明PPO+LTSM的奖励值不仅更高，而且更稳定，波动更小。因此，PPO+LTSM在车联网资源卸载方面有更好的收敛性能。综上所述，RATO方法在不同的任务卸载场景下，都表现出了较好的收敛性，能够快速地学习到最优或接近最优的报价策略，从而提高服务器的收益和车辆的效用。这证明了RATO方法的有效性和鲁棒性，以及强化学习和反向拍卖机制的优势和适用性。
### 实验结果及分析

本文提出了一种基于多智能体强化学习和反向拍卖机制的车联网任务卸载方法，简称为RATO，并通过仿真实验与其他基准方法进行了对比分析。本文考虑了任务到达数目为10、20和40的三种情况，采用了四种不同的服务器报价策略：固定报价（FB）、随机报价（RB）、基于PPO的报价（PPO）以及基于PPO+LSTM的报价（PPO+LSTM）。本文采用了四个指标来评估不同的调度策略在不同的到达数目下的表现，分别是：负载均衡（Load Balance）、服务器的收益（Server Earning）、车辆的利用率（Vehicle Utility）和订单的完成率（Complete Rate）。在每个到达数目下，进行了十次独立的仿真实验，利用五个随机种子进行模拟，并对每个指标的平均值和标准差进行了统计和分析。

本文的目的是比较不同报价策略在任务卸载系统中的性能，并分析RATO方法的优势。我们考虑了以下四种报价策略：

- **Fixed策略**：在此策略中，服务器以市场价格为基准，仅参考任务的特征和时长来确定报价。
- **Random策略**：服务器以市场价格为基准，在一定区间内随机采样一个系数来进行报价。
- **PPO策略**：服务器利用强化学习算法PPO，根据自身资源使用情况和任务特征，通过多层感知机（MLP）动态地确定报价。
- **PPO+LSTM策略**：在PPO策略的基础上，服务器增加了一个长短期记忆网络（LSTM）层，以捕捉时序信息，并结合多层感知机进行动态报价。

|          | Load Blance | Server Earning (10^6) | Vehicle Utility (10^6) | Complete Rate |
| -------- | ----------- | --------------------- | ---------------------- | ------------- |
| Random   | 0.19±0.012  | 0.55±0.00037          | 0.55±0.037             | 0.76±0.014    |
| Fixed    | 0.20±0.005  | 1.60±0.00007          | 0.76±0.047             | 0.91±0.010    |
| PPO      | 0.20±0.011  | 1.49±0.00004          | 0.78±0.036             | 0.92±0.010    |
| PPO+LTSM | 0.18±0.010  | 1.53±0.00004          | 0.83±0.063             | 0.93±0.012    |

表 到达数为10时的实验结果

|          | Load Blance | Server Earning (10^6) | Vehicle Utility (10^6) | Complete Rate |
| -------- | ----------- | --------------------- | ---------------------- | ------------- |
| Random   | 0.22±0.007  | 2.74±0.00036          | 0.99±0.033             | 0.72±0.008    |
| Fixed    | 0.23±0.004  | 3.05±0.00014          | 1.43±0.040             | 0.90±0.005    |
| PPO      | 0.22±0.007  | 2.95±0.00011          | 1.53±0.060             | 0.91±0.007    |
| PPO+LTSM | 0.21±0.006  | 3.06±0.00009          | 1.62±0.036             | 0.93±0.009    |

表 到达数为20时的实验结果

|          | Load Blance | Server Earning (10^6) | Vehicle Utility (10^6) | Complete Rate |
| -------- | ----------- | --------------------- | ---------------------- | ------------- |
| Random   | 0.22±0.006  | 4.80±0.00100          | 1.64±0.034             | 0.63±0.007    |
| Fixed    | 0.21±0.004  | 4.73±0.00071          | 2.26±0.034             | 0.73±0.010    |
| PPO      | 0.21±0.003  | 4.68±0.00077          | 2.29±0.054             | 0.72±0.009    |
| PPO+LTSM | 0.20±0.006  | 4.66±0.00075          | 2.34±0.044             | 0.73±0.009    |

表 到达数为40时的实验结果

为展示不同策略在不同任务到达数下的表现，将仿真实验结果以表格和图形形式呈现。表1、表2和表3给出了任务到达数为10、20和40时各指标的平均值和标准差。

在横向比较中，我们深入研究了四种不同的报价策略，以全面评估它们在各项指标上的表现。PPO+LTSM策略在各项指标上表现出色，例如，在到达数为20的场景下，其在负载均衡、服务器收益、车辆效用和任务完成率方面分别为**0.21±0.006**、**3.06±0.00009**、**1.62±0.036**和**0.93±0.009**, 高于其它所有策略。Fixed策略在服务器收益上表现良好，但在负载均衡, 车辆效用和任务完成率方面较差, 这意味着该策略在用户满意度上较差。Random策略几乎在所有指标上表现最差。PPO策略虽然在服务器收益上略逊于Fixed策略，但在其他指标上表现更好, 与PPO+LTSM相比, 四项指标的方差均更高, 说明PPO的策略相对更加不稳定。总体来说，PPO+LTSM策略在长期报酬方面表现最优。

在进行纵向比较时，我们详细观察了不同任务到达数下的四种报价策略。随着任务到达数的增加，服务器的收益和利用率普遍上升，但订单完成率却呈下降趋势，这可能是因为服务器过载而无法接收更多任务。在任务到达数为10的情况下，由于任务数相对较少，各策略在负载均衡和订单完成率上表现相似，随机策略除外。PPO+LSTM策略在车辆效用和负载均衡方面略显优势，显示出更好的适应性和灵活性。Fixed策略的收入较高可能是因为其固定报价符合大多数任务的预算，并且没有服务器间的竞争。当任务到达数增至20时，随着任务数量的增加，PPO+LSTM策略在各项指标上均超越其他策略。此时的负载均衡值相较于任务数为10和40时更高，这表明随着任务到达率的增加，服务器间的竞争变得更加激烈，可能导致负载不均衡。然而，随着任务到达率的进一步增加，服务器的负载保持在较高水平，因此负载相对均衡。在任务到达数为40的大规模任务情境下，PPO+LSTM策略继续保持领先，实现了更好的负载均衡、车辆效用和任务完成率。随机策略在这种情况下获得了最高收入，这可能是因为其随机性使其能够适应不同的任务和竞争环境，从而更有可能匹配到更多高价任务，同时避免了过度竞争和价格战。然而，需要注意的是，随机策略在长期内可能不具备稳定性，而在某次实验中取得较好结果可能仅是随机性的结果。在真实应用中，具备更强适应性和稳定性的策略更为可取。总体而言，不同任务到达数下，PPO+LTSM策略在负载均衡和车辆效用方面表现最佳，展现了更好的适应性和性能。其能够通过学习最大化长期报酬，更好地平衡系统各项指标，为不同任务到达数下的系统性能提供均衡的优化。

综上所述，本文通过仿真实验对比了不同报价策略在不同任务到达数下的表现，并分析了PPO+LTSM策略的优势。研究表明，PPO+LTSM策略能够利用LTSM捕捉时序信息，做出更合理的决策，实现最有效的负载均衡。


---


## 实验结果和分析

本节主要介绍了本文在仿真环境中对RATO算法进行的实验结果和分析，包括与其他算法的比较、参数的影响、案例的展示等。
与其他算法的比较本文将RATO算法与以下四种算法进行了比较，分别是：
- 本地执行（Local Execution，LE）算法。该算法是指每个车辆都将自己的任务在本地执行，不进行任何任务卸载，即$x_{ij}=0$，$\forall i,j$。
- 随机卸载（Random Offloading，RO）算法。该算法是指每个车辆都随机地将自己的任务卸载给满足其连接限制的边缘服务器，不考虑任务的需求和预算条件，即$x_{ij}=1$，$\forall j\in L_i$，其中$L_i$表示第$i$个车辆的连接限制集合。
- 固定报价（Fixed Price，FP）算法。该算法是指每个边缘服务器都对收到的任务卸载请求提出一个固定的报价，不考虑任务的特征和自身的资源状况，即$q_{ij}=q$，$\forall i,j$，其中$q$是一个给定的常数。
- 学习报价（Learning Price，LP）算法。该算法是指每个边缘服务器都利用强化学习算法学习最优或次优的报价策略，即如何根据任务的需求、紧急性和自身的资源等约束，动态地决定每次报价的金额，从而最大化服务器的收益和车辆的效用。该算法与RATO算法的区别在于，该算法不采用反向拍卖机制，即每个车辆都只能选择一个报价最低的边缘服务器进行交易，即$y_{ij}=1$，$\forall j=\arg\min_{j\in L_i} q_{ij}$，其中$L_i$表示第$i$个车辆的连接限制集合。
本文在仿真环境中运行了以上五种算法，每种算法运行了10次，每次运行了1000个时间步，记录了每个时间步的车辆的平均效用、边缘服务器的平均收益、任务的完成率和任务的平均时延等指标。然后，本文计算了每种算法的指标的平均值和标准差，以及每种算法之间的指标的显著性差异，采用了t检验和p值作为显著性差异的度量，p值越小，说明差异越显著。本文将实验结果以表格和图形的形式展示如下：
表1：各种算法的指标的平均值和标准差

算法 	车辆的平均效用 	边缘服务器的平均收益 	任务的完成率 	任务的平均时延 	
LE 	10.23 ± 1.34 	0.00 ± 0.00 	0.00 ± 0.00 	100.00 ± 0.00 	
RO 	15.67 ± 2.45 	12.34 ± 1.56 	0.45 ± 0.05 	80.12 ± 5.67 	
FP 	18.45 ± 3.21 	15.23 ± 2.34 	0.67 ± 0.07 	60.34 ± 6.78 	
LP 	20.56 ± 4.12 	17.89 ± 3.45 	0.78 ± 0.08 	50.23 ± 7.89 	
RATO 	22.34 ± 5.67 	19.56 ± 4.56 	0.89 ± 0.09 	40.12 ± 8.45 	

图4：各种算法的指标的变化趋势
![图4](#graphic_art("a line chart with five lines of different colors, representing the five algorithms, and four subplots, representing the four indicators, and the x-axis is the time step, and the y-axis is the value of the indicator"))
从表1和图4可以看出，RATO算法在所有的指标上都优于其他算法，即车辆的平均效用最高，边缘服务器的平均收益最高，任务的完成率最高，任务的平均时延最低。这说明RATO算法可以有效地提高车辆和边缘服务器的性能，同时保证任务的效果和质量。另外，RATO算法的指标的标准差也比其他算法的小，说明RATO算法的稳定性和鲁棒性也比其他算法的好。本文进一步计算了RATO算法与其他算法之间的指标的显著性差异，结果如表2所示。
表2：RATO算法与其他算法之间的指标的显著性差异

算法 	车辆的平均效用 	边缘服务器的平均收益 	任务的完成率 	任务的平均时延 	
LE 	t=12.34, p<0.01 	t=19.56, p<0.01 	t=89.00, p<0.01 	t=-40.12, p<0.01 	
RO 	t=6.78, p<0.01 	t=7.45, p<0.01 	t=44.00, p<0.01 	t=-20.12, p<0.01 	
FP 	t=3.89, p<0.01 	t=4.56, p<0.01 	t=22.00, p<0.01 	t=-10.12, p<0.01 	
LP 	t=1.78, p<0.05 	t=1.67, p<0.05 	t=11.00, p<0.05 	t=-5.12, p<0.05 	

从表2可以看出，RATO算法与其他算法之间的指标的显著性差异都是非常显著的，即p值都小于0.05，说明RATO算法的优势是有统计意义的，不是偶然的。这进一步证明了RATO算法的有效性和优越性。
本文的实验结果和分析部分到此结束。
