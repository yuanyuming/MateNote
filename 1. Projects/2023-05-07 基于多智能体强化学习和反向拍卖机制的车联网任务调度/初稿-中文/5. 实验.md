---
date created: 2023-11-20 15:54
date updated: 2023-11-29 17:20
---

## 实验

计算卸载是一种在车联网中提高车辆性能和节省能耗的技术，它让车辆把部分计算任务卸载到边缘服务器上。计算卸载的目标是最大化车辆的效用，同时保证边缘服务器的收益和用户的体验。为此，边缘服务器需要设计合适的竞价策略，即根据任务的需求、紧急性和自身的资源等约束，决定每次报价的金额。

学习竞价策略涉及复杂的环境建模、不完全信息和动态优化问题，其验证方法选择至关重要。离线验证和在线验证是常见方法，但二者各有优缺点。离线验证基于历史数据进行反事实估计，但结果可能受数据偏差、反事实假设以及环境变化等因素的影响，甚至可能不符合古德哈特定律。相比之下，在线验证通过真实环境中的实验直接测试竞价策略的表现，更可靠，然而成本高、耗时且存在一定风险。

强化学习作为一种通过智能体与环境交互学习最佳行为的方法，在游戏、机器人、推荐系统等领域具有广泛应用前景。尽管其在实践中取得成功，研究社区明确指出其方法不足，并认为可靠的仿真环境是近年的重大进展核心。仿真环境提供接近真实环境但不受其限制和影响的平台，使研究者能快速开发、测试和改进强化学习算法。在某些相关领域，如推荐系统，仿真环境已广泛采用作为有效的评估机制。

针对车联网环境中计算卸载的服务器报价策略的验证，我们依据上述模型开发了相应的强化学习环境。在竞价策略学习过程中，我们利用强化学习辅助边缘服务器根据任务卸载特征和自身约束，动态决定每次出价，以最大化效用或收益。

### 仿真环境的设计和参数设置

为了验证本文提出的基于多智能体强化学习和反向拍卖机制的车联网任务卸载方法（RATO）的有效性，我们开发了一个基于Python的开源仿真环境，称为VehicleJobScheduling。该仿真环境旨在模拟车辆任务卸载的过程，并提供一系列常用的评估指标。我们参考了Pettingzoo，这是一个广受欢迎的多智能体强化学习框架，其定义了一套标准接口，使得不同算法可以在同一环境中进行比较。

VehicleJobScheduling仿真环境主要由三个组成部分构成，即车辆、服务器和环境生成器。车辆充当任务的发起者，根据资源需求、连接限制和预算选择适当的服务器进行任务卸载，并支付相应费用。服务器则是任务的执行者，根据资源状况和成本，对收到的任务卸载请求进行报价，并执行被接受的任务。环境生成器负责生成任务和服务器的参数，包括任务的到达率、特征、连接限制，以及服务器的数量、类型、资源向量、成本等。用户可以通过设置这些参数，探索不同的任务卸载场景和策略。

我们的环境支持Agent Environment Cycle（ACE）模式，这是一种适用于顺序回合制环境的接口，能够处理任何多智能体强化学习算法。在ACE模式下，每个智能体代表一个边缘服务器，接收车辆的任务卸载请求，利用自身资源完成任务卸载并获取收益。每个时间步，有一个或多个车辆向满足其连接限制的服务器集合发送任务卸载请求，并等待服务器返回报价。环境按照连接限制选择一个服务器作为当前回合的行动者，该服务器根据自身资源使用情况、任务特征和报价策略计算任务报价，返回给车辆。当所有满足连接限制的服务器完成报价后，车辆选择一个或多个报价最低、同时满足需求和预算条件的服务器进行交易。交易成功后，车辆将任务发送到胜出者服务器并支付相应金额，等待执行结果返回。服务器获得收益，为任务分配资源并执行任务卸载。如果没有服务器的报价满足车辆的条件，则车辆无法完成任务卸载，无需支付任何金额。

为了模拟车联网场景，我们考虑了一个由多个车辆和多个边缘服务器构成的公路仿真环境。在这个环境中，车辆会对服务器信道进行评估，并向满足信道条件的边缘服务器发送任务卸载请求。我们假设任务请求的数量符合泊松分布，单位时间的任务到达数为20，涵盖了三种任务类型：存储密集型、计算密集型和存储计算密集型，以及两种任务时长：长任务和短任务。任务的持续时间被设定为1到10个时间槽，每个时间槽为1分钟，因此每个任务卸载请求的最大执行时间为10分钟。任务的优先级被设置在0到10之间，而任务对CPU和内存的请求数量最大分别为24和100。此外，我们设定了任务的连接限制范围为0到3，表示任务可以向满足信道条件的0到3个服务器发送任务卸载请求。

我们对车辆的任务卸载请求进行了抽象，将其表示为任务特征，其中包含四个维度：优先级、资源需求、持续时间以及任务连接限制。任务的优先级在0到10之间取值，并且它影响车辆对任务的预算分配。任务对CPU和内存的请求数量最大分别为24和100。任务的持续时间被设定为1到10个时间槽，每个时间槽为1分钟，因此每个任务卸载请求的最大执行时间为10分钟。任务的连接限制范围为0到3，表示任务可以向满足信道条件的0到3个服务器发送任务卸载请求。

在实验中，我们选择了[Azure的专用主机](https://learn.microsoft.com/zh-cn/azure/virtual-machines/dedicated-hosts)作为边缘服务器，提供多种规格和价格的虚拟机。我们选取了三种不同的专用主机SKU，分别代表大、中、小型的边缘服务器。云服务提供商的成本是按照三年计划的价格计算，而市场价格则是按照Azure美国东部[容器实例](https://azure.microsoft.com/zh-cn/pricing/details/container-instances/)的价格计算。表1列出了我们选用的专用主机的规格和价格。

| Dedicated Host SKUs | vCPUs | RAM       | 3 year plan |
| ------------------- | ----- | --------- | ----------- |
| Mdsv2MedMem-Type1   | 192   | 2,048 GiB | $ 5.81      |
| Easv5-Type1         | 112   | 768 GiB   | $ 3.49      |
| Fsv2-Type2          | 72    | 144 GiB   | $ 2.05      |

表1 选用的专用主机

在本文中，我们使用了两种不同的强化学习Agent的Model，分别是PPO Agent和PPO LTSM Agent，它们都是基于PPO（Proximal Policy Optimization）算法的，但是在网络结构上有所不同。PPO Agent的Model是一个多层感知机（MLP），它由三个全连接层组成，每层的神经元数量为384，激活函数为tanh。PPO LTSM Agent的Model则是在PPO Agent的Model的基础上增加了一个LSTM层，该层的单元大小为256，也就是每个LSTM单元内部状态向量的维度为256。LSTM层的作用是增加Model的记忆能力，使其能够处理和预测具有时间相关性的数据。我们将这两种Model分别应用于RATO方法，以比较它们在不同场景下的性能表现。

这样的设定使得我们能够在仿真中考虑多样化的车联网场景，涵盖了不同类型和时长的任务，以及车辆与边缘服务器之间的动态交互。这样的实验设置允许我们全面评估RATO方法在不同场景下的性能表现，考察不同服务器报价策略对系统效果的影响。通过这些实验，我们能够更深入地理解提出方法的优势和适用性。

表 强化学习Agent和环境超参数

: PettingZoo: Gym for Multi-Agent Reinforcement Learning

### 深度强化学习代理的收敛性

![[Pasted image 20231128145930.png]]

在本节中，我们评估了RATO方法在车联网资源卸载问题中的收敛性能。我们使用了PPO和PPO+LTSM两种基于多智能体强化学习的算法，分别训练了多个代理，以最大化奖励函数。奖励函数是服务器在一个回合内的收益，反映了任务卸载的效果。我们对代理进行了4480000次迭代的训练，并在每40000次迭代后对算法进行了评估，计算了训练策略在10次测试运行中的平均奖励。图1展示了两种算法在训练过程中的奖励评估情况。表1显示了PPO和PPO+LTSM两种算法在最后448000次迭代中的奖励值的平均值和标准差。我们可以看到，PPO+LTSM的平均奖励值为1.72×10^7，比PPO的1.68×10^7高出约2.4%。同时，PPO+LTSM的标准差为1.12×10^5，比PPO的2.65×10^5低出约57.7%。这说明PPO+LTSM的奖励值不仅更高，而且更稳定，波动更小。因此，PPO+LTSM在车联网资源卸载方面有更好的收敛性能。综上所述，RATO方法在不同的任务卸载场景下，都表现出了较好的收敛性，能够快速地学习到最优或接近最优的报价策略，从而提高服务器的收益和车辆的效用。这证明了RATO方法的有效性和鲁棒性，以及强化学习和反向拍卖机制的优势和适用性。

| 算法       | 平均奖励值     | 标准差       |
| -------- | --------- | --------- |
| PPO      | 1.68×10^7 | 2.65×10^5 |
| PPO+LTSM | 1.72×10^7 | 1.12×10^5 |

在本节中，我们评估了RATO方法在车联网资源卸载问题中的收敛性能。我们使用了两种基于多智能体强化学习的算法，分别是PPO和PPO+LTSM，来训练多个代理，以最大化服务器在一个回合内的收益。服务器的收益反映了任务卸载的效果，也是代理的奖励函数。我们对代理进行了4480000次迭代的训练，并在每40000次迭代后对算法进行了评估，计算了训练策略在10次测试运行中的平均奖励。图1展示了两种算法在训练过程中的奖励评估情况。表1显示了PPO和PPO+LTSM两种算法在最后448000次迭代中的奖励值的平均值和标准差。我们可以看到，PPO+LTSM的平均奖励值为1.72×10^7，比PPO的1.68×10^7高出约**2.4%**。同时，PPO+LTSM的标准差为1.12×10^5，比PPO的2.65×10^5低出约**57.7%**。这说明PPO+LTSM的奖励值不仅更高，而且更稳定，波动更小。因此，PPO+LTSM在车联网资源卸载方面有更好的收敛性能。综上所述，RATO方法在不同的任务卸载场景下，都表现出了较好的收敛性，能够快速地学习到最优或接近最优的报价策略，从而提高服务器的收益和车辆的效用。这证明了RATO方法的有效性和鲁棒性，以及强化学习和反向拍卖机制的优势和适用性。

### 实验结果及分析

本文提出了一种基于多智能体强化学习和反向拍卖机制的车联网任务卸载方法，简称为RATO，并通过仿真实验与其他基准方法进行了对比分析。本文考虑了任务到达数目为10、20和40的三种情况，采用了四种不同的服务器报价策略：固定报价（FB）、随机报价（RB）、基于PPO的报价（PPO）以及基于PPO+LSTM的报价（PPO+LSTM）。本文采用了四个指标来评估不同的调度策略在不同的到达数目下的表现，分别是：负载均衡（Load Balance）、服务器的收益（Server Earning）、车辆的利用率（Vehicle Utility）和订单的完成率（Complete Rate）。在每个到达数目下，进行了十次独立的仿真实验，利用五个随机种子进行模拟，并对每个指标的平均值和标准差进行了统计和分析。

本文的目的是比较不同的报价策略在任务卸载系统中的性能，以及分析RATO方法的优势。我们考虑了四种不同的报价策略，分别是：

- **Fixed策略**：服务器根据任务的距离和时长来确定报价，不考虑自身资源使用情况和任务特征。
- **Random策略**：服务器根据自身的成本和市场价格，设定一个报价区间，对每个任务卸载请求都返回一个在该区间内的随机报价。
- **PPO策略**：服务器根据自身的成本和市场价格，以及自身资源使用情况和任务特征，利用强化学习算法PPO来动态地确定报价。
- **PPO+LTSM策略**：服务器根据自身的成本和市场价格，以及自身资源使用情况和任务特征，利用强化学习算法PPO和长短期记忆网络（LSTM）来动态地确定报价，并利用LSTM来捕捉时序信息。

我们在不同的任务卸载场景下，分别对比了RATO方法与基准方法的收敛性。基准方法包括Fixed、Random和PPO三种策略。

|          | Load Blance | Server Earning (10^6) | Vehicle Utility (10^6) | Complete Rate |
| -------- | ----------- | --------------------- | ---------------------- | ------------- |
| Random   | 0.19±0.012  | 0.55±0.00037          | 0.55±0.037             | 0.76±0.014    |
| Fixed    | 0.20±0.005  | 1.60±0.00007          | 0.76±0.047             | 0.91±0.010    |
| PPO      | 0.20±0.011  | 1.49±0.00004          | 0.78±0.036             | 0.92±0.010    |
| PPO+LTSM | 0.18±0.010  | 1.53±0.00004          | 0.83±0.063             | 0.93±0.012    |

表 到达数为10时的实验结果

|          | Load Blance | Server Earning (10^6) | Vehicle Utility (10^6) | Complete Rate |
| -------- | ----------- | --------------------- | ---------------------- | ------------- |
| Random   | 0.22±0.007  | 2.74±0.00036          | 0.99±0.033             | 0.72±0.008    |
| Fixed    | 0.23±0.004  | 3.05±0.00014          | 1.43±0.040             | 0.90±0.005    |
| PPO      | 0.22±0.007  | 2.95±0.00011          | 1.53±0.060             | 0.91±0.007    |
| PPO+LTSM | 0.21±0.006  | 3.06±0.00009          | 1.62±0.036             | 0.93±0.009    |

表 到达数为20时的实验结果

|          | Load Blance | Server Earning (10^6) | Vehicle Utility (10^6) | Complete Rate |
| -------- | ----------- | --------------------- | ---------------------- | ------------- |
| Random   | 0.22±0.006  | 4.80±0.00100          | 1.64±0.034             | 0.63±0.007    |
| Fixed    | 0.21±0.004  | 4.73±0.00071          | 2.26±0.034             | 0.73±0.009    |
| PPO      | 0.21±0.003  | 4.68±0.00077          | 2.29±0.054             | 0.72±0.009    |
| PPO+LTSM | 0.20±0.006  | 4.66±0.00075          | 2.34±0.044             | 0.73±0.010    |

表 到达数为40时的实验结果

本文旨在分析任务到达数对各个指标的影响，包括服务器的收益、车辆的利用率、订单的完成率和负载均衡四个指标。我们考虑了四种不同的报价策略，分别是Fixed、Random、PPO和PPO+LTSM。

Fixed策略是一种简单的报价策略，它根据任务的距离和时长来确定报价，不考虑自身资源使用情况和任务特征。Fixed策略虽然在服务器的收益上也有不错的表现，但是它在车辆的利用率和订单的完成率上却明显落后于PPO+LTSM策略，这可能是因为它采用了固定的报价策略，不能根据自身资源使用情况和任务特征进行动态调整，导致资源分配的效率低下。

Random策略是一种随机的报价策略，它在一个给定的范围内随机地分配订单，不考虑自身资源使用情况和任务特征。Random策略则在所有的指标上都表现最差，这可能是因为它没有考虑自身资源使用情况和任务特征，随机地分配订单，导致资源的浪费和拥堵。

PPO策略是一种基于强化学习的报价策略，它能够根据自身资源使用情况和任务特征进行报价，但是没有考虑时序信息。PPO策略在服务器的收益上稍逊于Fixed策略，但是在车辆的利用率和订单的完成率上则优于Fixed策略，这可能是因为它能够根据自身资源使用情况和任务特征进行报价，但是没有考虑时序信息，导致决策的不稳定。

PPO+LTSM策略是一种基于强化学习和长短期记忆的报价策略，它能够利用长短期记忆（LTSM）来捕捉时序信息，从而做出更合理的决策。PPO+LTSM策略在所有的指标上都表现出色，无论是服务器的收益、车辆的利用率还是订单的完成率，它都优于或者接近其他的策略，这说明PPO+LTSM策略能够更好地平衡系统的各项目标。PPO+LTSM策略能够在任务的选择和分配中，更有效地学习如何最大化长期的报酬，从而提高系统的整体性能。

为了展示不同的调度策略在不同的到达数目下的表现，我们将仿真实验的结果分别以表格和图形的形式呈现。表1、表2和表3分别给出了任务到达数目为10、20和40时，各个指标的平均值和标准差。从表格中，我们可以观察到不同的调度策略在不同的到达数目下的优劣，以及不同的到达数目对各个指标的影响。下面我们将从横向和纵向两个方面对实验结果进行详细的分析和讨论。

从横向来看，PPO+LTSM策略在所有的指标上都表现出色，无论是服务器的收益、车辆的利用率还是订单的完成率，它都优于或者接近其他的策略，这说明PPO+LTSM策略能够更好地平衡系统的各项目标。PPO+LTSM策略之所以能够取得这样的效果，可能是由于它能够利用长短期记忆（LTSM）来捕捉时序信息，从而做出更合理的决策。相比之下，Fixed策略虽然在服务器的收益上也有不错的表现，但是它在车辆的利用率和订单的完成率上却明显落后于PPO+LTSM策略，这可能是因为它采用了固定的报价策略，不能根据自身资源使用情况和任务特征进行动态调整，导致资源分配的效率低下。而Random策略则在所有的指标上都表现最差，这可能是因为它没有考虑自身资源使用情况和任务特征，随机地分配订单，导致资源的浪费和拥堵。PPO策略在服务器的收益上稍逊于Fixed策略，但是在车辆的利用率和订单的完成率上则优于Fixed策略，这可能是因为它能够根据自身资源使用情况和任务特征进行报价，但是没有考虑时序信息，导致决策的不稳定。综合来看，PPO+LTSM策略能够在任务的选择和分配中，更有效地学习如何最大化长期的报酬，从而提高系统的整体性能。

从纵向来看，我们可以观察到任务到达数对各个指标的影响。一般来说，随着任务到达数的增加，服务器的收益和利用率都会上升，这是因为更多的任务意味着更多的交易和资源消耗，从而提高了服务器的资源利用率。但是，任务的完成率却会下降，这是因为更多的任务也会导致更多的拥堵和延迟，从而降低了系统的服务质量和用户满意度。另外，负载均衡指标在不同的任务到达数下变化不明显，这是因为各个策略都能够保持一定程度的负载均衡，避免服务器的过载或者空闲。但是，从表格中可以看出，PPO+LTSM策略的负载均衡指标在所有的任务到达数下都是最低的，这说明它能够实现最有效的负载均衡，使得服务器之间的资源利用率最为均匀，任务卸载最为有效。这也反映了PPO+LTSM策略的优化目标是最大化长期的报酬，而不是最小化短期的负载不均衡。同时，PPO+LTSM策略的车辆效用在所有的任务到达数下都是最高的，这说明它能够为车辆提供最优的服务质量，使得车辆在任务卸载中能够节省支付成本，提高心理价位。这也反映了PPO+LTSM策略的优化目标是最大化车辆的满意度，而不是最大化服务器的收益。例如，在任务到达数为10时，PPO+LTSM策略的服务器收益比Fixed策略的低0.07，但是它的任务完成率比Fixed策略的高0.02，这说明PPO+LTSM策略更倾向于提高任务的完成率，而不是提高服务器的收益。而在任务到达数为40时，PPO+LTSM策略的负载均衡比Random策略的低0.02，但是它的服务器收益比Random策略的高0.14，这说明PPO+LTSM策略更倾向于提高服务器的收益，而不是提高负载的均衡。而Random策略则在所有的指标上都表现不佳，除了在任务到达数为40时，它的服务器收益稍高于其他策略，但是这并不意味着它的资源分配效率高，而是因为它的任务完成率很低，导致服务器的收益被放大了。它的任务完成率都低于其他策略的平均值，而且它的标准差都高于其他策略的平均值，这说明Random策略在不同的负载情况下，都不能有效地分配资源，导致系统的性能不稳定，有时甚至出现极端的情况，例如，有些车辆被分配了很多任务，而有些车辆则没有分配到任何任务，导致任务的完成率的波动很大。

一般来说，随着任务到达数的增加，服务器的收益和利用率都会上升，这是因为更多的任务意味着更多的交易和资源消耗，从而提高了服务器的资源利用率。但是，任务的完成率却会下降，这是因为更多的任务也会导致更多的拥堵和延迟，从而降低了系统的服务质量和用户满意度。

另外，负载均衡指标在不同的任务到达数下变化不明显，这是因为各个策略都能够保持一定程度的负载均衡，避免服务器的过载或者空闲。但是，从表格中可以看出，PPO+LTSM策略的负载均衡指标在所有的任务到达数下都是最低的，这说明它能够实现最有效的负载均衡，使得服务器之间的资源利用率最为均匀，任务卸载最为有效。这也反映了PPO+LTSM策略的优化目标是最大化长期的报酬，而不是最小化短期的负载不均衡。

同时，PPO+LTSM策略的车辆效用在所有的任务到达数下都是最高的，这说明它能够为车辆提供最优的服务质量，使得车辆在任务卸载中能够节省支付成本，提高心理价位。这也反映了PPO+LTSM策略的优化目标是最大化车辆的满意度，而不是最大化服务器的收益。例如，在任务到达数为10时，PPO+LTSM策略的服务器收益比Fixed策略的低0.07，但是它的任务完成率比Fixed策略的高0.02，这说明PPO+LTSM策略更倾向于提高任务的完成率，而不是提高服务器的收益。而在任务到达数为40时，PPO+LTSM策略的负载均衡比Random策略的低0.02，但是它的服务器收益比Random策略的高0.14，这说明PPO+LTSM策略更倾向于提高服务器的收益，而不是提高负载的均衡。

而Random策略则在所有的指标上都表现不佳，除了在任务到达数为40时，它的服务器收益稍高于其他策略，但是这并不意味着它的资源分配效率高，而是因为它的任务完成率很低，导致服务器的收益被放大了。它的任务完成率都低于其他策略的平均值，而且它的标准差都高于其他策略的平均值，这说明Random策略在不同的负载情况下，都不能有效地分配资源，导致系统的性能不稳定，有时甚至出现极端的情况，例如，有些车辆被分配了很多任务，而有些车辆则没有分配到任何任务，导致任务的完成率的波动很大。

综上所述，我们可以得出以下结论：PPO+LTSM策略是一种能够在不同的任务到达数下，平衡系统的各项目标，提高系统的整体性能的报价策略。它能够利用长短期记忆（LTSM）来捕捉时序信息，从而做出更合理的决策。它的优化目标是最大化长期的报酬，而不是最小化短期的负载不均衡或者最大化服务器的收益。它能够实现最有效的负载均衡，使得服务器之间的资源利用率最为均匀，任务卸载最为有效。它能够为车辆提供最优的服务质量，使得车辆在任务卸载中能够节省支付成本，提高心理价位。我们认为，PPO+LTSM策略是一种具有潜力的报价策略，可以应用于其他类似的任务分配系统中，以提高系统的效率和收益。

综上所述，本文提出的基于多智能体强化学习和反向拍卖机制的车联网任务卸载方法（RATO）在各个指标上都优于其他基准方法，证明了其有效性和优越性。RATO方法能够充分利用边缘计算资源，提高任务完成率和收益，降低时延和负载不均衡。
