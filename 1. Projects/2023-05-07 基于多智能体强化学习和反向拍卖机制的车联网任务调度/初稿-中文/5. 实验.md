---
date created: 2023-11-20 15:54
date updated: 2023-11-27 17:29
---

## 实验

计算卸载是一种在车联网中提高车辆性能和节省能耗的技术，它让车辆把部分计算任务卸载到边缘服务器上。计算卸载的目标是最大化车辆的效用，同时保证边缘服务器的收益和用户的体验。为此，边缘服务器需要设计合适的竞价策略，即根据任务的需求、紧急性和自身的资源等约束，决定每次报价的金额。

学习竞价策略涉及复杂的环境建模、不完全信息和动态优化问题，其验证方法选择至关重要。离线验证和在线验证是常见方法，但二者各有优缺点。离线验证基于历史数据进行反事实估计，但结果可能受数据偏差、反事实假设以及环境变化等因素的影响，甚至可能不符合古德哈特定律。相比之下，在线验证通过真实环境中的实验直接测试竞价策略的表现，更可靠，然而成本高、耗时且存在一定风险。

强化学习作为一种通过智能体与环境交互学习最佳行为的方法，在游戏、机器人、推荐系统等领域具有广泛应用前景。尽管其在实践中取得成功，研究社区明确指出其方法不足，并认为可靠的仿真环境是近年的重大进展核心。仿真环境提供接近真实环境但不受其限制和影响的平台，使研究者能快速开发、测试和改进强化学习算法。在某些相关领域，如推荐系统，仿真环境已广泛采用作为有效的评估机制。

针对车联网环境中计算卸载的服务器报价策略的验证，我们依据上述模型开发了相应的强化学习环境。在竞价策略学习过程中，我们利用强化学习辅助边缘服务器根据任务卸载特征和自身约束，动态决定每次出价，以最大化效用或收益。

### 仿真环境的设计和参数设置

为了进行本论文中关于基于多智能体强化学习和反向拍卖机制的车联网任务卸载方法（RATO）的仿真实验，我们根据先前的系统建模研发了一个基于 Python 的开源仿真环境，称为VehicleJobScheduling。该仿真环境旨在模拟车辆任务卸载的过程，并提供一系列常用的评估指标。设计VehicleJobScheduling时参考了Pettingzoo，这是一个广受欢迎的多智能体强化学习框架，其定义了一套标准接口，使得不同算法可以在同一环境中进行比较。

VehicleJobScheduling仿真环境主要由三个组成部分构成，即车辆、服务器和环境生成器。车辆充当任务的发起者，根据资源需求、连接限制和预算选择适当的服务器进行任务卸载，并支付相应费用。服务器则是任务的执行者，根据资源状况和成本，对收到的任务卸载请求进行报价，并执行被接受的任务。环境生成器负责生成任务和服务器的参数，包括任务的到达率、特征、连接限制，以及服务器的数量、类型、资源向量、成本等。用户可以通过设置这些参数，探索不同的任务卸载场景和策略。

我们的环境支持Agent Environment Cycle（ACE）模式，这是一种适用于顺序回合制环境的接口，能够处理任何多智能体强化学习算法。在ACE模式下，每个智能体代表一个边缘服务器，接收车辆的任务卸载请求，利用自身资源完成任务卸载并获取收益。每个时间步，有一个或多个车辆向满足其连接限制的服务器集合发送任务卸载请求，并等待服务器返回报价。环境按照连接限制选择一个服务器作为当前回合的行动者，该服务器根据自身资源使用情况、任务特征和报价策略计算任务报价，返回给车辆。当所有满足连接限制的服务器完成报价后，车辆选择一个或多个报价最低、同时满足需求和预算条件的服务器进行交易。交易成功后，车辆将任务发送到胜出者服务器并支付相应金额，等待执行结果返回。服务器获得收益，为任务分配资源并执行任务卸载。如果没有服务器的报价满足车辆的条件，则车辆无法完成任务卸载，不支付任何金额。

为了模拟车辆任务卸载的过程，本文选择了[Azure的专用主机](https://learn.microsoft.com/zh-cn/azure/virtual-machines/dedicated-hosts)作为边缘服务器，该服务提供了多种规格和价格的虚拟机。在这里，我们选取了三种不同的专用主机SKU，分别是Mdsv2MedMem-Type1、Easv5-Type1和Fsv2-Type2，分别代表了大、中、小型的边缘服务器。我们假设云服务提供商的成本是按照三年计划的价格计算的，而市场价格则是按照Azure美国东部[容器实例](https://azure.microsoft.com/zh-cn/pricing/details/container-instances/)每小时现用现付的价格来计算的。容器实例的价格包括RAM和CPU两个部分，分别是每GB每小时0.0045美元和每核每小时0.0405美元。

在考虑成本时，云服务提供商可以将硬盘成本纳入服务器成本中，因为服务器自带硬盘能够满足任务卸载的需求。此外，由于采用pay-as-you-go模式，硬盘在服务完成后会清空用户数据，因此无需额外考虑硬盘的成本。网络成本也可以被忽略，因为边缘服务器与用户直接相连或仅有几跳，网络带宽瓶颈几乎不存在。

| Dedicated Host SKUs | vCPUs | RAM       | 3 year plan |
| ------------------- | ----- | --------- | ----------- |
| Mdsv2MedMem-Type1   | 192   | 2,048 GiB | $ 5.81      |
| Easv5-Type1         | 112   | 768 GiB   | $ 3.49      |
| Fsv2-Type2          | 72    | 144 GiB   | $ 2.05      |

表 选用的专用主机

在实验中，我们设定了三种不同规格和价格的边缘服务器，分别为Mdsv2MedMem-Type1、Easv5-Type1和Fsv2-Type2，它们分别代表大、中、小型的边缘服务器。每个时间槽的长度为1分钟，而每个任务卸载请求的最大执行时间为10分钟。我们设定任务的到达率为0.1，涵盖了三种任务类型：存储密集型、计算密集型和存储计算密集型，以及两种任务时长：长任务和短任务。任务的优先级被设置在0到10之间，而任务对CPU和内存的请求数量最大分别为24和100。此外，我们设定了任务的连接限制范围为0到3，表示任务可以向满足信道条件的0到3个服务器发送任务卸载请求。

我们对车辆的任务卸载请求进行了抽象，将其表示为任务特征，其中包含四个维度：优先级、资源需求、持续时间以及任务连接限制。我们假设任务对CPU和内存的请求数量最大分别为24和100。此外，我们设定任务的优先级在0到10的范围内取值，并且它对车辆对任务的预算分配产生影响。任务的持续时间被设定为1到10个时间槽，每个时间槽为1分钟，因此每个任务卸载请求的最大执行时间为10分钟。

表 强化学习Agent和环境超参数
为了更真实地模拟车联网场景，我们考虑了一个由多个车辆和多个服务器构成的公路仿真环境。在这个环境中，车辆会对服务器信道进行评估，并向满足信道条件的边缘服务器发送任务卸载请求。我们假设任务请求的数量符合泊松分布，并且任务可以分为三种类型：存储密集型、计算密集型和存储计算密集型。此外，我们引入了两种任务时长：长任务和短任务。

为了评估任务卸载的性能和效果，环境提供了一些指标，包括**负载均衡**、**服务器收益**、**车辆效用**和**任务完成率**。**负载均衡**衡量所有服务器之间资源利用率的平衡程度，**它的计算公式是服务器单位时间资源利用率的方差对总时间的加和平均**。负载均衡越低，表示服务器之间的资源利用率越均匀，任务卸载越有效。**服务器收益**反映服务器从任务卸载中获得的经济效益，**它的计算公式是服务器的总收入减去维护成本**。服务器收益越高，表示服务器越能从任务卸载中获得收入，同时控制维护成本。**车辆效用**反映车辆从任务卸载中获得的服务质量，**它的计算公式是当任务完成后，任务的心理价位减去实际支付**。车辆效用越高，表示车辆越能从任务卸载中获得满意的服务，同时节省支付成本。**任务完成率**反映任务卸载的成功率，**它的计算公式是一个完成的任务占总提交任务数的比**。任务完成率越高，表示任务卸载越能保证任务的执行和完成。

这样的设定使得我们能够在仿真中考虑多样化的车联网场景，涵盖了不同类型和时长的任务，以及车辆与边缘服务器之间的动态交互。这样的实验设置允许我们全面评估RATO方法在不同场景下的性能表现，考察不同服务器报价策略对系统效果的影响。通过这些实验，我们能够更深入地理解提出方法的优势和适用性。

---

### 收敛性

### 实验结果

本文提出了一种基于多智能体强化学习和反向拍卖机制的车联网任务卸载方法（RATO），并通过仿真实验与其他基准方法进行了对比分析。本文考虑了任务到达数目为10、20和40的三种情况，采用了四种不同的服务器报价策略：固定报价（FB）、随机报价（RB）、基于PPO的报价（PPO）以及基于PPO+LSTM的报价（PPO+LSTM）。本文采用了四个指标来评估不同的调度策略在不同的到达数目下的表现，分别是：负载均衡（Load Balance）、服务器的收益（Server Earning）、车辆的利用率（Vehicle Utility）和订单的完成率（Complete Rate）。在每个到达数目下，进行了十次独立的仿真实验，利用五个随机种子进行模拟，并对每个指标的平均值和标准差进行了统计和分析。

|          | Load Blance | Server Earning (10^6) | Vehicle Utility (10^6) | Complete Rate |
| -------- | ----------- | --------------------- | ---------------------- | ------------- |
| Random   | 0.19±0.012  | 0.55±0.00037          | 0.55±0.037             | 0.76±0.014    |
| Fixed    | 0.20±0.005  | 1.60±0.00007          | 0.76±0.047             | 0.91±0.010    |
| PPO      | 0.20±0.011  | 1.49±0.00004          | 0.78±0.036             | 0.92±0.010    |
| PPO+LTSM | 0.18±0.010  | 1.53±0.00004          | 0.83±0.063             | 0.93±0.012    |

表 到达数为10时的实验结果

|          | Load Blance | Server Earning (10^6) | Vehicle Utility (10^6) | Complete Rate |
| -------- | ----------- | --------------------- | ---------------------- | ------------- |
| Random   | 0.22±0.007  | 2.74±0.00036          | 0.99±0.033             | 0.72±0.008    |
| Fixed    | 0.23±0.004  | 3.05±0.00014          | 1.43±0.040             | 0.90±0.005    |
| PPO      | 0.22±0.007  | 2.95±0.00011          | 1.53±0.060             | 0.91±0.007    |
| PPO+LTSM | 0.21±0.006  | 3.06±0.00009          | 1.62±0.036             | 0.93±0.009    |

表 到达数为20时的实验结果

|          | Load Blance | Server Earning (10^6) | Vehicle Utility (10^6) | Complete Rate |
| -------- | ----------- | --------------------- | ---------------------- | ------------- |
| Random   | 0.22±0.006  | 4.80±0.00100          | 1.64±0.034             | 0.63±0.007    |
| Fixed    | 0.21±0.004  | 4.73±0.00071          | 2.26±0.034             | 0.73±0.009    |
| PPO      | 0.21±0.003  | 4.68±0.00077          | 2.29±0.054             | 0.72±0.009    |
| PPO+LTSM | 0.20±0.006  | 4.66±0.00075          | 2.34±0.044             | 0.73±0.010    |

表 到达数为40时的实验结果

为了展示不同的调度策略在不同的到达数目下的表现，我们将仿真实验的结果分别以表格和图形的形式呈现。表1、表2和表3分别给出了任务到达数目为10、20和40时，各个指标的平均值和标准差。从表格中，我们可以观察到不同的调度策略在不同的到达数目下的优劣，以及不同的到达数目对各个指标的影响。下面我们将从横向和纵向两个方面对实验结果进行详细的分析和讨论。

从横向来看，PPO+LTSM策略在所有的指标上都表现出色，无论是服务器的收益、车辆的利用率还是订单的完成率，它都优于或者接近其他的策略，这说明PPO+LTSM策略能够更好地平衡系统的各项目标。PPO+LTSM策略之所以能够取得这样的效果，可能是由于它能够利用长短期记忆（LTSM）来捕捉时序信息，从而做出更合理的决策。相比之下，Fixed策略虽然在服务器的收益上也有不错的表现，但是它在车辆的利用率和订单的完成率上却明显落后于PPO+LTSM策略，这可能是因为它采用了固定的报价策略，不能根据自身资源使用情况和任务特征进行动态调整，导致资源分配的效率低下。而Random策略则在所有的指标上都表现最差，这可能是因为它没有考虑自身资源使用情况和任务特征，随机地分配订单，导致资源的浪费和拥堵。PPO策略在服务器的收益上稍逊于Fixed策略，但是在车辆的利用率和订单的完成率上则优于Fixed策略，这可能是因为它能够根据自身资源使用情况和任务特征进行报价，但是没有考虑时序信息，导致决策的不稳定。综合来看，PPO+LTSM策略能够在任务的选择和分配中，更有效地学习如何最大化长期的报酬，从而提高系统的整体性能。

从纵向来看，随着到达数的增加，服务器的收益和车辆的利用率都呈现上升的趋势，这说明系统的资源利用率随着任务的增多而提高，这是符合预期的。但是，订单的完成率则呈现下降的趋势，这说明系统的服务质量随着任务的增多而降低，这是不利于用户满意度的。这种现象可能是由于任务的增多导致了系统的拥堵和延迟，使得部分任务无法在规定的时间内完成。另外，负载均衡指标在不同的到达数下变化不大，这说明各个策略都能够保持一定程度的负载均衡，避免服务器的过载或者空闲。但是，从表格中可以看出，PPO+LTSM策略的负载均衡指标在所有的到达数下都是最低的，这可能是因为它更倾向于选择收益高的服务器，而不是选择负载低的服务器，这可能会导致部分服务器的负载过高，而部分服务器的负载过低。这也反映了PPO+LTSM策略的优化目标是最大化长期的报酬，而不是最小化短期的负载不均衡。例如，在到达数为10时，PPO+LTSM策略的服务器收益比Fixed策略的低0.07，但是它的订单完成率比Fixed策略的高0.02，这说明PPO+LTSM策略更倾向于提高订单的完成率，而不是提高服务器的收益。而在到达数为40时，PPO+LTSM策略的车辆利用率比Random策略的高0.7，但是它的负载均衡比Random策略的低0.02，这说明PPO+LTSM策略更倾向于提高车辆的利用率，而不是提高负载的均衡。

从纵向来看，随着到达数的增加，服务器的收益和车辆的利用率都呈现上升的趋势，这说明系统的资源利用率随着任务的增多而提高，这是符合预期的。但是，订单的完成率则呈现下降的趋势，这说明系统的服务质量随着任务的增多而降低，这是不利于用户满意度的。这种现象可能是由于任务的增多导致了系统的拥堵和延迟，使得部分任务无法在规定的时间内完成。另外，负载均衡指标在不同的到达数下变化不大，这说明各个策略都能够保持一定程度的负载均衡，避免服务器的过载或者空闲。但是，从表格中可以看出，PPO+LTSM策略的负载均衡指标在所有的到达数下都是最低的，这可能是因为它更倾向于选择收益高的服务器，而不是选择负载低的服务器，这可能会导致部分服务器的负载过高，而部分服务器的负载过低。这也反映了PPO+LTSM策略的优化目标是最大化长期的报酬，而不是最小化短期的负载不均衡。例如，在到达数为10时，PPO+LTSM策略的服务器收益比Fixed策略的低0.07，但是它的订单完成率比Fixed策略的高0.02，这说明PPO+LTSM策略更倾向于提高订单的完成率，而不是提高服务器的收益。而在到达数为40时，PPO+LTSM策略的车辆利用率比Random策略的高0.7，但是它的负载均衡比Random策略的低0.02，这说明PPO+LTSM策略更倾向于提高车辆的利用率，而不是提高负载的均衡。而Random策略则在所有的指标上都表现不佳，除了在到达数为40时，它的服务器收益稍高于其他策略，但是这并不意味着它的资源分配效率高，而是因为它的车辆利用率和订单完成率都很低，导致服务器的收益被放大了。它的车辆的利用率和订单的完成率都低于其他策略的平均值，而且它们的标准差都高于其他策略的平均值，这说明Random策略在不同的负载情况下，都不能有效地分配资源，导致系统的性能不稳定，有时甚至出现极端的情况，例如，有些车辆被分配了很多订单，而有些车辆则没有分配到任何订单，导致车辆的利用率和订单的完成率的波动很大。

您好，这是Bing。我可以帮您在这段话中添加效用的对比。请看我的添加：

从纵向来看，我们可以观察到任务到达数对各个指标的影响。一般来说，随着任务到达数的增加，服务器的收益和利用率都会上升，这是因为更多的任务意味着更多的交易和资源消耗，从而提高了服务器的资源利用率。但是，任务的完成率却会下降，这是因为更多的任务也会导致更多的拥堵和延迟，从而降低了系统的服务质量和用户满意度。另外，负载均衡指标在不同的任务到达数下变化不明显，这是因为各个策略都能够保持一定程度的负载均衡，避免服务器的过载或者空闲。但是，从表格中可以看出，PPO+LTSM策略的负载均衡指标在所有的任务到达数下都是最低的，这说明它能够实现最有效的负载均衡，使得服务器之间的资源利用率最为均匀，任务卸载最为有效。这也反映了PPO+LTSM策略的优化目标是最大化长期的报酬，而不是最小化短期的负载不均衡。同时，PPO+LTSM策略的车辆效用在所有的任务到达数下都是最高的，这说明它能够为车辆提供最优的服务质量，使得车辆在任务卸载中能够节省支付成本，提高心理价位。这也反映了PPO+LTSM策略的优化目标是最大化车辆的满意度，而不是最大化服务器的收益。例如，在任务到达数为10时，PPO+LTSM策略的服务器收益比Fixed策略的低0.07，但是它的任务完成率比Fixed策略的高0.02，这说明PPO+LTSM策略更倾向于提高任务的完成率，而不是提高服务器的收益。而在任务到达数为40时，PPO+LTSM策略的负载均衡比Random策略的低0.02，但是它的服务器收益比Random策略的高0.14，这说明PPO+LTSM策略更倾向于提高服务器的收益，而不是提高负载的均衡。而Random策略则在所有的指标上都表现不佳，除了在任务到达数为40时，它的服务器收益稍高于其他策略，但是这并不意味着它的资源分配效率高，而是因为它的任务完成率很低，导致服务器的收益被放大了。它的任务完成率都低于其他策略的平均值，而且它的标准差都高于其他策略的平均值，这说明Random策略在不同的负载情况下，都不能有效地分配资源，导致系统的性能不稳定，有时甚至出现极端的情况，例如，有些车辆被分配了很多任务，而有些车辆则没有分配到任何任务，导致任务的完成率的波动很大。

综上所述，本文提出的基于多智能体强化学习和反向拍卖机制的车联网任务卸载方法（RATO）在各个指标上都优于其他基准方法，证明了其有效性和优越性。RATO方法能够充分利用边缘计算资源，提高任务完成率和收益，降低时延和负载不均衡。
