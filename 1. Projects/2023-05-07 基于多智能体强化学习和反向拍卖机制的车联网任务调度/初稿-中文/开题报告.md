一、 **研究背景、目的及意义**

随着物联网和智能交通系统的发展，车联网（IoV）作为一种新兴的技术，为车辆提供了各种感知、通信和计算功能，通过车辆之间,车辆与路测设施之间以及车辆与云端之间的信息交换和协同服务,以实现更安全、高效和智能的驾驶[1] 。然而，车辆本身的计算能力和电池容量有限，无法满足一些复杂或者对延迟敏感的任务需求,例如自动驾驶,视频分析等。移动边缘计算（MEC）作为一种新型的计算范式，通过在道路边缘部署多个MEC服务器，为车辆提供近距离的低延迟高带宽计算资源和服务[2] 。通过移动边缘计算技术，车辆可以将部分或全部任务卸载到MEC服务器上执行，从而节省自身的能耗和时间开销[3] 。

然而，在MEC环境中进行有效的任务卸载和调度面临着诸多挑战。首先，由于车辆具有高速移动性和动态变化性，如何预测车辆与MEC服务器之间的通信质量并根据实时状态进行任务卸载决策是一个难点。其次，由于不同车辆可能有不同的任务类型、优先级、截止时间等约束条件，如何在保证服务质量（QoS）的同时平衡各个MEC服务器之间的负载并最大化系统效用是一个关键问题。第三，在多用户多任务多服务器场景下，如何设计一种合理且公平的激励机制来鼓励用户之间相互合作并避免自私或恶意行为也是一个重要问题。

针对上述问题，在本文中我们提出了一种基于反向拍卖机制（RAM）和多智能体深度强化学习（MADRL）相结合的车联网工作流调度方法（RAM-DRL）。由于MEC服务器分布比较广泛但是信号连接距离有限，用户只能向相关可以连接到的服务器发起任务请求。本次论文考虑了每个用户每一次提交给系统的一个相关任务，这个任务只能由一个MEC服务器接收运行。当MEC服务器接收到用户的任务申请后，利用深度强化学习来训练每个MEC服务器上部署的策略网络，使其能够根据用户的任务需求和自身状态计算接受任务所产生的长期收益，并根据收益最大化原则向用户进行报价。本次论文利用反向拍卖机制来确定每个用户应该支付给MEC服务器的价格，并根据价格最低化原则来选择最优MEC服务器。本次论文将设计一个三层神经网络结构来近似每个用户 - 状态 - 行动对应的价值函数，并采用异步优势行动者-评论者算法（A3C）算法来更新网络参数并避免过度估计偏差，提高算法的稳定性和准确性。此外，本次论文还引入了一种基于中心训练分散部署（CTDE）的方法，来解决多智能体深度强化学习中的非平稳性和通信开销问题。本次论文在一个中心服务器上训练一个全局策略网络，并将其分发给各个MEC服务器，让MEC服务器在本地执行该策略并收集经验数据，然后将数据汇报给中心服务器进行网络更新。

本次选题的意义在于，它为车联网中的任务卸载和调度提供了一种新的思路和技术方案，可以有效地降低车辆的能耗、提高任务完成率和保证用户满意度。本文的方法可以适应车辆的高速移动性和动态变化性，以及不同车辆的任务类型、优先级、截止时间等约束条件。本文的方法还可以通过反向拍卖机制来激励用户之间相互合作并避免自私或恶意行为。本文的方法对于提升车联网的服务质量和系统效用，以及促进移动边缘计算技术在车联网中的广泛应用，具有重要的现实意义。

为了更清晰地说明本次论文提出的方法，本文的组织结构如下：

• 第一节介绍了背景和目标，以及本文主要贡献；

• 第二节介绍了国内外相关工作，包括移动边缘计算、车联网、反向拍卖机制和深度强化学习等方面；

• 第三节介绍了本次研究的主要内容；

• 第四节介绍了本次研究的方案规划,基于反向拍卖机制和深度强化学习相结合的车联网工作流调度方法（RAM-DRL）；

• 第五节介绍了本次研究可能的创新点和预期成果；

• 第六节介绍了前期已经完成的研究工作和可能存在的困难。

• 第七节介绍了论文工作进度及经费预算

二、 **国内外相关研究现状综述**

本文主要研究的问题是车联网（IoV）中的工作流调度问题，即根据车辆的计算任务需求和移动特性，在移动边缘计算（MEC）环境中有效地分配和调度车辆的计算任务，以达到优化系统性能和用户体验的目的。车联网工作流调度涉及到多个方面的问题，如任务卸载策略、资源分配算法、激励机制设计、时延分析等。近年来，随着车联网技术和移动边缘计算技术的发展，车联网工作流调度问题受到了国内外学者的广泛关注，并取得了一些研究成果。为了对本课题有一个全面的了解，我们首先回顾了国内外相关领域的研究现状，主要包括以下几个方面：传统资源调度和随机优化的分布式资源调度方法,强化学习的车联网资源调度,强化学习移动边缘计算资源调度,强化学习服务器资源调度,车联网中的拍卖机制,强化学习定价策略。

**(****1).传统资源调度和随机优化的分布式资源调度方法**

资源调度是指在有限的资源条件下，根据任务的需求和优先级，合理地分配和利用资源，以达到最优或次优的目标。资源调度问题在各种计算系统中都广泛存在，例如云计算、边缘计算、车联网等。传统的资源调度方法通常基于确定性或随机性的模型，通过数学规划、启发式算法、元启发式算法等技术来求解。传统资源调度方法通常基于集中式的优化模型，需要预先知道系统的参数和状态信息，然而在边缘云环境中，这些信息往往是不完全或不准确的，导致资源调度效率低下。为了解决这一问题，近年来出现了一些基于随机优化的分布式资源调度方法，它们可以根据实时的反馈信息动态地调整资源分配策略。目前已有一些针对边缘云计算场景的服务迁移和负载调度方法被提出，例如：Wang等人（2017）提出了一个联合计算卸载、资源分配和内容缓存的优化框架，并设计了一个基于ADMM的分布式算法来提高具有移动边缘计算的无线蜂窝网络的收益[4]。Rahul等人（2015）通过解耦原始MDP并采用李雅普诺夫优化技术，提出了一种高效、鲁棒、自适应且无需统计知识的边缘云服务迁移和负载调度算法[5]。Ibrahim等人（2019）提出了一种考虑车辆移动性和任务延迟要求的边缘计算工作负载调度方法，并通过拉格朗日松弛技术求解[6]。Zhang 等人（2022）考虑了多跳传输导向的车联网云动态工作流调度问题，提出了一种基于人工蜂群算法和贪心策略相结合的动态工作流调度方法[7]。这些方法在一些特定场景下表现出了较好的效果，但也面临着一些挑战，例如如何保证算法的收敛性和稳定性，如何平衡探索和利用之间的权衡，如何减少通信开销和计算复杂度等。

在分布式资源调度问题中，基于随机优化的方法是一种常见的解决方案。这类方法利用随机性来处理不确定性，通过迭代更新可行解来逼近最优解。然而，基于随机优化的方法也存在一些局限性，例如收敛速度慢、计算开销大、对参数敏感等。因此，近年来出现了一种新的方法，即深度强化学习。深度强化学习是一种结合了深度神经网络和强化学习的技术，可以训练神经网络来快速准确地解决多目标优化问题[8] 。相比于基于随机优化的方法，深度强化学习具有以下优势：一是能够自动学习复杂的策略函数，无需人为设计或调整参数；二是能够利用大量数据和高效算法来提高学习效率和质量；三是能够适应动态变化的环境和目标，并实现在线决策和反馈。

**(****2).****强化学习的车联网资源调度**

强化学习是一种基于试错学习和奖励反馈的机器学习方法，它可以让智能体在与环境交互的过程中自主地学习最优或近似最优的策略。强化学习具有以下几个特点：无需事先知道系统模型和参数；能够处理部分可观测和非马尔可夫性的环境；能够适应动态变化和不确定性的环境；能够实现长期目标和多目标之间的平衡。由于这些特点，强化学习被认为是一种适合于解决车联网资源调度问题的方法。强化学习的车联网资源调度是一种利用智能体的自主学习能力，根据车辆的状态、环境的变化和奖励信号，动态优化车辆之间和基础设施之间的通信资源分配的方法。强化学习的车联网资源调度可以提高车联网的性能，如降低时延、增加吞吐量、节省能耗等。强化学习的车联网资源调度面临着多个挑战，如高维状态空间、部分可观测性、非平稳性、多目标优化等。为了解决这些挑战，一些研究者提出了基于深度神经网络、多智能体协作、知识驱动等技术的强化学习算法，并在不同的场景中进行了验证和应用，如频谱共享、信道选择、功率控制、数据传输等。例如,Liu 等人（2019）设计了两种强化学习方法来优化计算卸载和资源分配问题[9]。Liu等人（2020）提出了一种基于优先级和价值函数的任务调度算法，考虑了任务之间的依赖性[10]。Song等人（2023）提出了一种基于潜在博弈理论和联邦深度强化学习的多异构服务器边缘计算卸载方法，能够有效地满足任务车辆用户的定制化服务需求[11]。Pang等人（2020）提出了一种考虑位置隐私保护的车联网计算资源协同调度策略，利用卡尔曼滤波预测车辆间距离，采用双重深度 Q 网络优化总成本[1]。然而，强化学习的车联网资源调度问题也存在一些挑战和难点，例如如何设计合适的状态空间、动作空间和奖励函数，如何处理高维度和连续性的状态和动作，如何实现多智能体之间的协调和博弈，如何提高学习效率和泛化能力等。

**(****3).** **强化学习移动边缘计算资源调度**

移动边缘计算是一种将云计算的功能和服务延伸到网络边缘的技术，它可以为移动用户提供低延迟、高带宽、高可靠性的计算资源和服务。移动边缘计算中的资源调度问题是指如何在有限的计算资源和网络资源下，根据用户的任务需求和服务质量要求，合理地分配和调度任务在本地执行或卸载到边缘服务器或云服务器上执行。这是一个具有多约束、多目标、动态性和不确定性的优化问题，传统的优化方法难以有效地解决。因此，一些学者尝试使用强化学习来解决移动边缘计算中的资源调度问题，并取得了一定的成果。强化学习是一种基于智能体与环境交互学习最优行为策略的机器学习方法，可以适应不确定性和动态性，并且不需要先验知识或模型。近年来出现了许多基于强化学习的MEC资源调度方法，接下来对其中部分进行介绍。

例如，Zheng 等人（2022）使用深度 Q 网络（DQN）或双重深度 Q 网络（DDQN）来训练移动边缘服务器上的智能体，使其能够根据自身状态和环境状态选择最优或次优的任务卸载策略[12]；Chen J等人（2021）使用深度确定性策略梯度算法（DDPG）来训练连续动作空间下的智能体，使其能够输出最优或次优的任务卸载比例或执行速度[13]；Chen M等利用异步优势行动者-评论者算法(A3C），提出了一个基于软件定义网络和信息中心网络的动态资源分配方案，以提高车辆网络的服务质量[14];Peng等使用多智能体强化学习（MARL）来训练多个边缘服务器之间的智能体，使其能够协作地进行车辆关联和资源分配决策[15]；Peng等人（2022）提出了一种基于深度强化学习和有向无环图的依赖任务卸载策略，能够在多用户多服务器边缘计算环境中，灵活地选择合适的卸载目标服务器，并有效地减少服务延迟和终端能耗[16]。然而，强化学习在移动边缘计算中的资源调度问题仍然面临着一些问题和挑战，例如如何处理状态空间和动作空间的爆炸性增长，如何平衡探索和利用之间的权衡，如何解决多智能体之间的非平稳性和通信开销等。

**(****4).** **强化学习服务器资源调度**

服务器资源调度是指在数据中心或云平台中，根据用户或应用程序的需求和服务质量要求，合理地分配和调度服务器上的计算资源、存储资源、网络资源等。这是一个涉及到数据中心或云平台的性能、效率、成本、节能等方面的重要问题。传统的服务器资源调度方法通常基于静态规划或启发式算法等技术来求解，但这些方法往往依赖于精确的系统模型和参数，难以适应复杂多变的环境。因此，一些学者利用强化学习来解决服务器资源调度问题，并取得了一些进展。强化学习服务器资源调度（Reinforcement Learning Server Resource Scheduling）是一种利用强化学习方法来优化服务器资源分配和任务执行的技术。它可以应对服务器状态的动态变化、多用户多任务的复杂需求、网络切片和边缘计算等新型网络架构的挑战，以及可再生能源和机器学习服务等新兴应用场景的特点。强化学习服务器资源调度的目标是在保证服务质量和满足约束条件的前提下，最大化系统效率和性能，最小化系统成本和延迟。强化学习服务器资源调度涉及多个层次和方面，包括资源配置、任务卸载、任务调度、并行度配置、协商策略等。近年来，有许多研究者提出了基于深度强化学习（Deep Reinforcement Learning）的服务器资源调度方法，并在不同的场景和数据集上进行了实验验证。

例如，有些学者使用强化学习来优化虚拟机（VM）或容器（Container）在物理机上的放置策略，以提高资源利用率和节约能耗[17–19]，其中Cheng 等人（2018）提出了一个基于深度强化学习的资源配置和任务调度系统，能够有效地降低云服务提供商的数据中心能耗和电费成本，并具有高效、可扩展、自适应和快速收敛等特点[17]，Zhao 等（2021）展示了如何利用深度强化学习在混合云环境中实现自适应的多目标任务调度，以最大化可再生能源的利用率和满足截止日期约束[19]；有些学者使用强化学习来优化服务器上的任务调度策略，以提高任务的执行效率和服务质量[20–25]，例如，Wu 等人（2020）针对服务器状态动态变化导致的资源分配不均衡问题，提出了一种基于深度强化学习和马尔可夫决策过程相结合的任务调度方法[23]，Rjoub等人（2021）提出了四种基于深度学习和强化学习的调度方法，并用真实数据集进行了实验比较。实验结果表明，深度强化学习结合长短期记忆网络（DRL-LSTM）的方法在减少任务执行成本和延迟方面优于其他三种方法[25]。然而，强化学习在服务器资源调度问题中也存在一些问题和挑战，例如如何处理大规模的状态空间和动作空间，如何处理部分可观测和非马尔可夫性的环境，如何处理多目标和多约束的优化问题，如何提高算法的收敛速度和稳定性等。

**(****5).****车联网中的拍卖机制**

拍卖机制是一种经济学中常用的激励机制，它可以实现资源或服务的有效分配和价格的公平确定。在车联网中，拍卖机制可以应用于多种场景，例如车辆之间或车辆与基础设施之间的资源或服务交易，例如频谱、缓存、计算、数据等。近年来，许多学者研究了车联网中的拍卖机制，并设计了各种拍卖模型和算法。例如，有些学者使用密封双向拍卖（SDBA）来实现车辆之间的频谱共享和交易；有些学者使用组合双向拍卖（CDBA）来实现车辆之间的缓存共享和交易；有些学者使用反向拍卖（RAM）来实现车辆与边缘服务器之间的计算任务卸载和交易；有些学者使用多属性拍卖（MAA）来实现车辆与数据提供者之间的数据获取和交易。然而，车联网中的拍卖机制也面临着一些问题和挑战，例如如何保证拍卖机制的真实性、有效性、个体合理性、社会福利最大化等性质，如何处理参与者的自私或恶意行为，如何减少拍卖过程中的计算复杂度和通信开销等。

车联网是指通过无线通信技术，实现车辆、道路、交通设施等的信息交互和资源共享的网络。车联网可以提高道路安全，优化交通管理，提升驾驶体验，促进智能出行等。在车联网中，拍卖机制是一种有效的资源分配和价格发现的方法，可以解决供需不平衡、竞争不公平、信息不对称等问题。拍卖机制可以应用于车联网中的多种场景，例如路侧单元（RSU）的接入控制、频谱资源的分配、数据服务的交易等。接下来将对部分研究进行介绍。Vishalatchi等人（2017）介绍了云计算中的虚拟机调度问题，以及一种基于拍卖机制的禁忌搜索算法来解决它。这可以作为一个基础和背景，让读者了解云计算中的资源分配问题和拍卖机制的作用。Ding等人（2016）从云计算转向网格计算，介绍了网格计算无线网络中的动态资源分配问题，以及一种具有预测能力和多属性特征的新颖反向在线拍卖算法来解决它。这可以展示拍卖机制在另一种计算网络中的适用性和创新性。Liwang等人（2019）从网格计算转向车联网，介绍了车联网中存在的计算卸载、资源共享和用户自私等问题，以及一种基于VCG原理的反向拍卖机制来优化计算卸载决策并满足经济属性。这可以展示拍卖机制在更具挑战性和前沿性的领域中的应用和效果。Zhang等人（2022）进一步考虑了公共区块链网络对车联网资源分配问题的影响和支持，提出了一种利用反向拍卖模型和VCG机制激励车辆记录驾驶数据，并使用边缘计算节点支持区块链技术的真实拍卖机制。这可以展示拍卖机制在结合其他先进技术时能够产生更高效、安全、可信等优点。

**(****6).****强化学习定价策略**

定价策略是指在市场交易中，根据供需关系、成本收益分析、竞争对手行为等因素，确定商品或服务的价格水平和变化规律。定价策略是一种重要的市场营销手段，它可以影响消费者的购买意愿和行为，从而影响供应商的收入和利润。在车联网中，定价策略可以应用于多种场景，例如车辆之间或车辆与基础设施之间的资源或服务交易，例如频谱、缓存、计算、数据等。近年来，许多学者研究了车联网中的定价策略，并设计了各种定价模型和算法。例如，有些学者使用基于需求函数的定价策略来实现车辆之间的频谱共享和交易；有些学者使用基于成本函数的定价策略来实现车辆之间的缓存共享和交易；有些学者使用基于效用函数的定价策略来实现车辆与边缘服务器之间的计算任务卸载和交易；有些学者使用基于博弈论的定价策略来实现车辆与数据提供者之间的数据获取和交易。然而，车联网中的定价策略也存在一些问题和挑战，例如如何根据市场环境和用户行为动态地调整价格，如何平衡供应商和消费者之间的利益，如何处理多方参与者之间的竞争和合作等。

强化学习定价策略是一种利用强化学习技术来优化定价决策的方法，它可以在不依赖用户响应函数的情况下，通过不断地探索和学习找到最优的定价策略，从而实现需求响应、能耗调度、竞争优势、成本效率等目标。强化学习定价策略在金融量化、电子市场、云计算等领域有着广泛的应用和研究。接下来将对使用强化学习进行定价的部分研究进行介绍。

Kutschinski等人（2003）研究了电子市场中多智能体强化学习定价策略，并提出了一个分布式代理平台来模拟和评估不同竞争环境下的定价效果。随后，Kim等人（2016）将强化学习技术应用到一个不确定微网环境中，解决了动态定价和能耗调度问题。Ghasemkhani等人（2018）提出了一种不依赖用户响应函数的定价算法，通过强化学习找到最优定价策略，实现需求响应目标。Krasheninnikova 等人（2019）将强化学习算法扩展到保险领域，使用马尔可夫决策过程来解决保险续费价格调整的多目标优化问题。Islam等人（2022）提出了一种基于深度强化学习的Spark作业调度算法，考虑了多个SLA目标，利用了云VM定价模型，动态调整了资源配置，用于解决云计算中的虚拟机（VM）调度问题。

综上所述，国内外相关领域的研究现状表明，车联网中的资源调度问题是一个具有重要意义和挑战性的课题，它涉及到多种技术和方法，例如传统优化、随机优化、强化学习、拍卖机制、定价策略等。这些技术和方法在一定程度上可以解决资源调度问题，但也存在一些不足或缺陷，需要进一步的研究和改进。因此，在本文中，我们将结合反向拍卖机制和多智能体深度强化学习，提出一种新颖的车联网工作流调度方法（RAM-DRL），并对其进行理论分析和仿真验证。下面将介绍本文研究的主要内容。

三、 **研究的主要内容**

本文的主要研究内容是提出一种基于反向拍卖机制（RAM）和多智能体深度强化学习（MADRL）相结合的车联网工作流调度方法（RAM-DRL），该方法可以有效地解决多用户多任务多服务器场景下的任务卸载和调度问题。具体来说，本文的研究内容包括以下几个方面：IoV 和 MEC中的工作流调度,反向拍卖机制,深度强化学习,中心训练分散部署（CTDE）。 首先，我们介绍了车联网（IoV）和移动边缘计算（MEC）的基本概念、特点和挑战，并分析了在 MEC 环境中进行有效的任务卸载和调度所面临的问题。其次，我们介绍了反向拍卖机制（RAM）的基本原理和优势，并将其应用于本文所考虑的车联网工作流调度问题中。再次，我们介绍了深度强化学习（DRL）的基本概念、框架和算法，并将其应用于本文所考虑的车联网工作流调度问题中。最后，我们介绍了一种基于中心训练分散部署（CTDE）的方法，来解决多智能体深度强化学习中的非平稳性和通信开销问题。

**IoV和MEC中的工作流调度**

车联网（IoV）是一种新兴的技术，它为车辆提供了感知、通信和计算功能，通过车辆之间、车辆与路测设施之间以及车辆与云端之间的信息交换和协同服务，实现更安全、高效和智能的驾驶。然而，车辆本身的计算能力和电池容量有限，无法满足一些复杂或者对延迟敏感的任务需求，例如自动驾驶、视频分析等。移动边缘计算（MEC）是一种新型的计算范式，它通过在道路边缘部署多个MEC服务器，为车辆提供近距离的低延迟高带宽计算资源和服务。IoV和MEC是两种相辅相成的技术。MEC可以让车辆把任务卸载到边缘服务器上，节约自己的能量和时间。这样，IoV产生的大量数据和多样化的服务需求就能得到更好的满足。MEC不仅为IoV提供了低时延、高带宽、高可靠性等网络质量，还利用边缘节点的存储和计算资源进行数据处理、分析和优化，实现数据就近处理、就近消费。

在智能交通系统等领域中，IoV和MEC有着广泛而深刻的应用场景和价值。MEC节点可以为车辆提供多种服务，包括感知、计算和通信等。例如，MEC节点可以根据车辆和道路的实时状态，进行车路协同调度，以提高交通效率和安全性；MEC节点还可以管理停车场的空位，并为车辆提供停车导航和支付服务，以节省时间和费用；此外，MEC节点也可以提供丰富的车载娱乐内容，并根据用户喜好进行个性化推荐，以增加乐趣和舒适度。

工作流调度（Workflow Scheduling）是指在一定的约束条件下，根据某种优化目标，为一组相关的任务分配合适的执行资源的过程。工作流调度的目标是在满足用户需求和资源限制的前提下，优化工作流调度的执行效果，如执行时间、执行成本、能耗、可靠性等。工作流调度需要考虑多种约束条件，如任务之间的依赖关系、资源之间的异构性、资源的可用性和动态性等。工作流调度的评价指标是根据不同的优化目标而定，常见的评价指标有完成所有任务所需的最短时间（Make Span）、完成所有任务所需的总费用（Cost）、完成所有任务所需的总能耗（Energy）、完成所有任务成功率（Reliability）等 。

在IoV（Internet of Vehicles）和MEC（Mobile Edge Computing）环境下，工作流调度面临着更多的特殊性和难点。首先，由于车辆具有高速移动性和动态变化性，如何预测车辆与 MEC 服务器之间的通信质量并根据实时状态进行任务卸载决策是一个难点。其次，由于不同车辆可能有不同的任务类型、优先级、截止时间等约束条件，如何在保证服务质量（QoS）的同时平衡各个 MEC 服务器之间的负载并最大化系统效用是一个关键问题。第三，在多用户多任务多服务器场景下，如何设计一种合理且公平的激励机制来鼓励用户之间相互合作并避免自私或恶意行为也是一个重要问题。本文的目标是提出一种基于反向拍卖机制和深度强化学习相结合的车联网工作流调度方法（RAM-DRL），该方法可以有效地解决上述问题，并在保证收益最大化的前提下，尽可能保证系统可靠性。

**反向拍卖**

普通的拍卖方式是由卖方发起，买方出价最高的获得商品或服务。与正向拍卖相反的是反向拍卖（Reverse Auction Mechanism），它是一种拍卖机制，其中卖方是多个，而买方是一个。在反向拍卖中，买方会提出一个需求，并邀请卖方报出自己的价格。然后，买方会根据自己的目标和预算，从报价中选择一个或多个合适的卖方，并支付给他们相应的价格。反向拍卖可以分为不同的类型，根据竞标者数量、竞标规则、竞标信息等因素进行分类。常见的反向拍卖类型有：排名反向拍卖，日本反向拍卖，荷兰反向拍卖，和开放式反向拍卖。在排名反向拍卖中，每个卖方只能看到自己的排名和最低报价，而不能看到其他卖方的报价。在日本反向拍卖中，买方会提出一个初始报价，并逐渐降低，直到只剩下一个或多个愿意接受的卖方。在荷兰反向拍卖中，买方会提出一个需求清单和一个预算价格，并邀请多个卖方参与竞标，最后可能选择一个或多个中标者。在开放式反向拍卖中，每个卖方都能看到所有的报价和排名，并根据市场情况调整自己的报价。反向拍卖的性质是基于市场竞争和价格信号来确定商品或服务的价值，而不是由政府或其他机构来设定。反向拍卖的优势是可以降低采购成本、提高采购效率、增加透明度和公平性、促进创新和质量提升等。

反向拍卖在激励车辆用户共享资源或执行任务方面也有重要的作用和效果。例如，当一个平台需要为其用户提供某种服务时，可以通过反向拍卖来寻找愿意以最低价格提供该服务的车辆用户，并与之签订标准合同。这样可以使平台节省成本，同时也可以激励车辆用户利用闲置资源或空闲时间来参与竞标，从而获得额外收入。另一方面，反向拍卖也可以用于分配任务给车辆用户，例如交通管理、环境监测、数据收集等。平台可以根据任务需求和预算，在反向拍卖中发布任务信息，并选择出价最低且符合条件的车辆用户来执行任务。这样可以使平台有效地完成任务目标，同时也可以激励车辆用户参与社会公益活动，并获得相应奖励。在车联网和移动边缘计算环境下，由于车辆具有高速移动性和动态变化性，如何设计一种合理且公平的反向拍卖机制来鼓励用户之间相互合作并避免自私或恶意行为也是一个重要问题。本次论文将利用深度强化学习来训练每个 MEC 服务器上部署的策略网络，使其能够根据用户的任务需求和自身状态计算接受任务所产生的长期收益，并根据收益最大化原则向用户进行报价。

**深度强化学习（DRL）**

深度强化学习（DRL）是一种基于神经网络的强化学习方法，它能够在复杂动态优化问题中表现出优异的性能。DRL的基本概念是通过让智能体与环境进行交互，从而学习到一个最优策略，该策略可以使智能体获得最大的长期收益。当存在多个智能体共享一个环境时，就形成了多智能体强化学习（MARL）的问题。MARL的目标是让每个智能体根据自己的奖励和其他智能体的行为来调整自己的策略，从而实现合作或竞争的目标。MARL的方法可以分为基于值函数的方法，基于策略的方法，以及基于演员-评论家的方法，它们分别利用不同的方式来评估或生成策略。DRL和MARL的模型有多种形式，例如深度 Q 网络（DQN），深度确定性策略梯度（DDPG），异步优势演员-评论家（A3C）等，它们分别适用于不同的问题场景和特点。DRL和MARL的算法主要使用神经网络来近似值函数或策略函数，并采用梯度下降或其他优化技术来更新网络参数。

本次论文利用深度强化学习（DRL）来训练每个MEC服务器上部署的策略网络，使其能够根据用户的任务需求和自身状态计算接受任务所产生的长期收益，并根据收益最大化原则向用户进行报价。相比于其他机器学习方法，DRL可以直接从环境中获取反馈信号，并通过不断地探索和利用来优化自身的行为。DRL不需要预先定义特征或标签，也不需要大量的先验知识或假设，因此更适合处理车联网这样的复杂、动态和不确定的环境。本次论文设计了一个三层神经网络结构来近似每个用户 - 状态 - 行动对应的价值函数，并采用异步优势行动者-评论者算法（A3C）算法来更新网络参数并避免过度估计偏差，提高算法的稳定性和准确性。

**中心训练分散部署（CTDE）**

中心训练分散部署（CTDE）是一种深度学习模型部署的方法，它将模型的训练和部署分开进行，以提高效率和灵活性。为了解决多智能体环境中存在着非平稳性和通信开销等问题，本次论文引入了一种基于中心训练分散部署（CTDE）的方法，来解决多智能体深度强化学习中的非平稳性和通信开销问题。这种方法与传统的集中式训练和去中心化训练相比，既能够利用全局信息来提高学习效率，又能够保证每个智能体在执行时只依赖于自身的局部观测，从而适应动态变化的环境。类似于MADDPG、QMIX、VDN等多智能体强化学习算法，本次论文采用了CTDE框架来解决多用户多任务多服务器场景下的任务卸载和调度问题。具体地，我们在一个中心服务器上训练一个全局策略网络，并将其分发给各个MEC服务器，让MEC服务器在本地执行该策略并收集经验数据，然后将数据汇报给中心服务器进行网络更新。这样做的好处是：一方面，中心服务器可以利用所有MEC服务器的联合观测和联合动作来学习一个更优的全局策略；另一方面，每个MEC服务器只需要与中心服务器进行周期性的通信，并不需要与其他MEC服务器进行实时通信，从而降低了通信开销和延迟。

车联网工作流调度问题涉及到资源调度、拍卖机制、使用强化学习进行资源调度和使用强化学习进行定价等方面。在这些方面，我将从现有文献中选取有一些代表性的文章，分别对它们进行说明。

关于传统的资源调度，Wang等提出了一个优化移动边缘计算的无线蜂窝网络的框架，综合考虑了计算卸载、资源分配和内容缓存，并用ADMM算法求解。该框架能够平衡网络收益和用户体验，同时考虑了网络拓扑、用户需求和内容流行度等因素。ADMM算法能够将原始问题分解为多个子问题，并通过迭代更新来求解。该方法能够提高分布式计算效率，但也需要选择合适的惩罚参数，并且可能收敛速度较慢。[4]

关于使用强化学习进行资源调度，有两篇文章值得关注。一篇是Liu等提出的一种利用车辆作为移动边缘服务器的网络架构，并用两种强化学习方法来优化长期效用。该架构能够利用车辆的闲置资源来执行用户设备的计算任务，从而降低任务延迟和节省能耗。两种强化学习方法分别是Q-learning方法和深度强化学习方法，它们都能够根据环境状态和奖励函数来学习最优的卸载和资源分配策略。深度强化学习方法相比于Q-learning方法有更快的收敛速度和更高的稳定性[9]。另一篇是Rjoub等提出了四种基于深度学习和强化学习的任务调度方法，其中DRL-LSTM方法是一种结合深度Q网络和RNN-LSTM的算法，能处理时间相关和动态变化的问题。用真实数据集评估后发现DRL-LSTM方法在资源消耗和任务等待时间上都最优，比其他三种方法和传统算法都好[25]。

关于在车联网中应用拍卖机制，Liwang等提出了一种基于反向拍卖和单边匹配的计算卸载机制，可以解决云端车辆网络中的资源共享和用户自私问题。该机制建立了一个计算卸载市场模型，拍卖者有一个多子任务的计算任务，投标者给出价格执行子任务，拍卖者根据目标函数选出中标者并分配子任务，按VCG原理支付金额。该机制具有真实性、个体理性和匹配稳定性等属性，并且在不同场景下效率高[26]。

关于使用强化学习解决定价策略问题，Krasheninnikova等用Q-learning算法求解了保险续费价格调整问题，把它建模为MDP或CMDP。该问题是一个序贯决策问题，需要根据客户保留率和收入变化来确定每个客户的续费价格。作者分析了两种不同的奖励函数，一种是单目标的，只考虑收入最大化；另一种是受约束的，要求客户保留率不低于给定阈值。作者用真实数据验证了方法的有效性，发现可以在最大化收入的同时可以在最大化收入的同时保证客户保留率不低于给定阈值，为保险行业提供了一种新颖的定价策略优化框架[27]。

现有文献中所采用的方法、技术、策略等主要包括以下几类：

• 基于数学规划或优化模型的方法，例如线性规划、非线性规划、整数规划、混合整数规划等。这类方法通常需要建立精确或近似的目标函数和约束条件，并利用求解器或启发式算法来求解最优解或次优解。这类方法具有理论上的严谨性和可解释性，但是也存在一些局限性，例如对问题假设过于理想化或简化，忽略了实际环境中存在的不确定性和随机性；求解过程复杂耗时，难以适应动态变化的场景；缺乏自适应能力和泛化能力等。

• 基于机器学习或人工智能的方法，例如监督学习、无监督学习、强化学习等。这类方法通常不需要建立精确或近似的目标函数和约束条件，而是通过数据驱动或交互式学习来获取知识并指导行为。这类方法具有一定程度上克服了数学规划或优化模型方法存在局限性方面 的优势 ，例如可以处理不确定性和随机性；可以快速响应动态变化；可以自适应地更新策略并提高泛化能力等。

• 基于博弈论或拍卖理论的方法。这类方法通常需要考虑用户之间存在着利益冲突或竞争关系，并利用博弈论或拍卖理论中相关概念和工具来分析用户之间的策略选择和均衡状态。这类方法具有一定程度上解决了多用户多任务多服务器场景下的资源分配和激励机制设计问题方面的优势，例如可以保证用户之间的公平性、效率性、真实性等 。但是，这类方法也存在一些局限性，例如对用户行为假设过于理想化或简化，忽略了用户之间的合作或信任关系；求解过程依赖于完全信息或共同知识，难以适应信息不对称或不完备的情况；缺乏自适应能力和泛化能力等。

在前面的部分中，我们综合分析了车联网、移动边缘计算、工作流调度、反向拍卖机制和深度强化学习等方面的相关概念、方法、模型和算法，并总结了现有文献中所采用的技术路线和策略。然而，在对现有文献进行比较评价的过程中，我们也发现了一些不足或缺陷，主要体现在以下几个方面：

• 现有文献大都是基于传统的强化学习方法来进行资源调度，而忽略了多智能体深度强化学习的优势和潜力。多智能体深度强化学习可以更好地处理高维、非线性、动态变化的环境，以及多目标、多约束、多冲突的问题。

• 现有文献大都没有考虑用户之间的竞争和合作关系，而只是单纯地从系统效用或用户效用的角度来进行优化。这可能导致用户之间出现自私或恶意行为，从而降低系统的整体性能和公平性。

• 现有文献大都没有充分利用车联网中的拍卖机制来设计合理且公平的激励机制，而只是简单地采用固定价格或成本价格来进行定价。这可能导致用户之间出现信息不对称或虚假申报等问题，从而影响资源分配的效率和公正性。

针对上述不足或缺陷，本文提出了一种基于反向拍卖机制和多智能体深度强化学习相结合的车联网工作流调度方法（RAM-DRL），该方法可以有效地解决多用户多任务多服务器场景下的任务卸载和调度问题。

本文的研究主要内容如下：

本次论文提出了一种基于逆向拍卖机制和深度强化学习相结合的车联网工作流调度方法（RAM-DRL），该方法可以有效地解决多用户多任务多服务器场景下的任务卸载和调度问题。

本次论文将设计一个三层神经网络结构来近似每个用户 - 状态 - 行动对应的价值函数，并采用异步优势行动者-评论者算法（A3C）算法来更新网络参数并避免过度估计偏差，提高算法的稳定性和准确性。

本次论文通过理论分析和仿真实验来验证我们提出的方法在降低UAV的能耗、提高任务完成率和保证用户公平性方面的有效性和优越性。

本文研究了在移动边缘计算环境中使用多智能体深度强化学习来实现车联网的工作流调度的反向拍卖机制。本文的主要内容如下：

• 本文首先介绍了车联网、移动边缘计算、反向拍卖机制和多智能体深度强化学习等相关概念和技术，以及国内外相关研究现状综述。

• 本文其次建立了一个基于反向拍卖机制的计算资源交易模型，考虑了车联网中任务之间的依赖性和移动性对资源分配和任务执行的影响，并分析了该模型需要满足的真实性、有效性和公平性等性质。

• 本文然后提出了一种基于多智能体深度强化学习（MADRL）来设计反向拍卖机制中的竞标策略和分配规则，使得每个车辆用户都能根据自身和其他用户的状态、行为和奖励来学习最优化自己的收益，并证明了该方法具有收敛性、稳定性和可扩展性等特点。

• 本文最后通过理论分析和仿真实验验证了所提出模型和方法在保证真实性、社会福利最大化、计算效率等方面相比其他方法具有显著优势，并讨论了未来可能存在或需要解决的问题。

四、 **研究方案，包括研究思路（技术路线）、研究方法（技术措施）、拟解决的关键问题、实施方案所需条件等**

为了实现本文的研究目标和内容，本次论文设计了如下的研究方案：

**（1）研究思路（技术路线）**

本文的研究思路是基于反向拍卖机制和深度强化学习相结合的方法，来解决车联网工作流调度问题。具体来说，本文将采取以下步骤：

1. 分析车联网中的工作流调度问题，建立数学模型，定义系统效用函数和用户支付函数。

2. 设计反向拍卖机制，确定每个用户应该支付给 MEC 服务器的价格，并根据价格最低化原则来选择最优 MEC 服务器。

3. 设计深度强化学习算法，训练每个 MEC 服务器上部署的策略网络，使其能够根据用户的任务需求和自身状态计算接受任务所产生的长期收益，并根据收益最大化原则向用户进行报价。

4. 引入中心训练分散部署（CTDE）的方法，解决多智能体深度强化学习中的非平稳性和通信开销问题。在一个中心服务器上训练一个全局策略网络，并将其分发给各个 MEC 服务器，让 MEC 服务器在本地执行该策略并收集经验数据，然后将数据汇报给中心服务器进行网络更新。

5. 通过理论分析和仿真实验来验证本文提出的方法在降低车辆的能耗、提高系统效用、保证服务质量等方面的能力。

**（2）研究方法（技术措施）**

本文的研究方法主要包括以下几个方面：

1. 反向拍卖机制。我们将利用反向拍卖机制来确定每个用户应该支付给 MEC 服务器的价格，并根据价格最低化原则来选择最优 MEC 服务器。这样可以激励用户之间相互合作并避免自私或恶意行为，同时也可以保证 MEC 服务器的收益最大化。

2. 深度强化学习。我们将利用深度强化学习来训练每个 MEC 服务器上部署的策略网络，使其能够根据用户的任务需求和自身状态计算接受任务所产生的长期收益，并根据收益最大化原则向用户进行报价。这样可以使 MEC 服务器自适应地调整自己的行为策略，以应对车辆的高速移动性和动态变化性。

3. 中心训练分散部署。我们将采用中心训练分散部署的方法，来解决多智能体深度强化学习中的非平稳性和通信开销问题。我们在一个中心服务器上训练一个全局策略网络，并将其分发给各个 MEC 服务器，让 MEC 服务器在本地执行该策略并收集经验数据，然后将数据汇报给中心服务器进行网络更新。这样可以减少 MEC 服务器之间的信息交换和同步，提高系统的效率和稳定性。

4. 交通仿真软件SUMO和无线通信仿真软件NS3。我们将利用交通仿真软件SUMO和无线通信仿真软件NS3来构建车联网环境，并进行仿真实验。SUMO可以模拟车辆的移动轨迹和路况变化，NS3可以模拟车辆与MEC服务器之间的无线信道特性和数据传输过程。通过这两种软件的联合仿真，我们可以更加真实地模拟车联网移动边缘计算场景，并评估我们提出的方法在不同参数设置下的性能表现。

为了证明我们提出的方法的有效性和优越性，我们将从以下几个方面进行理论分析：

1. 真实性。我们将证明我们的方法可以保证用户和MEC服务器之间的真实性，即用户和MEC服务器都没有动机去虚报自己的真实信息，从而达到自己的利益最大化。我们将利用反向拍卖机制的概念和性质，证明我们的方法可以满足真实性的条件，即用户和MEC服务器都是单调策略的，并且用户的支付等于其对应的临界价值。

2. 社会福利最大化。我们将证明我们的方法可以实现社会福利最大化，即用户和MEC服务器的总收益最大化。我们将利用深度强化学习的框架和算法，证明我们的方法可以收敛到一个最优策略，即使得每个MEC服务器在每个状态下都选择最优行动，从而使得系统的长期累积收益最大化。

3. 计算效率。我们将证明我们的方法具有较高的计算效率，即用户和MEC服务器之间的交互过程可以在较短的时间内完成，并且不需要太多的通信开销和计算资源。我们将利用中心训练分散部署的方法，证明我们的方法可以减少MEC服务器之间的信息交换和同步，提高系统的效率和稳定性。

**（****3****）拟解决的关键问题**

以往在本文中，本次论文拟解决以下几个关键问题：

• 如何预测车辆与 MEC 服务器之间的通信质量，并根据实时状态进行任务卸载决策？

• 如何在保证服务质量（QoS）的同时平衡各个 MEC 服务器之间的负载并最大化系统效用？

• 如何设计一种合理且公平的激励机制来鼓励用户之间相互合作并避免自私或恶意行为？

• 如何利用深度强化学习来训练每个 MEC 服务器上部署的策略网络，使其能够根据用户的任务需求和自身状态计算接受任务所产生的长期收益，并根据收益最大化原则向用户进行报价？

• 如何利用反向拍卖机制来确定每个用户应该支付给 MEC 服务器的价格，并根据价格最低化原则来选择最优 MEC 服务器？

• 如何引入一种基于中心训练分散部署（CTDE）的方法，来解决多智能体深度强化学习中的非平稳性和通信开销问题？

为了回答这些问题，本次论文将采用以下研究方法：

• 建立一个车联网工作流调度模型，考虑车辆的移动性、任务的类型、优先级、截止时间等约束条件，以及 MEC 服务器的计算能力、能耗、负载等状态。

• 基于反向拍卖机制（RAM）和多智能体深度强化学习（MADRL）相结合的方法，设计一个车联网工作流调度算法（RAM-DRL），实现任务卸载和调度的优化。

• 设计一个三层神经网络结构来近似每个用户 - 状态 - 行动对应的价值函数，并采用异步优势行动者-评论者算法（A3C）算法来更新网络参数并避免过度估计偏差，提高算法的稳定性和准确性。

• 引入一种基于中心训练分散部署（CTDE）的方法，让各个 MEC 服务器在本地执行全局策略网络并收集经验数据，然后将数据汇报给中心服务器进行网络更新。

**（4）实施方案所需条件**

先前为了保证本研究的顺利进行，我们需要满足以下几个条件：

1. 数据获取。本研究需要收集车联网和移动边缘计算相关的数据，包括车辆的位置、速度、任务类型、优先级、截止时间等，以及 MEC 服务器的位置、状态、资源容量等。这些数据可以通过软件模拟仿真获取，例如使用SUMO软件模拟车辆的移动轨迹，并使用NS3软件模拟车辆和服务器之间的通信过程。我们需要考虑车辆和服务器之间的通信质量、网络拓扑变化、数据安全性等因素，以保证数据的有效性和可靠性。

2. 算法设计。本研究需要设计一种基于反向拍卖机制和多智能体深度强化学习相结合的车联网工作流调度方法（RAM-DRL），该方法可以有效地解决多用户多任务多服务器场景下的任务卸载和调度问题。我们需要掌握反向拍卖机制和深度强化学习的相关理论和技术，并根据本研究的特点进行适当的改进和优化。我们需要考虑车辆和服务器之间的博弈行为、合作策略、激励机制等因素，以保证算法的公平性和效率。

3. 算法实现。本研究需要利用编程语言和工具实现我们设计的算法，并在合适的平台上进行测试和评估。我们需要具备一定的编程能力和调试经验，并选择合适的硬件设备和软件环境来支持算法的运行。我们将使用Python语言编写算法代码，并利用Pytorch框架搭建神经网络模型。我们将使用SUMO软件模拟车辆的移动轨迹，并使用NS3软件模拟车辆和服务器之间的通信过程。我们需要考虑算法的复杂度、可扩展性、鲁棒性等因素，以保证算法的可行性和稳定性。

4. 算法评估。本研究需要对我们设计的算法进行全面和客观的评估，并与其他相关算法进行对比分析。我们需要选择合适的评价指标和评价方法，并收集或构造不同场景下的数据集来验证算法的性能和优势。我们需要考虑算法在不同参数设置、不同网络条件、不同用户行为等情况下的表现，以保证算法的有效性和通用性。我们主要从真实性、可信、社会福利最大化和计算效率四个方面来评估反向拍卖机制的效果。

**五、论文可能的创新点及预期成果**

本次论文的创新点主要体现在以下几个方面：

• 本次论文提出了一种基于反向拍卖机制和多智能体深度强化学习相结合的车联网工作流调度方法（RAM-DRL），该方法可以有效地解决多用户多任务多服务器场景下的任务卸载和调度问题，提高系统效用和用户公平性。

• 本次论文将设计一个三层神经网络结构来近似每个用户 - 状态 - 行动对应的价值函数，并采用异步优势行动者-评论者算法（A3C）算法来更新网络参数并避免过度估计偏差，提高算法的稳定性和准确性。

• 本次论文引入了一种基于中心训练分散部署（CTDE）的方法，来解决多智能体深度强化学习中的非平稳性和通信开销问题，提高算法的可扩展性和实用性。

本次论文的预期成果主要包括以下几个方面：

• 本次论文将撰写一份不少于 2万字的研究报告，系统地阐述本文的研究内容、思路方法、创新之处和实验结果，并提交给国家社科基金项目评审部门。

• 本次论文将发表1-2篇期刊或会议论文，分别从不同角度和层面展示本文的研究成果，并为相关领域研究提供文献参考。

• 本次论文将开发一个车联网工作流调度模拟平台，实现本文提出的 RAM-DRL 算法，并与其他对比算法进行性能比较。

**六、论文前期已经完成的研究工作和可能存在的困难**

本文的前期研究工作主要包括以下几个方面：

• 通过查阅大量国内外相关文献，了解车联网、移动边缘计算、反向拍卖机制和多智能体深度强化学习等领域的研究背景、现状、发展趋势和存在的问题，为本文的选题和创新点提供理论依据和文献支持。

• 通过分析车联网工作流调度问题的特点和难点，提出了基于反向拍卖机制和多智能体深度强化学习相结合的方法框架，并设计一个三层神经网络结构来近似每个服务器 - 状态 - 行动对应的价值函数。

• 通过搭建一个车联网工作流调度模拟平台，初步实现了本文提出的 RAM-DRL 算法。

本文在后期研究过程中可能会遇到的困难和问题主要有以下几个方面：

• 如何更准确地预测车辆与 MEC 服务器之间的通信质量，并根据实时状态进行任务卸载决策，以适应车辆的高速移动性和动态变化性。

• 如何更合理地设计反向拍卖机制中的报价策略、价格确定规则和分配算法，以保证用户之间的公平性和合作性，同时避免自私或恶意行为。

• 如何更有效地利用中心训练分散部署（CTDE）的方法，来解决多智能体深度强化学习中的非平稳性和通信开销问题，同时保证各个 MEC 服务器之间的信息共享和协同学习。

为了解决上述困难和问题，我们拟采取以下初步设想：

• 借鉴车联网中的位置预测技术，结合 MEC 服务器的位置信息和信号覆盖范围，来预测车辆与 MEC 服务器之间的通信质量，并根据通信质量、任务需求、服务器状态等因素来动态调整任务卸载决策。

• 参考现有的反向拍卖机制中的报价策略、价格确定规则和分配算法，结合本文场景的特点，进行必要的改进或创新，并通过理论分析和仿真实验来验证其有效性和公平性。

• 借鉴现有的中心训练分散部署（CTDE）的方法，结合本文场景的特点，进行必要的改进或创新，并通过理论分析和仿真实验来验证其有效性和实用性。