---
date created: 2023-11-20 11:29
date updated: 2023-11-20 14:50
---

- 方法设计
  - 基于多智能体强化学习
  - 反向拍卖机制的任务调度
  - 分析其原理和优势
  - 给出算法流程和伪代码。

## 4. Method of MADRL and Reverse-Auction

为了解决边缘计算中的车联网工作流调度问题，我们提出了一个基于多智能体强化学习和反向拍卖机制的方法。我们首先把这个问题建模成了一个在线和NP-hard问题，然后使用强化学习方法来处理复杂的动态的高维度的决策问题。我们采用多智能体强化学习来实现分布式的资源分配，避免集中式管理的通信开销和信息的不准确性，提高系统的稳定度。我们还使用了一种基于反向拍卖的任务卸载方法，用于解决车联网中的资源分配问题。反向拍卖是一种一买方多卖方的拍卖形式，适用于买方市场的交易活动。在反向拍卖中，买方提出自己的需求和预算，卖方根据自身的成本和利润，竞争性地给出自己的报价。最终，买方选择一个或多个报价最低且满足需求的卖方进行交易。反向拍卖可以有效降低买方的采购成本，提高卖方之间的竞争力，实现资源的优化分配。接下来的内容将分别描述多智能体强化学习建模和算法，以及反向拍卖的角色和过程。

## 多智能体强化学习建模

在第三章的车联网任务卸载建模中, 边缘服务器被定义为智能体（agent）, 每个智能体通过与环境交互学习可以最大化自身收益的策略. 因为在多智能体环境中,一个智能体的动作会影响其他智能体的决策. 为了描述这种情况，我们使用扩展的马尔可夫决策过程（extended Markov decision process, eMDP）来建模多智能体强化学习. 由K个智能体组成的eMDP可以被定义为

$$
(\mathcal S,\mathcal A_1,\dots,\mathcal A_K, \mathcal R_1,\dots,\mathcal R_K, \mathcal P_{ss'})
$$

$\mathcal S$

为状态空间（state space），

$\mathcal A_i$为第i个智能体的动作空间（action space），$\mathcal R_i$为第i个智能体的奖励函数（reward function），$\mathcal P_{ss'}$为状态转移概率（transition probability）。在每一个时间间隙, 边缘服务器作为一个智能体, 观察当前环境的状态$s\in\mathcal S$, 根据策略$\pi_i(a_i|s)$选择一个合适的动作$a_i\in\mathcal A_i$. 然后获得相应的奖励$r_i=\mathcal R_i(s,a_1,\dots,a_K)$. 为了使用强化学习方法, 我们需要将问题转换为马尔可夫决策过程（Markov decision process, MDP），它包括状态空间（state space, $\mathcal S$），观测空间（observation space, $\mathcal O$），动作空间（action space, $\mathcal A$），和奖励函数（reward function, $\mathcal R$）.

Littman, M. L. (1994). Markov games as a framework for multi-agent reinforcement learning. In Machine learning proceedings 1994 (pp. 157-163). Morgan Kaufmann.

![[Pasted image 20230902100025.png]]
多智能体强化学习过程

![[Pasted image 20230717162154.png]]
**Figure 1**: Single-agent approaches (a) and (b) in comparison with multi-agent RL (c)

我们将车联网中的任务调度问题建模如下.

- 状态空间
- 观察空间
- 动作空间
- 奖励函数
- 环境建模

### 状态空间 State Space

在强化学习方法中, 状态空间是指整个系统的描述，包含了系统中所有的信息。本次研究的是多智能体强化学习，因此状态空间应当包含所有边缘服务器本身的状态, 边缘服务器i的状态可以表示为$S_i=(L_i,B_i)$， 其中:

- $R_i=(r_{it})_{T\times R}$，现有资源时间槽表示边缘服务器在一定时间内的资源占用情况。其中，$r_{it}$ 表示第 $i$ 个边缘服务器在第 $t$ 个时间段内剩余的第 $r$ 种资源量，$T$ 表示总时间段数，$R$ 表示总资源种类数。这个状态特征可以反映边缘服务器的资源可用性和限制性。
- $B_i=(task_j)$，表示边缘服务器当前等待被报价的任务信息。其中，$task_j$ 表示当前编号为 $j$ 的任务正在向第 $i$ 个边缘服务器请求报价，$task_j=\{p_j, r_j, d_j\}$, 其中$p_j$为任务优先级，$r_j$为任务的资源需求，$d_j$为任务持续时间。

可用 $S=\{S_1,S_2,\dots,S_m\}$ 来表示所有边缘服务器的状态，其中，$S$ 表示整个服务器集群的状态集合，$S_i$ 表示第 $i$ 个边缘服务器的状态。因为整个服务器集群是由多个服务器组成，所有边缘服务器状态的集合就是本次研究的状态空间。

观察空间 Observation Space
观察空间定义了一个智能体可以观察到的状态空间， 本次研究所定义的智能体之间不存在额外的通信与交互，因此每个智能体所能观察到的状态空间就是自身的服务器状态$S_i$。因此，边缘服务器i的观察空间为$O_i=S_i$。==（等待完善）==

### 动作空间 Action Space

动作空间定义了智能体可以向环境执行的动作, 在多智能体强化学习中，每个智能体都可以执行动作以影响环境的状态，我们先定义单个智能体的动作空间。单个智能体的动作空间具体为一个在$a_i \in [1/3,3]$的数值，$a_i$市场价格的浮动系数，用以决定最终的服务器报价。为了使用Value-based和Policy-based的方法, 我们同时建模了discrete space和continuous space的两种action space.

- discrete space
  - 将价格区间等分为若干份
- continuous space
  - $[1/3,3]$ 的连续区间
    action space到action 转换
- discrete space
  - 将价格区间等分为若干份, 每一份对应的是动作空间的一个值
- continuous space
  - $[1/3,3]$ 的连续区间

当环境接收到服务器i的$a_i$的值后, 服务器i最终报价为$bid_i = a_i \times avergeprice \times resource \times duration$。$avergeprice$是资源的平均价格, $resource$是资源使用量, $duration$是资源使用时间
因此,在时间步t，整个系统的动作空间为

$$
A_t=\{a_1, a_2,\dots,a_m\}
$$

其中，$a_i$代表第i个服务器对其当前请求报价的任务的报价的系数. 当对任务进行报价时, 环境仅收集任务对应满足条件限制的服务器的报价, 而忽略其他服务器的报价.

### 奖励函数 Reward Function

奖励函数被用来描述智能体在执行某一动作后,从一个状态转移到另一个状态的奖励. 奖励函数的设置会影响每个智能体的目标，智能体会通过奖励函数判断自己选择的action对于自己目标的达成时正向的还是负向的影响来最大化自己的累积奖励。
在本次论文中,边缘服务器的目标是最大化自己的收益, 边缘服务器会在每回合开始时, 减去自身服务器的成本，并且结算本回合结束的任务获得收益。
因此, 奖励函数可以被写为

$$
r_i = p_i-c_i
$$

其中，$p_i$ 表示本回合的任务支付，$c_i$ 表示服务器成本。

### 环境建模

为了实现本论文中多智能体强化学习的环境建模, 我们使用了Pettingzoo最为我们实现自定义环境的接口. Pettingzoo是一个用于表示一般的多智能体强化学习（MARL）问题的Python库，它包括了多种参考环境、有用的工具和创建自定义环境的方法。Pettingzoo支持Agent Environment Cycle（ACE）模式，它是一种适用于顺序的回合制环境的接口，能够处理任何MARL可以考虑的游戏。在ACE模式下，每个智能体都有自己的观察空间和动作空间，而且只有一个智能体可以在每个时间步骤中执行动作。用户可以使用以下方法来与ACE模式的环境进行交互：每一个step开始时使用env.agent_iter()的到当前需要执行动作的agent, 通过env.last()返回最后一个智能体的观察、奖励、终止、截断和信息, 利用返回的信息生成action, 通过env.step(action)执行一个动作并更新环境状态,直到所有的智能体都完成了他们的回合或者游戏结束。

为了利用多智能体强化学习求解这个优化问题，我们建立了如下多智能体强化学习模型：

1. 初始化任务生成模型参数，边缘服务器集群 S。
2. 对于每个时间槽 t，执行以下操作, 直到满足停止条件：
   1. 使用任务生成模型生成在本时间槽中需要卸载的所有任务$T_t$.
   2. 集群中每一个服务器更新资源使用情况, 向车辆返回结束的任务的执行结果, 结算执行完毕的任务获得奖励和维护费用.
   3. 从$T_t$中的选择一个没有参加过拍卖任务$task_i$, 执行如下操作, 直至全部任务都进行拍卖:
      1. 将$task_i$相关信息发送到满足限制的服务器集合$a_i$.
      2. 对于每一个在$a_i$中的边缘服务器$S_j$,执行如下操作,直到所有服务器完成报价.
         1. $S_j$收到任务请求后，根据自己的观测，得到动作, 将报价返回拍卖商.
      3. 拍卖商收到所有服务器的报价后，执行反向拍卖过程，并向车辆通知卸载结果和支付费用。==可以考虑二价拍卖==
      4. 车辆收到拍卖结果，检查是否满足其预算要求
         1. 如果满足预算需求, 则同意卸载，并在任务结束后进行支付==目前是执行后支付,考虑立即支付, 执行后支付应该会偏向于小任务, 立即支付会偏向于大任务, 未验证==
            1. 将车辆任务发送到边缘服务器
            2. 边缘服务为任务预留相关资源并开始执行, 更新资源的使用情况.
         2. 如果不满足，则不进行任务卸载,不进行支付.
   4. 当$T_t$中所有任务都进行拍卖后, 进入下一个时间槽$t+1$
3. 输出最终的环境状态，并结束环境。

#### 任务生成模型

完成对任务的建模后, 为模拟真实的任务调度场景, 接下来需要对任务的生成进行建模. 任务的生成一共由三部分组成, 一个时间步内任务的数量, 每个任务对资源的需求, 任务可卸载的服务器列表.

- 一个时间步的任务数量, 本文将其简化为符合泊松分布的随机变量, 每一回合从分布中抽取一个正整数作为该时间步的任务数量.
- 每个任务对资源的需求, 我们将任务的资源需求和执行时间都取自由两个正态分布组合而成的双峰分布, 因为通过分析现实的计算机集群工作负载现实世界中卸载的任务以小任务或大任务居多, 如果以简单的正态分布或者指数分布(负指数分布)可能无法准确建模, 另外选择这样的分布也是为了检验神经网络能否拟合复杂分布.==(这里任务分布待讨论)==
- 任务可卸载的服务器列表, 由于车联网中的车辆存在动态性, 位置存在随机性, 因此我们将车辆与边缘服务器之间的连接抽象成一个可连接服务器的集合. 由于服务器的位置是固定的, 因此其可连接的车辆范围也是固定的. 为在模型中体现车联网的连接性质, 在生成执行服务器限制时, 我们首先生成一个连接范围限制, 然后从连接范围限制的服务器集合中随机抽取若干服务器作为服务器限制集合. 这时的任务可卸载约束仅有连接所限制,属于服务器的无线设备自发无目的的向其范围内车辆发送的连接检测过程,这时服务器仍然未知任务的具体特征, 因此执行约束不在任务生成模型的建模之中.

因此本次研究系统模型中的任务生成过程如下.

- 设定一个时间步的长度为 $\Delta t$，比如 $\Delta t=1$ 秒或者 $\Delta t=0.1$ 秒；
- 在每个时间步 $n$ 内，根据泊松分布 $Poisson(\lambda)$ 生成一个正整数 $m$，表示这个时间步内有 $m$ 个任务被车辆提交到边缘服务器，其中 $\lambda$ 是泊松分布的参数，表示单位时间内任务到达的平均速率；
- 对于每个任务 $task_i$，根据双峰分布 $Bimodal(\mu_1,\sigma_1,\mu_2,\sigma_2,w)$ 生成它的资源需求和执行时间，表示这个任务需要多少计算资源和存储资源以及执行多长时间，其中 $\mu_1,\sigma_1,\mu_2,\sigma_2$ 是两个正态分布的均值和标准差，$w$ 是两个正态分布的权重；
- 根据车辆的位置和边缘服务器的位置和连接情况，生成一个可连接服务器的集合 $a_i$，表示这个任务可以被发送到哪些边缘服务器；
- 根据车辆的偏好或者其他因素，生成一个预算 $b_i$，表示这个任务的最高支付费用；
- 这样，我们就得到了一个完整的任务 $task_i=(n_i,p_i,r_i,d_i,t_i,a_i,b_i,f_i)$；
- 将这个时间步内生成的所有任务组成一个任务集合 $T_n=\{task_k,task_{k+1},...,task_{k+m}\}$，表示这个时间步内有 $m$ 个任务被车辆提交到边缘服务器。
- 重复这个过程，直到生成所有任务或达到停止条件。

#### 反向拍卖模型

在本文中，我们使用了一种基于反向拍卖的任务卸载方法，用于解决车联网中的资源分配问题。反向拍卖是一种一买方多卖方的拍卖形式，适用于买方市场的交易活动。在反向拍卖中，买方提出自己的需求和预算，卖方根据自身的成本和利润，竞争性地给出自己的报价。最终，买方选择一个或多个报价最低且满足需求的卖方进行交易。反向拍卖可以有效降低买方的采购成本，提高卖方之间的竞争力，实现资源的优化分配。接下来将分别介绍本模型中反向拍卖的角色和反向拍卖过程.

在本文中，我们使用了一种基于反向拍卖的任务卸载方法，用于解决车联网中的资源分配问题。接下来将分别介绍本模型中反向拍卖的角色和反向拍卖过程.

买家, 任务发起者, 是车联网中有任务卸载需求, 并愿意为其需求支付的车辆. 在某一时刻t, 车辆向满足其连接限制(车辆可以顺利通过无线网络将任务卸载)的边缘服务器发起任务$task_j$, $task_j$根据其本身属性, 生成报价请求$b_j=\{p_j,r_j, d_j\}$. 其中, $p_j$任务优先级，$r_j$任务的资源需求，$d_j$任务持续时间. 报价请求将会由车辆发送到满足连接约束的服务器中以请求报价.

卖家, 服务提供商, 也就是车联网中的边缘服务器. 边缘服务器可以接收车辆的任务卸载请求, 并利用其自身资源来完成任务卸载和获得收益. 服务器i的特征可以被定义为$S_i=(L_i,R_i,C_i)$, 其中$L_k$：服务器负载（Load）,$R_k$：服务器资源（Resource）, $C_k$：服务器成本（Cost）.

服务器报价模型是指服务器根据任务的特征和自身的状况，计算出自己对任务的报价。为通过强化学习完成定价, 我们定义了一个函数，用于计算服务器的报价：

$$
bid_{i,j} = a_i \times p \times r_j \times d_j
$$

其中, $bid_{i,j}$为服务器i对任务j的报价, $a_i$为服务器通过强化学习结合自身观察生成的动作, 作为报价的系数. $p$为市场平均单位时间资源价格向量, $r_j$为任务j对资源的需求量, $d_j$为任务的持续时间. 则$p \times r_j \times d_j$可以理解为市场整体的平均价格.

一个经纪人是一个中间代理，负责主持和指导拍卖过程。经纪人的责任是确保拍卖顺利进行，保持真实性和个体理性，从而最大化买方的效用，同时保护卖方的利益。在车联网中，一个基站或路侧单元可以通过一个拍卖控制器进行资源拍卖。为使服务器之间充分竞争, 报价可信, 防止出现服务器一起抬价和神经网络无法收敛, 我们设定了一个市场平均价格作为参考价格, 具体报价方式为服务器根据任务特征和服务器自身状况, 报价一个参考价格的偏差.

基于云计算的车联网场景，包含若干车辆和n个卖方$s_j∈\{s_1,s_2,...,s_N\}$. 车辆通过自身的通信单元与边缘服务器进行通信, 并且评估车辆与边缘服务器的通信状况, 以在需要进行任务卸载时生成边缘服务器限制条件。边缘服务器可以通过有线链接相互连接,并且与互联网连接以便进行任务卸载环境的搭建和信息交流。基本假设如下。

1. 假设时间是分时的，任务只能在一个时间段提交, 并且任务时长为整数个时间段。
2. 拍卖过程中, 一次仅有一个任务参与, 在任务拍卖结束后, 进行本时隙的下一个任务的拍卖. 如果本时隙没有任务, 则进入下一个时隙.
3. 在卸载市场中存在信息缺失：首先，卖方的特征和维护成本对经纪人和买方不可见；没有卖方知道其他参与者的出价，也没有卖方相互勾结。另外，买方的私人信息（例如，买方的预算）不能公开给卖方。
4. 车辆可以根据自身状态, 评估在拍卖和卸载的时间内的连接情况, 得到任务的边缘服务器限制条件. 任务可以在卸载时保持与边缘服务器的稳定连接.
5. 一个买方只能把他的任务卸载给一个卖方，但一个卖方可以同时为多个买方提供服务。随着时间前进, 任务在到达其持续时间限制时完成, 其占用服务器的资源被释放, 以便服务器继续为其他买方服务.
6. 任务卸载的结果对于车辆而言, 其效用是一致的. 对于用户而言, 服务器的处理过程是透明的, 无论是哪一个服务器接收了卸载对于任务执行的结果是一致的.
7. 拍卖在车辆的行驶过程中自动进行, 经纪人不存在自由意志, 仅仅只是执行拍卖流程.

在设计了拍卖过程的基本假设后,结合服务器的报价过程, 我们可以书写反向拍卖模型. 拍卖商, 作为主持和引导拍卖的代理人, 在本次模型中拍卖商是一个运行在车联网中的程序, 用来完成每次反向拍卖的流程.反向拍卖卸载过程如下：

- 车辆将任务特征发送到满足任务连接限制的服务器, 向服务器请求报价
- 服务器接收到任务请求后, 先检查自身资源能否满足本次任务的资源请求, 如果无法满足则退出拍卖, 不进行报价. 如果可以满足资源请求, 则进入报价流程.
- 服务器根据自身资源使用情况和任务特征, 计算出自身报价.
- 车辆在收到报价后, 检查是否存在满足卸载需求的服务器, 如果不存在, 则本次拍卖结束, 任务卸载失败. 如果存在则继续.
- 根据服务器产生的报价, 进行反向拍卖, 得到胜者和支付.
- 检查支付是否满足预算需求, 如果不满足, 本次拍卖结束, 任务卸载失败.如果满足, 继续.
- 开始卸载到胜者服务器, 将支付设定为拍卖结果的支付.
- 任务执行完成后, 服务器获得支付, 并将结果返回车辆.

反向拍卖模型是指车辆根据服务器的报价，选择合适的服务器进行任务卸载。我们假设车辆只能将任务卸载到一个服务器上，并且要满足任务的能耗、延迟和预算限制。我们定义了一个函数E，用于计算将任务卸载到服务器所消耗的能耗，以及一个函数L，用于计算将任务卸载到服务器所产生的延迟。我们的目标是在满足限制条件的前提下，最小化车辆支付给服务器的总价格。我们用以下数学符号和公式来表示这个问题：

$$
\begin{aligned}
& \min_{x} V.P = \sum_{i=1}^n x_i * Si.p \\
& \text{s.t.} \\
& \sum_{i=1}^n x_i = 1 \\
& x_i \in \{0, 1\} \\
& Si.p \leq T.b \\
& T.e \leq E(T, Si) \\
& T.l \geq L(T, Si) \\
\end{aligned}
$$

其中xi是一个决策变量，表示是否将任务T分配给服务器Si。

## 多智能体强化学习算法

算法的简介, ==测试一下那种效果最好进行介绍, 剩下作为对比==

- dqn
- ddpg
- mappo
- maddpg ==未成功==
- a2c
- sac

为了解决提出的RL环境中的作业调度问题，我们使用了两种基于DRL的算法。第一种是基于Q-Learning的方法，叫做Deep Q-Learning (DQN)。另一种是一种策略梯度算法，叫做REINFORCE。我们选择这些算法是因为它们可以处理具有离散状态和动作空间的RL环境。而且，这两种算法的工作过程也不同，其中DQN优化状态-动作值，而REINFORCE直接更新策略。从Spark作业调度的角度来看，RL环境将提供与我们运行的真实工作负载类似的作业规范。此外，集群资源也相同，因此VM资源可用性也将作为状态空间的一部分被使用和更新。每次DRL代理采取一个动作（放置一个执行器），都会得到一个即时奖励。下一个状态也将依赖于前一个状态，因为VM和作业规范将在每次放置后进行更新。最终，DRL代理应该能够学习资源可用性和需求约束，并完成所有作业的所有执行器的调度，以完成情节并获得情节奖励。

RL模拟反馈回路不断地收集数据，对于单个（单智能体案例）或多个（多智能体案例）策略进行训练，确保策略的权重保持同步。因此，收集的环境数据包含观察到的状态、采取的动作、、获得的奖励和所谓的**结束**标志，表示代理在模拟中经历的不同的回合。

模拟的动作 -> 奖励 -> 下一个状态 -> 训练 -> 重复，直到终止状态，称为一个**回合**，或者在RLlib中，一个**演示**。

- RLlib使用rollout workers来并行地从环境中采集数据，每个worker可以使用一个或多个策略来与环境交互，产生一系列的动作、奖励、下一个状态和结束标志，这些数据构成了一个episode或rollout。
- RLlib将每个worker产生的rollout汇总到一个训练批次中，然后根据选择的算法和损失函数计算梯度，并更新模型参数。
- RLlib重复这个过程，直到满足终止条件，如达到最大迭代次数、最大训练时间或最佳性能等。

#### 5.1.2 Deep Q-Learning (DQN)

Q-学习可以作为动态规划（DP）问题来求解，其中我们可以将_Q_-函数表示为一个二维矩阵，包含每一对$s$和$a$的值。然而，在高维空间中（状态和动作对的总数很大），表格Q-学习的解决方案是不可行的。因此，通常用一个神经网络来训练参数$\theta$，来近似Q-值，即$Q(s,a;\theta) \approx Q^*(s,a)$。这里，每一步$i$需要最小化以下损失函数

$$
{{{ L}_{i}(\theta_{i})=\mathbb{E}_{s,a,r,s^{\prime}\sim\rho(\cdot)}\left[(y_{i}-Q(s,a;\theta_{i}))^{2}\right],}}\tag{16}
$$

其中$y_{i}=r+\gamma\mathrm{max}_{a^{\prime}} Q (s^{\prime}, a^{\prime};\theta_{i-1})$。

这里，$\rho$是从环境中采样的转移$\{s,a,r,s'\}$的分布。$y_i$被称为时间差分（TD）目标，*$y_i$ _ $Q$被称为TD误差。

注意，目标$y_i$是一个变化的目标。在监督学习中，我们有一个固定的目标。因此，我们可以训练一个神经网络，通过减少损失函数，让它在每一步向目标靠近。然而，在强化学习中，由于我们逐渐地学习环境的知识，目标$y_i$总是在改进，对于网络来说它就像一个移动的目标，从而使它不稳定。目标网络具有固定的网络参数，因为它是从之前的迭代中采样的。因此，目标网络的参数用于更新当前网络，以实现稳定的训练。

此外，我们希望我们的输入数据是独立同分布（i.i.d.）的。然而，在同一条轨迹（或者说情节）中，迭代之间是相关的。在训练迭代中，我们更新模型参数，使$Q(s,a)$更接近真实值。这些更新会影响其他估计，并使网络不稳定。因此，可以使用一个循环的*回放缓冲区*来保存环境中的之前的转移（状态、动作、奖励样本）。因此，从回放缓冲区中抽取一个小批量样本来训练深度神经网络，使得数据更加独立和类似于i.i.d.

DQN是一种离线算法，它在从环境中收集数据时使用了不同的策略。原因是如果一直使用正在改进的策略，由于状态-动作空间覆盖不足，算法可能会收敛到次优策略。因此，使用了一个$\epsilon$-贪婪策略，它以概率$1-\epsilon$选择贪婪动作，以概率$\epsilon$选择随机动作，这样它就可以观察到任何未探索过的状态，确保算法不会陷入局部最大值。我们使用回放缓冲区和目标网络的DQN[38]算法在算法1中总结。

![[Pasted image 20230726102735.png]]

### 5.1 REINFORCE Agent

DQN优化状态-动作值，从而间接地优化策略。然而，策略梯度方法直接对策略进行建模和优化。策略通常用一个关于$\theta$的参数化函数表示，写成$\pi_\theta$。相应地，$\pi(a_t|s_t)$是在时刻$t$给定状态$s_t$时选择动作$a_t$的概率。智能体能够获得的奖励的数量取决于这个策略。

在传统的策略梯度算法中，每次迭代收集一批样本，然后使用收集到的样本对策略应用公式(17)中显示的更新。

$$
\nabla_{\theta}\mathrm{E}_{\pi_{\theta}}\left[\sum_{t=0}^{T}\gamma^{t}r_{t}\right]=\mathbb{E}_{\pi_\theta}\left[\sum_{t=0}^{T}\nabla_{\theta}l o g\pi_{\theta}(a_{t}|s_{t})R_{t}\right].\tag{17}
$$

这里，$\gamma$是折扣因子，而$s_t$、$a_t$和$r_t$分别用来表示时刻$t$的状态、动作和奖励。$T$是任意单个回合的长度。$R_t$是折扣累积回报，可以按照公式(18)计算

$$
R_{t}=\sum_{t^{\prime}=t}^{T}\gamma^{t^{\prime}-t}r_{t}.\tag{18}
$$

这里，t'从当前时刻$t$开始，这意味着如果采取当前的动作，我们将得到一个即时奖励$r_t$，它也影响了我们能够累积到回合结束时的奖励量。
公式(17)中显示的期望回报使用了最大似然度，它衡量了观察到的数据的可能性。在强化学习的背景下，它意味着我们可以期望在当前策略下得到当前轨迹的可能性。当似然度与奖励相乘时，如果产生正向奖励，则策略的似然度增加。另一方面，如果给出较少或负向奖励，则策略的似然度减少。总之，模型试图保持效果较好的策略，并倾向于丢弃效果不好的策略。然而，由于公式显示为一个期望值，它不能直接使用。因此，使用一个基于采样的估计器来代替，如公式(19)所示

$$
\nabla_{\theta}J(\theta)\approx\frac{1}{N}\sum_{i=1}^{N}\left(\sum_{t=1}^{T}{\nabla}_{\theta}l o g\pi_{\theta}(a_{i,t}|s_{i,t})\right)\left(\sum_{t=1}^{T}r(s_{i,t},a_{i,t})\right).\tag{19}
$$

这里，$\nabla_\theta J(\theta)$是目标目标函数$J$的策略梯度，用$\theta$参数化。我们还假设在每次迭代中，采样了N条轨迹$( \tau_1,...,\tau_n)$，其中每条轨迹$\tau_i$是一个状态、动作和奖励的列表：$\tau_i = s_i,a_i,r_i$对于时间步$t=0$到$t=T_i$。在这项工作中，我们使用REINFORCE [39]算法，如算法2所示。这个算法通过利用蒙特卡罗采样（通过计算一个完整回合的奖励来学习）来工作。在收集步骤（第2行）之后，算法使用更新的策略梯度和一个学习参数a来更新底层网络（第4行）。注意，在采样轨迹时，使用了E-贪婪策略。

![[Pasted image 20230726103631.png]]

### PPO Algorithm

**策略梯度方法**是一类基于策略的强化学习算法，它们通过对策略参数进行梯度下降来优化一个期望累积折扣奖励的目标函数。策略梯度方法的核心思想是利用按照策略产生的执行轨迹来估计梯度，并使用经验回报或优势函数作为梯度的方向。策略梯度方法可以处理连续的动作空间，并且可以在每个时间步更新策略参数。

策略梯度方法的工作原理是通过计算策略梯度的估计量，并将其应用到随机梯度上升算法中。最常用的梯度估计量的形式如下，其中$\pi_{\theta}$是一个随机策略，$\hat{A}_{t}$是时间步$t$处的优势函数的估计量。

${\hat{g}}={\hat{\mathbb{E}}}_{t}\left[\nabla_{\theta}\log\pi_{\theta}(a_{t}\mid s_{t}){\hat{A}}_{t}\right]\tag1$

这里，期望$\hat{\mathbb{E}}_{t}[\ldots]$表示在一个有限的样本批次上的经验平均，在一个交替采样和优化的算法中。使用自动微分软件的实现方法是通过构造一个目标函数，其梯度是策略梯度估计量；估计量$\hat{g}$是通过对目标函数求导得到的

$$
L^{PG}(\theta)=\hat{\mathbb{E}}_{t}\Big[\log\pi_{\theta}(a_{t}\mid s_{t})\hat{A}_{t}\Big]. \tag2
$$

这个目标函数可以看作是根据[34]给出的期望累积折扣奖励的目标函数的对数似然，其中$Q^{\pi_{\theta}}(s,a)$表示从状态$s$开始，（确定性地）选择动作$a$，然后按照策略$π_θ$行动所获得的期望累积折扣奖励。

虽然使用相同的轨迹对这个损失$L^{PG}$进行多步优化是很有吸引力的，但这样做并没有很好的理由，而且从经验上看，它经常导致破坏性的大规模策略更新。

一个典型的策略梯度方法是REINFORCE算法[35]，它使用了蒙特卡罗方法来估计回报，并且可以直观地理解为强化那些导致更好回报的动作。REINFORCE算法通过梯度下降更新策略参数：

$$
\theta\leftarrow\theta+\alpha\sum_{t}\nabla_{\theta}\log\pi_{\theta}(s_{t},a_{t})v_{t},\tag3
$$

这里，$α$是步长。方程[3]沿着$\nabla_{\theta}\log\pi_{\theta}(s_{t},a_{t})v_{t}$这个方向走一步；这个方向表示如何改变策略参数以增加${π_θ}(s_t,a_t)$（状态$s_t$下动作$a_t$的概率）。步长的大小取决于回报$v_t$有多大。

在我们的设计中，我们使用了PPO算法[32]，它使用了剪裁函数来限制新旧策略之间的比率，并且使用了一种更一般的优势函数估计方式。PPO算法可以进行多次的小批量更新，而不是像标准的策略梯度方法那样每个数据样本只进行一次梯度更新。PPO算法在实现简易性、样本复杂度和调整简易性之间取得了平衡，在各种强化学习任务上表现出了优异的性能。更多细节如下。

**PPO算法**是一种基于策略梯度的强化学习算法，它的目标是在与环境交互采样数据后，使用随机梯度上升优化一个“替代”目标函数，从而改进策略。PPO算法有两种主流的实现方式，一种是使用KL散度作为惩罚项来限制策略更新的幅度，另一种是使用剪裁函数来限制新旧策略之间的比率。PPO算法还使用了一种更一般的优势函数估计方式，并且从环境中获取较长的样本序列，然后用于执行多个训练周期。PPO算法在实现简易性、样本复杂度和调整简易性之间取得了平衡，在各种强化学习任务上表现出了优异的性能。

PPO算法是由OpenAI团队发表在2017年的论文[1]中提出的。它是在TRPO之后很久才提出的一种改进算法。TRPO也是一种基于策略梯度的强化学习算法，它通过最大化一个近似的目标函数来更新策略，同时保证与前一策略的KL散度在一个信任域内。TRPO虽然在连续控制任务上表现良好，但是它不适合共享参数或有辅助损失函数的情况。

PPO算法对经典A2C方法的核心改进是更改用于估算策略梯度的公式。A2C方法是一种基于演员-评论家框架的强化学习算法，它使用一个演员网络来输出动作概率分布，一个评论家网络来输出状态值函数。A2C方法使用所执行动作的对数概率梯度来更新演员网络，即：

$$
\nabla_{\theta}J(\theta)=\mathbb{E}_{t}[\nabla_{\theta}\log\pi_{\theta}(a_{t}\mid s_{t})A_{t}]
$$

其中$A_t$是优势函数，表示动作$a_t$相对于状态$s_t$的平均价值的优势程度。A2C方法每个数据样本只进行一次梯度更新，因此可能导致高方差和低效率。

PPO方法不是使用所执行动作的对数概率梯度，而是使用了一个不同的目标：由优势值进行缩放的新策略和旧策略之间的比率。这个目标可以写为：

$$
J_{\theta}=E_{t}\left[\frac{\pi_{\theta}(a_{t}\mid s_{t})}{\pi_{{\theta_{\mathrm{old}}}}(a_{_t}\mid s_{t})}A^{\mathrm{GAE}(\gamma, \lambda)}_{t}\right]
$$

其中$\pi_{{\theta_{\mathrm{old}}}}$表示更新前的旧策略。这个目标与交叉熵方法有相同的原理：重要性采样。为了限制更新，PPO算法使用了裁剪后的目标：

$$
J_{\theta}^{\mathrm{clip}}=\mathbb{E}_{t}\left[\min(r_{t}(\theta)A^{\mathrm{GAE}(\gamma, \lambda)}_{t},\mathrm{clip}(r_{t}(\theta),1-\varepsilon,1+\varepsilon)A^{\mathrm{GAE}(\gamma, \lambda)}_{t})\right]
$$

此目标将新旧策略之间的比率限制在$[1–ε, 1+ε]$区间内，因此通过更改$ε$，我们可以限制更新的大小。这种裁剪方法可以避免使用KL散度惩罚或自适应更新，从而简化了算法。

PPO算法使用了一种更一般的优势函数估计方式，即GAE（Generalized Advantage Estimation）。GAE是由John Schulman等人在2015年的论文[2]中提出的。GAE的基本思想是利用TD($\lambda$)方法来平衡偏差和方差，同时减少优势函数的方差。GAE的定义如下：

$$
A^{\mathrm{GAE}(\gamma, \lambda)}_{t}=\sum_{l=0}^{\infty}(\gamma \lambda)^{l} \delta_{t+l}
$$

其中$\delta_t$是TD误差，即$\delta_t = R_t + \gamma V_\theta(S_{t+1}) - V_\theta(S_t)$。可以看出，GAE是对TD误差的加权累加，其中权重随着$l$的增加而指数衰减。当$\lambda=0$时，GAE退化为一步TD误差；当$\lambda=1$时，GAE退化为MC估计。因此，通过调节$\lambda$的值，我们可以在偏差和方差之间找到一个平衡点。GAE还有一个重要的性质是，它可以消除优势函数的常数偏移量。

PPO算法从环境中获取较长的样本序列，然后用于执行多个训练周期。PPO算法在实现简易性、样本复杂度和调整简易性之间取得了平衡，在每一步都试图计算一个最小化代价函数的更新，同时保证与前一策略的偏离相对较小。PPO算法在各种强化学习任务上表现出了优异的性能。

[1]: [Proximal Policy Optimization Algorithms]
[2]: [High-Dimensional Continuous Control Using Generalized Advantage Estimation]

---

### PPO算法

在上文中我们已经介绍了MDP建模和环境建模，本节我们将介绍一种基于策略梯度的强化学习算法——PPO算法[1]。PPO算法的目标是在与环境交互采样数据后，使用随机梯度上升优化一个“替代”目标函数，从而改进策略。PPO算法的核心思想是利用按照策略产生的执行轨迹来估计梯度，并使用经验回报或优势函数作为梯度的方向。PPO算法可以处理连续的动作空间，并且可以在每个时间步更新策略参数。

PPO算法的目标函数是由优势值进行缩放的新策略和旧策略之间的比率，即：

$$
J_{\theta}=E_{t}\left[\frac{\pi_{\theta}(a_{t}\mid s_{t})}{\pi_{{\theta_{\mathrm{old}}}}(a_{_t}\mid s_{t})}A^{\mathrm{GAE}(\gamma, \lambda)}_{t}\right]
$$

其中$\pi_{{\theta_{\mathrm{old}}}}$表示更新前的旧策略。这个目标与交叉熵方法有相同的原理：重要性采样。为了限制更新，PPO算法使用了裁剪后的目标：

$$
J_{\theta}^{\mathrm{clip}}=\mathbb{E}_{t}\left[\min(r_{t}(\theta)A^{\mathrm{GAE}(\gamma, \lambda)}_{t},\mathrm{clip}(r_{t}(\theta),1-\varepsilon,1+\varepsilon)A^{\mathrm{GAE}(\gamma, \lambda)}_{t})\right]
$$

此目标将新旧策略之间的比率限制在$[1–ε, 1+ε]$区间内，因此通过更改$ε$，我们可以限制更新的大小。这种裁剪方法可以避免使用KL散度惩罚或自适应更新，从而简化了算法。

PPO算法有两种主流的实现方式，一种是使用KL散度作为惩罚项来限制策略更新的幅度，另一种是使用剪裁函数来限制新旧策略之间的比率。PPO算法还使用了一种更一般的优势函数估计方式，即GAE（Generalized Advantage Estimation）。GAE是由John Schulman等人在2015年的论文[2]中提出的。GAE的基本思想是利用TD($\lambda$)方法来平衡偏差和方差，同时减少优势函数的方差。GAE的定义如下：

$$
A^{\mathrm{GAE}(\gamma, \lambda)}_{t}=\sum_{l=0}^{\infty}(\gamma \lambda)^{l} \delta_{t+l}
$$

其中$\delta_t$是TD误差，即$\delta_t = R_t + \gamma V_\theta(S_{t+1}) - V_\theta(S_t)$。可以看出，GAE是对TD误差的加权累加，其中权重随着$l$的增加而指数衰减。当$\lambda=0$时，GAE退化为一步TD误差；当$\lambda=1$时，GAE退化为MC估计。因此，通过调节$\lambda$的值，我们可以在偏差和方差之间找到一个平衡点。GAE还有一个重要的性质是，它可以消除优势函数的常数偏移量。

PPO算法从环境中获取较长的样本序列，然后用于执行多个训练周期。PPO算法在每一步都试图最小化一个代价函数，同时保证与前一策略的偏离相对较小。PPO算法在实现简易性、样本复杂度和调整简易性之间取得了平衡，在各种强化学习任务上表现出了优异的性能。

**Algorithm 1** PPO, Actor-Critic Style

- for iteration=1,2,... do
  - for actor=1,2,... do
    - Run policy $\pi_{\theta_{old}}$ in environment for T timesteps
    - Compute advantage estimates $\hat{A}_1, \dots, \hat{A}_T$
  - end for
  - Optimize surrogate $L$ wrt $\theta$, with $K$ epochs and minibatch size $M\le NT$
  - $\theta_{old}\gets \theta$
- end for

为了解决第三章提出的车联网环境中的任务卸载问题，我们采用以上描述的PPO算法。这种算法可以处理具有连续状态和动作空间的RL环境，而不需要像Q-Learning等基于值函数的算法那样估计状态-动作值，而是直接学习最优策略。

在我们的车辆任务卸载场景中，我们自定义的RL环境将提供与真实车联网场景相似的任务特征，包括任务的类型、大小、持续时间等。此外，边缘服务器的资源也会被考虑，因此CPU和内存的可用性也将作为状态空间的一部分被使用和更新。

我们使用多智能体强化学习的框架，让每个边缘服务器都部署一个PPO代理，根据自己的状态和动作来更新自己的策略。这样，不同的服务器之间的策略可以根据任务需求和市场竞争进行自适应调整，从而提高自身的收入和车辆的效用。同时，多智能体强化学习也可以处理多个智能体之间的交互和博弈，使得每个服务器能够考虑其他服务器的行为和影响。为了简化模型，我们假设服务器之间没有额外的通信，而且只进行一次博弈。

我们使用RLlib来实现和管理训练过程，它可以并行地从环境中采集数据，汇总到一个训练批次中，然后根据选择的算法和损失函数计算梯度，并更新模型参数。我们重复这个过程，直到满足终止条件，如达到最大迭代次数、最大训练时间或最佳性能等。在训练过程中，RL模拟反馈回路不断地收集数据，包括观察到的状态、采取的动作、获得的奖励和所谓的结束标志，表示代理在模拟中经历的不同的回合。每次PPO代理采取一个动作（报价一个任务），都会得到一个即时奖励。下一个状态也将依赖于前一个状态，因为CPU和内存的可用性将在每次任务卸载后进行更新。最终，PPO代理应该能够学习任务的需求和优先级，并完成所有任务的报价，以最大化自身的收入和车辆的效用。

![[da316cb2d97f69253083b7f353cd11e4_MD5.png]]

# 多智能体强化学习Training Algorithm

为使用MADRL解决车联网中的任务调度问题, 我们使用了两类基于DRL的算法, 分别为Value-based 和 Policy-based. 为同时测试这两类方法, 我们使用了离散的和连续的两种动作空间. 我们假定环境中的固定参数都相同, 每一个边缘服务器具有一个Agent来根据策略$\pi_i(a_i|s_i)$来确定动作.

![[Pasted image 20230719095250.png]]

深度强化学习的工作流程如下：首先，它创建一组rollout workers，每个worker可以并行地从环境中采集数据，并使用一个或多个策略来与环境交互。每个worker生成的数据包含一系列的动作、奖励、下一个状态和结束标志，这些数据构成了一个episode或rollout。然后，RLlib将所有worker生成的rollout汇总到一个训练批次中，接着根据选择的算法和损失函数计算梯度，并更新模型参数。最后，RLlib重复这个过程，直到满足终止条件，如达到最大迭代次数、最大训练时间或最佳性能等 。

- RLlib使用rollout workers来并行地从环境中采集数据，每个worker可以使用一个或多个策略来与环境交互，产生一系列的动作、奖励、下一个状态和结束标志，这些数据构成了一个episode或rollout。
- RLlib将每个worker产生的rollout汇总到一个训练批次中，然后根据选择的算法和损失函数计算梯度，并更新模型参数。
- RLlib重复这个过程，直到满足终止条件，如达到最大迭代次数、最大训练时间或最佳性能等。

使用分布式执行以增加灵活性

---

## Competitor algorithms

![[Pasted image 20230726102735.png]]
![[Pasted image 20230726103631.png]]

---

- 我们提出了一个基于多智能体强化学习和反向拍卖机制的车联网任务卸载方法
- 我们将问题建模为一个在线和NP-hard的有时序的多维背包问题
- 我们使用部分可观察马尔可夫决策过程来建模多智能体强化学习环境
- 我们定义了状态空间、观察空间、动作空间和奖励函数
- 我们的目标是最大化边缘服务器的收益

---

## 4. Method of MADRL and Reverse-Auction

为了解决边缘计算中的车联网工作流调度问题，我们提出了一个基于多智能体强化学习和反向拍卖机制的方法。我们首先将这个问题建模成了一个在线和NP-hard问题，因为车联网的任务卸载可以看作一个有时序的多维背包问题。然后使用强化学习方法来处理复杂的动态的高维度的决策问题。我们采用多智能体强化学习来实现分布式的资源分配，避免集中式管理的通信开销和信息的不准确性，提高系统的稳定度。我们还使用了一种基于反向拍卖的任务卸载方法，用于解决车联网中的资源分配问题。反向拍卖是一种一买方多卖方的拍卖形式，适用于买方市场的交易活动。买方提出自己的需求和任务优先级，卖方根据自身资源使用情况和任务特征给出报价。买方只选择一个卖方进行交易。反向拍卖可以有效降低买方的采购成本，提高卖方之间的竞争力，实现资源的优化分配。

### 多智能体强化学习环境建模

在第三章的车联网任务卸载建模中, 边缘服务器被定义为智能体（agent）, 每个智能体通过与环境交互学习可以最大化自身收益的策略. 因为在多智能体环境中,一个智能体的动作会影响其他智能体的决策. 为了描述这种情况，我们使用部分可观察马尔可夫决策过程（partially observable Markov decision process, POMDP）来建模多智能体强化学习. 由K个智能体组成的POMDP可以被定义为

$$
(\mathbf S,\mathbf O_1,\dots,\mathbf O_K,\mathbf A_1,\dots,\mathbf A_K, \mathbf R_1,\dots,\mathbf R_K, \mathbf P_{ss'}, \mathbf P_{o|s})
$$

其中$\mathbf S$为状态空间（state space），$\mathbf O_i$为第i个智能体的观测空间（observation space），$\mathbf A_i$为第i个智能体的动作空间（action space），$\mathbf R_i$为第i个智能体的奖励函数（reward function），$\mathbf P_{ss'}$为状态转移概率（transition probability），$\mathbf P_{o|s}$为观测生成概率（observation likelihood）。在每一个时间间隙, 边缘服务器作为一个智能体, 观察当前环境生成的观测$o_i\in\mathbf O_i$, 根据策略$\pi_i(a_i|o)$选择一个合适的动作$a_i\in\mathbf A_i$. 然后获得相应的奖励$r_i=\mathbf R_i(s,a_1,\dots,a_K)$. 在本章后续部分，我们将详细介绍如何定义$\mathbf S,\mathbf O,\mathbf A,\mathbf R$以及如何设计多智能体强化学习环境.

Littman, M. L. (1994). Markov games as a framework for multi-agent reinforcement learning. In Machine learning proceedings 1994 (pp. 157-163). Morgan Kaufmann.

#### 状态空间 State Space

在强化学习方法中, 状态空间是指整个系统的描述，包含了系统中所有的信息。本次研究的是多智能体强化学习，因此状态空间应当包含所有服务器集群和任务队列的状态。状态空间可以用一个集合$\mathbf S$表示，即

$$
\mathcal S=\{\mathcal B,\mathcal S, \mathbf r_{\mathcal B}\}
$$

其中$\mathcal B$表示卖家服务器,$\mathcal S$表示买家车辆提交的任务请求, $\mathbf r_{\mathcal B}=\{r_{\mathbf s_1},\dots,r_{\mathbf s_m}\}$表示卖家服务器的资源使用情况, $r_{\mathbf s_i}=(r_{\mathbf s_i}(1),\dots,r_{\mathbf s_i}(T))$。

#### 观察空间 Observation Space

在多智能体强化学习中，观察空间是指一个智能体可以观察到的状态空间的子集。在本次研究中，我们假设智能体之间不存在额外的通信与交互，因此智能体$i$所能观察到的状态空间就是自身的服务器在从当前时间$t$到之后一定时间的资源使用情况$\mathbf r_{\mathbf s_i,t}=(r_{\mathbf s_i}(t),\dots,r_{\mathbf s_i}(t+\epsilon))$和当前等待被智能体报价的任务$\theta_j$的任务执行特征$\mathbf f_j$. 因此，边缘服务器i的观察空间为

$$
O_i=\{r_{\mathbf s_i,t},\mathbf f_j\}
$$

。这意味着每个智能体只能根据自己的服务器负载和当前任务的特征来做出决策，而无法获取其他智能体的状态信息。

#### 动作空间 Action Space

动作空间是指智能体可以向环境执行的动作的集合。在多智能体强化学习中，每个智能体都可以执行动作以影响环境的状态，从而获得奖励。不同的任务可能需要不同类型的动作空间，例如离散的或连续的。在本次研究中，我们考虑了一个多智能体服务器报价任务，其中每个智能体代表一个边缘服务器，需要对收到的任务进行报价，以竞争任务的分配。

我们假设每个智能体的动作空间是一个在$[1,2]$的数值，表示其对任务报价的浮动系数。这意味着每个智能体可以选择任意一个在$[1,2]$区间内的实数作为其报价系数$a_i$。这样可以使得智能体有更大的自由度和灵活性来调整自己的报价策略。我们可以用一个区间$\mathbf A_i$表示第$i$个智能体的动作空间，即

$$
\mathbf A_i=[1,2]
$$

当环境接收到智能体$i$的$a_i$的值后, 则其对当前等待被拍卖的任务$\theta_j$的最终报价为

$$
q_i = a_i \times \mathbf p \cdot \mathbf r_j \times \tau_j
$$

，其中$\mathbf p$是资源的市场平均价格向量, $\mathbf r_j$是资源使用量, $\tau_j$是资源使用时间。

然而，并不是所有的服务器都可以对任意的任务进行报价，因为任务有执行服务器限制$\mathcal S_j \subseteq \mathcal S$，表示只有$\mathcal S_j$中的服务器才能满足任务$j$的需求。因此，在时间步$t$，整个系统的动作空间为

$$
A_t=\{a_1 \sigma_{j,1},\dots,a_m\sigma_{j,m}\}
$$

其中，$a_i\sigma_{j,i}$代表第$i$个服务器对其当前请求报价的任务$j$的报价系数。如果某个服务器不满足任务$j$的执行服务器条件限制，则其对应的报价系数为0，表示无法执行任何动作。当对任务$j$进行报价时, 环境仅收集任务对应满足条件限制的服务器的报价, 而忽略其他服务器的报价。

#### 奖励函数 Reward Function

在多智能体强化学习中，奖励函数定义了智能体的目标和评估标准，描述了智能体在执行某一动作后，从一个状态转移到另一个状态所获得的即时奖励。智能体会根据奖励函数来判断自己的动作对于自己目标的达成是有利还是有害，从而调整自己的行为策略，以最大化自己的累积奖励。本次论文考虑了一个多智能体服务器报价任务，其中每个智能体代表一个边缘服务器，需要对收到的任务进行报价，以竞争任务的分配。边缘服务器的目标是最大化自己的收益，即任务支付与服务器成本的差额。因此，我们设计了如下的奖励函数：

$$
\mathbf R_i = p_{t,i}-c_i
$$

这个奖励函数表示第$i$个边缘服务器在一个时间步内获得的奖励，它由两部分组成：$p_{t,i}$表示边缘服务器在一个时间步内完成的任务支付之和，$c_i$表示边缘服务器在一个时间步内的固定成本。

### 基于反向拍卖的车联网任务卸载方法

任务卸载涉及到多个参与者之间的资源分配问题，即如何将车辆的任务分配给合适的边缘服务器，并使得各方都能获得最大化的效用。为了解决这一问题，本文借鉴了经济学中的反向拍卖（Reverse Auction）理论，提出了一种基于反向拍卖的任务卸载方法（Reverse Auction-based Task Offloading, RATO）。反向拍卖是一种一买方多卖方的拍卖形式，适用于买方市场（Buyer’s Market）的交易活动。在反向拍卖中，买方提出自己的需求和预算，卖方根据自身的成本和利润，竞争性地给出自己的报价。最终，买方选择一个或多个报价最低且满足需求的卖方进行交易。反向拍卖可以有效降低买方的采购成本，提高卖方之间的竞争力，实现资源的优化分配。

本文将分别介绍本模型中反向拍卖的角色和过程。

**买方**是车联网中有任务卸载需求，并愿意为其需求支付的车辆所产生的一次请求$\mathbf b_j$。在某一时刻 $t$，车辆自身评估连接状态并向满足其连接限制的边缘服务器发起任务 $\theta_j$的卸载请求, 并将其任务属性 $\mathbf{f}_j$ 发送到满足连接约束的服务器集合$\mathcal S_j$中以请求报价。

**卖方**是车联网中的边缘服务器。边缘服务器可以接收车辆的任务卸载请求，并利用其自身资源来完成任务卸载和获得收益。卖家服务器 $\mathbf s_i$ 的特征可以被定义为 $\mathbf{s}_i = (\mathbf{r}_i, c_i)$，其中资源向量 $\mathbf{r}_i$ 和成本 $c_i$。服务器在接收到报价请求后, 确定能否接受任务卸载, 如果可以接受则根据当前时刻对可用资源$\mathbf r_{s_i,t}$和任务特征$\mathbf f_j$的观测, 通过强化学习学习到的策略, 对任务进行报价.

为简化问题,本文做出以下基本假设：

- 假设时间是分时的，任务只能在一个时间段提交，并且任务时长为整数个时间段。
- 车辆可以根据自身状态，评估在拍卖和卸载的时间内的连接情况，得到任务的边缘服务器限制条件。
- 在卸载市场中存在信息缺失：
  - 卖方的特征和维护成本对买方不可见。增加拍卖的竞争性，提高买方的效用。
  - 没有卖方知道其他参与者的出价，也没有卖方相互勾结。
  - 买方的预算不能公开给卖方。
- 拍卖过程中，一次仅有一个任务参与，在任务拍卖结束后，进入本时隙的下一个任务的拍卖。防止卖方串通抬价。
- 一个买方只能把他的任务卸载给一个卖方，但一个卖方可以同时为多个买方提供服务。
- 任务卸载的结果对于车辆而言，其效用是一致的。

在基于云计算的车联网场景中，我们考虑了 $m$ 个卖家 $\mathcal S$ 和 $n$ 个买家 $\mathcal B$ 的情况。每个买家代表一个车辆，它通过自身的通信单元与边缘服务器进行通信，并且根据与边缘服务器的通信状况生成相应的限制条件，以便在需要进行任务卸载时选择合适的卖家。每个卖家代表一个边缘服务器，它可以通过有线链接与其他边缘服务器和互联网相连，以返回卸载结果。同时，边缘服务器也可以通过有线连接返回任务执行结果和传输与车辆之间的数据。算法1是反向拍卖过程的具体描述.

算法1：基于反向拍卖的任务卸载方法

输入：车辆集合$\mathcal B$，服务器集合$\mathcal S$，时间槽长度$\epsilon$

输出：任务卸载结果$\mathcal R$

1. 初始化：对于每个服务器$\mathbf s_i \in \mathcal S$，初始化其资源向量$\mathbf r_i$，成本$c_i$，报价策略$\pi_i$；初始化卸载任务集合$\mathcal U=\{\mathcal U_1,\dots,\mathcal U_T\}$。
2. 对于每个时间槽$t=1,2,\dots,T$，执行以下步骤：
   1. 对于服务器集群内的每一个服务器$\mathbf s_i \in \mathcal S$, 更新自己的资源使用情况和结算上一个时间槽的收益$p_{t-1,i}-c_i$
   2. 从$\mathcal U_t$中的按照时间顺序选择买家车辆请求$\mathbf b_j$，则执行以下步骤：
      1. 车辆评估自身的连接状态，并生成任务的服务器限制向量$\mathbf \sigma_j$。
      2. 车辆向满足限制条件的服务器集合$\mathcal S_j = \{\mathbf s_i | \forall \mathbf s_i \in \mathcal S, \sigma_{j,i} = 1\}$发送任务卸载请求$\mathbf f_j$。
      3. 对于每个服务器$\mathbf s_i \in \mathcal S_j$，如果其可用资源$r_{s_i}(t)$能够满足任务的资源需求$r_{\theta_j}(t)$，则执行以下步骤：
         1. 服务器根据自身在$[t,t+\epsilon]$时间段的资源使用情况$\mathbf r_{s_i,t}$和任务特征$\mathbf f_j$，以及报价策略$\pi_i$，计算出自己对任务的报价$q_{j,i}$。
         2. 服务器将报价$q_{j,i}$返回给车辆。
      4. 车辆收集所有服务器的报价$\mathcal Q_j = \{q_{j,i} | \forall \mathbf s_i \in \mathcal S_j\}$，并将其非降序排列。
      5. 车辆选择报价最低的服务器作为胜者$\mathbf s_k =\mathop{\arg\min}\limits_{\mathbf s_i}\ q_{j,i}$，并检查其报价$p_j = q_{j,k}$是否满足预算条件$p_j \leq \psi_j$。如果不满足，则本次拍卖失败，任务卸载失败；如果满足，则本次拍卖成功，任务卸载成功，并执行以下步骤：
         1. 车辆将任务发送到胜出者服务器$\mathbf s_k$并支付$p_j$，等待执行结果返回。
         2. 服务器$\mathbf s_k$获得收益, 为任务分配资源并执行任务卸载，更新自身的资源使用情况$\mathbf r_{s_i,t}$
      6. 更新任务卸载结果$\mathcal R$
3. 当到达总时间步$T$时间后, 卸载结束

### PPO算法

在上文中我们已经介绍了MDP建模和环境建模，本节我们将介绍一种基于策略梯度的强化学习算法——PPO算法[1]。PPO算法的目标是在与环境交互采样数据后，使用随机梯度上升优化一个“替代”目标函数，从而改进策略。PPO算法的核心思想是利用按照策略产生的执行轨迹来估计梯度，并使用经验回报或优势函数作为梯度的方向。PPO算法可以处理具有连续状态和动作空间的强化学习环境。

PPO算法的目标函数是由优势值进行缩放的新策略和旧策略之间的比率，即：

$$
J_{\theta}=E_{t}\left[\frac{\pi_{\theta}(a_{t}\mid s_{t})}{\pi_{{\theta_{\mathrm{old}}}}(a_{_t}\mid s_{t})}A^{\mathrm{GAE}(\gamma, \lambda)}_{t}\right]
$$

其中$\pi_{{\theta_{\mathrm{old}}}}$表示更新前的旧策略。这个目标与交叉熵方法有相同的原理：重要性采样。为了限制更新，PPO算法使用了裁剪后的目标：

$$
J_{\theta}^{\mathrm{clip}}=\mathbb{E}_{t}\left[\min(r_{t}(\theta)A^{\mathrm{GAE}(\gamma, \lambda)}_{t},\mathrm{clip}(r_{t}(\theta),1-\varepsilon,1+\varepsilon)A^{\mathrm{GAE}(\gamma, \lambda)}_{t})\right]
$$

此目标将新旧策略之间的比率限制在$[1–ε, 1+ε]$区间内，因此通过更改$ε$，我们可以限制更新的大小。这种裁剪方法可以避免使用KL散度惩罚或自适应更新，从而简化了算法。

PPO算法有两种主流的实现方式，一种是使用KL散度作为惩罚项来限制策略更新的幅度，另一种是使用剪裁函数来限制新旧策略之间的比率。PPO算法还使用了一种更一般的优势函数估计方式，即GAE（Generalized Advantage Estimation）。GAE是由John Schulman等人在2015年的论文[2]中提出的。GAE的基本思想是利用TD($\lambda$)方法来平衡偏差和方差，同时减少优势函数的方差。GAE的定义如下：

$$
A^{\mathrm{GAE}(\gamma, \lambda)}_{t}=\sum_{l=0}^{\infty}(\gamma \lambda)^{l} \delta_{t+l}
$$

其中$\delta_t$是TD误差，即$\delta_t = R_t + \gamma V_\theta(S_{t+1}) - V_\theta(S_t)$。可以看出，GAE是对TD误差的加权累加，其中权重随着$l$的增加而指数衰减。当$\lambda=0$时，GAE退化为一步TD误差；当$\lambda=1$时，GAE退化为MC估计。因此，通过调节$\lambda$的值，我们可以在偏差和方差之间找到一个平衡点。GAE还有一个重要的性质是，它可以消除优势函数的常数偏移量。

为了解决第三章提出的车联网环境中的任务卸载问题，我们采用以上描述的PPO算法。这种算法可以根据任务的需求和优先级，直接学习最优策略，而不需要像Q-Learning等基于值函数的算法那样估计状态-动作值。

我们使用多智能体强化学习的框架，让每个边缘服务器都部署一个PPO代理，根据自己的状态和动作来更新自己的策略。这样，不同的服务器之间的策略可以根据任务需求和市场竞争进行自适应调整，从而提高自身的收入和车辆的效用。同时，多智能体强化学习也可以处理多个智能体之间的交互和博弈，使得每个服务器能够考虑其他服务器的行为和影响。为了简化模型，我们假设服务器之间没有额外的通信，而且只进行一次博弈。

接下来，我们将介绍我们的强化学习仿真环境的具体设计, 展示我们的实验结果和分析，比较PPO算法与其他基准算法的性能和效率。


---

**行动空间。** 行动空间是一个智能体可以在环境中执行的动作集合。我们假设每个智能体的行动空间是在范围$[1,2]$内的数值，表示其用于竞标任务的浮点系数。这意味着每个智能体可以选择范围$[1,2]$内的任意实数作为其竞标系数$a_i$。这为智能体提供了更多调整其竞标策略的自由度和灵活性。我们可以使用范围$\mathbf A_i$来表示第$i$个智能体的行动空间，如下所示：$\mathbf A_i=[1,2]$ 当环境从智能体$i$处接收到$a_i$的值时，那么它对于当前等待拍卖的任务$\theta_j$的最终出价为：$q_i = a_i \times \mathbf p \cdot \mathbf r_j \times \tau_j$ 其中$\mathbf p$是资源的市场平均价格向量，$\mathbf r_j$是资源使用情况，$\tau_j$是资源使用时间。然而，并非所有服务器都可以对任何任务进行竞标，因为任务有执行服务器限制$\mathcal S_j \subseteq \mathcal S$，这意味着只有在$\mathcal S_j$中的服务器才能满足任务$j$的要求。因此，在时间步$t$中，整个系统的行动空间是：$A_t=\{a_1\sigma_{j,1},\dots,a_m\sigma_{j,m}\}$ 其中$a_i\sigma_{j,i}$表示第$i$个服务器对当前被请求的任务$\theta_j$的竞标系数。如果服务器不符合任务$\theta_j$的执行服务器条件，则其相应的竞标系数为0，表示它无法采取任何行动。在对$\theta_j$任务进行竞标时，环境仅收集满足任务条件的服务器的竞标，并忽略其他服务器的竞标。