{
	"nodes":[
		{"id":"286bdbd3744bbcd2","type":"text","text":"- There have been several research works on VEC:\n  - Zhang et al. [12] proposed a task offloading scheme based on the Stackelberg game theory to maximize the utilities of both vehicles and MEC servers.\n  - Zhang et al. [13] introduced an efficient predictive combination-mode offloading mechanism to reduce the offloading cost.\n  - Dai et al. [14] developed a joint optimal VEC server selection and offloading algorithm to maximize the system utility.\n  - Zhou et al. [15] proposed an energy-efficient resource allocation algorithm based on the alternating direction method of multipliers.\n  - Zhu et al. [16] developed a dynamic task allocation solution to ensure the quality of service.\n\n- The prior studies assume that applications consisting of independent tasks are offloaded to RSUs for execution.\n\n- However, task execution order depends on task dependency, and the effect of task dependency on the execution time of applications has not been considered in the previous research works.\n\n- For an augmented vehicular reality system with the following major components: object tracking, model mapping, object recognition, perspective transformation, and merging processing, there are some task dependencies among the components.\n\n- For example, only after one vehicle is tracked, the surrounding environmental model of the vehicle can be built, and only after one vehicle is recognized, the process of perspective transformation and merging processing can be executed.\n\n- To ensure that multiple vehicular applications can be completed in time, it is necessary to take task dependency into account for task scheduling policies design.\n\n- To overcome the above drawbacks, in this article, we consider the task dependency and the completion time constraints when scheduling tasks into multiple MEC servers.\n\n- The goal of the work is to identify task scheduling decisions that minimize the average completion time of multiple applications, subject to their respective completion time constraints.\n\n- We first present a VEC architecture.\n\n- Then, we specify the completion time constraint of each application and the task dependency requirements of tasks.\n\n- Finally, we propose an efficient task scheduling algorithm to minimize the average completion time of multiple applications.\n\n- To overcome the above drawbacks, in this article, we consider the task dependency and the completion time constraints when scheduling tasks into multiple MEC servers.\n\n- The goal of the work is to identify task scheduling decisions that minimize the average completion time of multiple applications, subject to their respective completion time constraints.\n\n- We first present a VEC architecture.\n\n- Then, we specify the completion time constraint of each application and the task dependency requirements of tasks.\n\n- Finally, we propose an efficient task scheduling algorithm to minimize the average completion time of multiple applications.\n\n","x":-36895,"y":-2774,"width":460,"height":1757,"color":"6"},
		{"id":"1dab1d1b7467470a","type":"text","text":"![[Pasted image 20231219151604.png]]","x":-36895,"y":-959,"width":460,"height":909,"color":"6"},
		{"id":"7c8ebbea53acbd4d","type":"text","text":"- **Prior Research Works on VEC:**\n  - Zhang et al. [12]: Proposed a Stackelberg game theory-based task offloading scheme to maximize utilities of vehicles and MEC servers.\n  - Zhang et al. [13]: Introduced an efficient predictive combination-mode offloading mechanism to reduce offloading cost.\n  - Dai et al. [14]: Developed a joint optimal VEC server selection and offloading algorithm to maximize system utility.\n  - Zhou et al. [15]: Proposed an energy-efficient resource allocation algorithm based on the alternating direction method of multipliers.\n  - Zhu et al. [16]: Developed a dynamic task allocation solution to ensure quality of service.\n\n- **Task Dependency Considerations:**\n  - Previous studies assumed independent task offloading to RSUs for execution.\n  - Overlooked task execution order dependency and its impact on application execution time.\n\n- **Task Dependency in Augmented Vehicular Reality System:**\n  - Components: object tracking, model mapping, object recognition, perspective transformation, and merging processing.\n  - Dependencies: Vehicle tracking precedes environmental model building, and vehicle recognition precedes perspective transformation and merging processing.\n\n- **Objectives of the Article:**\n  - Consider task dependency and completion time constraints in scheduling tasks across multiple MEC servers.\n  - Goal: Identify task scheduling decisions minimizing average completion time of multiple applications while adhering to respective completion time constraints.\n\n- **Methodology:**\n  - Presentation of VEC architecture.\n  - Specification of completion time constraints for each application and task dependency requirements.\n  - Proposal of an efficient task scheduling algorithm to minimize average completion time of multiple applications.\n\n- **Key Contributions of the Article:**\n  - Integration of task dependency and completion time constraints into task scheduling for multiple MEC servers.\n  - Development of an efficient algorithm aimed at minimizing the average completion time of multiple applications.\n  - Focus on addressing drawbacks of prior research by considering dependencies and constraints in task scheduling.","x":-36395,"y":-2380,"width":781,"height":970,"color":"5"},
		{"id":"5c84ccba978a197b","type":"text","text":"- **VEC的先前研究工作:**\n  - Zhang等人[12]: 提出了一种基于Stackelberg博弈理论的任务卸载方案，以最大化车辆和MEC服务器的效用。\n  - Zhang等人[13]: 引入了一种高效的预测组合模式卸载机制，以减少卸载成本。\n  - Dai等人[14]: 开发了一种联合最优的VEC服务器选择和卸载算法，以最大化系统效用。\n  - Zhou等人[15]: 提出了一种基于交替方向乘子法的能效资源分配算法。\n  - Zhu等人[16]: 制定了一种动态任务分配解决方案，以确保服务质量。\n\n- **考虑任务依赖性:**\n  - 以前的研究假设任务独立卸载到RSUs进行执行。\n  - 忽略了任务执行顺序依赖性及其对应用执行时间的影响。\n\n- **在增强现实车载系统中的任务依赖性:**\n  - 组件: 目标跟踪、模型映射、目标识别、透视变换和合并处理。\n  - 依赖关系: 车辆跟踪在环境模型构建之前，车辆识别在透视变换和合并处理之前。\n\n- **文章的目标:**\n  - 考虑跨多个MEC服务器调度任务时的任务依赖性和完成时间约束。\n  - 目标: 在遵循各自完成时间约束的情况下，确定最小化多个应用程序的平均完成时间的任务调度决策。\n\n- **方法论:**\n  - 展示VEC体系结构。\n  - 为每个应用程序规定完成时间约束和任务依赖性要求。\n  - 提出了一种有效的任务调度算法，以最小化多个应用程序的平均完成时间。\n\n- **文章的主要贡献:**\n  - 将任务依赖性和完成时间约束整合到多个MEC服务器的任务调度中。\n  - 开发了一种旨在最小化多个应用程序的平均完成时间的高效算法。\n  - 通过考虑任务调度中的依赖关系和约束，专注于解决先前研究的缺点。","x":-35574,"y":-2248,"width":744,"height":706,"color":"1"},
		{"id":"52e1447da0b981a3","type":"text","text":"- **VEC的先前研究工作:**\n  - Zhang等人[12]: 提出了一种基于Stackelberg博弈理论的任务卸载方案，以最大化车辆和MEC服务器的效用。\n  - Zhang等人[13]: 引入了一种高效的预测组合模式卸载机制，以减少卸载成本。\n  - Dai等人[14]: 开发了一种联合最优的VEC服务器选择和卸载算法，以最大化系统效用。\n  - Zhou等人[15]: 提出了一种基于交替方向乘子法的能效资源分配算法。\n  - Zhu等人[16]: 制定了一种动态任务分配解决方案，以确保服务质量。\n\n- **考虑任务依赖性:**\n  - 以前的研究假设任务独立卸载到RSUs进行执行。\n  - 忽略了任务执行顺序依赖性及其对应用执行时间的影响。\n\n- **在增强现实车载系统中的任务依赖性:**\n  - 组件: 目标跟踪、模型映射、目标识别、透视变换和合并处理。\n  - 依赖关系: 车辆跟踪在环境模型构建之前，车辆识别在透视变换和合并处理之前。\n\n- **文章的目标:**\n  - 考虑跨多个MEC服务器调度任务时的任务依赖性和完成时间约束。\n  - 目标: 在遵循各自完成时间约束的情况下，确定最小化多个应用程序的平均完成时间的任务调度决策。\n\n- **方法论:**\n  - 展示VEC体系结构。\n  - 为每个应用程序规定完成时间约束和任务依赖性要求。\n  - 提出了一种有效的任务调度算法，以最小化多个应用程序的平均完成时间。\n\n- **文章的主要贡献:**\n  - 将任务依赖性和完成时间约束整合到多个MEC服务器的任务调度中。\n  - 开发了一种旨在最小化多个应用程序的平均完成时间的高效算法。\n  - 通过考虑任务调度中的依赖关系和约束，专注于解决先前研究的缺点。","x":-34790,"y":-2236,"width":880,"height":682,"color":"3"},
		{"id":"565aeb9be3eadabb","type":"text","text":"- VEC的先前研究工作:\n- 考虑任务依赖性:\n- 在增强现实车载系统中的任务依赖性:\n- 文章的目标:\n- 方法论:\n- 文章的主要贡献:","x":-33870,"y":-2014,"width":480,"height":238,"color":"4"},
		{"id":"8a7016c5db0d8166","type":"text","text":"\n### Dependency-Aware Task Scheduling in Vehicular Edge Computing\n\nLiu, Yujiong, Shangguang Wang, Qinglin Zhao, Shiyu Du, Ao Zhou, Xiao Ma和Fangchun Yang. 《Dependency-Aware Task Scheduling in Vehicular Edge Computing》. *IEEE Internet of Things Journal* 7, 期 6 (2020年6月): 4961–71. <https://doi.org/10.1109/JIOT.2020.2972041>.\n\n\n\nThere has been some research works on VEC. Zhang et al. [12] proposed a task offloading scheme based on the Stackelberg game theory to maximize the utilities of both vehicles and MEC servers. Zhang et al. [13] introduced an efficient predictive combination-mode offloading mechanism to reduce the offloading cost. Dai et al. [14] developed a joint optimal VEC server selection and offloading algorithm to maximize the system utility. Zhou et al. [15] proposed an energy-efficient resource allocation algorithm based on the alternating direction method of multipliers. Zhu et al. [16] developed a dynamic task allocation solution to ensure the quality of service. The prior studies assume that applications consisting of independent tasks are offloaded to RSUs for execution. However, task execution order depends on task dependency and the effect of task dependency on the execution time of applications has not been considered in the previous research works. For an augmented vehicular reality system with the following major components: object tracking, model mapping, object recognition, perspective transformation, and merging processing, there are some task dependencies among the components, e.g., only after one vehicle is tracked, the surrounding environmental model of the vehicle can be built and only after one vehicle is recognized, the process of perspective transformation and merging processing can be executed. To ensure that multiple vehicular applications can be completed in time, it is necessary to take task dependency into account for task scheduling policies design.\n\nTo overcome the above drawbacks, in this article, we consider the task dependency and the completion time constraints when scheduling tasks into multiple MEC servers. The goal of the work is to identify task scheduling decision that minimizes the average completion time of multiple applications, subject to their respective completion time constraints. We first present a VEC architecture. Then, we specify the completion time constraint of each application and the task dependency requirements of tasks. Finally, we propose an efficient task scheduling algorithm to minimize the average completion time of multiple applications.\n\nThe main contributions of this article are as follows.\n\n1. We propose a VEC architecture which consists of multiple vehicles, multiple RSUs, and multiple MEC servers. Each vehicle has a computation-intensive and delay-sensitive application. Each RSU is equipped with multiple MEC servers. Multiple vehicles can offload their computation-intensive and delay-sensitive applications to MEC servers on RSUs for execution where applications are independent of each other but tasks (belonging to the same application) have processing dependence.\n2. We formalize the task scheduling decision problem as an optimization problem which is NP-hard, and then propose an efficient multiple applications multiple tasks scheduling (MAMTS) algorithm to solve the optimization problem. Furthermore, we prioritize multiple applications to meet their respective completion time constraints and prioritize multiple tasks for satisfying their processing dependency requirements.\n3. We evaluate the proposed task scheduling algorithm with extensive simulations. The simulation results show that our proposed algorithm can significantly reduce the average completion time of multiple applications compared with benchmark algorithms.","x":-37640,"y":-2160,"width":527,"height":1805},
		{"id":"23fd185fed78b759","type":"text","text":"- **Background:**\n  - The continuous improvement of relevant standards and the increase in intelligent vehicles point towards a future where more vehicles achieve network interconnection through relevant protocols.\n\n- **Challenge:**\n  - With the growing number of vehicles, addressing road hazards is a crucial challenge in the development of the Internet of Vehicles (IoV) [10].\n    - Vehicle safety business communication demands high timeliness and reliability, especially in scenarios like automatic driving where the delay requirement may need to be lower than 10 ms [11,12].\n    - Factors such as channel congestion, channel interference, shadow fading, and intelligent computing processing significantly affect communication performance in the vehicle communication process based on IEEE 802.11P and LTE-V protocols.\n\n- **Importance:**\n  - Research on the transmission strategy of IoV security services gains importance, particularly in scheduling computing and communication resources to enhance vehicle safety business communication performance [11,12].\n\n- **Solution:**\n  - The proposed scheduling strategy is based on an IoV system with multi-area, multi-user, and multi-Mobile Edge Computing (MEC) server configuration.\n    - Introduces a vehicle distance prediction method based on Kalman filtering, considering the mobility of IoV users.\n    - Formulates the optimization goal as minimizing the total cost of communication delay and energy consumption for all users.\n    - Employs the Double Deep Q-Network (DQN) algorithm to solve the optimal scheduling strategy for minimizing the total consumption cost of the system.","x":-36395,"y":204,"width":781,"height":706,"color":"5"},
		{"id":"8bc59f8490d174fd","type":"text","text":"- **背景:**\n  - 相关标准的不断改进和智能车辆数量的增加表明未来将有更多车辆通过相关协议实现网络互连。\n\n- **挑战:**\n  - 随着车辆数量的增加，解决道路危险是发展车联网（IoV）的关键挑战[10]。\n    - 车辆安全业务通信要求高度的及时性和可靠性，特别是在自动驾驶等场景中，延迟要求可能需要低于10毫秒[11,12]。\n    - 诸如信道拥塞、信道干扰、阴影衰落和智能计算处理等因素显著影响基于IEEE 802.11P和LTE-V协议的车辆通信过程中的通信性能。\n\n- **重要性:**\n  - 对IoV安全服务传输策略的研究变得重要，特别是在调度计算和通信资源以提高车辆安全业务通信性能方面[11,12]。\n\n- **解决方案:**\n  - 所提出的调度策略基于具有多区域、多用户和多移动边缘计算（MEC）服务器配置的IoV系统。\n    - 引入基于卡尔曼滤波的车辆距离预测方法，考虑IoV用户的移动性。\n    - 将优化目标明确为最小化所有用户的通信延迟和能耗的总成本。\n    - 使用Double Deep Q-Network (DQN)算法来解决最小化系统总消耗成本的最佳调度策略。","x":-35574,"y":288,"width":744,"height":538,"color":"1"},
		{"id":"702cb167ea561453","type":"text","text":"- **背景:**\n  - 相关标准的不断改进和智能车辆数量的增加表明未来将有更多车辆通过相关协议实现网络互连。\n\n- **挑战:**\n  - 随着车辆数量的增加，解决道路危险是发展车联网（IoV）的关键挑战[10]。\n    - 车辆安全业务通信要求高度的及时性和可靠性，特别是在自动驾驶等场景中，延迟要求可能需要低于10毫秒[11,12]。\n    - 诸如信道拥塞、信道干扰、阴影衰落和智能计算处理等因素显著影响基于IEEE 802.11P和LTE-V协议的车辆通信过程中的通信性能。\n\n- **重要性:**\n  - 对IoV安全服务传输策略的研究变得重要，特别是在调度计算和通信资源以提高车辆安全业务通信性能方面[11,12]。\n\n- **解决方案:**\n  - 所提出的调度策略基于具有多区域、多用户和多移动边缘计算（MEC）服务器配置的IoV系统。\n    - 引入基于卡尔曼滤波的车辆距离预测方法，考虑IoV用户的移动性。\n    - 将优化目标明确为最小化所有用户的通信延迟和能耗的总成本。\n    - 使用Double Deep Q-Network (DQN)算法来解决最小化系统总消耗成本的最佳调度策略。","x":-34790,"y":324,"width":880,"height":466,"color":"3"},
		{"id":"2486e59ebcacff71","type":"text","text":"- 背景:\n- 挑战:\n- 重要性:\n- 解决方案:","x":-33870,"y":462,"width":200,"height":189,"color":"4"},
		{"id":"e283e054643c750e","type":"text","text":"### A Collaborative Scheduling Strategy for IoV Computing Resources Considering Location Privacy Protection in Mobile Edge Computing Environment\nPang, Meiyu, Li Wang和Ningsheng Fang. 《A Collaborative Scheduling Strategy for IoV Computing Resources Considering Location Privacy Protection in Mobile Edge Computing Environment》. Journal of Cloud Computing 9, 期 1 (2020年12月): 52. https://doi.org/10.1186/s13677-020-00201-x.\n\nWith the continuous improvement of relevant standards and continuous increase of intelligent vehicles, it is foreseeable that more and more vehicles will realize network interconnection by relevant protocols in the future. With the increasing number of vehicles, road hazards have become a problem that must be faced in the development of IoV [10]. Besides, the communication transmission of vehicle safety business has higher timeliness and reliability requirements. In some application scenarios of IoV, such as automatic driving, the delay requirement even needs to be lower than 10 ms. This makes the research on the transmission strategy of IoV security services more and more important [11,12]. In the vehicle communication process based on IEEE 802.11P and LTE-V protocols, channel congestion, channel interference, shadow fading and intelligent computing processing are main factors that affect the communication performance of vehicles. How to schedule computing resources and communication resources in IoV to improve the communication performance of vehicle safety business has important research value. Besides, the proposed scheduling strategy is based on an IoV system of multi-area multi-user multi-MEC server. A vehicle distance prediction method based on Kalman filtering is proposed combined with the mobility of IoV users in this paper. Furthermore, the total cost of communication delay and energy consumption of all users is formulated as the optimization goal, the Double DQN algorithm is used to solve the optimal scheduling strategy for minimizing the total consumption cost of system.\n\n\n","x":-37462,"y":72,"width":527,"height":970},
		{"id":"5d6cf91d30077f64","type":"text","text":"- **Background** The continuous improvement of relevant standards and the increase of intelligent vehicles indicate a future where more vehicles will achieve network interconnection through relevant protocols.\n- **Challenge** With the growing number of vehicles, addressing road hazards becomes a crucial challenge in the development of the Internet of Vehicles (IoV) [10].\n\t- Vehicle safety business communication requires high timeliness and reliability, especially in scenarios like automatic driving where the delay requirement may need to be lower than 10 ms [11,12].\n\t- In the vehicle communication process based on IEEE 802.11P and LTE-V protocols, factors such as channel congestion, channel interference, shadow fading, and intelligent computing processing significantly affect communication performance.\n- **Importance** Research on the transmission strategy of IoV security services gains importance, particularly in scheduling computing and communication resources to enhance vehicle safety business communication performance [11,12].\n- **Solution** The proposed scheduling strategy is based on an IoV system with multi-area, multi-user, and multi-Mobile Edge Computing (MEC) server configuration.\n\t- A vehicle distance prediction method based on Kalman filtering is introduced, considering the mobility of IoV users.\n\t- The optimization goal is formulated as minimizing the total cost of communication delay and energy consumption for all users.\n\t- The Double Deep Q-Network (DQN) algorithm is employed to solve the optimal scheduling strategy for minimizing the total consumption cost of the system.","x":-36895,"y":40,"width":460,"height":1033,"color":"6"},
		{"id":"aba8b6138abd39c0","type":"text","text":"- **MEC-Based IoT Networks:**\n  - In MEC-based IoT networks, devices can offload computation tasks to the MEC server, improving task processing speed and saving device energy [15]–[17].\n  - The primary technical challenge is determining when, whether, and how many computation tasks should be offloaded.\n  - A significant body of literature focuses on designing optimal strategies under various performance requirements [18]–[21].\n  - For long-term energy efficiency in IoT networks, [18] proposes an efficient edge computing infrastructure.\n  - Addressing stochastic task arrivals, wireless channel variations, congested air interfaces, and prohibitive feedback, [19] generates asymptotically optimal schedules tolerant to out-of-date network knowledge, relieving stringent requirements on feedback.\n  - Optimal scheduling solutions and energy-efficient resource allocation policies for MEC are proposed in [20] and [21], respectively.\n\n- **Vehicular Edge Computing Networks:**\n  - Researchers extensively explore the structure and resource allocation methods in vehicular edge computing networks, given the significant role of vehicles as User Equipment (UE) in IoT systems [22]–[27].\n  - C. Wang et al. propose a scalable SDN-enabled MEC architecture integrating a heterogeneous vehicular network to decrease overall delay and offload traffic load from the backbone network [22].\n  - To address latency and transmission cost of computation offloading, a cloud-based MEC offloading framework is proposed for vehicular networks in [23].\n  - In [24], a target server selection policy is presented, based on the MEC-assisted vehicular offloading mode, aiming to improve task offloading reliability in the case of vehicular data transmission failure.\n  - Novel MEC-based vehicular networks are proposed in [25], [26], where computation offloading policies are carefully designed for different scenarios.\n  - Reinforcement learning is employed for long-term resource provision in vehicular clouds, addressing dynamic demands for resources and stringent Quality of Service (QoS) requirements [27].\n  - However, in existing literature, vehicles predominantly serve as users in the MEC network, where edge servers are statically deployed and may lead to \"service holes\" due to an explosion of service requests from a tremendous number of User Equipments (UEs).\n\n- **Introduction to Vehicle Edge Computing (VEC) Network:**\n  - The paper focuses on designing a Vehicle Edge Computing (VEC) network, empowering vehicles to provide computation services alongside traditional edge servers.\n  - Traditional edge servers, connected to road-side units, small-cell base stations, etc., have fixed locations, and the proposed architecture aims to extend computation service range, enhancing the flexibility of the Mobile Edge Computing (MEC) network.\n\n- **Efficient Computation Offloading Scheme and Optimization Problem:**\n  - The paper proposes an efficient computation offloading scheme for User Equipments (UEs), considering the delay of computation tasks generated by UEs.\n  - An optimization problem is formulated to maximize the total utility of the proposed VEC network.\n\n- **Addressing Stochastic Traffic and Communication Uncertainty:**\n  - The stochastic traffic and communication uncertainty in the vehicular communication environment are crucial challenges to be addressed.\n  - The Q-learning method, a model-free reinforcement learning approach, is chosen due to its suitability for solving problems in the vehicle edge computing network where environment elements are not known [32].\n  - The paper introduces deep reinforcement learning (DRL), which uses deep neural networks to approximate the Q-function, providing greater performance and more robust learning [33]–[36].\n  - The proposed problem is further formulated as a semi-Markov process, and two reinforcement learning methods (Q-learning and DRL) are proposed to determine the policies of computation offloading and resource allocation.\n\n- **Main Contributions:**\n  1. The proposal of a vehicle edge computing network architecture where vehicles can provide computation services for UEs, enhancing the flexibility and scalability of the MEC-based IoT system.\n  2. Introduction of an efficient offloading scheme for the vehicle edge computing network, considering both delay and limited computation capabilities of vehicles and edge servers. Formulation of an optimization problem to maximize the total utility of the network.\n  3. Addressing stochastic traffic and uncertain communication conditions by reformulating the problem as a semi-Markov process. Proposing Q-learning-based reinforcement learning and DRL methods to determine policies for computation offloading and resource allocation.","x":-36395,"y":-4873,"width":781,"height":1755,"color":"5"},
		{"id":"ab53462f070e57c8","type":"text","text":"- **基于MEC的物联网（IoT）网络:**\n  - 在基于MEC的物联网中，设备可以将计算任务卸载到MEC服务器，提高任务处理速度并节省设备能量 [15]–[17]。\n  - 主要技术挑战是确定何时、是否以及有多少计算任务应该被卸载。\n  - 大量文献集中在设计在不同性能要求下的最佳策略 [18]–[21]。\n  - 为了实现物联网网络的长期能效，[18] 提出了一种高效的边缘计算基础设施。\n  - 针对随机任务到达、无线信道变化、拥塞的空中接口和禁止性反馈，[19] 生成了对网络知识陈旧具有容忍性的渐近最优调度，减轻了对反馈的严格要求。\n  - 在 [20] 和 [21] 中分别提出了MEC的最优调度解决方案和能效资源分配策略。\n\n- **车辆边缘计算网络:**\n  - 鉴于车辆在物联网系统中作为用户设备（UE）的重要角色，研究人员广泛探索了车辆边缘计算网络的结构和资源分配方法 [22]–[27]。\n  - C. Wang等人提出了一种可扩展的SDN启用的MEC架构，集成了异构车辆网络，以减少总体延迟并卸载骨干网络的流量负载 [22]。\n  - 为了解决计算卸载的延迟和传输成本，[23] 提出了一种面向车辆网络的基于云的MEC卸载框架。\n  - 在 [24] 中，提出了一种基于MEC辅助的车辆卸载模式的目标服务器选择策略，旨在提高在车辆数据传输失败的情况下任务卸载的可靠性。\n  - 在 [25]、[26] 中提出了新颖的基于MEC的车辆网络，为不同场景精心设计了计算卸载策略。\n  - 强化学习被用于车辆云中的长期资源供应，以解决资源的动态需求和严格的服务质量（QoS）要求 [27]。\n  - 然而，在现有文献中，车辆主要充当MEC网络中的用户，其中边缘服务器静态部署可能导致由于庞大数量的用户设备（UEs）的服务请求而产生“服务空白”。\n\n- **车辆边缘计算（VEC）网络介绍:**\n  - 本文重点设计了一种车辆边缘计算（VEC）网络，使车辆能够在传统边缘服务器之外提供计算服务。\n  - 传统的边缘服务器连接到路侧单元、小区基站等固定位置，而提出的架构旨在扩展计算服务范围，增强移动边缘计算（MEC）网络的灵活性。\n\n- **高效的计算卸载方案和优化问题:**\n  - 本文提出了一种针对用户设备（UEs）的高效计算卸载方案，考虑到由UEs生成的计算任务的延迟。\n  - 制定了一个优化问题，旨在最大化提出的VEC网络的总效用。\n\n- **解决随机流量和通信不确定性:**\n  - 车辆通信环境中的随机流量和通信不确定性是需要解决的关键挑战。\n  - 选择了Q学习方法，这是一种无模型的强化学习方法，由于其适用于车辆边缘计算网络中环境元素未知的问题 [32]。\n  - 本文引入了深度强化学习（DRL），使用深度神经网络来近似Q函数，提供更高的性能和更强大的学习 [33]–[36]。\n  - 将提出的问题进一步制定为半马尔可夫过程，并提出了两种强化学习方法（Q学习和DRL）来确定计算卸载和资源分配的策略。\n\n- **主要贡献:**\n  1. 提出了一种车辆边缘计算网络架构，使车辆能够为UE提供计算服务，增强了MEC-based IoT系统的灵活性和可伸缩性。\n  2. 引入了一种高效的车辆边缘计算网络的卸载方案，考虑了车辆和边缘服务器的延迟和有限的计算能力。制定了一个优化问题，以最大化网络的总效用。\n  3. 通过将问题进一步制定为半马尔可夫过程，解决了随机流量和不确定通信条件。提出了基于Q学习的强化学习和DRL方法，确定计算卸载和资源分配的策略。","x":-35574,"y":-4633,"width":744,"height":1275,"color":"1"},
		{"id":"b6ba70757acfcb52","type":"text","text":"- **基于MEC的物联网（IoT）网络:**\n  - 在基于MEC的物联网中，设备可以将计算任务卸载到MEC服务器，提高任务处理速度并节省设备能量 [15]–[17]。\n  - 主要技术挑战是确定何时、是否以及有多少计算任务应该被卸载。\n  - 大量文献集中在设计在不同性能要求下的最佳策略 [18]–[21]。\n  - 为了实现物联网网络的长期能效，[18] 提出了一种高效的边缘计算基础设施。\n  - 针对随机任务到达、无线信道变化、拥塞的空中接口和禁止性反馈，[19] 生成了对网络知识陈旧具有容忍性的渐近最优调度，减轻了对反馈的严格要求。\n  - 在 [20] 和 [21] 中分别提出了MEC的最优调度解决方案和能效资源分配策略。\n\n- **车辆边缘计算网络:**\n  - 鉴于车辆在物联网系统中作为用户设备（UE）的重要角色，研究人员广泛探索了车辆边缘计算网络的结构和资源分配方法 [22]–[27]。\n  - C. Wang等人提出了一种可扩展的SDN启用的MEC架构，集成了异构车辆网络，以减少总体延迟并卸载骨干网络的流量负载 [22]。\n  - 为了解决计算卸载的延迟和传输成本，[23] 提出了一种面向车辆网络的基于云的MEC卸载框架。\n  - 在 [24] 中，提出了一种基于MEC辅助的车辆卸载模式的目标服务器选择策略，旨在提高在车辆数据传输失败的情况下任务卸载的可靠性。\n  - 在 [25]、[26] 中提出了新颖的基于MEC的车辆网络，为不同场景精心设计了计算卸载策略。\n  - 强化学习被用于车辆云中的长期资源供应，以解决资源的动态需求和严格的服务质量（QoS）要求 [27]。\n  - 然而，在现有文献中，车辆主要充当MEC网络中的用户，其中边缘服务器静态部署可能导致由于庞大数量的用户设备（UEs）的服务请求而产生“服务空白”。\n\n- **车辆边缘计算（VEC）网络介绍:**\n  - 本文重点设计了一种车辆边缘计算（VEC）网络，使车辆能够在传统边缘服务器之外提供计算服务。\n  - 传统的边缘服务器连接到路侧单元、小区基站等固定位置，而提出的架构旨在扩展计算服务范围，增强移动边缘计算（MEC）网络的灵活性。\n\n- **高效的计算卸载方案和优化问题:**\n  - 本文提出了一种针对用户设备（UEs）的高效计算卸载方案，考虑到由UEs生成的计算任务的延迟。\n  - 制定了一个优化问题，旨在最大化提出的VEC网络的总效用。\n\n- **解决随机流量和通信不确定性:**\n  - 车辆通信环境中的随机流量和通信不确定性是需要解决的关键挑战。\n  - 选择了Q学习方法，这是一种无模型的强化学习方法，由于其适用于车辆边缘计算网络中环境元素未知的问题 [32]。\n  - 本文引入了深度强化学习（DRL），使用深度神经网络来近似Q函数，提供更高的性能和更强大的学习 [33]–[36]。\n  - 将提出的问题进一步制定为半马尔可夫过程，并提出了两种强化学习方法（Q学习和DRL）来确定计算卸载和资源分配的策略。\n\n- **主要贡献:**\n  1. 提出了一种车辆边缘计算网络架构，使车辆能够为UE提供计算服务，增强了MEC-based IoT系统的灵活性和可伸缩性。\n  2. 引入了一种高效的车辆边缘计算网络的卸载方案，考虑了车辆和边缘服务器的延迟和有限的计算能力。制定了一个优化问题，以最大化网络的总效用。\n  3. 通过将问题进一步制定为半马尔可夫过程，解决了随机流量和不确定通信条件。提出了基于Q学习的强化学习和DRL方法，确定计算卸载和资源分配的策略。","x":-34790,"y":-4573,"width":880,"height":1155,"color":"3"},
		{"id":"f81d51fbbd9d7a86","type":"text","text":"\n1. 基于MEC的物联网（IoT）网络\n2. 车辆边缘计算网络\n3. 车辆边缘计算（VEC）网络介绍\n4. 高效的计算卸载方案和优化问题\n5. 解决随机流量和通信不确定性\n6. 主要贡献","x":-33870,"y":-4092,"width":480,"height":194,"color":"4"},
		{"id":"5911a2a71cba17fe","type":"text","text":"### Deep Reinforcement Learning for Offloading and Resource Allocation in Vehicle Edge Computing and Networks\n\nLiu, Yi, Huimin Yu, Shengli Xie和Yan Zhang. 《Deep Reinforcement Learning for Offloading and Resource Allocation in Vehicle Edge Computing and Networks》. *IEEE Transactions on Vehicular Technology* 68, 期 11 (2019年11月): 11158–68. <https://doi.org/10.1109/TVT.2019.2935450>.\n\nIn MEC based IoT network, the devices can offload all/part of the computation tasks to the MEC server which can speed up the processing of the tasks and save energy for devices [15]–[17]. Then, the main technical problem becomes whether/when/how many computation tasks should be offloaded. Numerous literatures are devoted to design the optimal strategy to solve this problem under different performance requirements [18]–[21]. Considering the long-time energy efficiency while using IoT network, an efficient edge computing infrastructure is proposed in [18]. Due to stochastic task arrivals and wireless channels, congested air interface, and prohibitive feedbacks from thousands of IoT devices, authors in [19] generate asymptotically optimal schedules tolerant to out-of-date network knowledge, thereby relieving stringent requirements on feedbacks. The optimal schedule and energy efficient resource allocation policies for MEC are proposed in [20] and [21], respectively.\n\nSince the vehicle is an important type of User Equipment (UE) in IoT system, the vehicular edge computing network structure and related resource allocation methods are studied by many researchers [22]–[27]. C. Wang et al., propose a scalable SDN-enabled MEC architecture that integrates a heterogeneous vehicular network to decrease the overall delay and offload the traffic load from the backbone network [22]. To reduce both the latency and the transmission cost of the computation offloading, a cloud-based MEC offloading framework is proposed for vehicular networks in [23]. In [24], based on the MEC-assisted vehicular offloading mode, a target server selection policy is presented to improve task offloading reliability in the case of vehicular data transmission failure. Some novel MEC-based vehicular networks are proposed in [25], [26], in which the computation offloading policies are carefully designed according to different scenarios. The reinforcement learning is used for long-term resource provision in vehicular cloud to deal with dynamic demands for the resources and stringent QoS requirements [27]. However, in the existing literatures, the vehicles play the role as users in the MEC network in which the edge servers are statically deployed and may cause “service hole” due to the explosion of service requests of tremendous number of UEs.\n\nThe focus of this paper is to design a Vehicle Edge Computing (VEC) network in which the vehicles are able to provide computation services as well as the traditional edge servers. As the traditional edge server, generally connected to road side units, small-cell base stations, etc., has fixed locations, the proposed architecture can extend the computation services range and improve flexibility of the MEC network. Then, we propose an efficient computation offloading scheme for UEs while considering the delay of the computation tasks generated by UEs. Accordingly, we formulate an optimization problem to maximize the total utility of the proposed VEC network.\n\nTo solve the problem, the stochastic traffic and communication uncertainty in vehicular communication environment should be carefully addressed. The Q-learning method is one of the model-free reinforcement learning methods which are not based on the environment elements are already known [32]. Such feature makes Q-learning method suitable for solving the proposed problem in the vehicle edge computing network. The crucial part of Q-learning is to accurately and efficiently estimate the Q value, which may lead to the curse of dimensionality as the increasing of state space. Deep reinforcement learning (DRL), which approximates the Q-function by using deep neural networks, has more advantageous than Q-learning for providing greater performance and more robust learning [33]–[36]. Hence, the proposed problem is further formulated as a semi-Markov process and two reinforcement learning methods: Q-learning based method and deep reinforcement learning (DRL) method are proposed to determine the policies of computation offloading and resource allocation.\n\nIn this paper, we propose a VEC network to enhance the flexibility and scalability of the MEC based IoT system with main contributions summarized as follows:\n\n1. We propose a vehicle edge computing network architecture in which the vehicles can provide computation services for UEs as well as the traditional edge server.\n2. We propose an efficient offloading scheme for the vehicle edge computing network while considering both delay and limited computation capabilities of vehicles and edge servers. Accordingly, we formulate an optimization problem to maximize the total utility of the vehicle edge computing network.\n3. Taking into account of the stochastic traffic and uncertain communication conditions, we reformulated the proposed problem as a semi-Markov process and propose Q-learning based reinforcement learning method and DRL method to find the policies of computation offloading and resource allocation.","x":-37640,"y":-5216,"width":527,"height":2443},
		{"id":"4377f960d6d505c5","type":"text","text":"### A Truthful Reverse-Auction Mechanism for Computation Offloading in Cloud-Enabled Vehicular Network\n\nLiwang, Minghui, Shijie Dai, Zhibin Gao, Yuliang Tang和Huaiyu Dai. 《A Truthful Reverse-Auction Mechanism for Computation Offloading in Cloud-Enabled Vehicular Network》. _IEEE Internet of Things Journal_ 6, 期 3 (2019年6月): 4214–27. [https://doi.org/10.1109/JIOT.2018.2875507](https://doi.org/10.1109/JIOT.2018.2875507).\n\n\nHowever, mobile edge servers may still experience signal coverage limitations and resource constraints in cases of high user density, especially during high-traffic periods. Therefore, mobile device cloud (MDC) [7], [8] technology has been applied as a strategy for offloading computation-intensive applications to nearby mobile devices with idle resources. Compared with location-fixed mobile edge clouds, the offloading scheme in an MDC environment possesses advantages of infrastructure independency and economic efficiency.\n\nIn this paper, we investigate a Vickrey–Clarke–Groves (VCG)-based reverse auction mechanism for V2V computation offloading that addresses the aforementioned issues while considering specific features such as opportunistic connections and different V2V channel conditions in cloud-enabled vehicular networks. Specifically, we envision an offloading market containing several auction groups1 with multiple buyers (service requestors) and sellers (service providers). Groups are managed by centralized brokers (RSUs) with innovative policies that can preserve truthfulness and individual rationality. This paper makes the following contributions.\n\n\n","x":-37872,"y":-6150,"width":527,"height":788},
		{"id":"c58793506f600720","type":"text","text":"- **Introduction:**\n  - Mobile edge servers face signal coverage limitations and resource constraints, especially during high user density and traffic periods.\n  - Mobile device cloud (MDC) technology is applied for offloading computation-intensive applications to nearby mobile devices with idle resources.\n  - Advantages of MDC include infrastructure independency and economic efficiency.\n\n- **Research Objective:**\n  - Investigating a Vickrey–Clarke–Groves (VCG)-based reverse auction mechanism for V2V computation offloading in cloud-enabled vehicular networks.\n  - Addressing issues such as signal coverage limitations, resource constraints, and specific features like opportunistic connections and different V2V channel conditions.\n\n- **Auction Mechanism:**\n  - Envisioning an offloading market with multiple auction groups, each containing buyers (service requestors) and sellers (service providers).\n  - Groups are managed by centralized brokers (RSUs) with innovative policies ensuring truthfulness and individual rationality.\n\n- **Contributions of the Paper:**\n  - Investigates VCG-based reverse auction mechanism for V2V computation offloading.\n  - Addresses issues in cloud-enabled vehicular networks, including opportunistic connections and varied V2V channel conditions.\n  - Envisions an offloading market with centralized brokers managing auction groups for economic efficiency.","x":-36395,"y":-5599,"width":781,"height":613,"color":"5"},
		{"id":"f2b4116eb9283cc6","type":"text","text":"- **引言:**\n  - 移动边缘服务器在高用户密度和高流量期间尤其面临信号覆盖限制和资源约束。\n  - 移动设备云（MDC）技术用于将计算密集型应用卸载到附近具有闲置资源的移动设备。\n  - MDC的优势包括基础设施独立性和经济效益。\n\n- **研究目标:**\n  - 调查一种基于Vickrey–Clarke–Groves（VCG）的反向拍卖机制，用于在云启用的车载网络中进行V2V计算卸载。\n  - 处理信号覆盖限制、资源约束以及诸如机会连接和不同的V2V信道条件等特定特征。\n\n- **拍卖机制:**\n  - 设想一个具有多个拍卖组的卸载市场，每个组包含买方（服务请求者）和卖方（服务提供者）。\n  - 这些组由中央经纪人（RSUs）管理，采用创新策略确保拍卖的真实性和个体合理性。\n\n- **论文的贡献:**\n  - 调查了基于VCG的反向拍卖机制，用于V2V计算卸载。\n  - 解决了云启用的车载网络中的问题，包括机会连接和不同的V2V信道条件。\n  - 设想了一个具有中央经纪人管理的卸载市场，以实现经济效益。","x":-35574,"y":-5539,"width":744,"height":493,"color":"1"},
		{"id":"41c040ef7453b8cd","type":"text","text":"- **引言:**\n  - 移动边缘服务器在高用户密度和高流量期间尤其面临信号覆盖限制和资源约束。\n  - 移动设备云（MDC）技术用于将计算密集型应用卸载到附近具有闲置资源的移动设备。\n  - MDC的优势包括基础设施独立性和经济效益。\n\n- **研究目标:**\n  - 调查一种基于Vickrey–Clarke–Groves（VCG）的反向拍卖机制，用于在云启用的车载网络中进行V2V计算卸载。\n  - 处理信号覆盖限制、资源约束以及诸如机会连接和不同的V2V信道条件等特定特征。\n\n- **拍卖机制:**\n  - 设想一个具有多个拍卖组的卸载市场，每个组包含买方（服务请求者）和卖方（服务提供者）。\n  - 这些组由中央经纪人（RSUs）管理，采用创新策略确保拍卖的真实性和个体合理性。\n\n- **论文的贡献:**\n  - 调查了基于VCG的反向拍卖机制，用于V2V计算卸载。\n  - 解决了云启用的车载网络中的问题，包括机会连接和不同的V2V信道条件。\n  - 设想了一个具有中央经纪人管理的卸载市场，以实现经济效益。","x":-34790,"y":-5515,"width":880,"height":445,"color":"3"},
		{"id":"bb8396cee58a06db","type":"text","text":"- 引言:\n- 研究目标:\n- 拍卖机制:\n- 论文的贡献:","x":-33824,"y":-5397,"width":240,"height":209,"color":"4"},
		{"id":"756c3e54b6e06e7e","type":"text","text":"\n![[Pasted image 20231219141124.png]]\nTABLE II OPTIMAL VCG-BASED REVERSE AUCTION\n\n![[Pasted image 20231219141141.png]]\nTABLE III PROPOSED MAT C H I N G ALGORITHM IN REVERSE AUCTION","x":-36937,"y":-6661,"width":460,"height":814,"color":"6"},
		{"id":"c542af179514c130","type":"text","text":"\n- **Introduction:**\n  - Mobile edge servers face signal coverage limitations and resource constraints, especially during high user density and traffic periods.\n  - Mobile device cloud (MDC) technology is applied for offloading computation-intensive applications to nearby mobile devices with idle resources.\n  - Advantages of MDC include infrastructure independency and economic efficiency.\n- **Research Objective:**\n  - Investigating a Vickrey–Clarke–Groves (VCG)-based reverse auction mechanism for V2V computation offloading in cloud-enabled vehicular networks.\n  - Addressing issues such as signal coverage limitations, resource constraints, and specific features like opportunistic connections and different V2V channel conditions.\n- **Auction Mechanism:**\n  - Envisioning an offloading market with multiple auction groups, each containing buyers (service requestors) and sellers (service providers).\n  - Groups are managed by centralized brokers (RSUs) with innovative policies ensuring truthfulness and individual rationality.\n- **Contributions of the Paper:**\n  - Investigates VCG-based reverse auction mechanism for V2V computation offloading.\n  - Addresses issues in cloud-enabled vehicular networks, including opportunistic connections and varied V2V channel conditions.\n  - Envisions an offloading market with centralized brokers managing auction groups for economic efficiency.","x":-36937,"y":-5791,"width":460,"height":997,"color":"6"},
		{"id":"df604f2483501e93","type":"text","text":"- **Introduction to MEC and its Certification:**\n  - MEC is effective in reducing service delivery and network operation delay costs by deploying servers close to mobile users.\n  - Certified by the European 5G Infrastructure Public Private Partnership (PPP) as a key technology for the next-generation 5G network.\n\n- **Advantages of MEC:**\n  - Low latency\n  - High bandwidth\n  - Real-time wireless network information\n  - Location awareness [7,8].\n\n- **MEC-Based Internet of Vehicles (IoV):**\n  - MEC facilitates the offloading of computing tasks from mobile vehicles to network edge nodes, addressing ultra-low latency requirements of IoV [9].\n  - It sinks cloud services to the edge of the wireless access network, providing computing services near moving vehicles [10].\n\n- **Challenges in MEC-Based Vehicle-Connected Networks:**\n  - In MEC-based vehicle-connected networks, various computationally intensive and delay-sensitive tasks exist.\n  - Each task has distinct resource requirements, encompassing computing and communication resources.\n\n- **Need for Offloading Strategy and Resource Control:**\n  - A suitable strategy is required to control offloading tasks, ensuring the normal operation of the system.\n  - Considering the impact of offloading decisions and resource allocation on computing offloading performance.\n\n- **Proposed Solution:**\n  - In response to the mentioned challenges, a computing offloading resource allocation scheme is proposed.\n  - The scheme utilizes reinforcement learning in a MEC system to optimize the offloading decisions and resource allocation, addressing the dynamic and diverse nature of the tasks and resource requirements.","x":-36395,"y":-7654,"width":781,"height":843,"color":"5"},
		{"id":"59e271b6d698153c","type":"text","text":"### A Computing Offloading Resource Allocation Scheme Using Deep Reinforcement Learning in Mobile Edge Computing Systems\n\nLi, Xuezhu. 《A Computing Offloading Resource Allocation Scheme Using Deep Reinforcement Learning in Mobile Edge Computing Systems》. _Journal of Grid Computing_ 19, 期 3 (2021年9月): 35. [https://doi.org/10.1007/s10723-021-09568-w](https://doi.org/10.1007/s10723-021-09568-w).\n\nMEC can effectively reduce the delay cost of service delivery and network operation by deploying servers close to mobile users. And it has been certified by European 5G Infrastructure Public Private Partnership (PPP) as one of key technologies of the next-generation 5G network. The advantages of MEC include low latency, high bandwidth, real time wireless network information and location awareness [7,8]. The MEC-based Internet of Vehicles (IoV) allows mobile vehicles to offload computing tasks to network edge nodes for processing, which helps to achieve the ultra-low latency requirements of IoV [9]. MEC sinks cloud services to the edge of wireless access network and provides computing services near moving vehicles [10]. However, in the MEC-based vehicle-connected network, there are various computationally intensive and delay sensitive computational tasks. Moreover, each task has different resource requirements, including computing resources required for task execution and communication resources required for task transmission. In this case, a suitable strategy is needed to control offloading tasks and ensure the normal operation of system. Considering the impact of offloading decision and resource allocation on computing offloading performance. In response to the above problems, a computing offloading resource allocation scheme using reinforcement learning in a MEC system is proposed,\n\n","x":-37569,"y":-7642,"width":527,"height":820},
		{"id":"fd4a3013b817968e","type":"text","text":"- **MEC及其认证介绍:**\n  - MEC通过在移动用户附近部署服务器，有效降低服务交付和网络运营延迟成本。\n  - 被欧洲5G基础设施公私合作伙伴关系（PPP）认证为下一代5G网络的关键技术。\n\n- **MEC的优势:**\n  - 低延迟\n  - 高带宽\n  - 实时无线网络信息\n  - 位置感知 [7,8]。\n\n- **基于MEC的车联网（IoV）:**\n  - MEC促进了从移动车辆卸载计算任务到网络边缘节点，满足IoV的超低延迟需求 [9]。\n  - 它将云服务下沉到无线接入网络的边缘，为行驶中的车辆提供计算服务 [10]。\n\n- **MEC-Based车联网的挑战:**\n  - 在基于MEC的车联网中，存在各种计算密集型和延迟敏感的任务。\n  - 每个任务都具有不同的资源需求，包括计算和通信资源。\n\n- **需要卸载策略和资源控制:**\n  - 需要合适的策略来控制卸载任务，确保系统的正常运行。\n  - 考虑卸载决策和资源分配对计算卸载性能的影响。\n\n- **提出的解决方案:**\n  - 针对上述挑战，提出了一种计算卸载资源分配方案。\n  - 该方案利用MEC系统中的强化学习来优化卸载决策和资源分配，以应对任务和资源需求的动态多样性。","x":-35574,"y":-7534,"width":744,"height":603,"color":"1"},
		{"id":"7263f79cc514c264","type":"text","text":"- **MEC及其认证介绍:**\n  - MEC通过在移动用户附近部署服务器，有效降低服务交付和网络运营延迟成本。\n  - 被欧洲5G基础设施公私合作伙伴关系（PPP）认证为下一代5G网络的关键技术。\n\n- **MEC的优势:**\n  - 低延迟\n  - 高带宽\n  - 实时无线网络信息\n  - 位置感知 [7,8]。\n\n- **基于MEC的车联网（IoV）:**\n  - MEC促进了从移动车辆卸载计算任务到网络边缘节点，满足IoV的超低延迟需求 [9]。\n  - 它将云服务下沉到无线接入网络的边缘，为行驶中的车辆提供计算服务 [10]。\n\n- **MEC-Based车联网的挑战:**\n  - 在基于MEC的车联网中，存在各种计算密集型和延迟敏感的任务。\n  - 每个任务都具有不同的资源需求，包括计算和通信资源。\n\n- **需要卸载策略和资源控制:**\n  - 需要合适的策略来控制卸载任务，确保系统的正常运行。\n  - 考虑卸载决策和资源分配对计算卸载性能的影响。\n\n- **提出的解决方案:**\n  - 针对上述挑战，提出了一种计算卸载资源分配方案。\n  - 该方案利用MEC系统中的强化学习来优化卸载决策和资源分配，以应对任务和资源需求的动态多样性。","x":-34790,"y":-7522,"width":920,"height":579,"color":"3"},
		{"id":"74ab79f4daac717d","type":"text","text":"\n1. MEC及其认证介绍\n2. MEC的优势\n3. 基于MEC的车联网（IoV）\n4. MEC-Based车联网的挑战\n5. 需要卸载策略和资源控制\n6. 提出的解决方案","x":-33784,"y":-7329,"width":335,"height":194,"color":"4"},
		{"id":"bb560557ff125046","type":"text","text":"- To address the challenges on implementing MEC-based vehicular networks, many research works have been performed recently, including:\n  - Design of architecture\n  - Task offloading scheme\n  - Resource management scheme, and so on.\n\n- For example, the MEC-based hierarchical vehicular network framework, comprised of:\n  - Vehicle level’s on-board computing/storing resources\n  - Server level’s resources (resources placed at the MEC and cloud-computing servers), has been investigated in [10], [12], [14]–[16].\n\n- To better manage the spectrum/computing/storing resources among and make task offloading decisions to vehicle users, task offloading and resource management schemes have been proposed in [10], [15]–[17].\n\n- Since task offloading and spectrum/computing resource allocation are coupled with each other, the objectives of the most existing works have been achieved by jointly optimizing these two parts with traditional optimization methods [10], [15].\n\n- However, only one or two dimensions of resources have been considered in most of the existing schemes, which cannot be directly adopted to support some vehicular applications where high dimensional resources are involved, such as the computing tasks generated by the leading vehicle for platoon/convoy control [7].\n\n- Moreover, there are also some works focusing on multi-dimensional resources management in the scenarios with low mobility users [18], [19].\n\n- For MEC-based vehicular networks, the computational complexity of multi-dimensional resource management problem increases due to the high vehicle mobility and time-varying demand on resources, therefore increasing the time consumption on the resource management scheme itself.\n\n- Thus, it is infeasible to adopt the pure optimization approach-based schemes to achieve multi-dimensional resource management in MEC-based vehicular networks, especially for the scenarios with delay-sensitive applications.\n\n- How to design practical and QoS-oriented multi-dimensional resource management schemes for the MEC-based vehicular networks still needs efforts.\n\n- As is known, artificial intelligence (AI) technology, especially reinforcement learning (RL), can be exploited to solve resource management problems quickly [20]–[23].\n\n- Q-learning [16], [24], deep Q-learning [25]–[27], actor-critic [18], [28], and other deep RL algorithms have been widely exploited for resource management in wireless communication networks.\n\n- Inspired by the existing works and considering the dynamic vehicular network environment caused by high vehicle mobility and heterogeneous applications, we investigate how to exploit deep RL to jointly manage the spectrum, computing, and storing resources to support delay-sensitive applications in the MEC-based vehicular network [12] in this paper.\n\n- Specifically, the main contributions of this work can be summarized as follows\n\n1) According to the location of the MEC server, two typical multi-dimensional resource management frameworks are proposed with placing the MEC server at a macro-cell base station (MBS) and an edge node (EN), respectively. \n2) Leveraging optimization theory, optimization problems are formulated to maximize the number of offloaded tasks with satisfied QoS requirements and constrained total amounts of available spectrum, computing, and storing resources. \n3) To rapidly solve the formulated problems and obtain optimal spectrum slicing among base stations (BSs) and optimal spectrum/computing/storing allocation among vehicles, the formulated optimization problems are transformed with deep RL. \n4) A deep deterministic policy gradient (DDPG)-based algorithm is proposed to solve the transformed RL problems. As the complexity of the transformed RL problems increases with the sizes of environment state and action, a hierarchical DDPG (HDDPG)-based algorithm is developed by combining the DDPG and the hierarchical learning architecture.\n\n","x":-19355,"y":-4352,"width":527,"height":2016,"color":"6"},
		{"id":"462aeef697230fbf","type":"text","text":"- **Challenges and Research Focus in MEC-Based Vehicular Networks:**\n  - Addressing challenges in implementing MEC-based vehicular networks has been a focus of recent research, covering aspects such as:\n    - Design of architecture\n    - Task offloading schemes\n    - Resource management schemes, etc.\n\n- **Hierarchical Vehicular Network Framework:**\n  - An example is the investigation of the MEC-based hierarchical vehicular network framework, which includes on-board computing/storing resources at the vehicle level and resources at the server level (MEC and cloud-computing servers) [10], [12], [14]–[16].\n\n- **Task Offloading and Resource Management Schemes:**\n  - Task offloading and resource management schemes have been proposed to manage spectrum/computing/storing resources and make task offloading decisions to vehicle users [10], [15]–[17].\n  - Existing works typically achieve objectives by jointly optimizing task offloading and spectrum/computing resource allocation using traditional optimization methods [10], [15].\n\n- **Limitations of Existing Schemes:**\n  - Most existing schemes consider only one or two dimensions of resources, which may not directly support vehicular applications with high-dimensional resources, such as computing tasks for platoon/convoy control [7].\n  - Some works focus on multi-dimensional resource management in scenarios with low mobility users [18], [19].\n\n- **Computational Complexity Challenges:**\n  - In MEC-based vehicular networks, the computational complexity of multi-dimensional resource management increases due to high vehicle mobility and time-varying demand on resources.\n  - Pure optimization approach-based schemes become infeasible, especially for scenarios with delay-sensitive applications.\n\n- **Introduction of AI, Reinforcement Learning, and Main Contributions:**\n  - Artificial intelligence (AI) technology, particularly reinforcement learning (RL), is explored as a solution for rapid resource management [20]–[23].\n  - Q-learning [16], [24], deep Q-learning [25]–[27], and actor-critic [18], [28] algorithms have been widely used for resource management in wireless communication networks.\n  - Inspired by these approaches, and considering the dynamic vehicular network environment, the paper investigates the use of deep RL to jointly manage spectrum, computing, and storing resources to support delay-sensitive applications in the MEC-based vehicular network [12].\n\n- **Main Contributions of the Paper:**\n  1. Proposal of two multi-dimensional resource management frameworks based on the MEC server's location (macro-cell base station (MBS) and edge node (EN)).\n  2. Formulation of optimization problems, leveraging optimization theory, to maximize the number of offloaded tasks with satisfied Quality of Service (QoS) requirements, and constrained total amounts of available spectrum, computing, and storing resources.\n  3. Transformation of formulated problems using deep RL to rapidly obtain optimal spectrum slicing among base stations (BSs) and optimal spectrum/computing/storing allocation among vehicles.\n  4. Introduction of a deep deterministic policy gradient (DDPG)-based algorithm to solve the transformed RL problems. A hierarchical DDPG (HDDPG)-based algorithm is developed to address the increasing complexity of the transformed RL problems.","x":-20176,"y":-3987,"width":781,"height":1287,"color":"5"},
		{"id":"ec67400a03415c26","type":"text","text":"### DRL-Based Long-Term Resource Planning for Task Offloading Policies in Multiserver Edge Computing Networks\n\nLi, Haiyuan, Karcius Day R. Assis, Shuangyi Yan和Dimitra Simeonidou. 《DRL-Based Long-Term Resource Planning for Task Offloading Policies in Multiserver Edge Computing Networks》. *IEEE Transactions on Network and Service Management* 19, 期 4 (2022年12月): 4151–64. <https://doi.org/10.1109/TNSM.2022.3191748>.\n\nIn previous studies, some of the task offloading algorithms assumed infinite computing resources on the MEC server for offloading operations [6], [7]. However, exploding computingintensive tasks and the imbalanced request distribution are challenging the computing capability of MEC servers in bearer networks [8], [9]. The limited MEC resources require offload policies to consider long-term rewards and bring collaboration between multiple MECs. On the other side, to improve the utilization of limited MEC server resources, a large body of literature has focused on designing offloading resource management algorithms based on centralized [7], [10]–[12] and distributed approaches [8], [13], [14].\n\nAccording to the number of servers, MEC systems in the current papers are divided into single-server [13], [15]–[17] or multiple-server [9]–[11], [18], [19] systems. Meanwhile, the techniques to tackle computation offloading and resource allocation in those models are classified into optimizationbased [10], [11], [13], [19] or machined learning based [16], [17], [20], respectively.\n\nRegarding the single-server system, Lyu et al. [13]introduced a semi-distributed approach that jointly optimizes offloading decisions and resource allocation problems. With a heuristic offloading algorithm (HODA), they achieved superior system utility with an acceptable complexity of $O(K^3)$. Nevertheless, the single-server system cannot deal with the coexistence between idle and over occupied servers because of the inevitable unbalanced request distribution [21], [22]. Under the same system model, Li et al. [16] designed a centralized based DRL-based model to simultaneously resolve offloading decisions and computing resource allocations. However, excess users will cause the explosion of the action space of the DRL based algorithm, which causes the model fail to converge. In comparison, Chen et al. [7] applied multi-agent policy based DRL model that distributed the artificial intelligence (AI) agent to the edge devices to reduce the dimension of action space. However, this approach comes at the expense of complexity and might bring burdens on the devices because of their weak computing abilities.\n\nIn order to further improve the workload carrying capability of the MEC network, multiple access in RAN was introduced as a promising technology to achieve the multi-server system. Li et al. [10], Nduwayezu et al. [12] and Xue and An [11] explored the centralized joint optimization approach for the multi-access system. First fit (FF) algorithm [23] was used to select the offloading destination for the offloading requests in the network. In comparison, Apostolopoulos et al. proposed a distributed approach towards determining the optimal data oflfoading of each user within a multiple MEC servers system by non-cooperative game among the users [8]. However, their assumption that all edge devices have access to the same set of servers neglected the geographical proximity factor in practical networks. In contrast, Kan et al. [24] designed a model where edge devices can only connect to their proximate servers. The multi-server system was achieved by relaying the workload among servers via the wired interface (Mp3) which is designed and standardized for the workload transition between MEC servers by the European Telecommunications Standards Institute (ETSI) [25]. Furthermore, Qian et al. designed a distributed algorithm and a centralized online DRL-based solution for statistic and dynamic channel scenarios, respectively, which can realized the selection of access point with minimized transmission cost [14].\n\nAlthough extensive research has been carried out on the offloading management technology in single and multiple server scenarios, few studies have been reported with consideration of long-term resource balance for offloading operations. Offloading without long-term resource balance will prefer instant high reward at the expense of the resource shorting for following offloading requests.\n\nIn summary, three critical issues need to be resolved in designing efficient offloading management policies. First, the prior joint offloading algorithms in multi-server systems ignored the geographic proximity between edge devices and servers. FF-based solutions did not fundamentally deal with the unbalanced traffic distribution. Second, the possible action space explosion in DRL-based solutions remained unclear. Last but not least, the lack of consideration of long-term reward in the current studies hindered further improvement in network resource utilization. To overcome these obstacles, the main contributions of this paper are as follows:\n\nTo the best of our knowledge, this is the first time that the optimization-based method and DRL have been geared together to solve two interrelated subproblems and find the policies that maximize long-term offloading benefits. Regarding the joint solution between edge devices in one time slot, our results show that there is a great benefit in the execution time by sorting the execution order of requests and reusing the released resources for the on processing workloads. In addition, a period of continuous time slots is taken to illustrate the long-term reward of random allocation, over allocation, and three DRL-based allocation policies. We prove that over-allocation used by the prior studies can not get reliable results as it pursues higher rewards at the expense of future loss. In comparison, DRL-based algorithms can adapt to the diversity of future network states and obtain resource allocation strategies with better performance. Furthermore, of these three DRL algorithms, compared to single-agent DQN, the cooperative multi-agent model is able to more accurately account for interactions between servers on limited resources and achieve higher average rewards. In the end, the value of server cooperation is also justified.","x":-18760,"y":-2243,"width":527,"height":2789},
		{"id":"126967b0e284f732","type":"text","text":"- In previous studies, some of the task offloading algorithms assumed infinite computing resources on the MEC server for offloading operations [6], [7].\n\n- However, exploding computing-intensive tasks and the imbalanced request distribution are challenging the computing capability of MEC servers in bearer networks [8], [9].\n\n- The limited MEC resources require offload policies to consider long-term rewards and bring collaboration between multiple MECs.\n\n- On the other side, to improve the utilization of limited MEC server resources, a large body of literature has focused on designing offloading resource management algorithms based on:\n  - Centralized approaches [7], [10]–[12]\n  - Distributed approaches [8], [13], [14].\n\n- According to the number of servers, MEC systems in the current papers are divided into:\n  - Single-server [13], [15]–[17]\n  - Multiple-server [9]–[11], [18], [19] systems.\n\n- Meanwhile, the techniques to tackle computation offloading and resource allocation in those models are classified into:\n  - Optimization-based [10], [11], [13], [19]\n  - Machine learning-based [16], [17], [20], respectively.\n\n- Regarding the single-server system, Lyu et al. [13] introduced a semi-distributed approach that jointly optimizes offloading decisions and resource allocation problems.\n\n  - With a heuristic offloading algorithm (HODA), they achieved superior system utility with an acceptable complexity of O(K^3).\n\n  - Nevertheless, the single-server system cannot deal with the coexistence between idle and over-occupied servers because of the inevitable unbalanced request distribution [21], [22].\n\n- Under the same system model, Li et al. [16] designed a centralized-based DRL-based model to simultaneously resolve offloading decisions and computing resource allocations.\n\n  - However, excess users will cause the explosion of the action space of the DRL-based algorithm, which causes the model to fail to converge.\n\n- In comparison, Chen et al. [7] applied a multi-agent policy-based DRL model that distributed the artificial intelligence (AI) agent to the edge devices to reduce the dimension of the action space.\n\n  - However, this approach comes at the expense of complexity and might bring burdens on the devices because of their weak computing abilities.\n\n- In order to further improve the workload carrying capability of the MEC network, multiple access in RAN was introduced as a promising technology to achieve the multi-server system.\n\n  - Li et al. [10], Nduwayezu et al. [12], and Xue and An [11] explored the centralized joint optimization approach for the multi-access system.\n\n  - The First Fit (FF) algorithm [23] was used to select the offloading destination for the offloading requests in the network.\n\n- In comparison, Apostolopoulos et al. proposed a distributed approach towards determining the optimal data offloading of each user within a multiple MEC servers system by non-cooperative game among the users [8].\n\n  - However, their assumption that all edge devices have access to the same set of servers neglected the geographical proximity factor in practical networks.\n\n- In contrast, Kan et al. [24] designed a model where edge devices can only connect to their proximate servers.\n\n  - The multi-server system was achieved by relaying the workload among servers via the wired interface (Mp3) designed and standardized for the workload transition between MEC servers by the European Telecommunications Standards Institute (ETSI) [25].\n\n- Furthermore, Qian et al. designed a distributed algorithm and a centralized online DRL-based solution for statistic and dynamic channel scenarios, respectively, which can realize the selection of access point with minimized transmission cost [14].\n\n- Although extensive research has been carried out on the offloading management technology in single and multiple server scenarios, few studies have been reported with consideration of long-term resource balance for offloading operations.\n\n- Offloading without long-term resource balance will prefer instant high reward at the expense of the resource shorting for following offloading requests.\n\n- In summary, three critical issues need to be resolved in designing efficient offloading management policies.\n\n  1. First, the prior joint offloading algorithms in multi-server systems ignored the geographic proximity between edge devices and servers. FF-based solutions did not fundamentally deal with the unbalanced traffic distribution.\n\n  2. Second, the possible action space explosion in DRL-based solutions remained unclear.\n\n  3. Last but not least, the lack of consideration of long-term reward in the current studies hindered further improvement in network resource utilization.\n\n- To overcome these obstacles, the main contributions of this paper are as follows:\n\n- To the best of our knowledge, this is the first time that the optimization-based method and DRL have been geared together to solve two interrelated subproblems and find the policies that maximize long-term offloading benefits.\n\n- Regarding the joint solution between edge devices in one time slot, our results show that there is a great benefit in the execution time by sorting the execution order of requests and reusing the released resources for the on-processing workloads.\n\n- In addition, a period of continuous time slots is taken to illustrate the long-term reward of random allocation, over allocation, and three DRL-based allocation policies.\n\n- We prove that over-allocation used by the prior studies cannot get reliable results as it pursues higher rewards at the expense of future loss.\n\n- In comparison, DRL-based algorithms can adapt to the diversity of future network states and obtain resource allocation strategies with better performance.\n\n- Furthermore, of these three DRL algorithms, compared to single-agent DQN, the cooperative multi-agent model is able to more accurately account for interactions between servers on limited resources and achieve higher average rewards.\n\n- In the end, the value of server cooperation is also justified.\n\n\n","x":-19355,"y":-2243,"width":527,"height":3125,"color":"6"},
		{"id":"1ebaf5ca1336fdd6","type":"text","text":"- **Overview of Previous Studies in MEC-Based Vehicular Networks:**\n  - Some task offloading algorithms in prior studies assumed infinite computing resources on MEC servers, but the growing demand challenges the computing capability of MEC servers in bearer networks.\n\n- **Resource Management Approaches:**\n  - A substantial body of literature focuses on designing offloading resource management algorithms based on centralized and distributed approaches for both single-server and multiple-server systems.\n\n- **Challenges and Limitations in Single-Server Systems:**\n  - Challenges in single-server systems include unbalanced request distribution and difficulty in handling coexistence between idle and over-occupied servers.\n\n- **Approaches in Single-Server Systems:**\n  - Lyu et al. [13] introduced a semi-distributed approach with a heuristic offloading algorithm (HODA) achieving superior system utility with acceptable complexity.\n  - Li et al. [16] designed a centralized DRL-based model, facing challenges with the explosion of the action space.\n  - Chen et al. [7] applied a multi-agent policy-based DRL model to reduce the dimension of the action space, but this approach added complexity to devices with weak computing abilities.\n\n- **Multi-Server Systems and Resource Management Approaches:**\n  - Multiple access in Radio Access Network (RAN) is introduced to enhance the workload carrying capability of the MEC network.\n  - Centralized joint optimization approaches [10], [11], [12], and a distributed approach involving a non-cooperative game among users [8] are explored.\n\n- **Considerations in Multi-Server Systems:**\n  - Geographic proximity is considered in some models [24], [8].\n  - A model is designed where edge devices connect only to proximate servers, and a multi-server system is achieved by relaying workload among servers via a standardized wired interface (Mp3) [25].\n\n- **Distributed Algorithm and DRL-Based Solutions:**\n  - Qian et al. designed a distributed algorithm and a centralized online DRL-based solution for statistic and dynamic channel scenarios, respectively, optimizing the selection of access points with minimized transmission cost [14].\n\n- **Critical Issues and Challenges in Existing Approaches:**\n  1. Geographic proximity between edge devices and servers is often neglected, impacting traffic distribution.\n  2. DRL-based solutions may face an unclear action space explosion.\n  3. Lack of consideration for long-term resource balance hinders further improvement in network resource utilization.\n\n- **Contributions of the Paper:**\n  - First-time integration of optimization-based method and DRL to solve interrelated subproblems and maximize long-term offloading benefits.\n  - Demonstrated benefits of sorting execution order and reusing released resources in joint solutions among edge devices in one time slot.\n  - Illustration of long-term reward for various allocation policies, proving that over-allocation pursued higher rewards at the expense of future loss.\n  - Comparison of three DRL algorithms, showing that the cooperative multi-agent model achieves higher average rewards by accurately considering interactions between servers on limited resources.\n\n- **Conclusion:**\n  - The paper contributes to overcoming challenges in offloading management, introducing novel methodologies for efficient resource utilization in MEC-based vehicular networks.","x":-20176,"y":-1365,"width":781,"height":1369,"color":"5"},
		{"id":"f0b8b4a82e76ec76","type":"text","text":"- **MEC-Based Vehicular Networks中的挑战和研究重点:**\n  - 解决在实施基于MEC的车联网中的挑战一直是最近研究的焦点，涵盖了以下方面：\n    - 架构设计\n    - 任务卸载方案\n    - 资源管理方案等。\n\n- **分层车联网框架:**\n  - 一个例子是对基于MEC的分层车联网框架的研究，该框架包括车辆级别的车载计算/存储资源和服务器级别的资源（MEC和云计算服务器）[10]，[12]，[14]–[16]。\n\n- **任务卸载和资源管理方案:**\n  - 已经提出了任务卸载和资源管理方案，以管理频谱/计算/存储资源并对车辆用户进行任务卸载决策 [10]，[15]–[17]。\n  - 现有的工作通常通过联合优化任务卸载和频谱/计算资源分配来实现目标，使用传统的优化方法 [10]，[15]。\n\n- **现有方案的局限性:**\n  - 大多数现有方案仅考虑一两个维度的资源，这可能无法直接支持具有高维资源需求的车联网应用，例如编队/车队控制的计算任务 [7]。\n  - 一些工作侧重于低移动性用户场景下的多维资源管理 [18]，[19]。\n\n- **计算复杂性挑战:**\n  - 在基于MEC的车联网中，由于车辆高度移动和资源需求的时变性，多维资源管理的计算复杂性增加。\n  - 基于纯优化方法的方案在具有对延迟敏感应用的场景中变得不可行，特别是对于计算复杂性挑战较大的情况。\n\n- **引入人工智能、强化学习和主要贡献:**\n  - 探讨了人工智能（AI）技术，尤其是强化学习（RL），作为快速资源管理的解决方案 [20]–[23]。\n  - Q学习 [16]，[24]，深度Q学习 [25]–[27]，以及演员-评论家 [18]，[28] 算法在无线通信网络中的资源管理中被广泛使用。\n  - 在这些方法的启发下，并考虑到动态的车辆网络环境，本文研究了使用深度RL来共同管理频谱、计算和存储资源，以支持基于MEC的车联网中对延迟敏感的应用 [12]。\n\n- **论文的主要贡献:**\n  1. 提出了两个基于MEC服务器位置（宏基站（MBS）和边缘节点（EN））的多维资源管理框架。\n  2. 利用优化理论制定了优化问题，以最大化卸载任务的数量，同时满足服务质量（QoS）要求，并受到可用频谱、计算和存储资源的总量限制。\n  3. 使用深度RL对所制定的问题进行转化，以快速获得基站（BS）之间的最佳频谱切片和车辆之间的最佳频谱/计算/存储分配。\n  4. 引入了基于深度确定性策略梯度（DDPG）的算法来解决转化后的RL问题。为应对转化后的RL问题的增加复杂性，开发了一种基于层次DDPG（HDDPG）的算法。","x":-20960,"y":-3855,"width":744,"height":1023,"color":"1"},
		{"id":"919e01de9971ff0b","type":"text","text":"- MEC-Based Vehicular Networks中的挑战和研究重点:\n- 分层车联网框架:\n- 任务卸载和资源管理方案:\n- 现有方案的局限性:\n- 计算复杂性挑战:\n- 引入人工智能、强化学习和主要贡献:\n- 论文的主要贡献:","x":-22400,"y":-3484,"width":480,"height":280,"color":"4"},
		{"id":"e53bd267ca288808","type":"text","text":"- MEC-Based Vehicular Networks的先前研究概述:\n- 资源管理方法:\n- 单服务器系统中的挑战和局限性:\n- 单服务器系统中的方法:\n- 多服务器系统和资源管理方法:\n- 多服务器系统中的考虑:\n- 分布式算法和基于DRL的解决方案:\n- 现有方法中的关键问题和挑战:\n- 论文的贡献:\n- 结论:","x":-22400,"y":-840,"width":480,"height":320,"color":"4"},
		{"id":"70341e2d8d93c34b","type":"text","text":"- **MEC-Based Vehicular Networks的先前研究概述:**\n  - 先前研究中，一些任务卸载算法假定MEC服务器具有无限的计算资源，但不断增长的需求挑战了MEC服务器在承载网络中的计算能力。\n\n- **资源管理方法:**\n  - 大量文献集中在为单服务器和多服务器系统设计基于集中和分布式方法的卸载资源管理算法。\n\n- **单服务器系统中的挑战和局限性:**\n  - 单服务器系统中的挑战包括请求分布不平衡和难以处理空闲和过度占用服务器之间的共存。\n\n- **单服务器系统中的方法:**\n  - Lyu等人[13]引入了一种半分布式方法，采用了启发式卸载算法（HODA），具有可接受的复杂性，实现了卓越的系统效用。\n  - Li等人[16]设计了一个基于集中式DRL的模型，面临着行动空间激增的挑战。\n  - Chen等人[7]应用了基于多智能体策略的DRL模型，以减小行动空间的维度，但这种方法增加了计算能力较弱的设备的复杂性。\n\n- **多服务器系统和资源管理方法:**\n  - 引入了无线接入网络（RAN）中的多用户访问，以增强MEC网络的工作负载承载能力。\n  - 研究了集中式联合优化方法[10]，[11]，[12]以及涉及用户之间的非合作游戏的分布式方法[8]。\n\n- **多服务器系统中的考虑:**\n  - 一些模型中考虑了地理近距离[24]，[8]。\n  - 设计了一种模型，其中边缘设备仅连接到附近的服务器，通过标准化的有线接口（Mp3）在服务器之间中继工作负载，实现了多服务器系统[25]。\n\n- **分布式算法和基于DRL的解决方案:**\n  - Qian等人设计了一个分布式算法和一个用于统计和动态信道场景的集中式在线DRL解决方案，分别优化了选择具有最小化传输成本的接入点[14]。\n\n- **现有方法中的关键问题和挑战:**\n  1. 往往忽视了边缘设备和服务器之间的地理近距离，影响了流量分布。\n  2. 基于DRL的解决方案可能面临不明确的行动空间激增。\n  3. 缺乏对长期资源平衡的考虑，阻碍了网络资源利用的进一步提高。\n\n- **论文的贡献:**\n  - 首次将基于优化的方法和DRL集成，解决相互关联的子问题并最大化长期卸载效益。\n  - 在一个时间槽中的边缘设备之间的联合解决方案中，演示了排序执行顺序和重复利用释放的资源的好处。\n  - 通过对比不同分配策略的长期奖励，证明了过度分配在牺牲未来损失的情况下追求更高的奖励。\n  - 比较了三种DRL算法，表明协作多智能体模型通过准确考虑有限资源上的服务器之间的交互而实现了更高的平均奖励。\n\n- **结论:**\n  - 本文有助于克服卸载管理中的挑战，引入了在基于MEC的车辆网络中实现有效资源利用的新方法论。","x":-21880,"y":-1161,"width":880,"height":961,"color":"3"},
		{"id":"f528ccf3526e0288","type":"text","text":"- **MEC-Based Vehicular Networks的先前研究概述:**\n  - 先前研究中，一些任务卸载算法假定MEC服务器具有无限的计算资源，但不断增长的需求挑战了MEC服务器在承载网络中的计算能力。\n\n- **资源管理方法:**\n  - 大量文献集中在为单服务器和多服务器系统设计基于集中和分布式方法的卸载资源管理算法。\n\n- **单服务器系统中的挑战和局限性:**\n  - 单服务器系统中的挑战包括请求分布不平衡和难以处理空闲和过度占用服务器之间的共存。\n\n- **单服务器系统中的方法:**\n  - Lyu等人[13]引入了一种半分布式方法，采用了启发式卸载算法（HODA），具有可接受的复杂性，实现了卓越的系统效用。\n  - Li等人[16]设计了一个基于集中式DRL的模型，面临着行动空间激增的挑战。\n  - Chen等人[7]应用了基于多智能体策略的DRL模型，以减小行动空间的维度，但这种方法增加了计算能力较弱的设备的复杂性。\n\n- **多服务器系统和资源管理方法:**\n  - 引入了无线接入网络（RAN）中的多用户访问，以增强MEC网络的工作负载承载能力。\n  - 研究了集中式联合优化方法[10]，[11]，[12]以及涉及用户之间的非合作游戏的分布式方法[8]。\n\n- **多服务器系统中的考虑:**\n  - 一些模型中考虑了地理近距离[24]，[8]。\n  - 设计了一种模型，其中边缘设备仅连接到附近的服务器，通过标准化的有线接口（Mp3）在服务器之间中继工作负载，实现了多服务器系统[25]。\n\n- **分布式算法和基于DRL的解决方案:**\n  - Qian等人设计了一个分布式算法和一个用于统计和动态信道场景的集中式在线DRL解决方案，分别优化了选择具有最小化传输成本的接入点[14]。\n\n- **现有方法中的关键问题和挑战:**\n  1. 往往忽视了边缘设备和服务器之间的地理近距离，影响了流量分布。\n  2. 基于DRL的解决方案可能面临不明确的行动空间激增。\n  3. 缺乏对长期资源平衡的考虑，阻碍了网络资源利用的进一步提高。\n\n- **论文的贡献:**\n  - 首次将基于优化的方法和DRL集成，解决相互关联的子问题并最大化长期卸载效益。\n  - 在一个时间槽中的边缘设备之间的联合解决方案中，演示了排序执行顺序和重复利用释放的资源的好处。\n  - 通过对比不同分配策略的长期奖励，证明了过度分配在牺牲未来损失的情况下追求更高的奖励。\n  - 比较了三种DRL算法，表明协作多智能体模型通过准确考虑有限资源上的服务器之间的交互而实现了更高的平均奖励。\n\n- **结论:**\n  - 本文有助于克服卸载管理中的挑战，引入了在基于MEC的车辆网络中实现有效资源利用的新方法论。","x":-20960,"y":-1233,"width":744,"height":1105,"color":"1"},
		{"id":"38d1769d3407d63b","type":"text","text":"\n- Considering the stringent delay requirements of some applications and huge latency on wireless and wired communications between a vehicle and the cloud computing server, offloading vehicles’ tasks to the cloud computing server is sometimes inapplicable [13]–[15].\n- Meanwhile, physically increasing the amount of onboard resources would result in a serious increase in the manufacturing cost of vehicles.\n- The MEC server, as an extension of the cloud computing server, shifts computing and caching capabilities close to user devices [16]–[19].\n- It allows vehicles to offload tasks to it via multiple wireless communication technologies.\n- By saving the time consumption on communications between the MEC and the cloud computing server, the sensitive delay requirement of an offloaded task can be satisfied.\n- However, the amount of computing/caching resources is generally preset in each MEC-mounted base station (BS) while the resource demand from vehicle users is time-varying, underused, or wasted resource issues remain in the vehicular scenarios supported by only MEC-mounted BSs, especially when there exists bursty traffic caused by some social activities or events.\n- Taking the flexibility advantage of UAVs, mounting MEC servers in UAVs can help to address the above issues by dispatching the MEC-mounted UAVs to assist the designated BSs [11], [20].\n- Related applications have been also considered in different projects launched by many leading companies [21].\n\n- To implement MEC- and UAV-assisted vehicular networks, recent efforts have been made, with a focus on the deployment of MEC-mounted UAVs.\n- [22] has studied how to deploy and schedule the MEC-mounted UAVs to support vehicular applications.\n- Resource management is another research emphasis, with most works adopting optimization and reinforcement learning (RL) methods.\n- In [12], the transmit powers of vehicles and the trajectories of UAVs are jointly optimized to maximize the resource efficiency on MEC-mounted UAVs.\n- [23] proposes a deep RL-based adaptive computation offloading method to balance the tradeoff between energy consumption and data transmission delay in an MEC-based vehicular network.\n- [24] introduces a framework using MEC-mounted UAVs to support mobile users in the extended 5G network, with an RL method adopted to manage the resources carried by the UAV.\n- [25] proposes deep deterministic policy gradient (DDPG)- and hierarchical DDPG (HDDPG)-based schemes to jointly manage spectrum, computing, and caching resources available to an MEC-mounted BS.\n- However, most existing works have studied vehicular networks supported either by MEC-mounted BSs or UAVs.\n- There is still a need for efforts in efficiently allocating resources to support applications with various resource demands and heterogeneous quality-of-service (QoS) requirements in vehicular networks with MEC-mounted BSs and UAVs.\n\n\n- In this paper, the focus is on investigating multi-dimensional resource management in MEC- and UAV-assisted vehicular networks.\n- MEC servers are mounted at a macro eNodeB (MeNB) and in some UAVs to provide resource access to vehicles with limited onboard resources.\n- RL methods are adopted, inspired by existing works [23]–[26], to achieve real-time resource management in the considered scenario.\n- Due to sensitive delay requirements in certain vehicular applications (e.g., autonomous driving) and the wireless transmission time between a UAV and a controller, installing a central controller at the MeNB or an edge node for centralized resource management is sometimes infeasible.\n- To address this, a distributed cooperative scheme is developed based on a multi-agent RL method to manage the multi-dimensional resources available to the MEC-mounted MeNB and UAVs.\n- The main contributions of this work are summarized as follows:\n\n\n- In this paper, the focus is on investigating multi-dimensional resource management in MEC- and UAV-assisted vehicular networks.\n- MEC servers are mounted at a macro eNodeB (MeNB) and in some UAVs to provide resource access to vehicles with limited onboard resources.\n- RL methods are adopted, inspired by existing works [23]–[26], to achieve real-time resource management in the considered scenario.\n- Due to sensitive delay requirements in certain vehicular applications (e.g., autonomous driving) and the wireless transmission time between a UAV and a controller, installing a central controller at the MeNB or an edge node for centralized resource management is sometimes infeasible.\n- To address this, a distributed cooperative scheme is developed based on a multi-agent RL method to manage the multi-dimensional resources available to the MEC-mounted MeNB and UAVs.\n- The main contributions of this work are summarized as follows:\n1. To support as many offloaded tasks as possible while satisfying their QoS requirements, we formulate an individual optimization problem to each MEC server to jointly manage the MEC-mounted MeNB’s and UAVs’ spectrum, computing, and caching resources;\n2. Because of the vehicle association pattern variables, the formulated problems are coupled with each other and non-convex. To rapidly solve these problems to satisfy the sensitive delay requirements of the offloaded tasks, we transform each formulated problem according to the main idea of RL;\n3. We convert the transformed problems as a multi-agent problem by letting each MEC server act as an agent and develop a multi-agent DDPG (MADDPG) algorithm to solve it. Through training the MADDPG model offline, the vehicle association and resource allocation decisions can be made in real time by each MEC server.\n\n","x":-32494,"y":1881,"width":527,"height":2952,"color":"6"},
		{"id":"838ff88d8ddde355","type":"text","text":"- **Challenges in Vehicular Networks and Introduction of MEC Servers:**\n  - Considering the stringent delay requirements of some applications and huge latency on wireless and wired communications between a vehicle and the cloud computing server, offloading vehicles’ tasks to the cloud computing server is sometimes inapplicable [13]–[15].\n  - Meanwhile, physically increasing the amount of onboard resources would result in a serious increase in the manufacturing cost of vehicles.\n  - The MEC server, as an extension of the cloud computing server, shifts computing and caching capabilities close to user devices [16]–[19].\n  - It allows vehicles to offload tasks to it via multiple wireless communication technologies.\n  - By saving the time consumption on communications between the MEC and the cloud computing server, the sensitive delay requirement of an offloaded task can be satisfied.\n  - However, the amount of computing/caching resources is generally preset in each MEC-mounted base station (BS) while the resource demand from vehicle users is time-varying, underused, or wasted resource issues remain in the vehicular scenarios supported by only MEC-mounted BSs, especially when there exists bursty traffic caused by some social activities or events.\n  - Taking the flexibility advantage of UAVs, mounting MEC servers in UAVs can help to address the above issues by dispatching the MEC-mounted UAVs to assist the designated BSs [11], [20].\n  - Related applications have been also considered in different projects launched by many leading companies [21].\n\n- **Recent Efforts and Resource Management in MEC- and UAV-assisted Vehicular Networks:**\n  - To implement MEC- and UAV-assisted vehicular networks, recent efforts have been made, with a focus on the deployment of MEC-mounted UAVs.\n    - [22] has studied how to deploy and schedule the MEC-mounted UAVs to support vehicular applications.\n    - Resource management is another research emphasis, with most works adopting optimization and reinforcement learning (RL) methods.\n    - In [12], the transmit powers of vehicles and the trajectories of UAVs are jointly optimized to maximize the resource efficiency on MEC-mounted UAVs.\n    - [23] proposes a deep RL-based adaptive computation offloading method to balance the tradeoff between energy consumption and data transmission delay in an MEC-based vehicular network.\n    - [24] introduces a framework using MEC-mounted UAVs to support mobile users in the extended 5G network, with an RL method adopted to manage the resources carried by the UAV.\n    - [25] proposes deep deterministic policy gradient (DDPG)- and hierarchical DDPG (HDDPG)-based schemes to jointly manage spectrum, computing, and caching resources available to an MEC-mounted BS.\n    - However, most existing works have studied vehicular networks supported either by MEC-mounted BSs or UAVs.\n    - There is still a need for efforts in efficiently allocating resources to support applications with various resource demands and heterogeneous quality-of-service (QoS) requirements in vehicular networks with MEC-mounted BSs and UAVs.\n\n- **Focus and Contributions of the Paper:**\n  - In this paper, the focus is on investigating multi-dimensional resource management in MEC- and UAV-assisted vehicular networks.\n  - MEC servers are mounted at a macro eNodeB (MeNB) and in some UAVs to provide resource access to vehicles with limited onboard resources.\n  - RL methods are adopted, inspired by existing works [23]–[26], to achieve real-time resource management in the considered scenario.\n  - Due to sensitive delay requirements in certain vehicular applications (e.g., autonomous driving) and the wireless transmission time between a UAV and a controller, installing a central controller at the MeNB or an edge node for centralized resource management is sometimes infeasible.\n  - To address this, a distributed cooperative scheme is developed based on a multi-agent RL method to manage the multi-dimensional resources available to the MEC-mounted MeNB and UAVs.\n  - The main contributions of this work are summarized as follows:\n    1. To support as many offloaded tasks as possible while satisfying their QoS requirements, we formulate an individual optimization problem to each MEC server to jointly manage the MEC-mounted MeNB’s and UAVs’ spectrum, computing, and caching resources;\n    2. Because of the vehicle association pattern variables, the formulated problems are coupled with each other and non-convex. To rapidly solve these problems to satisfy the sensitive delay requirements of the offloaded tasks, we transform each formulated problem according to the main idea of RL;\n    3. We convert the transformed problems as a multi-agent problem by letting each MEC server act as an agent and develop a multi-agent DDPG (MADDPG) algorithm to solve it. Through training the MADDPG model offline, the vehicle association and resource allocation decisions can be made in real time by each MEC server.","x":-31724,"y":2259,"width":781,"height":1849,"color":"5"},
		{"id":"3c9f9ad2c6c2d15e","type":"text","text":"- **挑战和MEC服务器引入在车联网中的应用：**\n  - 针对一些应用的严格延迟要求以及车辆与云计算服务器之间的无线和有线通信的巨大延迟，有时将车辆任务卸载到云计算服务器是不可行的[13]–[15]。\n  - 与此同时，物理增加车载资源的数量会导致车辆制造成本的严重增加。\n  - MEC服务器作为云计算服务器的扩展，将计算和缓存功能移到用户设备附近 [16]–[19]。\n  - 它允许车辆通过多种无线通信技术将任务卸载到它。\n  - 通过减少MEC和云计算服务器之间的通信时间，可以满足卸载任务的敏感延迟要求。\n  - 然而，每个MEC装载的基站（BS）中的计算/缓存资源量通常是预设的，而车辆用户的资源需求是时变的，这在仅由MEC装载的BS支持的车辆场景中会导致资源的浪费或低使用率问题，尤其是在由一些社交活动或事件引起的突发流量存在时。\n  - 利用无人机（UAV）的灵活性，将MEC服务器安装在无人机上可以通过派遣MEC装载的无人机来协助指定的BS来解决上述问题 [11]， [20]。\n  - 许多领先公司启动的不同项目中也考虑了相关应用 [21]。\n\n- **MEC-和UAV辅助车联网中的最新努力和资源管理：**\n  - 为了实施MEC-和UAV辅助车联网，最近进行了努力，重点放在部署MEC装载的无人机上。\n    - [22] 研究了如何部署和调度MEC装载的无人机以支持车辆应用。\n    - 资源管理是另一个研究重点，大多数工作采用优化和强化学习（RL）方法。\n    - 在 [12] 中，联合优化了车辆的发送功率和无人机的轨迹，以最大化MEC装载的无人机上的资源效率。\n    - [23] 提出了一种基于深度强化学习（RL）的自适应计算卸载方法，以平衡能耗和数据传输延迟在基于MEC的车联网中的权衡。\n    - [24] 引入了一个框架，使用MEC装载的无人机支持扩展5G网络中的移动用户，采用RL方法来管理无人机携带的资源。\n    - [25] 提出了基于深度确定性策略梯度（DDPG）和层次DDPG（HDDPG）的方案，以共同管理MEC装载的BS可用的频谱、计算和缓存资源。\n    - 然而，大多数现有的工作都研究了由MEC装载的BS或无人机支持的车联网。\n    - 在具有MEC装载的BS和无人机的车联网中，仍然需要努力有效地分配资源，以支持具有各种资源需求和异构服务质量（QoS）要求的应用。\n\n- **论文的焦点和贡献：**\n  - 本文的重点是调查MEC-和UAV辅助车联网中的多维资源管理。\n  - MEC服务器被安装在宏eNodeB（MeNB）和一些无人机上，以为具有有限车载资源的车辆提供资源访问。\n  - 我们采用受现有工作 [23]–[26] 启发的RL方法，实现所考虑场景中的实时资源管理。\n  - 由于某些车联网应用（例如自动驾驶）中的敏感延迟要求以及无人机与控制器之间的无线传输时间，将中央控制器安装在MeNB或边缘节点进行集中资源管理有时是不可行的。\n  - 为了解决这个问题，我们开发了一个基于多智能体RL方法的分布式合作方案，以管理MEC装载的MeNB和无人机的多维资源。\n  - 本文的主要贡献总结如下：\n    1. 为了支持尽可能多的卸载任务并满足它们的QoS要求，我们为每个MEC服务器制定了一个个体优化问题，以共同管理MEC装载的MeNB和无人机的频谱、计算和缓存资源；\n    2. 由于车辆关联模式变量的存在，所制定的问题相互耦合且非凸。为了迅速解决这些问题以满足卸载任务的敏感延迟要求，我们根据RL的主要思想转换了每个制定的问题；\n    3. 我们通过让每个MEC服务器充当一个智能体，将转换后的问题转化为多智能体问题，并开发了一个多智能体DDPG（MADDPG）算法来解决它。通过离线训练MADDPG模型，每个MEC服务器可以实时做出车辆关联和资源分配决策。","x":-30807,"y":2619,"width":744,"height":1321,"color":"1"},
		{"id":"e38860a49d379893","type":"text","text":"- **挑战和MEC服务器引入在车联网中的应用：**\n  - 针对一些应用的严格延迟要求以及车辆与云计算服务器之间的无线和有线通信的巨大延迟，有时将车辆任务卸载到云计算服务器是不可行的[13]–[15]。\n  - 与此同时，物理增加车载资源的数量会导致车辆制造成本的严重增加。\n  - MEC服务器作为云计算服务器的扩展，将计算和缓存功能移到用户设备附近 [16]–[19]。\n  - 它允许车辆通过多种无线通信技术将任务卸载到它。\n  - 通过减少MEC和云计算服务器之间的通信时间，可以满足卸载任务的敏感延迟要求。\n  - 然而，每个MEC装载的基站（BS）中的计算/缓存资源量通常是预设的，而车辆用户的资源需求是时变的，这在仅由MEC装载的BS支持的车辆场景中会导致资源的浪费或低使用率问题，尤其是在由一些社交活动或事件引起的突发流量存在时。\n  - 许多领先公司启动的不同项目中也考虑了相关应用 [21]。\n- **论文的焦点和贡献：**\n  - 本文的重点是多维资源管理。\n  - MEC服务器被安装在宏eNodeB（MeNB）和一些无人机上，以为具有有限车载资源的车辆提供资源访问。\n  - 我们采用受现有工作 [23]–[26] 启发的RL方法，实现所考虑场景中的实时资源管理。\n  - 由于某些车联网应用（例如自动驾驶）中的敏感延迟要求以及无人机与控制器之间的无线传输时间，将中央控制器安装在MeNB或边缘节点进行集中资源管理有时是不可行的。\n  - 为了解决这个问题，我们开发了一个基于多智能体RL方法的分布式合作方案，以管理MEC装载的MeNB和无人机的多维资源。\n  - 本文的主要贡献总结如下：\n    1. 为了支持尽可能多的卸载任务并满足它们的QoS要求，我们为每个MEC服务器制定了一个个体优化问题，以共同管理MEC装载的MeNB和无人机的频谱、计算和缓存资源；\n    2. 由于车辆关联模式变量的存在，所制定的问题相互耦合且非凸。为了迅速解决这些问题以满足卸载任务的敏感延迟要求，我们根据RL的主要思想转换了每个制定的问题；\n    3. 我们通过让每个MEC服务器充当一个智能体，将转换后的问题转化为多智能体问题，并开发了一个多智能体DDPG（MADDPG）算法来解决它。通过离线训练MADDPG模型，每个MEC服务器可以实时做出车辆关联和资源分配决策。","x":-29910,"y":2965,"width":880,"height":742,"color":"3"},
		{"id":"03a2f309483a3fd2","type":"text","text":"- 挑战和MEC服务器引入在车联网中的应用:\n- 论文的焦点和贡献:","x":-28870,"y":3297,"width":480,"height":120,"color":"4"},
		{"id":"60cf73ff2492e3e8","type":"text","text":"### Multi-Agent Reinforcement Learning Based Resource Management in MEC- and UAV-Assisted Vehicular Networks\n\nPeng, Haixia, 和Xuemin Shen. 《Multi-Agent Reinforcement Learning Based Resource Management in MEC- and UAV-Assisted Vehicular Networks》. *IEEE Journal on Selected Areas in Communications* 39, 期 1 (2021年1月): 131–41. <https://doi.org/10.1109/JSAC.2020.3036962>.\n\nConsidering the stringent delay requirements of some applications and huge latency on wireless and wired communications between a vehicle and the cloud computing server, offloading vehicles’ tasks to the cloud computing server is sometimes inapplicable [13]–[15]. Meanwhile, physically increasing the amount of onboard resources would result in a serious increase in the manufacturing cost of vehicles. The MEC server, as an extension of the cloud computing server, shifts computing and caching capabilities close to user devices [16]–[19], and allows vehicles to offload tasks to it via multiple wireless communication technologies. By saving the time consumption on communications between the MEC and the cloud computing server, the sensitive delay requirement of an offloaded task can be satisfied. However, the amount of computing/caching resources is generally preset in each MEC-mounted base station (BS) while the resource demand from vehicle users is time-varying, underused or wasted resource issues remain in the vehicular scenarios supported by only MEC-mounted BSs, especially when there exists bursty traffic caused by some social activities or events. Taking the flexibility advantage of UAVs, mounting MEC servers in UAVs can help to address the above issues by dispatching the MEC-mounted UAVs to assist the designated BSs [11], [20]. Related applications have been also considered in different projects launched by many leading companies [21].\n\nTo implement MEC- and UAV-assisted vehicular networks, many efforts have been made recently. Some of them have been focused on the deployment of MEC-mounted UAVs. For example, [22] has studied how to deploy and schedule the MEC-mounted UAVs to support vehicular applications. Resource management, as another research emphasis, has also attracted lots of attention from the existing works, where most of them adopt the optimization and reinforcement learning (RL) methods. In [12], the transmit powers of vehicles and the trajectories of UAVs have been jointly optimized to maximize the resource efficiency on MEC-mounted UAVs. In [23], a deep RL based adaptive computation offloading method has been proposed to balance the tradeoff between energy consumption and data transmission delay in an MEC-based vehicular network. In [24], a framework using MEC-mounted UAVs has been proposed to support mobile users in the extended 5G network, and an RL method is adopted to manage the resources carried by the UAV. To jointly manage the spectrum, computing, and caching resources available to an MEC-mounted BS, deep deterministic policy gradient (DDPG)- and hierarchical DDPG (HDDPG)-based schemes have been proposed in [25]. However, only vehicular networks supported either by MEC-mounted BSs or UAVs have been studied by most of the existing works. How to perform efficient resource allocation to support applications with various resource demand and heterogeneous quality-of-service (QoS) requirements in vehicular networks with MEC-mounted BSs and UAVs still needs efforts.\n\nIn this paper, we investigate multi-dimensional resource management in the MEC- and UAV-assisted vehicular networks, where MEC servers are mounted at a macro eNodeB (MeNB) and in some UAVs to provide resource access to vehicles with limited onboard resources. Inspired by existing works [23]–[26], we adopt RL methods to achieve real-time resource management in the considered scenario. Considering the sensitive delay requirements of some vehicular applications (e.g., autonomous driving) and the wireless transmission time between a UAV and a controller, installing a central controller either at the MeNB or an edge node to enable a centralized resource management scheme is infeasible sometimes. Thus, we develop a distributed cooperative scheme based on a multi-agent RL method to manage the multi-dimensional resources available to the MEC-mounted MeNB and UAVs. The main contributions of this work are summarized as follows,\n\n1. To support as many offloaded tasks as possible while satisfying their QoS requirements, we formulate an individual optimization problem to each MEC server to jointly manage the MEC-mounted MeNB’s and UAVs’ spectrum, computing, and caching resources;\n2. Because of the vehicle association pattern variables, the formulated problems are coupled with each other and non-convex. To rapidly solve these problems to satisfy the sensitive delay requirements of the offloaded tasks, we transform each formulated problem according to the main idea of RL;\n3. We convert the transformed problems as a multi-agent problem by letting each MEC server act as an agent and develop a multi-agent DDPG (MADDPG) algorithm to solve it. Through training the MADDPG model offline, the vehicle association and resource allocation decisions can be made in real time by each MEC server.\n","x":-33270,"y":2164,"width":527,"height":2387},
		{"id":"b9441fbc2f7662a2","type":"text","text":"- **Importance of Energy Efficiency and Task Offloading in VEC:**\n  - In Vehicle Edge Computing (VEC), saving energy consumption and accelerating computation through task offloading to Roadside Units (RSUs) with edge servers is critical [7].\n  - Existing works in VEC mainly focus on task offloading algorithm design to minimize response time or energy consumption.\n\n- **Task Offloading Algorithm Design:**\n  - Adaptive algorithms [5,8] aim to minimize average response time.\n  - Contract theoretical modeling [9] proposes an algorithm to minimize network delay.\n  - Deep reinforcement learning [10] maximizes computation and communication rates.\n  - Energy-efficient resource allocation algorithm [11] and algorithms minimizing the weighted sum of energy consumption and latency [12] are proposed.\n\n- **Task Scheduling Challenges in VEC:**\n  - Geographical differences in vehicle locations result in an imbalance in edge server load, leading to high latency and increased energy consumption.\n  - Few existing works consider task scheduling algorithms in VEC systems.\n\n- **Existing Task Scheduling Algorithms in VEC:**\n  - Energy-efficient scheduling algorithms [7] and those based on ant colony optimization [13] are proposed.\n  - Algorithms considering partial offloading scheduling and power allocation [14].\n  - Task scheduling algorithms for computation-intensive and time-sensitive applications [15].\n\n- **Energy Efficiency Strategies for RSUs:**\n  - RSUs switch to a sleep state during low task request periods to save energy [16–19].\n  - Algorithms designed to maximize energy efficiency by implementing sleeping strategies for base stations [20,21].\n  - Efficient algorithms minimizing energy consumption through joint cell association and on–off schemes [22].\n  - User-centric energy-aware mobility management schemes [23].\n\n- **Gap in Existing Works:**\n  - Existing works mainly focus on task offloading algorithm design, with limited attention to task scheduling.\n  - None of the works consider the possibility of edge servers transitioning to a sleep state.\n\n- **Contributions of the Paper:**\n  1. Introduction of a novel task scheduling problem in VEC, aiming to minimize total task delay, considering dynamic state switching of edge servers in RSUs.\n  2. Modeling task requests as an independent Poisson stream and edge servers as simple M/M/1 queuing systems. Formulation of the NP-hard problem of minimizing total task delay.\n  3. Proposal of a greedy algorithm for task scheduling by selecting edge servers to minimize current task response time. Introduction of a customized tabu search algorithm to refine solutions generated by the greedy algorithm.\n  4. Introduction of a deep Q-network based algorithm utilizing deep reinforcement learning to learn optimal scheduling policies without prior knowledge of dynamic statistics.\n  5. Simulation results show the superiority of proposed algorithms over a random algorithm. The deep Q-network-based algorithm outperforms others, achieving significant reductions in total task response time.","x":-26310,"y":2762,"width":781,"height":1225,"color":"5"},
		{"id":"9f283274c02f05c4","type":"text","text":"- **VEC中能源效率和任务卸载的重要性:**\n  - 在车辆边缘计算（VEC）中，通过任务卸载到边缘服务器的道路边单元（RSU）来节省能耗并加速计算是至关重要的[7]。\n  - VEC中的现有工作主要集中在任务卸载算法设计上，以最小化响应时间或能耗。\n\n- **任务卸载算法设计:**\n  - 自适应算法[5,8]旨在最小化平均响应时间。\n  - 契约理论建模[9]提出了一种最小化网络延迟的算法。\n  - 深度强化学习[10]最大化计算和通信速率。\n  - 能效资源分配算法[11]和最小化能耗和延迟加权和的算法[12]被提出。\n\n- **VEC中的任务调度挑战:**\n  - 车辆位置的地理差异导致边缘服务器负载不平衡，导致高延迟和能耗增加。\n  - 少数现有工作考虑VEC系统中的任务调度算法。\n\n- **VEC中现有的任务调度算法:**\n  - 提出了能效调度算法[7]和基于蚁群优化的算法[13]。\n  - 考虑部分卸载调度和功率分配的算法[14]。\n  - 面向计算密集型和时间敏感应用的任务调度算法[15]。\n\n- **RSU的能源效率策略:**\n  - RSU在低任务请求时段转入休眠状态以节省能量[16–19]。\n  - 通过实施基站的休眠策略来最大化能源效率的算法[20,21]。\n  - 通过联合小区关联和开关方案最小化能耗的高效算法[22]。\n  - 面向用户的能量感知移动管理方案[23]。\n\n- **现有工作中的差距:**\n  - 现有工作主要集中在任务卸载算法设计上，对任务调度的关注有限。\n  - 这些工作中没有考虑边缘服务器转入休眠状态的可能性。\n\n- **论文的贡献:**\n  1. 引入VEC中的一种新颖的任务调度问题，旨在最小化总任务延迟，考虑RSU中边缘服务器的动态状态切换。\n  2. 将任务请求建模为独立的泊松流，将边缘服务器建模为简单的M/M/1排队系统。制定了最小化总任务延迟的NP难问题。\n  3. 提出了一种贪婪算法，通过选择边缘服务器来最小化当前任务响应时间。引入了一种定制的禁忌搜索算法，以改进贪婪算法生成的解决方案。\n  4. 引入了一种基于深度强化学习的深度Q网络算法，用于学习在没有动态统计先验知识的情况下的最优调度策略。\n  5. 仿真结果显示了所提出算法在随机算法上的优越性。基于深度Q网络的算法优于其他算法，实现了总任务响应时间的显著减少。","x":-24470,"y":2984,"width":880,"height":985,"color":"3"},
		{"id":"f241a24d8f473b61","type":"text","text":"- **VEC中能源效率和任务卸载的重要性:**\n  - 在车辆边缘计算（VEC）中，通过任务卸载到边缘服务器的道路边单元（RSU）来节省能耗并加速计算是至关重要的[7]。\n  - VEC中的现有工作主要集中在任务卸载算法设计上，以最小化响应时间或能耗。\n\n- **任务卸载算法设计:**\n  - 自适应算法[5,8]旨在最小化平均响应时间。\n  - 契约理论建模[9]提出了一种最小化网络延迟的算法。\n  - 深度强化学习[10]最大化计算和通信速率。\n  - 能效资源分配算法[11]和最小化能耗和延迟加权和的算法[12]被提出。\n\n- **VEC中的任务调度挑战:**\n  - 车辆位置的地理差异导致边缘服务器负载不平衡，导致高延迟和能耗增加。\n  - 少数现有工作考虑VEC系统中的任务调度算法。\n\n- **VEC中现有的任务调度算法:**\n  - 提出了能效调度算法[7]和基于蚁群优化的算法[13]。\n  - 考虑部分卸载调度和功率分配的算法[14]。\n  - 面向计算密集型和时间敏感应用的任务调度算法[15]。\n\n- **RSU的能源效率策略:**\n  - RSU在低任务请求时段转入休眠状态以节省能量[16–19]。\n  - 通过实施基站的休眠策略来最大化能源效率的算法[20,21]。\n  - 通过联合小区关联和开关方案最小化能耗的高效算法[22]。\n  - 面向用户的能量感知移动管理方案[23]。\n\n- **现有工作中的差距:**\n  - 现有工作主要集中在任务卸载算法设计上，对任务调度的关注有限。\n  - 这些工作中没有考虑边缘服务器转入休眠状态的可能性。\n\n- **论文的贡献:**\n  1. 引入VEC中的一种新颖的任务调度问题，旨在最小化总任务延迟，考虑RSU中边缘服务器的动态状态切换。\n  2. 将任务请求建模为独立的泊松流，将边缘服务器建模为简单的M/M/1排队系统。制定了最小化总任务延迟的NP难问题。\n  3. 提出了一种贪婪算法，通过选择边缘服务器来最小化当前任务响应时间。引入了一种定制的禁忌搜索算法，以改进贪婪算法生成的解决方案。\n  4. 引入了一种基于深度强化学习的深度Q网络算法，用于学习在没有动态统计先验知识的情况下的最优调度策略。\n  5. 仿真结果显示了所提出算法在随机算法上的优越性。基于深度Q网络的算法优于其他算法，实现了总任务响应时间的显著减少。","x":-25379,"y":3008,"width":744,"height":961,"color":"1"},
		{"id":"691ef2784504755b","type":"text","text":"- VEC中能源效率和任务卸载的重要性:\n- 任务卸载算法设计:\n- VEC中的任务调度挑战:\n- VEC中现有的任务调度算法:\n- RSU的能源效率策略:\n- 现有工作中的差距:\n- 论文的贡献:","x":-23430,"y":3357,"width":480,"height":240,"color":"4"},
		{"id":"93ea1152bc6b1264","type":"text","text":"### Efficient Task Scheduling for Servers with Dynamic States in Vehicular Edge Computing\n\nWu, Yalan, Jigang Wu, Long Chen, Jiaquan Yan和Yuchong Luo. 《Efficient Task Scheduling for Servers with Dynamic States in Vehicular Edge Computing》. *Computer Communications* 150 (2020年1月): 245–53. <https://doi.org/10.1016/j.comcom.2019.11.019>.\n\nIn VEC, it is critical to save energy consumption and accelerate the computation process by offloading a particular task to the roadside unit (RSU) which is placed with an edge server [7]. Therefore, most existing works in VEC focus on the algorithm design of task offloading, to minimize the response time of tasks or energy consumption. Authors in [5,8] propose adaptive algorithms for task offloading to minimize the average response time of tasks. Authors in [9] propose an efficient algorithm by contract theoretical modeling to minimize the network delay. Authors in [10] propose an intelligent offloading ∗ Corresponding author. E-mail address: <asjgwucn@outlook.com> (J. Wu). system by leveraging deep reinforcement learning to maximum the total computation and communication rates. Authors in [11] propose an energy-efficient algorithm for resource allocation in VEC. In addition, the offloading algorithm is proposed to minimize the weighted sum of energy consumption and latency [12].\n\nDue to geographical difference of vehicles, the tasks are distributed in different locations. Therefore, the load of edge servers is serious unbalanced, which results in high latency of tasks and energy consumption. However, the task scheduling algorithm in edge servers is only considered in few existing works on VEC. A heuristic algorithm for energy-efficient scheduling is proposed to minimize the energy consumption [7]. A scheduling algorithm based on ant colony optimization is proposed to minimize the completion time of jobs [13]. An algorithm for partial offloading scheduling and power allocation is proposed to minimize the weighted sum of the execution delay and energy consumption in mobile edge computing [14]. Meanwhile, authors in [15] propose a task scheduling algorithm for the applications with computation intensive and time-sensitive to minimize the completion time of tasks.\n\nThe RSUs, placed with edge servers, are densely distributed in the VEC system to guarantee the quality of service. Besides, the spatial distribution of vehicles has much difference. For example, vehicles are mainly distributed in city, especially in hot spots, instead of the remote districts. Thus, the task requests are greatly varied in space. For saving energy, some RSUs switch their state into sleep state, when there are few task requests [16–19]. Authors in [20,21] propose algorithms to maximize the energy efficiency by designing the sleeping strategies for base stations. Authors in [22] propose an efficient algorithm to minimize the energy consumption by jointing the cell association and on–off scheme. Authors in [23] propose a novel user-centric energyaware mobility management scheme to minimize the delay of tasks in the mobile edge computing, in which the candidate base stations randomly switch their states between sleep and work.\n\nTo the best of our knowledge, the existing works in VEC mainly target on the area in the algorithm designing for task offloading, only a few of them investigate the algorithms for task scheduling. Meanwhile, none of the existing works in VEC consider that the edge servers may turn to sleep state in some cases. Therefore, in this paper, we investigate the problem of scheduling the tasks in VEC to minimize the response time, in the case of that the edge servers may switch their state between sleep and work according to the traffic of the requests. The contributions of this paper are summarized as follows.\n\n- We propose a novel problem of task scheduling for minimizing the total delay of tasks in VEC, with considering that the edge servers in RSUs can dynamically switch their states between sleep and work. Meanwhile, we model task requests generated by vehicles as an independent Poisson stream, and we model an edge server in the RSU as a simple M/M/1 queuing system. Besides, the proposed problem of minimizing the total delay of tasks is formulated and the NP-hardness is proved in this paper.\n- To solve the proposed problem, we contribute a greedy algorithm by carefully choosing the edge server so that the response time of the task arrived at the current time is minimum. Meanwhile, we customize a tabu search algorithm, which can successfully refine the solution generated by the greedy algorithm.\n- We also propose a deep Q-network based algorithm by utilizing the deep reinforcement learning algorithm to learn the optimal scheduling policy for minimizing the total delay of tasks, without having a priori knowledge of dynamic statistics.\n- Simulation results show that, our proposed algorithms outperform the random algorithm also proposed in this paper in terms of the total response time of tasks. Especially, the deep Q-network based algorithm performs better than the other algorithms in terms of the total response time of tasks. For example, for the case of that the maximum tolerant response time of each task is 14 s, the total response time of tasks decreases by 24.13%, 28.73% and 35.95%, compared with the customized tabu search algorithm, the greedy algorithm and the random algorithm, respectively.","x":-27567,"y":2253,"width":527,"height":2448},
		{"id":"b8dca4a353594e1f","type":"text","text":"先前研究概述","x":-29580,"y":-4440,"width":250,"height":60,"color":"6"},
		{"id":"46b1742079e5c58b","type":"text","text":"现有方案局限性","x":-29580,"y":-3980,"width":250,"height":50,"color":"6"},
		{"id":"43217431945deb29","type":"text","text":"方法论","x":-29580,"y":-3740,"width":250,"height":60,"color":"6"},
		{"id":"8005a0253235b71b","type":"text","text":"论文的贡献","x":-29580,"y":-3540,"width":250,"height":60,"color":"6"},
		{"id":"896f0266a998ed96","type":"text","text":"**先前研究概述:**\n- 解决在实施基于MEC的车联网中的挑战一直是最近研究的焦点，涵盖了以下方面：\n  - 架构设计\n  - 任务卸载方案\n  - 资源管理方案等。\n\n\n- 一个例子是对基于MEC的分层车联网框架的研究，该框架包括车辆级别的车载计算/存储资源和服务器级别的资源（MEC和云计算服务器）[10]，[12]，[14]–[16]。\n\n- 已经提出了任务卸载和资源管理方案，以管理频谱/计算/存储资源并对车辆用户进行任务卸载决策 [10]，[15]–[17]。\n\n- 大多数现有方案仅考虑一两个维度的资源，这可能无法直接支持具有高维资源需求的车联网应用，例如编队/车队控制的计算任务 [7]。\n\n- 在基于MEC的车联网中，由于车辆高度移动和资源需求的时变性，多维资源管理的计算复杂性增加。基于纯优化方法的方案在具有对延迟敏感应用的场景中变得不可行，特别是对于计算复杂性挑战较大的情况。\n\n- 探讨了人工智能（AI）技术，尤其是强化学习（RL），作为快速资源管理的解决方案 [20]–[23]。 Q学习 [16]，[24]，深度Q学习 [25]–[27]，以及演员-评论家 [18]，[28] 算法在无线通信网络中的资源管理中被广泛使用。 在这些方法的启发下，并考虑到动态的车辆网络环境，本文研究了使用深度RL来共同管理频谱、计算和存储资源，以支持基于MEC的车联网中对延迟敏感的应用 [12]。\n","x":-28630,"y":-2860,"width":640,"height":617},
		{"id":"dee1f05eac0ddf50","type":"text","text":"- **传统方法处理MEC中的JCORA问题:**\n  - 包括凸逼近[5]，[6]，博弈论[7]–[9]和元启发式算法[4]，[10]。\n  - 面临挑战，如指数增长的搜索空间和沉重的计算负担，尤其是在大规模场景中。\n\n- **在JCORA问题中使用深度强化学习（DRL）:**\n  - 正在引起越来越多的研究兴趣。\n  - 利用深度神经网络（DNNs）的强大功能[3]，[12]–[20]。\n  - 深度确定性策略梯度（DDPG）[16]，[18]在处理具有连续动作空间的动态优化问题方面显示出潜力。\n\n- **传统DDPG的缺点:**\n  - 演员和评论家网络都依赖于全连接网络（FCNs）。\n  - FCNs存在缺点：大量的可训练参数，使训练困难，以及仅具有提取全局判别状态和动作策略特征的有限能力。\n  - 忽视任务序列的时间变化，可能忽视用于函数逼近的有用的形状特征。\n  - 从回放缓冲区均匀采样经验转换，将所有经验视为相等，而不考虑其重要性。","x":-27956,"y":-2767,"width":606,"height":524},
		{"id":"c86bfb4ac1499def","type":"text","text":"- **基于MEC的物联网（IoT）网络:**\n  - 在基于MEC的物联网中，设备可以将计算任务卸载到MEC服务器，提高任务处理速度并节省设备能量 [15]–[17]。\n  - 主要技术挑战是确定何时、是否以及有多少计算任务应该被卸载。\n  - 大量文献集中在设计在不同性能要求下的最佳策略 [18]–[21]。\n  - 为了实现物联网网络的长期能效，[18] 提出了一种高效的边缘计算基础设施。\n  - 针对随机任务到达、无线信道变化、拥塞的空中接口和禁止性反馈，[19] 生成了对网络知识陈旧具有容忍性的渐近最优调度，减轻了对反馈的严格要求。\n  - 在 [20] 和 [21] 中分别提出了MEC的最优调度解决方案和能效资源分配策略。\n\n- **车辆边缘计算网络:**\n  - 鉴于车辆在物联网系统中作为用户设备（UE）的重要角色，研究人员广泛探索了车辆边缘计算网络的结构和资源分配方法 [22]–[27]。\n  - C. Wang等人提出了一种可扩展的SDN启用的MEC架构，集成了异构车辆网络，以减少总体延迟并卸载骨干网络的流量负载 [22]。\n  - 为了解决计算卸载的延迟和传输成本，[23] 提出了一种面向车辆网络的基于云的MEC卸载框架。\n  - 在 [24] 中，提出了一种基于MEC辅助的车辆卸载模式的目标服务器选择策略，旨在提高在车辆数据传输失败的情况下任务卸载的可靠性。\n  - 在 [25]、[26] 中提出了新颖的基于MEC的车辆网络，为不同场景精心设计了计算卸载策略。\n  - 强化学习被用于车辆云中的长期资源供应，以解决资源的动态需求和严格的服务质量（QoS）要求 [27]。\n  - 然而，在现有文献中，车辆主要充当MEC网络中的用户，其中边缘服务器静态部署可能导致由于庞大数量的用户设备（UEs）的服务请求而产生“服务空白”。\n\n- **车辆边缘计算（VEC）网络介绍:**\n  - 本文重点设计了一种车辆边缘计算（VEC）网络，使车辆能够在传统边缘服务器之外提供计算服务。\n  - 传统的边缘服务器连接到路侧单元、小区基站等固定位置，而提出的架构旨在扩展计算服务范围，增强移动边缘计算（MEC）网络的灵活性。\n\n- **高效的计算卸载方案和优化问题:**\n  - 本文提出了一种针对用户设备（UEs）的高效计算卸载方案，考虑到由UEs生成的计算任务的延迟。\n  - 制定了一个优化问题，旨在最大化提出的VEC网络的总效用。\n\n- **解决随机流量和通信不确定性:**\n  - 车辆通信环境中的随机流量和通信不确定性是需要解决的关键挑战。\n  - 选择了Q学习方法，这是一种无模型的强化学习方法，由于其适用于车辆边缘计算网络中环境元素未知的问题 [32]。\n  - 本文引入了深度强化学习（DRL），使用深度神经网络来近似Q函数，提供更高的性能和更强大的学习 [33]–[36]。\n  - 将提出的问题进一步制定为半马尔可夫过程，并提出了两种强化学习方法（Q学习和DRL）来确定计算卸载和资源分配的策略。","x":-27960,"y":-2200,"width":640,"height":1193},
		{"id":"c5590e89aca3ebca","type":"text","text":"- **MEC及其认证介绍:**\n  - MEC通过在移动用户附近部署服务器，有效降低服务交付和网络运营延迟成本。\n  - 被欧洲5G基础设施公私合作伙伴关系（PPP）认证为下一代5G网络的关键技术。\n\n- **MEC的优势:**\n  - 低延迟\n  - 高带宽\n  - 实时无线网络信息\n  - 位置感知 [7,8]。\n\n- **基于MEC的车联网（IoV）:**\n  - MEC促进了从移动车辆卸载计算任务到网络边缘节点，满足IoV的超低延迟需求 [9]。\n  - 它将云服务下沉到无线接入网络的边缘，为行驶中的车辆提供计算服务 [10]。\n\n- **MEC-Based车联网的挑战:**\n  - 在基于MEC的车联网中，存在各种计算密集型和延迟敏感的任务。\n  - 每个任务都具有不同的资源需求，包括计算和通信资源。\n\n- **需要卸载策略和资源控制:**\n  - 需要合适的策略来控制卸载任务，确保系统的正常运行。\n  - 考虑卸载决策和资源分配对计算卸载性能的影响。","x":-28630,"y":-2200,"width":640,"height":596},
		{"id":"c5ab5d07e127b93e","type":"text","text":"- **MEC-Based Vehicular Networks的先前研究概述:**\n  - 先前研究中，一些任务卸载算法假定MEC服务器具有无限的计算资源，但不断增长的需求挑战了MEC服务器在承载网络中的计算能力。\n\n- **资源管理方法:**\n  - 大量文献集中在为单服务器和多服务器系统设计基于集中和分布式方法的卸载资源管理算法。\n\n- **单服务器系统中的挑战和局限性:**\n  - 单服务器系统中的挑战包括请求分布不平衡和难以处理空闲和过度占用服务器之间的共存。\n\n- **单服务器系统中的方法:**\n  - Lyu等人[13]引入了一种半分布式方法，采用了启发式卸载算法（HODA），具有可接受的复杂性，实现了卓越的系统效用。\n  - Li等人[16]设计了一个基于集中式DRL的模型，面临着行动空间激增的挑战。\n  - Chen等人[7]应用了基于多智能体策略的DRL模型，以减小行动空间的维度，但这种方法增加了计算能力较弱的设备的复杂性。\n\n- **多服务器系统和资源管理方法:**\n  - 引入了无线接入网络（RAN）中的多用户访问，以增强MEC网络的工作负载承载能力。\n  - 研究了集中式联合优化方法[10]，[11]，[12]以及涉及用户之间的非合作游戏的分布式方法[8]。\n\n- **多服务器系统中的考虑:**\n  - 一些模型中考虑了地理近距离[24]，[8]。\n  - 设计了一种模型，其中边缘设备仅连接到附近的服务器，通过标准化的有线接口（Mp3）在服务器之间中继工作负载，实现了多服务器系统[25]。\n\n- **分布式算法和基于DRL的解决方案:**\n  - Qian等人设计了一个分布式算法和一个用于统计和动态信道场景的集中式在线DRL解决方案，分别优化了选择具有最小化传输成本的接入点[14]。","x":-27290,"y":-2200,"width":640,"height":814},
		{"id":"da4eb9597b38757c","type":"text","text":"- **VEC中能源效率和任务卸载的重要性:**\n  - 在车辆边缘计算（VEC）中，通过任务卸载到边缘服务器的道路边单元（RSU）来节省能耗并加速计算是至关重要的[7]。\n  - VEC中的现有工作主要集中在任务卸载算法设计上，以最小化响应时间或能耗。\n\n- **任务卸载算法设计:**\n  - 自适应算法[5,8]旨在最小化平均响应时间。\n  - 契约理论建模[9]提出了一种最小化网络延迟的算法。\n  - 深度强化学习[10]最大化计算和通信速率。\n  - 能效资源分配算法[11]和最小化能耗和延迟加权和的算法[12]被提出。\n\n- **VEC中的任务调度挑战:**\n  - 车辆位置的地理差异导致边缘服务器负载不平衡，导致高延迟和能耗增加。\n  - 少数现有工作考虑VEC系统中的任务调度算法。\n\n- **VEC中现有的任务调度算法:**\n  - 提出了能效调度算法[7]和基于蚁群优化的算法[13]。\n  - 考虑部分卸载调度和功率分配的算法[14]。\n  - 面向计算密集型和时间敏感应用的任务调度算法[15]。\n\n- **RSU的能源效率策略:**\n  - RSU在低任务请求时段转入休眠状态以节省能量[16–19]。\n  - 通过实施基站的休眠策略来最大化能源效率的算法[20,21]。\n  - 通过联合小区关联和开关方案最小化能耗的高效算法[22]。\n  - 面向用户的能量感知移动管理方案[23]。\n\n- **现有工作中的差距:**\n  - 现有工作主要集中在任务卸载算法设计上，对任务调度的关注有限。\n  - 这些工作中没有考虑边缘服务器转入休眠状态的可能性。","x":-27290,"y":-1359,"width":727,"height":705},
		{"id":"a5358b1a7cbd4bb4","type":"text","text":"- **挑战和MEC服务器引入在车联网中的应用：**\n  - 针对一些应用的严格延迟要求以及车辆与云计算服务器之间的无线和有线通信的巨大延迟，有时将车辆任务卸载到云计算服务器是不可行的[13]–[15]。\n  - 与此同时，物理增加车载资源的数量会导致车辆制造成本的严重增加。\n  - MEC服务器作为云计算服务器的扩展，将计算和缓存功能移到用户设备附近 [16]–[19]。\n  - 它允许车辆通过多种无线通信技术将任务卸载到它。\n  - 通过减少MEC和云计算服务器之间的通信时间，可以满足卸载任务的敏感延迟要求。\n  - 然而，每个MEC装载的基站（BS）中的计算/缓存资源量通常是预设的，而车辆用户的资源需求是时变的，这在仅由MEC装载的BS支持的车辆场景中会导致资源的浪费或低使用率问题，尤其是在由一些社交活动或事件引起的突发流量存在时。\n  - 许多领先公司启动的不同项目中也考虑了相关应用 [21]。","x":-27956,"y":-960,"width":636,"height":440},
		{"id":"776809ee9fe5bb02","type":"text","text":"- **VEC的先前研究工作:**\n  - Zhang等人[12]: 提出了一种基于Stackelberg博弈理论的任务卸载方案，以最大化车辆和MEC服务器的效用。\n  - Zhang等人[13]: 引入了一种高效的预测组合模式卸载机制，以减少卸载成本。\n  - Dai等人[14]: 开发了一种联合最优的VEC服务器选择和卸载算法，以最大化系统效用。\n  - Zhou等人[15]: 提出了一种基于交替方向乘子法的能效资源分配算法。\n  - Zhu等人[16]: 制定了一种动态任务分配解决方案，以确保服务质量。\n\n- **考虑任务依赖性:**\n  - 以前的研究假设任务独立卸载到RSUs进行执行。\n  - 忽略了任务执行顺序依赖性及其对应用执行时间的影响。\n\n- **在增强现实车载系统中的任务依赖性:**\n  - 组件: 目标跟踪、模型映射、目标识别、透视变换和合并处理。\n  - 依赖关系: 车辆跟踪在环境模型构建之前，车辆识别在透视变换和合并处理之前。","x":-28635,"y":-1553,"width":650,"height":467},
		{"id":"454086d54a814e32","type":"text","text":"- **背景:**\n  - 相关标准的不断改进和智能车辆数量的增加表明未来将有更多车辆通过相关协议实现网络互连。\n\n- **挑战:**\n  - 随着车辆数量的增加，解决道路危险是发展车联网（IoV）的关键挑战[10]。\n    - 车辆安全业务通信要求高度的及时性和可靠性，特别是在自动驾驶等场景中，延迟要求可能需要低于10毫秒[11,12]。\n    - 诸如信道拥塞、信道干扰、阴影衰落和智能计算处理等因素显著影响基于IEEE 802.11P和LTE-V协议的车辆通信过程中的通信性能。\n\n- **重要性:**\n  - 对IoV安全服务传输策略的研究变得重要，特别是在调度计算和通信资源以提高车辆安全业务通信性能方面[11,12]。","x":-28630,"y":-1040,"width":640,"height":386},
		{"id":"82fa82fe463e1a92","type":"text","text":"- **MEC-Based Vehicular Networks中的挑战和研究重点:**\n  - 解决在实施基于MEC的车联网中的挑战一直是最近研究的焦点，涵盖了以下方面：\n    - 架构设计\n    - 任务卸载方案\n    - 资源管理方案等。\n\n- **分层车联网框架:**\n  - 一个例子是对基于MEC的分层车联网框架的研究，该框架包括车辆级别的车载计算/存储资源和服务器级别的资源（MEC和云计算服务器）[10]，[12]，[14]–[16]。\n\n- **任务卸载和资源管理方案:**\n  - 已经提出了任务卸载和资源管理方案，以管理频谱/计算/存储资源并对车辆用户进行任务卸载决策 [10]，[15]–[17]。\n  - 现有的工作通常通过联合优化任务卸载和频谱/计算资源分配来实现目标，使用传统的优化方法 [10]，[15]。\n\n- **现有方案的局限性:**\n  - 大多数现有方案仅考虑一两个维度的资源，这可能无法直接支持具有高维资源需求的车联网应用，例如编队/车队控制的计算任务 [7]。\n  - 一些工作侧重于低移动性用户场景下的多维资源管理 [18]，[19]。\n\n- **计算复杂性挑战:**\n  - 在基于MEC的车联网中，由于车辆高度移动和资源需求的时变性，多维资源管理的计算复杂性增加。\n  - 基于纯优化方法的方案在具有对延迟敏感应用的场景中变得不可行，特别是对于计算复杂性挑战较大的情况。\n\n- **引入人工智能、强化学习和主要贡献:**\n  - 探讨了人工智能（AI）技术，尤其是强化学习（RL），作为快速资源管理的解决方案 [20]–[23]。\n  - Q学习 [16]，[24]，深度Q学习 [25]–[27]，以及演员-评论家 [18]，[28] 算法在无线通信网络中的资源管理中被广泛使用。\n  - 在这些方法的启发下，并考虑到动态的车辆网络环境，本文研究了使用深度RL来共同管理频谱、计算和存储资源，以支持基于MEC的车联网中对延迟敏感的应用 [12]。\n\n- **论文的主要贡献:**\n  1. 提出了两个基于MEC服务器位置（宏基站（MBS）和边缘节点（EN））的多维资源管理框架。\n  2. 利用优化理论制定了优化问题，以最大化卸载任务的数量，同时满足服务质量（QoS）要求，并受到可用频谱、计算和存储资源的总量限制。\n  3. 使用深度RL对所制定的问题进行转化，以快速获得基站（BS）之间的最佳频谱切片和车辆之间的最佳频谱/计算/存储分配。\n  4. 引入了基于深度确定性策略梯度（DDPG）的算法来解决转化后的RL问题。为应对转化后的RL问题的增加复杂性，开发了一种基于层次DDPG（HDDPG）的算法。","x":-21880,"y":-3819,"width":880,"height":951,"color":"3"},
		{"id":"405938e52d4d732c","type":"text","text":"\n![[Pasted image 20231219142015.png]]\n\n![[Pasted image 20231219142031.png]]\n","x":-18080,"y":-5334,"width":533,"height":1643,"color":"6"},
		{"id":"bdc8ca778f55da5e","type":"text","text":"### Deep Reinforcement Learning Based Resource Management for Multi-Access Edge Computing in Vehicular Networks\n\nPeng, Haixia, 和Xuemin Shen. 《Deep Reinforcement Learning Based Resource Management for Multi-Access Edge Computing in Vehicular Networks》. *IEEE Transactions on Network Science and Engineering* 7, 期 4 (2020年10月1日): 2416–28. <https://doi.org/10.1109/TNSE.2020.2978856>.\n\nTo address the challenges on implementing MEC-based vehicular networks, many research works have been performed recently, including design of architecture, task offloading scheme, resource management scheme, and so on. For example, the MEC-based hierarchical vehicular network framework, comprised of vehicle level’s on-board computing/ storing resources and server level’s resources (resources placed at the MEC and cloud-computing servers), has been investigated in [10], [12], [14]–[16]. To better manage the spectrum/computing/storing resources among and make task offloading decisions to vehicle users, task offloading and resource management schemes have been proposed in [10], [15]–[17]. Since task offloading and spectrum/computing resource allocation are coupled with each other, the objectives of the most existing works have been achieved by jointly optimizing these two parts with traditional optimization methods [10], [15]. However, only one or two dimensions of resources have been considered in most of the existing schemes, which cannot be directly adopted to support some vehicular applications where high dimensional resources are involved, such as the computing tasks generated by the leading vehicle for platoon/convoy control [7]. Moreover, there are also some works focusing on multi-dimensional resources management in the scenarios with low mobility users [18], [19]. For MEC-based vehicular networks, the computational complexity of multi-dimensional resource management problem increases due to the high vehicle mobility and time-varying demand on resources, therefore increasing the time consumption on the resource management scheme itself. Thus, it is infeasible to adopt the pure optimization approachbased schemes to achieve multi-dimensional resource management in MEC-based vehicular networks, especially for the scenarios with delay-sensitive applications. How to design practical and QoS-oriented multi-dimensional resource management schemes for the MEC-based vehicular networks still needs efforts.\n\nAs is known, artificial intelligence (AI) technology, especially reinforcement learning (RL), can be exploited to solve resource management problems quickly [20]–[23]. Q-learning [16], [24], deep Q-learning [25]–[27], actor-critic [18], [28], and other deep RL algorithms have been widely exploited for resource management in wireless communication networks. Inspired by the existing works and considering the dynamic vehicular network environment caused by high vehicle mobility and heterogeneous applications, we investigate how to exploit deep RL to jointly manage the spectrum, computing, and storing resources to support delay-sensitive applications in the MEC-based vehicular network [12] in this paper. Specifically, the main contributions of this work can be summarized as follows,\n\n1) According to the location of the MEC server, two typical multi-dimensional resource management frameworks are proposed with placing the MEC server at a macro-cell base station (MBS) and an edge node (EN), respectively. \n2) Leveraging optimization theory, optimization problems are formulated to maximize the number of offloaded tasks with satisfied QoS requirements and constrained total amounts of available spectrum, computing, and storing resources. \n3) To rapidly solve the formulated problems and obtain optimal spectrum slicing among base stations (BSs) and optimal spectrum/computing/storing allocation among vehicles, the formulated optimization problems are transformed with deep RL. \n4) A deep deterministic policy gradient (DDPG)-based algorithm is proposed to solve the transformed RL problems. As the complexity of the transformed RL problems increases with the sizes of environment state and action, a hierarchical DDPG (HDDPG)-based algorithm is developed by combining the DDPG and the hierarchical learning architecture.","x":-18760,"y":-4352,"width":527,"height":1942},
		{"id":"1d1686ac908b30bd","type":"text","text":"\n- Traditional approaches for dynamic Joint Computation Offloading and Resource Allocation (JCORA) problems in Mobile Edge Computing (MEC) include convex approximation [5], [6], game theory [7]–[9], and metaheuristics [4], [10].\n- These methods face challenges such as exponentially increasing search space and heavy computational burdens, particularly in large-scale scenarios.\n- The use of deep reinforcement learning (DRL) to tackle JCORA problems has gained increasing research interest, leveraging the power of deep neural networks (DNNs) [3], [12]–[20].\n- Deep Deterministic Policy Gradient (DDPG) [16], [18] has shown excellent potential in dealing with dynamic optimization problems with a continuous action space.\n- More details on these approaches can be found in Section II.\n- In traditional Deep Deterministic Policy Gradient (DDPG), both the actor and critic networks rely on fully connected networks (FCNs).\n- FCNs have two significant drawbacks: a large number of trainable parameters, making training difficult and consuming computing resources, and the limitation of extracting only global discriminative state and action policy features.\n- FCNs neglect the temporal variation of task sequences, potentially overlooking useful shapelets for function approximation.\n- Additionally, in traditional DDPG, experience transitions are uniformly sampled from the replay buffer, treating all experiences equally regardless of their significance.\n- This uniform sampling approach ignores the importance of valuable experiences, often resulting in poor stability and slow convergence during training.\n- To address the drawbacks mentioned earlier, this article introduces Temporal Attentional Deterministic Policy Gradient (TADPG), an enhanced agent derived from DDPG.\n- TADPG is designed to handle the decentralized Joint Computation Offloading and Resource Allocation (JCORA) problem within a dynamic Mobile Edge Computing (MEC) environment.\n- The TADPG agent is deployed on each Mobile Device (MD), resulting in lower control costs between the MD and its corresponding MEC server.\n- This makes TADPG more suitable for large-scale scenarios compared to centralized JCORA methods.\n- The main contributions of this work can be summarized as follows.","x":-19355,"y":-5919,"width":460,"height":1407,"color":"6"},
		{"id":"828f8d6eef86ddf6","type":"text","text":"### A DRL Agent for Jointly Optimizing Computation Offloading and Resource Allocation in MEC\n\nChen, Juan, Huanlai Xing, Zhiwen Xiao, Lexi Xu和Tao Tao. 《A DRL Agent for Jointly Optimizing Computation Offloading and Resource Allocation in MEC》. _IEEE Internet of Things Journal_ 8, 期 24 (2021年12月15日): 17508–24. [https://doi.org/10.1109/JIOT.2021.3081694](https://doi.org/10.1109/JIOT.2021.3081694).\n\n\nTraditional approaches, such as convex approximation [5], [6], game theory [7]–[9], and metaheuristics [4], [10], are not suitable for addressing dynamic JCORA problems in MEC. These methods usually suffer from exponentially increasing search space and heavy computational burdens, especially in large-scale scenarios. Adopting deep reinforcement learning (DRL) to handle JCORA problems has attracted increasingly more research interests because of the powerful deep neural networks (DNNs) [3], [12]–[20]. Deep deterministic policy gradient (DDPG) [16], [18] shows excellent potential when coping with dynamic optimization problems with a continuous action space. More details can be found in Section II. \n\nIn the traditional DDPG, both the actor and critic networks are based on fully connected networks (FCNs). However, FCNs generally suffer from two drawbacks. One is the huge number of trainable parameters, which increases the difficulty for training and significantly consumes computing resources. The other is that FCNs only extract the global discriminative state and action policy features, ignoring the temporal variation of task sequences that may contain useful shapelets for function approximation. Moreover, experience transitions are uniformly sampled from the replay buffer. This approach treats all experiences equally, regardless of their significance. However, ignoring the importance of valuable experiences usually leads to poor stability and slow convergence during training. \n\nTo overcome the disadvantages above, this article proposes temporal attentional deterministic policy gradient (TADPG), an improved DDPG agent for tackling the decentralized JCORA problem in a dynamic MEC environment. Running a TADPG agent on each MD requires fewer control costs between this MD and its corresponding MEC server. It is thus more suitable for large-scale scenarios, compared with those centralized JCORA methods. Our main contributions are summarized as follows.\n\n","x":-18855,"y":-5773,"width":527,"height":1116},
		{"id":"d1e05b13e7f28935","type":"text","text":"- **Traditional Approaches for JCORA Problems in MEC:**\n  - Include convex approximation [5], [6], game theory [7]–[9], and metaheuristics [4], [10].\n  - Face challenges like exponentially increasing search space and heavy computational burdens, especially in large-scale scenarios.\n\n- **Use of Deep Reinforcement Learning (DRL) in JCORA Problems:**\n  - Gaining increasing research interest.\n  - Leverages the power of deep neural networks (DNNs) [3], [12]–[20].\n  - Deep Deterministic Policy Gradient (DDPG) [16], [18] shows potential in dealing with dynamic optimization problems with a continuous action space.\n\n- **Details in Section II:**\n  - More information on the traditional approaches and DDPG can be found in Section II.\n\n- **Drawbacks of Traditional DDPG:**\n  - Both actor and critic networks rely on fully connected networks (FCNs).\n  - FCNs have drawbacks: large number of trainable parameters, making training difficult, and limited ability to extract only global discriminative state and action policy features.\n  - Neglect the temporal variation of task sequences, potentially overlooking useful shapelets for function approximation.\n  - Experience transitions uniformly sampled from the replay buffer, treating all experiences equally regardless of significance.\n\n- **Introduction of TADPG:**\n  - Temporal Attentional Deterministic Policy Gradient (TADPG) introduced to address drawbacks of traditional DDPG.\n  - Designed for the decentralized JCORA problem in a dynamic MEC environment.\n  - TADPG agents deployed on each Mobile Device (MD) for lower control costs compared to centralized methods.\n  - More suitable for large-scale scenarios.\n\n- **Main Contributions:**\n  - TADPG's main contributions include...\n    (Please provide details on the main contributions.)\n ","x":-20176,"y":-5612,"width":781,"height":794,"color":"5"},
		{"id":"e32a341f9d7f31bf","type":"text","text":"- **传统方法处理MEC中的JCORA问题:**\n  - 包括凸逼近[5]，[6]，博弈论[7]–[9]和元启发式算法[4]，[10]。\n  - 面临挑战，如指数增长的搜索空间和沉重的计算负担，尤其是在大规模场景中。\n\n- **在JCORA问题中使用深度强化学习（DRL）:**\n  - 正在引起越来越多的研究兴趣。\n  - 利用深度神经网络（DNNs）的强大功能[3]，[12]–[20]。\n  - 深度确定性策略梯度（DDPG）[16]，[18]在处理具有连续动作空间的动态优化问题方面显示出潜力。\n\n- **第二部分的详细信息:**\n  - 有关传统方法和DDPG的更多信息，请参阅第二部分。\n\n- **传统DDPG的缺点:**\n  - 演员和评论家网络都依赖于全连接网络（FCNs）。\n  - FCNs存在缺点：大量的可训练参数，使训练困难，以及仅具有提取全局判别状态和动作策略特征的有限能力。\n  - 忽视任务序列的时间变化，可能忽视用于函数逼近的有用的形状特征。\n  - 从回放缓冲区均匀采样经验转换，将所有经验视为相等，而不考虑其重要性。\n\n- **引入TADPG:**\n  - 引入时序注意力确定性策略梯度（TADPG）以解决传统DDPG的缺点。\n  - 设计用于动态MEC环境中的分散JCORA问题。\n  - TADPG代理部署在每个移动设备（MD）上，以降低与集中式方法相比的控制成本。\n  - 更适用于大规模场景。\n\n- **主要贡献:**\n  - TADPG的主要贡献包括...\n    (请提供主要贡献的详细信息。)","x":-20960,"y":-5554,"width":744,"height":677,"color":"1"},
		{"id":"b9ca629f67a6ebcc","type":"text","text":"- **传统方法处理MEC中的JCORA问题:**\n  - 包括凸逼近[5]，[6]，博弈论[7]–[9]和元启发式算法[4]，[10]。\n  - 面临挑战，如指数增长的搜索空间和沉重的计算负担，尤其是在大规模场景中。\n\n- **在JCORA问题中使用深度强化学习（DRL）:**\n  - 正在引起越来越多的研究兴趣。\n  - 利用深度神经网络（DNNs）的强大功能[3]，[12]–[20]。\n  - 深度确定性策略梯度（DDPG）[16]，[18]在处理具有连续动作空间的动态优化问题方面显示出潜力。\n\n- **传统DDPG的缺点:**\n  - 演员和评论家网络都依赖于全连接网络（FCNs）。\n  - FCNs存在缺点：大量的可训练参数，使训练困难，以及仅具有提取全局判别状态和动作策略特征的有限能力。\n  - 忽视任务序列的时间变化，可能忽视用于函数逼近的有用的形状特征。\n  - 从回放缓冲区均匀采样经验转换，将所有经验视为相等，而不考虑其重要性。\n\n- **引入TADPG:**\n  - 引入时序注意力确定性策略梯度（TADPG）以解决传统DDPG的缺点。\n  - 设计用于动态MEC环境中的分散JCORA问题。\n  - TADPG代理部署在每个移动设备（MD）上，以降低与集中式方法相比的控制成本。\n  - 更适用于大规模场景。\n\n- **主要贡献:**\n  - TADPG的主要贡献包括...\n    (请提供主要贡献的详细信息。)","x":-21880,"y":-5516,"width":880,"height":601,"color":"3"},
		{"id":"11ad9eb6f2b70b30","type":"text","text":"- 传统方法处理MEC中的JCORA问题:\n- 在JCORA问题中使用深度强化学习（DRL）:\n- 传统DDPG的缺点:\n- 引入TADPG:\n- 主要贡献:","x":-22400,"y":-5315,"width":480,"height":200,"color":"4"},
		{"id":"f22230870406c474","type":"text","text":"- In VEC, it is critical to save energy consumption and accelerate the computation process by offloading a particular task to the roadside unit (RSU) which is placed with an edge server [7].\n\n- Therefore, most existing works in VEC focus on the algorithm design of task offloading, aiming to minimize the response time of tasks or energy consumption.\n\n- Authors in [5,8] propose adaptive algorithms for task offloading to minimize the average response time of tasks.\n\n- Authors in [9] propose an efficient algorithm by contract theoretical modeling to minimize the network delay.\n\n- Authors in [10] propose an intelligent offloading system by leveraging deep reinforcement learning to maximize the total computation and communication rates.\n\n- Authors in [11] propose an energy-efficient algorithm for resource allocation in VEC.\n\n- In addition, the offloading algorithm is proposed to minimize the weighted sum of energy consumption and latency [12].\n\n- Due to geographical differences in vehicles, the tasks are distributed in different locations. This results in a serious imbalance in the load of edge servers, leading to high latency of tasks and increased energy consumption.\n\n- However, the task scheduling algorithm in edge servers is only considered in a few existing works on VEC.\n\n- A heuristic algorithm for energy-efficient scheduling is proposed to minimize energy consumption [7].\n\n- A scheduling algorithm based on ant colony optimization is proposed to minimize the completion time of jobs [13].\n\n- An algorithm for partial offloading scheduling and power allocation is proposed to minimize the weighted sum of the execution delay and energy consumption in mobile edge computing [14].\n\n- Meanwhile, authors in [15] propose a task scheduling algorithm for applications with computation-intensive and time-sensitive requirements to minimize the completion time of tasks.\n\n- The RSUs, placed with edge servers, are densely distributed in the VEC system to guarantee the quality of service.\n\n- Besides, the spatial distribution of vehicles has much difference. For example, vehicles are mainly distributed in the city, especially in hot spots, instead of the remote districts. Thus, the task requests are greatly varied in space.\n\n- For saving energy, some RSUs switch their state into a sleep state when there are few task requests [16–19].\n\n- Authors in [20,21] propose algorithms to maximize the energy efficiency by designing sleeping strategies for base stations.\n\n- Authors in [22] propose an efficient algorithm to minimize the energy consumption by jointing the cell association and on–off scheme.\n\n- Authors in [23] propose a novel user-centric energy-aware mobility management scheme to minimize the delay of tasks in mobile edge computing, in which the candidate base stations randomly switch their states between sleep and work.\n\n- To the best of our knowledge, the existing works in VEC mainly target the algorithm designing for task offloading, with only a few investigating algorithms for task scheduling.\n\n- Meanwhile, none of the existing works in VEC consider that edge servers may turn to a sleep state in some cases.\n\n- Therefore, in this paper, we investigate the problem of scheduling tasks in VEC to minimize the response time, considering that edge servers may switch their state between sleep and work according to the traffic of the requests.\n\n- The contributions of this paper are summarized as follows:\n\n- We propose a novel problem of task scheduling for minimizing the total delay of tasks in VEC, with considering that the edge servers in RSUs can dynamically switch their states between sleep and work. Meanwhile, we model task requests generated by vehicles as an independent Poisson stream, and we model an edge server in the RSU as a simple M/M/1 queuing system. Besides, the proposed problem of minimizing the total delay of tasks is formulated and the NP-hardness is proved in this paper.\n- To solve the proposed problem, we contribute a greedy algorithm by carefully choosing the edge server so that the response time of the task arrived at the current time is minimum. Meanwhile, we customize a tabu search algorithm, which can successfully refine the solution generated by the greedy algorithm.\n- We also propose a deep Q-network based algorithm by utilizing the deep reinforcement learning algorithm to learn the optimal scheduling policy for minimizing the total delay of tasks, without having a priori knowledge of dynamic statistics.\n- Simulation results show that, our proposed algorithms outperform the random algorithm also proposed in this paper in terms of the total response time of tasks. Especially, the deep Q-network based algorithm performs better than the other algorithms in terms of the total response time of tasks. For example, for the case of that the maximum tolerant response time of each task is 14 s, the total response time of tasks decreases by 24.13%, 28.73% and 35.95%, compared with the customized tabu search algorithm, the greedy algorithm and the random algorithm, respectively.\n\n","x":-26940,"y":2255,"width":527,"height":2468,"color":"6"},
		{"id":"48d95710b6d6f974","type":"text","text":"- MEC can effectively reduce the delay cost of service delivery and network operation by deploying servers close to mobile users.\n\n- It has been certified by the European 5G Infrastructure Public Private Partnership (PPP) as one of the key technologies of the next-generation 5G network.\n\n- The advantages of MEC include:\n  - Low latency\n  - High bandwidth\n  - Real-time wireless network information\n  - Location awareness [7,8].\n\n- The MEC-based Internet of Vehicles (IoV) allows mobile vehicles to offload computing tasks to network edge nodes for processing, helping to achieve the ultra-low latency requirements of IoV [9].\n\n- MEC sinks cloud services to the edge of the wireless access network and provides computing services near moving vehicles [10].\n\n- However, in the MEC-based vehicle-connected network, there are various computationally intensive and delay-sensitive computational tasks.\n\n- Each task has different resource requirements, including:\n  - Computing resources required for task execution\n  - Communication resources required for task transmission.\n\n- In this case, a suitable strategy is needed to control offloading tasks and ensure the normal operation of the system.\n\n- Considering the impact of offloading decision and resource allocation on computing offloading performance.\n\n- In response to the above problems, a computing offloading resource allocation scheme using reinforcement learning in a MEC system is proposed.\n","x":-37020,"y":-7624,"width":567,"height":785,"color":"6"}
	],
	"edges":[
		{"id":"17c31e7664079810","fromNode":"38d1769d3407d63b","fromSide":"right","toNode":"838ff88d8ddde355","toSide":"left"},
		{"id":"8f1e0a889fd9123c","fromNode":"4377f960d6d505c5","fromSide":"right","toNode":"c542af179514c130","toSide":"left"},
		{"id":"8f17088ec94f94ee","fromNode":"4377f960d6d505c5","fromSide":"right","toNode":"756c3e54b6e06e7e","toSide":"left"},
		{"id":"efcdfdfcde5eb97e","fromNode":"c542af179514c130","fromSide":"right","toNode":"c58793506f600720","toSide":"left"},
		{"id":"dcf4bd84090c21b3","fromNode":"59e271b6d698153c","fromSide":"right","toNode":"48d95710b6d6f974","toSide":"left"},
		{"id":"ac695e35c682a4f8","fromNode":"48d95710b6d6f974","fromSide":"right","toNode":"df604f2483501e93","toSide":"left"},
		{"id":"1b7a3500e29351d5","fromNode":"5d6cf91d30077f64","fromSide":"right","toNode":"23fd185fed78b759","toSide":"left"},
		{"id":"bb18e380bd468a16","fromNode":"286bdbd3744bbcd2","fromSide":"right","toNode":"7c8ebbea53acbd4d","toSide":"left"},
		{"id":"97d20b55b47d6372","fromNode":"bb560557ff125046","fromSide":"right","toNode":"462aeef697230fbf","toSide":"left"},
		{"id":"d8cd93a6a876c228","fromNode":"ec67400a03415c26","fromSide":"bottom","toNode":"126967b0e284f732","toSide":"top"},
		{"id":"a92b10a8ba783810","fromNode":"126967b0e284f732","fromSide":"right","toNode":"1ebaf5ca1336fdd6","toSide":"left"},
		{"id":"fc5087a9394a456f","fromNode":"93ea1152bc6b1264","fromSide":"bottom","toNode":"f22230870406c474","toSide":"top"},
		{"id":"d71a7318860f648f","fromNode":"f22230870406c474","fromSide":"right","toNode":"b9441fbc2f7662a2","toSide":"left"},
		{"id":"c94f84b9f8064d23","fromNode":"1d1686ac908b30bd","fromSide":"right","toNode":"d1e05b13e7f28935","toSide":"left"},
		{"id":"ad904f4740938096","fromNode":"838ff88d8ddde355","fromSide":"right","toNode":"3c9f9ad2c6c2d15e","toSide":"left"},
		{"id":"36347266198437a8","fromNode":"aba8b6138abd39c0","fromSide":"right","toNode":"ab53462f070e57c8","toSide":"left"},
		{"id":"b905cefec6015f60","fromNode":"462aeef697230fbf","fromSide":"right","toNode":"f0b8b4a82e76ec76","toSide":"left"},
		{"id":"6c09298351b64ecd","fromNode":"1ebaf5ca1336fdd6","fromSide":"right","toNode":"f528ccf3526e0288","toSide":"left"},
		{"id":"f4ddd67a7d6c729f","fromNode":"b9441fbc2f7662a2","fromSide":"right","toNode":"f241a24d8f473b61","toSide":"left"},
		{"id":"86985ea123e03a35","fromNode":"df604f2483501e93","fromSide":"right","toNode":"fd4a3013b817968e","toSide":"left"},
		{"id":"cf3bd8df0a9b6795","fromNode":"23fd185fed78b759","fromSide":"right","toNode":"8bc59f8490d174fd","toSide":"left"},
		{"id":"2095fec706b84856","fromNode":"d1e05b13e7f28935","fromSide":"right","toNode":"e32a341f9d7f31bf","toSide":"left"},
		{"id":"aec870e80a8e6c3e","fromNode":"3c9f9ad2c6c2d15e","fromSide":"right","toNode":"e38860a49d379893","toSide":"left"},
		{"id":"97a8ae86cbccc830","fromNode":"60cf73ff2492e3e8","fromSide":"bottom","toNode":"38d1769d3407d63b","toSide":"top"},
		{"id":"f09f519a8c86be2a","fromNode":"e32a341f9d7f31bf","fromSide":"right","toNode":"b9ca629f67a6ebcc","toSide":"left"},
		{"id":"b293d56e4d6e7053","fromNode":"f2b4116eb9283cc6","fromSide":"right","toNode":"41c040ef7453b8cd","toSide":"left"},
		{"id":"13c483359d526b29","fromNode":"ab53462f070e57c8","fromSide":"right","toNode":"b6ba70757acfcb52","toSide":"left"},
		{"id":"f888c53445a76b71","fromNode":"fd4a3013b817968e","fromSide":"right","toNode":"7263f79cc514c264","toSide":"left"},
		{"id":"2f1dd651bc8b842e","fromNode":"f0b8b4a82e76ec76","fromSide":"right","toNode":"82fa82fe463e1a92","toSide":"left"},
		{"id":"dfbee6d22c7b0842","fromNode":"f528ccf3526e0288","fromSide":"right","toNode":"70341e2d8d93c34b","toSide":"left"},
		{"id":"d2c8a0a8230022e1","fromNode":"5c84ccba978a197b","fromSide":"right","toNode":"52e1447da0b981a3","toSide":"left"},
		{"id":"0c3e5149534c60fa","fromNode":"f241a24d8f473b61","fromSide":"right","toNode":"9f283274c02f05c4","toSide":"left"},
		{"id":"a0116521adec6313","fromNode":"8bc59f8490d174fd","fromSide":"right","toNode":"702cb167ea561453","toSide":"left"},
		{"id":"448a9695b5e6f0bf","fromNode":"82fa82fe463e1a92","fromSide":"right","toNode":"919e01de9971ff0b","toSide":"left","color":"1"},
		{"id":"49e71e5acdc7abf5","fromNode":"70341e2d8d93c34b","fromSide":"left","toNode":"e53bd267ca288808","toSide":"top"},
		{"id":"640f7f4dc7eed49a","fromNode":"52e1447da0b981a3","fromSide":"right","toNode":"565aeb9be3eadabb","toSide":"left"},
		{"id":"aac6f959dd5a12f5","fromNode":"9f283274c02f05c4","fromSide":"right","toNode":"691ef2784504755b","toSide":"left"},
		{"id":"f9afacbe43463c44","fromNode":"702cb167ea561453","fromSide":"right","toNode":"2486e59ebcacff71","toSide":"left"},
		{"id":"795b6906d615a108","fromNode":"e38860a49d379893","fromSide":"right","toNode":"03a2f309483a3fd2","toSide":"left"},
		{"id":"3d2ab405956e3f5b","fromNode":"b9ca629f67a6ebcc","fromSide":"right","toNode":"11ad9eb6f2b70b30","toSide":"left"},
		{"id":"47587c4dbff51487","fromNode":"41c040ef7453b8cd","fromSide":"right","toNode":"bb8396cee58a06db","toSide":"left"},
		{"id":"e618f67b955d75b1","fromNode":"7263f79cc514c264","fromSide":"right","toNode":"74ab79f4daac717d","toSide":"left","color":"2"},
		{"id":"affb9a974e59a848","fromNode":"919e01de9971ff0b","fromSide":"right","toNode":"b8dca4a353594e1f","toSide":"left"},
		{"id":"e7263976bc4e1575","fromNode":"e53bd267ca288808","fromSide":"top","toNode":"b8dca4a353594e1f","toSide":"left"},
		{"id":"f96926d287c56289","fromNode":"565aeb9be3eadabb","fromSide":"right","toNode":"b8dca4a353594e1f","toSide":"left"},
		{"id":"0313f58c1e0228aa","fromNode":"691ef2784504755b","fromSide":"top","toNode":"b8dca4a353594e1f","toSide":"left"},
		{"id":"f5507991a8cb8b94","fromNode":"2486e59ebcacff71","fromSide":"right","toNode":"b8dca4a353594e1f","toSide":"left"},
		{"id":"718bc600efb5f481","fromNode":"03a2f309483a3fd2","fromSide":"right","toNode":"b8dca4a353594e1f","toSide":"left"},
		{"id":"d05e078768b7e517","fromNode":"74ab79f4daac717d","fromSide":"right","toNode":"b8dca4a353594e1f","toSide":"left"},
		{"id":"b64d25932e43e7c9","fromNode":"f81d51fbbd9d7a86","fromSide":"right","toNode":"b8dca4a353594e1f","toSide":"left"},
		{"id":"6dc2aa64bf8bf05d","fromNode":"b8dca4a353594e1f","fromSide":"right","toNode":"896f0266a998ed96","toSide":"left"},
		{"id":"79c9023f5c2f2b4c","fromNode":"82fa82fe463e1a92","fromSide":"top","toNode":"896f0266a998ed96","toSide":"top","color":"1"},
		{"id":"614dfd3a3f693728","fromNode":"46b1742079e5c58b","fromSide":"right","toNode":"896f0266a998ed96","toSide":"left"},
		{"id":"6f989811536dd31d","fromNode":"e53bd267ca288808","fromSide":"top","toNode":"46b1742079e5c58b","toSide":"left"},
		{"id":"99c43994ddd0d274","fromNode":"919e01de9971ff0b","fromSide":"right","toNode":"46b1742079e5c58b","toSide":"left"},
		{"id":"4d3fd81e9f2ceff1","fromNode":"691ef2784504755b","fromSide":"top","toNode":"46b1742079e5c58b","toSide":"left"},
		{"id":"38e67970639289ab","fromNode":"2486e59ebcacff71","fromSide":"right","toNode":"46b1742079e5c58b","toSide":"left"},
		{"id":"e859807e2891567b","fromNode":"03a2f309483a3fd2","fromSide":"right","toNode":"46b1742079e5c58b","toSide":"left"},
		{"id":"357bcc6092519fdb","fromNode":"11ad9eb6f2b70b30","fromSide":"right","toNode":"46b1742079e5c58b","toSide":"left"},
		{"id":"f022a4147f4d9e80","fromNode":"74ab79f4daac717d","fromSide":"right","toNode":"46b1742079e5c58b","toSide":"left"},
		{"id":"cdbc2b5443c81926","fromNode":"f81d51fbbd9d7a86","fromSide":"right","toNode":"46b1742079e5c58b","toSide":"left"},
		{"id":"3ac92b58d15e21ed","fromNode":"7c8ebbea53acbd4d","fromSide":"right","toNode":"5c84ccba978a197b","toSide":"left"},
		{"id":"7035e47f931f60e4","fromNode":"b6ba70757acfcb52","fromSide":"right","toNode":"f81d51fbbd9d7a86","toSide":"left"},
		{"id":"4f47b84f4966ba14","fromNode":"b8dca4a353594e1f","fromSide":"right","toNode":"c5ab5d07e127b93e","toSide":"left"},
		{"id":"3d520dcf28581e78","fromNode":"46b1742079e5c58b","fromSide":"right","toNode":"c5ab5d07e127b93e","toSide":"left"},
		{"id":"92608055bbb6c80f","fromNode":"b8dca4a353594e1f","fromSide":"right","toNode":"c5590e89aca3ebca","toSide":"left"},
		{"id":"d3d6879d4a6ab9ac","fromNode":"46b1742079e5c58b","fromSide":"right","toNode":"c5590e89aca3ebca","toSide":"left"},
		{"id":"7e02dd8443fa28ef","fromNode":"70341e2d8d93c34b","fromSide":"bottom","toNode":"c5ab5d07e127b93e","toSide":"right","color":"1"},
		{"id":"a5546def9b6bac45","fromNode":"7263f79cc514c264","fromSide":"bottom","toNode":"c5590e89aca3ebca","toSide":"top","color":"1"},
		{"id":"427681995e689933","fromNode":"c58793506f600720","fromSide":"right","toNode":"f2b4116eb9283cc6","toSide":"left"},
		{"id":"2ef74234afd16f1d","fromNode":"5911a2a71cba17fe","fromSide":"right","toNode":"aba8b6138abd39c0","toSide":"left"},
		{"id":"181b7f24c7ebfcc5","fromNode":"8a7016c5db0d8166","fromSide":"right","toNode":"286bdbd3744bbcd2","toSide":"left"},
		{"id":"442bc1348991daaa","fromNode":"8a7016c5db0d8166","fromSide":"right","toNode":"1dab1d1b7467470a","toSide":"left"},
		{"id":"b2e7297d72d7cd42","fromNode":"e283e054643c750e","fromSide":"right","toNode":"5d6cf91d30077f64","toSide":"left"},
		{"id":"a16a43bc8d226988","fromNode":"828f8d6eef86ddf6","fromSide":"right","toNode":"1d1686ac908b30bd","toSide":"left"},
		{"id":"686b10e41eb14c6a","fromNode":"bdc8ca778f55da5e","fromSide":"right","toNode":"405938e52d4d732c","toSide":"left"},
		{"id":"36de60c240d956ed","fromNode":"bdc8ca778f55da5e","fromSide":"right","toNode":"bb560557ff125046","toSide":"left"},
		{"id":"bd75829960c9a35e","fromNode":"b8dca4a353594e1f","fromSide":"right","toNode":"c86bfb4ac1499def","toSide":"left"},
		{"id":"12b897f395bd6952","fromNode":"46b1742079e5c58b","fromSide":"right","toNode":"c86bfb4ac1499def","toSide":"left"},
		{"id":"467e44847e759dbb","fromNode":"c86bfb4ac1499def","fromSide":"top","toNode":"b6ba70757acfcb52","toSide":"bottom","color":"1"},
		{"id":"5cd39d8f55c802a4","fromNode":"52e1447da0b981a3","fromSide":"top","toNode":"776809ee9fe5bb02","toSide":"left","color":"1"},
		{"id":"148270d4501e9f3b","fromNode":"702cb167ea561453","fromSide":"bottom","toNode":"454086d54a814e32","toSide":"bottom","color":"1"},
		{"id":"c19aa13e85442630","fromNode":"e38860a49d379893","fromSide":"top","toNode":"a5358b1a7cbd4bb4","toSide":"bottom","color":"1"},
		{"id":"3d12ce8bb9d4902d","fromNode":"9f283274c02f05c4","fromSide":"top","toNode":"da4eb9597b38757c","toSide":"bottom","color":"1"},
		{"id":"d167c928a72a9f26","fromNode":"b9ca629f67a6ebcc","fromSide":"bottom","toNode":"dee1f05eac0ddf50","toSide":"top","color":"1"}
	]
}