---
UID: 20230902165215
aliases: null
source: null
cssclass: null
created: 2023-08-28 09:24
updated: 2023-09-11 16:52
date updated: 2023-12-05 18:43
date created: 2023-11-20 15:05
---

## 1. Introduction



In recent years, with the rapid development of Internet of Things (IoT) and Information and Communication Technology (ICT), modern vehicles are undergoing a technological revolution. The introduction of wireless embedded sensing devices and 5G-related technologies enables Intelligent Transportation Systems (ITS) to become a reality and an indispensable part of smart cities. In this context, Internet of Vehicles (IoV) as an important component of ITS, provides real-time traffic flow information for drivers through information exchange and collaborative services between vehicles and between vehicles and infrastructure, thereby improving road safety, traffic efficiency and driving experience. This paper mainly focuses on the task scheduling problem in IoV, that is, how to migrate the computation-intensive tasks of vehicles to edge servers for execution, to improve the performance and user experience of vehicles.

Internet of Vehicles (IoV) constitutes a self-organizing network composed of mobile vehicles and fixed infrastructure, aiming to provide intelligent, safe and efficient transportation services for vehicles. With the advancement of vehicle technology, such as the popularization of computation-intensive applications such as autonomous driving, video analysis and personal assistants, vehicles can perform more and more computation-intensive and data-intensive tasks, including but not limited to navigation, video streaming and in-vehicle entertainment. However, with the increase of these task demands, the computing power and battery capacity of vehicles gradually become limited, and it is difficult to meet the complex or delay-sensitive task demands. To solve this problem, Mobile Edge Computing (MEC) emerges as a new computing paradigm. By deploying multiple MEC servers at the road edge, it provides low-latency, high-bandwidth computing resources and services for vehicles. This technology enables vehicles to offload part or all of their tasks to MEC servers for execution, thereby reducing their own energy consumption and time overhead, and bringing more safe, efficient and intelligent driving experience to the IoV system.

Task scheduling in IoV is an important and challenging work. It requires allocating computation-intensive services to On-Board Units (OBUs) or edge servers, to improve the performance and user experience of vehicles. These services cover multiple aspects such as speech recognition, natural language processing, computer vision, machine learning, augmented reality, etc., and have different characteristics and demands such as type, size, priority, latency, energy consumption, etc. However, with the increase of service demands, the computing power and battery capacity of vehicles gradually become limited, and it is difficult to meet the complex or delay-sensitive service demands. To solve this problem, Mobile Edge Computing (MEC) emerges as a new computing paradigm. By deploying multiple MEC servers at the road edge, it provides low-latency, high-bandwidth computing resources and services for vehicles. This technology enables vehicles to offload part or all of their services to MEC servers for execution, thereby reducing their own energy consumption and time overhead, and bringing more safe, efficient and intelligent driving experience to the IoV system.

This paper proposes an innovative method that combines multi-agent reinforcement learning and reverse auction mechanism for the task scheduling problem in IoV. The goal of this method is to achieve resource matching between services, vehicles and edge servers by dynamically allocating the execution plans of computation-intensive services, and to promote cooperation between vehicles and edge servers through incentive mechanism, to improve the efficiency and performance of task scheduling and reduce the total cost. Specifically, this paper adopts a multi-agent reinforcement learning algorithm, which enables vehicles to learn and update their task scheduling strategies autonomously according to local and global information, so as to make full use of system resources and improve execution effects. At the same time, a reverse auction mechanism is introduced, which realizes effective cooperation between vehicles and edge servers through resource allocation and price negotiation. This mechanism aims to balance competition and cooperation, improve system efficiency and fairness, and ensure reasonable resource allocation. This paper names this method as MADRL, a task scheduling method for IoV based on multi-agent deep reinforcement learning.

MADRL method uses neural network and Markov decision process (MDP) model to train the policy network on each edge server, which can intelligently calculate the long-term reward of accepting tasks, and bid to users according to the principle of maximizing rewards. By combining deep reinforcement learning and reverse auction mechanism, MADRL method can achieve system adaptability, robustness, scalability and distributivity in complex, dynamic and uncertain multi-agent environments. Therefore, MADRL method can adapt to complex, dynamic and uncertain multi-agent environments, and improve system performance and efficiency.

In addition, this paper adopts a reverse auction mechanism, which has rich and diverse application scenarios in IoV, including resource or service transactions, such as spectrum, cache, computing and data, etc. In IoV, reverse auction mechanism can not only motivate vehicle users to share resources or execute tasks, but also apply to social welfare task allocation, such as traffic management, environmental monitoring, data collection, etc. Through reverse auction, the platform can find vehicle users or servers who are willing to provide services or execute tasks at the lowest price, and sign standard contracts with them, thus achieving cost savings. The following sections of this paper will introduce the design and implementation of the proposed method in detail, and verify its effectiveness and superiority through simulation experiments.

The main contributions of this paper are as follows:

- Apply reverse auction to the task scheduling problem in IoV, and realize distributed, adaptive and incentive-compatible task scheduling.
- Design a bidding strategy based on PPO+LSTM, which uses the memory ability of LSTM to capture the temporal characteristics and long-term dependencies of task scheduling, and improve the performance and effect of bidding strategy.
- Develop an open-source simulation environment based on Python - VehicleJobScheduling, which simulates the process of vehicle task scheduling, and provides some common evaluation indicators.
- Through simulation experiments, verify the effectiveness and superiority of the proposed method, as well as the advantages and applicability of reinforcement learning and reverse auction mechanism, and compare with other benchmark methods.

The structure of this paper is arranged as follows: Section 2 reviews the related works, Section 3 elaborates the System Model and Problem Formulation, Section 4 describes the Method of MADRL and Reverse-Auction, Section 5 shows the Performance Evaluation, Section 6 summarizes the main conclusions and future work of this paper.

## Related Works

This is a  placeholder. This is a  placeholder. This is a  placeholder.  This is a  placeholder.  This is a  placeholder. This is a  placeholder.  This is a  placeholder.  This is a  placeholder. This is a  placeholder.  This is a  placeholder.  This is a  placeholder. This is a  placeholder.  This is a  placeholder.  This is a  placeholder. This is a  placeholder.  This is a  placeholder.  This is a  placeholder. This is a  placeholder.  This is a  placeholder.  This is a  placeholder. This is a  placeholder.  This is a  placeholder.

This is a  placeholder. This is a  placeholder. This is a  placeholder.  This is a  placeholder.  This is a  placeholder. This is a  placeholder.  This is a  placeholder.  This is a  placeholder. This is a  placeholder.  This is a  placeholder.  This is a  placeholder. This is a  placeholder.  This is a  placeholder.  This is a  placeholder. This is a  placeholder.  This is a  placeholder.  This is a  placeholder. This is a  placeholder.  This is a  placeholder.  This is a  placeholder. This is a  placeholder.  This is a  placeholder. This is a  placeholder. This is a  placeholder. This is a  placeholder.  This is a  placeholder.  This is a  placeholder. This is a  placeholder.  This is a  placeholder.  This is a  placeholder. This is a  placeholder.  This is a  placeholder.  This is a  placeholder. This is a  placeholder.  This is a  placeholder.  This is a  placeholder. This is a  placeholder.  This is a  placeholder.  This is a  placeholder. This is a  placeholder.  This is a  placeholder.  This is a  placeholder. This is a  placeholder.  This is a  placeholder.

This is a  placeholder. This is a  placeholder. This is a  placeholder.  This is a  placeholder.  This is a  placeholder. This is a  placeholder.  This is a  placeholder.  This is a  placeholder. This is a  placeholder.  This is a  placeholder.  This is a  placeholder. This is a  placeholder.  This is a  placeholder.  This is a  placeholder. This is a  placeholder.  This is a  placeholder.  This is a  placeholder. This is a  placeholder.  This is a  placeholder.  This is a  placeholder. This is a  placeholder.  This is a  placeholder. This is a  placeholder. This is a  placeholder. This is a  placeholder.  This is a  placeholder.  This is a  placeholder. This is a  placeholder.  This is a  placeholder.  This is a  placeholder. This is a  placeholder.  This is a  placeholder.  This is a  placeholder. This is a  placeholder.  This is a  placeholder.  This is a  placeholder. This is a  placeholder.  This is a  placeholder.  This is a  placeholder. This is a  placeholder.  This is a  placeholder.  This is a  placeholder. This is a  placeholder.  This is a  placeholder.

This is a  placeholder. This is a  placeholder. This is a  placeholder.  This is a  placeholder.  This is a  placeholder. This is a  placeholder.  This is a  placeholder.  This is a  placeholder. This is a  placeholder.  This is a  placeholder.  This is a  placeholder. This is a  placeholder.  This is a  placeholder.  This is a  placeholder. This is a  placeholder.  This is a  placeholder.  This is a  placeholder. This is a  placeholder.  This is a  placeholder.  This is a  placeholder. This is a  placeholder.  This is a  placeholder. This is a  placeholder. This is a  placeholder. This is a  placeholder.  This is a  placeholder.  This is a  placeholder. This is a  placeholder.  This is a  placeholder.  This is a  placeholder. This is a  placeholder.  This is a  placeholder.  This is a  placeholder. This is a  placeholder.  This is a  placeholder.  This is a  placeholder. This is a  placeholder.  This is a  placeholder.  This is a  placeholder. This is a  placeholder.  This is a  placeholder.  This is a  placeholder. This is a  placeholder.  This is a  placeholder.

This is a  placeholder. This is a  placeholder. This is a  placeholder.  This is a  placeholder.  This is a  placeholder. This is a  placeholder.  This is a  placeholder.  This is a  placeholder. This is a  placeholder.  This is a  placeholder.  This is a  placeholder. This is a  placeholder.  This is a  placeholder.  This is a  placeholder. This is a  placeholder.  This is a  placeholder.  This is a  placeholder. This is a  placeholder.  This is a  placeholder.  This is a  placeholder. This is a  placeholder.  This is a  placeholder.

## 3. System Model and Problem Formulation

Vehicular networks are a critical application scenario for mobile edge computing. Vehicles need to perform different computing tasks during driving, such as speech recognition, route planning, computer vision, machine learning, and augmented reality. These tasks require different computing and storage resources, while the vehicle's own computing power and battery capacity are constrained. Therefore, vehicles need to choose to offload some tasks to edge servers to be processed to save power and improve performance. However, the network connection between vehicles and edge servers is unreliable, and the resources of edge servers are also limited. These challenges make it difficult to achieve efficient task offloading and resource allocation.

We propose a vehicular network task scheduling model, consisting of $m$ sellers $\mathcal S=\{\mathbf s_1,\dots,\mathbf s_m\}$ and $n$ buyers $\mathcal B=\{\mathbf b_1,\dots,\mathbf b_n\}$.

- A seller is a server that can provide task offloading services, represented by a resource vector $\mathbf r_i=(r_{j,1},\dots,r_{j,d})$ and a cost $c_i$, where $d$ is the total number of resource types, $\mathbf r_{j,k}$ is the number of the $k$-th type of resource owned by server $\mathbf b_j$, and $c_i$ is the unit time maintenance cost.
- A buyer is a vehicle that submits a task request for offloading, represented by a task $\theta_j=(\mathbf f_j,\mathbf \sigma_j)$, where $\theta_j=(l_j,r_j,d_j,\tau_j)$ represents a task, $\mathbf f_j=(l_j,r_j,d_j,\tau_j)$ is the task execution features, $\mathbf \sigma_j=(\sigma_{j,1},\dots,\sigma_{j,m})$ is the task execution server restriction vector indicating whether task $\theta_j$ can be executed on server $\mathbf s_i$. $l_j$ is the task priority, $\mathbf r_j=(r_{j,1},\dots,r_{j,d})$ is the task resource requirements, $r_{j,k}$ is the task $\theta_j$'s demand for the $k$-th type of resource; $d_j$ is the task duration; $\tau_j$ is the task arrival time. $\psi_j$ is the budget of buyer $b_j$ for $\theta_j$.

At time step $t$, we use $\mathcal U_t$ to represent all the tasks submitted by vehicles at the $t$th time step, with the task sequence sorted in non-decreasing order of time. Assuming that the tasks in $\mathcal U_t$ are auctioned one by one in the order of arrival, then in this auction, the buyer is $b_j$, which represents the vehicle that initiates the task request, and the seller is $\mathcal S_j = \{\mathbf s_i | \forall \mathbf s_i \in \mathcal S, \sigma_{j,i} = 1\}$, which represents the set of servers that can satisfy the server restriction vector.

At the beginning of the auction, the buyer $\mathbf b_j$ sends the task feature $\mathbf f_j$ of $\theta_j$ to all the participating sellers $\mathcal S_j$. The sellers return bids $\mathcal Q_j = \{q_{j,i} | \forall \mathbf s_i \in \mathcal S_j\}$ based on their own status and the received task feature. $\mathcal Q_j$ represents the set of bids for $\theta_j$ from the servers, and $q_{j,i}$ represents the bid of the seller server $\mathbf s_i$ for the task $\theta_j$. The auction results are then generated, with the server allocation vector $\mathbf x_j = (x_{j,1}, \dots, x_{j,m})$ and payment $p_j = \min(q_j)$. If $x_{j,i} = 1$, it means that it is assigned to $\mathbf s_i$. If $x_{j,i} = 0$, it means that it is not assigned to $\mathbf s_i$. The buyer pays and starts task offloading. After all the tasks in $\mathcal U_t$ have been auctioned, the next time step $t + 1$ is entered. When all the time steps are completed, the auction process is over.

To describe the resource usage of task offloading, we need to model the usage of resources by tasks and servers during execution. We assume that the resource usage of task $\theta_j$ at time $t$ is a vector, where $r_{{\theta_j},k}(t)$ denotes the usage of the $k$-th resource at time $t$. $r_{{\theta_j},k}(t)=\begin{cases} r_{j,k}&\text{if }\tau_j \leq t < \tau_j+d_j \\ 0,&\text{otherwise} \end{cases}$ Similarly, we assume that the available resources of seller server $\mathbf s_i$ at time $t$ is also a vector, where $r_{{\mathbf s_i},k}(t)$ denotes the remaining amount of the $k$-th resource at time $t$. We assume that task $\theta_j$ will be assigned to server $\mathbf s_i$ immediately after $\tau_j$ and will be executed within $\tau_j+d_j$ time steps. During this time, task $\theta_j$ will occupy the resources of server $k$ and will release the resources at $\tau_j+d_j$.

Therefore, we can define a service status variable $s_{i,k}(t)$ for task $\theta_j$ as follows: $s_{j,i}(t)=\begin{cases} 1, & \text{if } x_{j,i}=1 \text{ and } \tau_j \leq t < \tau_j+d_j\\ 0, & \text{otherwise} \end{cases}$ where $x_{j,i}$ denotes whether task $\theta_i$ is assigned to server $\mathbf s_i$. The service status variable $s_{j,i}(t)$ denotes whether task $\theta_i$ is using the resources of server $\mathbf s_i$ at time $t$. # Based on the service status variable, we can calculate the resource usage of task $\theta_i$ on server $\mathbf s_i$ at time $t$ as follows: $r_{j,i}(t)=s_{j,i}(t)\cdot r_{\theta_j}(t)$From this, we can obtain the available resources of server $\mathbf s_i$ at time $t$ as follows: $r_{s_i}(t)=\mathbf r_i-\sum_{i=1}^{|\mathcal U_t|} r_{j,i}(t)$where $|\mathcal U_t|$ denotes the total number of tasks at time $t$, and $\mathbf r_i$ denotes the total resource capacity of server $\mathbf s_i$. The available resources of server $\mathbf b_j$ at time $t$, $r_i(t)$, denotes the amount of resources that server $j$ can still accept for new task offloading requests at time $t$.

The server $\mathbf b_j$ will earn a reward when it receives a task offload. Then, the total reward of edge server $\mathbf s_i$ at the end of the total time step $T$ is: $R_i = \sum_{t=1}^T (p_{t,i} - c_{i})$ Where, $p_{t,i}=\sum_{i=1}^{|\mathcal U_t|} p_{i} x_{j,i}$ is the total payment received in the $t$-th time step, $p_{j}$ is the payment of task $\theta_j$, and $c_{i}$ is the maintenance cost of the server per round.

The goal is to design a task bidding strategy for all buyers $\mathbf b$ and sellers $\mathbf s$, so that each edge server can maximize the overall revenue of the seller $R=\sum^{m}_{i=1} R_i$ under the premise of meeting the task budget and server execution constraints. Based on the above definitions, we can model the task scheduling problem in the vehicular network as an optimization problem:

$$
\begin{aligned} &Objective: \max_{q_j} R= \sum^m_{i=1} \sum_{t=1}^T (\sum_{j=1}^{|\mathcal U_t|} p_{j} x_{j,i}- c_j)\\ &\\ &\text{s.t.}\ \ \begin{aligned} & C1:\sum_{j=1}^n x_{j,i}\sigma_{j,i} \le 1, \forall i \in \mathbf b, j \in \mathbf s \\ & C2:p_j \sum_{i=1}^m x_{j,i}\sigma_{j,i} \leq b_j, \forall i \in \mathbf b, j \in \mathbf s \\ & C3: r_{s_i}(t)\ge 0, \forall i \in \mathbf b, s_i \in \mathbf s, t \in [0, T]\\ \end{aligned} \end{aligned} 
$$

where:

- $R$ is the overall revenue of the seller
- $x_{j,i}$ is the indicator variable for whether task $\theta_j$ is assigned to edge server $\mathbf s_i$
- $\sigma_{j,i}$ is the service quality of edge server $\mathbf s_i$ for task $\theta_j$
- $p_j$ is the payment for task $\theta_j$ assigned to edge server $\mathbf s_i$
- $c_j$ is the maintenance cost of edge server $\mathbf s_i$
- $r_{s_i}(t)$ is the available resources of edge server $\mathbf s_i$ at time $t$

The constraints are as follows:

- C1: Each task can only be offloaded to one edge server in $a_j$.
- C2: The final payment for each offloaded task cannot exceed its budget $b_j$.
- C3: The available resources of each edge server at any time point $r_k(t)\ge 0$.

In the next chapter, we will introduce the task scheduling algorithm based on multi-agent reinforcement learning and reverse auction.

## 4. Method of MADRL and Reverse-Auction

To address the workflow scheduling problem in edge computing for vehicular networks, we propose a method based on multi-agent reinforcement learning and reverse auction mechanism. We first model the problem as an online and NP-hard problem, since the task offloading in vehicular networks can be regarded as a temporal multi-dimensional knapsack problem. Then, we use reinforcement learning to deal with complex dynamic high-dimensional decision-making problems. We adopt multi-agent reinforcement learning to achieve distributed resource allocation, avoiding the communication overhead and information inaccuracy of centralized management, and improving the stability of the system. We also propose a task offloading method based on reverse auction for solving the resource allocation problem in vehicular networks. Reverse auction is a one-buyer-multi-seller auction mechanism, suitable for trading activities in the buyer market. The buyer submits its demand and task priority, and the seller makes a bid based on its own resource usage and task features. The buyer only chooses one seller to trade with. Reverse auction can effectively reduce the buyer's procurement costs, improve the competitiveness of sellers, and achieve optimal resource allocation.

### Multi-agent reinforcement learning environment modeling

In the task offloading modeling of Chapter 3, edge servers are defined as agents, each of which learns a policy that maximizes its own reward through interaction with the environment. Because in a multi-agent environment, the action of one agent affects the decision of other agents. In order to describe this situation, we use a partially observable Markov decision process (POMDP) to model multi-agent reinforcement learning. A POMDP consisting of K agents can be defined as $(\mathbf S,\mathbf O_1,\dots,\mathbf O_K,\mathbf A_1,\dots,\mathbf A_K, \mathbf R_1,\dots,\mathbf R_K, \mathbf P_{ss'}, \mathbf P_{o|s})$ where $\mathbf S$ is the state space, $\mathbf O_i$ is the observation space of the $i$-th agent, $\mathbf A_i$ is the action space of the $i$-th agent, $\mathbf R_i$ is the reward function of the $i$-th agent, $\mathbf P_{ss'}$ is the state transition probability, and $\mathbf P_{o|s}$ is the observation likelihood. In each time interval, the edge server, as an agent, observes the observation $o_i\in\mathbf O_i$ generated by the current environment, and selects a suitable action $a_i\in\mathbf A_i$ according to the policy $\pi_i(a_i|o)$. Then, it obtains the corresponding reward $r_i=\mathbf R_i(s,a_1,\dots,a_K)$. In the subsequent parts of this chapter, we will introduce in detail how to define $\mathbf S,\mathbf O,\mathbf A,\mathbf R$ and how to design a multi-agent reinforcement learning environment.

**State Space.** The state space is a description of the entire system, including all the information in the system. The state space can be represented by a set $\mathbf S$, as follows: $\mathcal S=\{\mathcal B,\mathcal S, \mathbf r_{\mathcal B}\}$ where $\mathcal B$ represents the seller servers, $\mathcal S$ represents the task requests submitted by the buyer vehicles, $\mathbf r_{\mathcal B}=\{r_{\mathbf s_1},\dots,r_{\mathbf s_m}\}$ represents the resource usage of the seller servers, and $r_{\mathbf s_i}=(r_{\mathbf s_i}(1),\dots,r_{\mathbf s_i}(T))$.

**Observation Space.** The observation space is the subset of the state space that an agent can observe. In this study, we assume that there is no additional communication or interaction between agents. Therefore, the observation space for agent $i$ is the resource usage of its server from the current time $t$ to a certain time in the future $\mathbf r_{\mathbf s_i,t}=(r_{\mathbf s_i}(t),\dots,r_{\mathbf s_i}(t+\epsilon))$ and the task execution features $\mathbf f_j$ of the task $\theta_j$ that is waiting to be quoted by the agent. Therefore, the observation space for edge server $i$ is $O_i=\{r_{\mathbf s_i,t},\mathbf f_j\}$ This means that each agent can only make decisions based on its own server load and the features of the current task, and cannot obtain information about the state of other agents.

**Action space.** Action space is the range of actions that an agent can choose in the environment. We assume that each agent's action space is a real number in $[1,2]$, representing its bidding coefficient $a_i$ for the task. This allows the agent to adjust its bidding strategy flexibly. For the current auction task $\theta_j$, the agent $i$'s bid is: $q_i = a_i \times \mathbf p \cdot \mathbf r_j \times \tau_j$ where $\mathbf p$ is the average price of the resources, $\mathbf r_j$ is the resource usage, $\tau_j$ is the resource usage time. However, the task $\theta_j$ has execution server constraints $\mathcal S_j \subseteq \mathcal S$, only the servers in $\mathcal S_j$ can bid. Therefore, the system's action space is: $A_t=\{a_1\sigma_{j,1},\dots,a_m\sigma_{j,m}\}$ where $a_i\sigma_{j,i}$ is the server $i$'s bidding coefficient for the task $\theta_j$. If the server does not meet the task condition, then $a_i\sigma_{j,i}=0$, indicating that it cannot bid. The environment only collects the bids of the servers that meet the condition.

**Rewards.** The reward function defines the agent's goal and evaluation criteria. The goal of the edge server is to maximize its profit, which is the difference between task payment and server cost. Therefore, we designed the following reward function: $\mathbf R_i = p_{t,i}-c_i$ . It consists of two parts: $p_{t,i}$ represents the sum of task payments completed by the edge server in one time step, and $c_i$ represents the fixed cost of the edge server in one time step.

### Reverse Auction-based Task Offloading Method

Task offloading involves the resource allocation problem among multiple participants, that is, how to assign the tasks of vehicles to suitable edge servers and make all parties achieve maximum utility. To solve this problem, this paper draws on the reverse auction theory in economics and proposes a reverse auction-based task offloading method (RATO). Reverse auction is a form of auction with one buyer and multiple sellers, suitable for transactions in a buyer's market. In a reverse auction, the buyer proposes his demand and budget, and the sellers compete to offer their bids based on their own costs and profits. Finally, the buyer selects one or more sellers with the lowest bids that meet his demand to trade. Reverse auction can effectively reduce the buyer's procurement cost, increase the competitiveness among sellers, and achieve optimal resource allocation.

This paper will introduce the roles and process of reverse auction in this model respectively.

**Buyer** is a vehicle in vehicular network that has task offloading demand and is willing to pay for its demand. At a certain moment $t$, the vehicle evaluates its connection status and initiates a task offloading request for task $\theta_j$ to the edge servers that meet its connection constraints, and sends its task attributes $\mathbf{f}_j$ to the server set $\mathcal S_j$ that meets the connection constraints to request bids.

**Seller** is an edge server in vehicular network. The edge server can receive the task offloading request from the vehicle and use its own resources to complete the task offloading and obtain revenue. The seller server $\mathbf s_i$'s characteristics can be defined as $\mathbf{s}_i = (\mathbf{r}_i, c_i)$, where resource vector $\mathbf{r}_i$ and cost $c_i$. After receiving the bid request, the server determines whether it can accept the task offloading. If it can accept it, it will use the reinforcement learning learned policy to bid for the task based on the current observation of available resources $\mathbf r_{s_i,t}$ and task characteristics $\mathbf f_j$.

To simplify the problem, this paper makes the following basic assumptions:

- Assume that time is divided into slots, tasks can only be submitted in one slot, and task duration is an integer number of slots.
- Vehicles can evaluate their connection status based on their own state, and obtain the edge server constraint conditions for tasks during the auction and offloading time.
- There is information asymmetry in the offloading market:
  - The seller's characteristics and maintenance costs are not visible to the buyer. Increase the competitiveness of the auction and improve the buyer's utility.
  - No seller knows other participants' bids, nor do sellers collude with each other.
  - The buyer's budget cannot be disclosed to the seller.
- During the auction process, only one task participates at a time. After the task auction is over, enter the next task's auction in this slot. Prevent sellers from colluding to raise prices.
- A buyer can only offload his task to one seller, but a seller can provide services to multiple buyers at the same time.
- The result of task offloading is consistent for vehicles in terms of utility.

In the cloud computing-based vehicular network scenario, we consider the case of $m$ sellers $\mathcal S$ and $n$ buyers $\mathcal B$. Each buyer represents a vehicle, which communicates with the edge server through its own communication unit, and generates corresponding constraints according to the communication status with the edge server, so as to select a suitable seller when it needs to perform task offloading. Each seller represents an edge server, which can connect with other edge servers and the Internet through wired links, and return the offloading results. At the same time, the edge server can also return the task execution results and transmit data between vehicles through wired connections. Algorithm 1 is a specific description of the reverse auction process.

Algorithm 1: Task offloading method based on reverse auction
Input: Vehicle set $\mathcal B$, server set $\mathcal S$
Output: Task offloading result $\mathcal R$

1. Initialization: For each server $\mathbf s_i \in \mathcal S$, initialize its resource vector $\mathbf r_i$, cost $c_i$, and bidding strategy $\pi_i$; initialize the offloading task set $\mathcal U=\{\mathcal U_1,\dots,\mathcal U_T\}$.
2. For each time slot $t=1,2,\dots,T$, perform the following steps:
   1. For each server $\mathbf s_i \in \mathcal S$ in the server cluster, update its resource usage and settle the revenue of the previous time slot $p_{t-1,i}-c_i$
   2. Select the buyer vehicle request $\mathbf b_j$ from $\mathcal U_t$ in chronological order, then perform the following steps:
      1. The vehicle evaluates its own connection status and generates the server constraint vector $\mathbf \sigma_j$ for the task.
      2. The vehicle sends the task offloading request $\mathbf f_j$ to the set of servers that meet the constraint condition $\mathcal S_j = \{\mathbf s_i | \forall \mathbf s_i \in \mathcal S, \sigma_{j,i} = 1\}$.
      3. For each server $\mathbf s_i \in \mathcal S_j$, if its available resource $r_{s_i}(t)$ can meet the resource demand of the task $r_{\theta_j}(t)$, then perform the following steps:
         1. The server calculates its own bid for the task $q_{j,i}$ according to its own resource usage situation $\mathbf r_{s_i,t}$ and task characteristics $\mathbf f_j$ in the time period $[t,t+\epsilon]$, as well as the bidding strategy $\pi_i$.
         2. The server returns the bid $q_{j,i}$ to the vehicle.
      4. The vehicle collects all the bids from the servers $\mathcal Q_j = \{q_{j,i} | \forall \mathbf s_i \in \mathcal S_j\}$ and sorts them in non-decreasing order.
      5. The vehicle selects the lowest bidder as the winner $\mathbf s_k =\mathop{\arg\min}\limits_{\mathbf s_i}\ q_{j,i}$, and checks whether its bid $p_j = q_{j,k}$ satisfies the budget condition $p_j \leq \psi_j$. If not, then this auction fails and the task offloading fails; if yes, then this auction succeeds and the task offloading succeeds, and perform the following steps:
         1. The vehicle sends the task to the winning server $\mathbf s_k$ and pays $p_j$, waiting for the execution result to return.
         2. The server $\mathbf s_k$ obtains revenue, allocates resources for the task and performs task offloading, updates its own resource usage situation $\mathbf r_{s_i,t}$
      6. Update task offloading result $\mathcal R$
3. When reaching total time step $T$, offloading ends

### PPO Algorithm

In the previous section, we have introduced the MDP modeling and the environment modeling. In this section, we will introduce a policy gradient-based reinforcement learning algorithm - PPO algorithm[1]. The goal of the PPO algorithm is to improve the policy by optimizing a "surrogate" objective function using stochastic gradient ascent after sampling data from the environment. The core idea of the PPO algorithm is to use the trajectories generated by the policy to estimate the gradient, and use the empirical return or the advantage function as the direction of the gradient. The PPO algorithm can handle reinforcement learning environments with continuous state and action spaces.

The objective function of the PPO algorithm is the ratio between the new policy and the old policy scaled by the advantage value, i.e.,

$$
J_{\theta}=E_{t}\left[\frac{\pi_{\theta}(a_{t}\mid s_{t})}{\pi_{{\theta_{\mathrm{old}}}}(a_{_t}\mid s_{t})}A^{\mathrm{GAE}(\gamma, \lambda)}_{t}\right]
$$

where $\pi_{{\theta_{\mathrm{old}}}}$ denotes the old policy before the update. This objective has the same principle as the cross-entropy method: importance sampling. To limit the update, the PPO algorithm uses a clipped objective:

$$
J_{\theta}^{\mathrm{clip}}=\mathbb{E}_{t}\left[\min(r_{t}(\theta)A^{\mathrm{GAE}(\gamma, \lambda)}_{t},\mathrm{clip}(r_{t}(\theta),1-\varepsilon,1+\varepsilon)A^{\mathrm{GAE}(\gamma, \lambda)}_{t})\right]
$$

This objective constrains the ratio between the new and old policies within the interval $[1–ε, 1+ε]$, and thus by changing $ε$, we can limit the size of the update. This clipping method can avoid using KL divergence penalty or adaptive update, which simplifies the algorithm.

There are two main implementations of the PPO algorithm, one is to use KL divergence as a penalty term to limit the magnitude of the policy update, and the other is to use a clipping function to limit the ratio between the new and old policies. The PPO algorithm also uses a more general way of estimating the advantage function, namely GAE (Generalized Advantage Estimation). GAE was proposed by John Schulman et al. in their 2015 paper[2]. The basic idea of GAE is to use the TD($\lambda$) method to balance the bias and variance, while reducing the variance of the advantage function. GAE is defined as follows:

$$
A^{\mathrm{GAE}(\gamma, \lambda)}_{t}=\sum_{l=0}^{\infty}(\gamma \lambda)^{l} \delta_{t+l}
$$

where $\delta_t$ is the TD error, i.e., $\delta_t = R_t + \gamma V_\theta(S_{t+1}) - V_\theta(S_t)$. It can be seen that GAE is a weighted accumulation of TD errors, where the weights decay exponentially as $l$ increases. When $\lambda=0$, GAE degenerates to one-step TD error; when $\lambda=1$, GAE degenerates to MC estimation. Therefore, by adjusting the value of $\lambda$, we can find a balance point between bias and variance. GAE also has an important property that it can eliminate the constant bias of the advantage function.

To solve the task offloading problem in the vehicular network environment proposed in Chapter 3, we adopt the PPO algorithm described above. This algorithm can directly learn the optimal policy according to the task requirements and priorities, without having to estimate the state-action value like Q-Learning and other value function-based algorithms.

We use the multi-agent reinforcement learning framework, where each edge server deploys a PPO agent, and updates its own policy according to its own state and action. In this way, the policies of different servers can be adaptively adjusted according to the task demand and market competition, thereby improving their own revenue and the utility of the vehicles. At the same time, multi-agent reinforcement learning can also handle the interaction and game between multiple agents, so that each server can consider the behavior and influence of other servers. To simplify the model, we assume that there is no additional communication between the servers, and only one game is played.

Next, we will introduce the specific design of our reinforcement learning simulation environment, show our experimental results and analysis, and compare the performance and efficiency of the PPO algorithm with other baseline algorithms.

Algorithm 2: Multi-Agent Policy Update using PPO

Input: Edge servers set $\mathcal S$, experience replay buffer $\mathcal D$
Output: Updated policy parameters for each server

1. Initialize policy network parameters $\theta_i$ for each server $\mathbf s_i \in \mathcal S$
2. Initialize other hyperparameters, e.g., learning rate, discount factor, clip parameter, etc.
3. for each training iteration do
   1. Sample a mini-batch of experiences from $\mathcal D$: $\{(s_i, a_i, r_i, s'_i)\}$
   2. for each server $\mathbf s_i \in \mathcal S$ do
      1. Compute advantage estimates $A_i$ using the collected experiences
      2. Compute policy probabilities for selected actions using the current policy $\pi_{\theta_i}(a_i|s_i)$
      3. Compute old policy probabilities for selected actions using the old policy $\pi_{\theta_{i_{old}}}(a_i|s_i)$
      4. Compute the ratio $r_i = \frac{\pi_{\theta_i}(a_i|s_i)}{\pi_{\theta_{i_{\text{old}}}}(a_i|s_i)}$
      5. Compute the clipped surrogate objective: $L_i(\theta_i) = \mathbb{E}\left[\min(r_i A_i, \text{clip}(r_i, 1-\epsilon, 1+\epsilon) A_i)\right]$
   3. Update the policy network parameters $\theta_i$ using gradient ascent on $L_i(\theta_i)$
   4. end for
   5. Update the old policy parameters: $\theta_{i_{\text{old}}} \leftarrow \theta_i$
4. end for

## 5. Performance Evaluation

Computational offloading is a technique in vehicular networks aimed at enhancing vehicle performance and conserving energy by transferring a portion of computational tasks to edge servers. The objective of computational offloading is to maximize vehicle utility while ensuring the profitability of edge servers and user experience. To achieve this, edge servers need to design appropriate bidding strategies, determining the bid amount for each transaction based on task requirements, urgency, and their own resource constraints.

Learning bidding strategies involves complex environment modeling, incomplete information, and dynamic optimization problems, making the choice of validation methods crucial. Offline validation and online validation are common approaches, each with its pros and cons. Offline validation relies on historical data for counterfactual estimation, but results may be influenced by data biases, counterfactual assumptions, and environmental changes, potentially violating the Goodhart's Law. In contrast, online validation directly tests the performance of bidding strategies in a real environment through experiments, offering more reliability. However, it comes with higher costs, time consumption, and some level of risk.

Reinforcement learning, as a method for learning optimal behavior through interactions between agents and environments, has shown promising applications in various fields such as gaming, robotics, and recommendation systems. Despite its practical successes, the research community has acknowledged its limitations, emphasizing the significance of reliable simulation environments as a major recent advancement. Simulation environments provide a platform that closely resembles the real environment but is free from its constraints and influences, enabling researchers to rapidly develop, test, and enhance reinforcement learning algorithms. In certain relevant domains, such as recommendation systems, simulation environments have been widely adopted as effective evaluation mechanisms.

For the validation of server bidding strategies in the context of computational offloading in vehicular networks, we have developed a corresponding reinforcement learning environment based on the aforementioned model. In the process of bidding strategy learning, we leverage reinforcement learning to assist edge servers in dynamically determining bid amounts based on task offloading features and their own constraints, aiming to maximize utility or profitability.

### Simulation environment design and parameter setting

To validate the effectiveness of the proposed Vehicle-to-Edge task Offloading (RATO) method based on multi-agent reinforcement learning and reverse auction mechanism, we developed an open-source simulation environment named "VehicleJobScheduling" using Python. This simulation environment aims to simulate the process of vehicle task offloading and provides a set of commonly used evaluation metrics. In its design, we drew inspiration from PettingZoo, a popular multi-agent reinforcement learning framework that defines a standard interface, enabling different algorithms to be compared within the same environment.

: PettingZoo: Gym for Multi-Agent Reinforcement Learning

The VehicleJobScheduling simulation environment is primarily composed of three components: vehicles, servers, and environment generator. In this environment, vehicles act as task initiators, selecting an appropriate server for task offloading based on resource requirements, connection constraints, and budget, and subsequently paying the corresponding fee. Servers, on the other hand, act as task executors, quoting and executing accepted task offloading requests based on resource availability and cost. The environment generator is responsible for generating parameters for tasks and servers, including task arrival rate, features, connection constraints, as well as the number, type, resource vectors, and costs of servers. Users can explore different task offloading scenarios and strategies by setting these parameters.

Our simulation environment supports the Agent Environment Cycle (ACE) mode, an interface suitable for sequential turn-based environments that can accommodate any multi-agent reinforcement learning algorithm. In ACE mode, each agent represents an edge server, receiving task offloading requests from vehicles, using its own resources to complete task offloading, and earning profits. At each time step, one or more vehicles send task offloading requests to the set of servers that meet their connection constraints, waiting for server quotations. The environment, based on the connection constraints, selects a server as the current round's actor. This server calculates task quotations based on its resource usage, task features, and quotation strategy, and returns them to the vehicles. Once all servers meeting the connection constraints have submitted their quotations, vehicles select one or more servers with the lowest quotations that simultaneously meet their requirements and budget conditions for transactions. After a successful transaction, vehicles send tasks to the winning server, pay the corresponding amount, and wait for the execution results. Servers gain profits, allocate resources for tasks, and execute task offloading. If no server's quotation meets the vehicle's conditions, the vehicle is unable to complete task offloading and incurs no cost.

To evaluate the performance and effect of task offloading ,the environment provides some indicators ,including load balance ,server profit ,vehicle utility and task completion rate .Load balance measures the degree of resource utilization balance among all servers ,server profit reflects the economic benefit obtained from task offloading by servers ,vehicle utility reflects the service quality obtained from task offloading by vehicles ,task completion rate reflects the success rate of task offloading .These indicators help evaluate different strategies of task offloading .

We abstracted the task unloading requests for vehicles, representing them as task features with four dimensions: priority, resource requirements, duration, and task connection constraints. The priority of tasks ranges from 0 to 10, influencing the budget allocation for vehicle-task assignments. The maximum requests for CPU and memory per task are 24 and 100, respectively. Task duration is set from 1 to 10 time slots, each slot representing 1 minute, resulting in a maximum execution time of 10 minutes per task. Task connection constraints range from 0 to 3, indicating the number of servers to which a task unloading request can be sent based on channel conditions.

In our experiments, we chose [Azure Dedicated Hosts](https://learn.microsoft.com/en-us/azure/virtual-machines/dedicated-hosts) as edge servers, offering various virtual machine specifications and prices. We selected three different Dedicated Host SKUs representing large, medium, and small edge servers. The cost from cloud service providers is calculated based on a three-year plan, while market prices are derived from Azure's US East [Container Instances](https://azure.microsoft.com/en-us/pricing/details/container-instances/) pricing. Table 1 presents the specifications and prices of the chosen Dedicated Hosts.

| Dedicated Host SKUs | vCPUs | RAM       | 3 year plan |
| ------------------- | ----- | --------- | ----------- |
| Mdsv2MedMem-Type1   | 192   | 2,048 GiB | $ 5.81      |
| Easv5-Type1         | 112   | 768 GiB   | $ 3.49      |
| Fsv2-Type2          | 72    | 144 GiB   | $ 2.05      |

Table 1 Chosen Dedicated Hosts

| Parameter                      | Range                                     |
| ------------------------------ | ----------------------------------------- |
| Task arrival rate distribution | Poisson distribution                      |
| Tasks per unit time            | 20                                        |
| Task duration                  | 1 to 10 time slots, each slot is 1 minute |
| Task priority                  | 0 to 10                                   |
| Task CPU requests              | Maximum 24                                |
| Task memory requests           | Maximum 100                               |
| Task connection constraints    | 0 to 3                                    |

Table 2 Environmental hyperparameters

| Agent Model Type                 | PPO Agent                    | PPO LTSM Agent                      |
| -------------------------------- | ---------------------------- | ----------------------------------- |
| Network architecture             | Multi-Layer Perceptron (MLP) | MLP + Long Short-Term Memory (LSTM) |
| Fully connected layer parameters | [384,384,384]                | [384,384,384]                       |
| LSTM unit size                   | -                            | 256                                 |
| Activation function              | tanh                         | tanh                                |

Table 3 Reinforcement learning agent parameters

In this study, we employed two different reinforcement learning agent models, namely the PPO Agent and PPO LTSM Agent. Both are based on the Proximal Policy Optimization (PPO) algorithm, differing in their network structures. The PPO Agent utilizes a Multi-Layer Perceptron (MLP) with three fully connected layers, each with 384 neurons and a tanh activation function. The PPO LTSM Agent extends the PPO Agent's model by incorporating an LSTM layer with a unit size of 256, enhancing the model's memory capacity to handle and predict time-correlated data. We applied these two models to the RATO method to compare their performance in different scenarios.

This configuration allows us to consider diverse vehicular networking scenarios in simulation, encompassing tasks of varying types and durations, as well as dynamic interactions between vehicles and edge servers. Subsequently, we will conduct a numerical analysis of the RATO method in this simulation environment to comprehensively evaluate its performance in different scenarios. We will specifically focus on the impact of different server pricing strategies on system effectiveness to gain deeper insights into the advantages and applicability of the proposed method.

### Convergence Analysis of Deep Reinforcement Learning Agents

![[Pasted image 20231128145930.png]]

Figure 1: Mean Reward Estimations of PPO and PPO+LTSM over the Last 448,000 Iterations

In this section, we assess the convergence performance of two approaches employing Proximal Policy Optimization (PPO) reinforcement learning agents—PPO and PPO+LTSM—in the context of vehicular network resource offloading. We utilize two multi-agent reinforcement learning algorithms, PPO and PPO+LTSM, to train multiple agents with the objective of maximizing the server's revenue within a single round. The server's revenue reflects the effectiveness of task offloading and serves as the reward function for the agents. We conduct training for the agents over 4,480,000 iterations and evaluate the algorithms every 40,000 iterations, calculating the average reward of the training policy over 10 test runs. Figure 1 illustrates the reward evaluation during the training process, while Table 1 presents the mean and standard deviation of the rewards for PPO and PPO+LTSM over the last 448,000 iterations.

| Algorithm | Average Reward | Standard Deviation |
| --------- | -------------- | ------------------ |
| PPO       | $1.68×10^7$    | $2.65×10^5$        |
| PPO+LTSM  | $1.72×10^7$    | $1.12×10^5$        |

Table 1 summarizes the variance and mean of the rewards for the last 10% of iterations during the training process for PPO and PPO+LTSM.

It can be observed that PPO+LTSM has an average reward of $1.72×10^7$, approximately 2.4% higher than PPO's $1.68×10^7$. Additionally, PPO+LTSM exhibits a standard deviation of $1.12×10^5$, approximately 57.7% lower than PPO's $2.65×10^5$. These results indicate that not only does PPO+LTSM achieve a higher average reward, but it also demonstrates greater stability with significantly reduced fluctuations. Consequently, PPO+LTSM exhibits superior convergence performance in vehicular network resource offloading. In conclusion, the RATO method demonstrates commendable convergence in various task offloading scenarios, rapidly learning optimal or near-optimal bidding strategies, thereby enhancing server revenue and vehicle utility. This validates the effectiveness and robustness of the RATO method, highlighting the advantages and applicability of reinforcement learning and reverse auction mechanisms.

### Experimental Results and Analysis

This paper introduces a vehicular network task offloading method based on multi-agent reinforcement learning and reverse auction mechanism, referred to as RATO. Comparative analyses were conducted through simulation experiments with other benchmark methods. The study considered three scenarios with task arrival numbers of 10, 20, and 40, employing four different server bidding strategies: Fixed Bid (FB), Random Bid (RB), PPO-based Bid (PPO), and PPO+LSTM-based Bid (PPO+LSTM). Four metrics—Load Balance, Server Earning, Vehicle Utility, and Complete Rate—were utilized to evaluate the performance of different scheduling strategies under varying arrival numbers. Ten independent simulation experiments were conducted for each arrival number, using five random seeds for simulation. The mean and standard deviation of each metric were calculated and analyzed.

The objective of this study is to compare the performance of different bidding strategies in a task offloading system and analyze the advantages of the RATO method. Four bidding strategies were considered:

- **Fixed Strategy**: In this strategy, servers use market prices as a reference and determine bids solely based on task features and duration.
- **Random Strategy**: Servers use market prices as a reference and randomly sample a coefficient within a certain range to determine bids.
- **PPO Strategy**: Servers employ the PPO reinforcement learning algorithm to dynamically determine bids based on their resource usage and task features through a Multi-Layer Perceptron (MLP).
- **PPO+LSTM Strategy**: Building upon the PPO strategy, servers add a Long Short-Term Memory (LSTM) layer to capture temporal information and dynamically bid using a combination of an MLP.

|          | Load Balance | Server Earning (10^6) | Vehicle Utility (10^6) | Complete Rate |
| -------- | ------------ | --------------------- | ---------------------- | ------------- |
| Random   | 0.19±0.012   | 0.55±0.00037          | 0.55±0.037             | 0.76±0.014    |
| Fixed    | 0.20±0.005   | 1.60±0.00007          | 0.76±0.047             | 0.91±0.010    |
| PPO      | 0.20±0.011   | 1.49±0.00004          | 0.78±0.036             | 0.92±0.010    |
| PPO+LTSM | 0.18±0.010   | 1.53±0.00004          | 0.83±0.063             | 0.93±0.012    |

Table 4: Experimental Results for Arrival Number 10

|          | Load Balance | Server Earning (10^6) | Vehicle Utility (10^6) | Complete Rate |
| -------- | ------------ | --------------------- | ---------------------- | ------------- |
| Random   | 0.22±0.007   | 2.74±0.00036          | 0.99±0.033             | 0.72±0.008    |
| Fixed    | 0.23±0.004   | 3.05±0.00014          | 1.43±0.040             | 0.90±0.005    |
| PPO      | 0.22±0.007   | 2.95±0.00011          | 1.53±0.060             | 0.91±0.007    |
| PPO+LTSM | 0.21±0.006   | 3.06±0.00009          | 1.62±0.036             | 0.93±0.009    |

Table 5: Experimental Results for Arrival Number 20

|          | Load Balance | Server Earning (10^6) | Vehicle Utility (10^6) | Complete Rate |
| -------- | ------------ | --------------------- | ---------------------- | ------------- |
| Random   | 0.22±0.006   | 4.80±0.00100          | 1.64±0.034             | 0.63±0.007    |
| Fixed    | 0.21±0.004   | 4.73±0.00071          | 2.26±0.034             | 0.73±0.010    |
| PPO      | 0.21±0.003   | 4.68±0.00077          | 2.29±0.054             | 0.72±0.009    |
| PPO+LTSM | 0.20±0.006   | 4.66±0.00075          | 2.34±0.044             | 0.73±0.009    |

Table 6: Experimental Results for Arrival Number 40

In this section, we present the simulation experiment results in tabular and graphical form to showcase the performance of different strategies under varying task arrival numbers. Tables 1, 2, and 3 provide the mean and standard deviation of each metric for task arrival numbers 10, 20, and 40.

In the horizontal comparison, we delve into the analysis of four distinct bidding strategies to comprehensively evaluate their performance across various metrics. The PPO+LTSM strategy excels in all metrics, such as Load Balance, Server Earning, Vehicle Utility, and Task Completion Rate, with values of 0.21±0.006, 3.06±0.00009, 1.62±0.036, and 0.93±0.009, respectively, outperforming all other strategies. The Fixed strategy performs well in server earnings but exhibits lower performance in Load Balance, Vehicle Utility, and Task Completion Rate, indicating lower user satisfaction. The Random strategy performs consistently poorly across all metrics. While the PPO strategy slightly lags behind the Fixed strategy in server earnings, it outperforms in other metrics. However, compared to PPO+LTSM, PPO exhibits higher variance in all four metrics, indicating relatively greater instability. Overall, the PPO+LTSM strategy demonstrates superior long-term performance.

In the vertical comparison, we closely observe the four bidding strategies under different task arrival numbers. As the task arrival number increases, server earnings and utilization generally rise, while the completion rate experiences a downward trend, possibly due to server overload preventing additional tasks. For the scenario with a task arrival number of 10, where tasks are relatively scarce, all strategies, except for the Random strategy, exhibit similar performance in Load Balance and Task Completion Rate. PPO+LSTM shows a slight advantage in Vehicle Utility and Load Balance, demonstrating better adaptability and flexibility. The higher income of the Fixed strategy may be attributed to its fixed bids aligning with the budgets of most tasks and encountering less competition among servers. When the task arrival number increases to 20, PPO+LTSM surpasses other strategies in all metrics as the competition between servers intensifies. The Load Balance value is higher at this point, suggesting increased server competition and potential load imbalance. However, as the task arrival rate further increases, server load remains consistently high, leading to relatively balanced loads. In the scenario with a task arrival number of 40, PPO+LTSM continues to maintain its lead, achieving better Load Balance, Vehicle Utility, and Task Completion Rate. The Random strategy attains the highest income in this scenario, possibly due to its adaptability to different tasks and competitive environments, avoiding excessive competition and price wars. Nevertheless, it is essential to note that the Random strategy may lack stability over the long term, and its occasional good performance could be a result of randomness. In real-world applications, strategies with stronger adaptability and stability are preferable. In summary, under different task arrival numbers, the PPO+LTSM strategy performs optimally in Load Balance and Vehicle Utility, demonstrating superior adaptability and performance. It maximizes long-term rewards, effectively balancing various system metrics, and providing balanced optimization for system performance under different task arrival numbers.

In conclusion, this paper conducted simulation experiments to compare the performance of different bidding strategies under varying task arrival numbers and analyzed the advantages of the PPO+LTSM strategy. The study reveals that the PPO+LTSM strategy utilizes LSTM to capture temporal information, enabling more rational decision-making and achieving the most effective load balance.

## 6. Conclusion

This paper proposes a vehicular network task offloading method that uses multi-agent reinforcement learning and reverse auction mechanism, aiming to improve the task completion rate and vehicle utility by letting edge servers accept vehicle tasks at appropriate prices. Compared with other methods, the proposed method is not only more effective, but also more stable. However, it should be noted that there are some limitations in this paper, such as not considering the communication reliability, the uncertainty of channel gain, and the cooperativeness of participants. Future work will focus on solving these problems, to make the task offloading more reliable, efficient, and fair. In the early experiments of the paper, I compared several reinforcement learning methods and network structures, and found that they had some defects, affecting the quality of learning and decision making. Finally, I chose the PPO+LSTM strategy, because it can compensate for the shortcomings of other methods, and achieve a suitable efficiency and stability. Future work also includes considering various influencing factors in a more realistic vehicular network environment, such as the mobility of vehicles, the latency and bandwidth of the network, etc., to improve the reliability of the system. At the same time, more reinforcement learning algorithms and models will be tried, and more auction mechanisms and rules will be studied in depth, such as combinatorial auction, VCG auction, etc. In general, the vehicular network task offloading method based on multi-agent reinforcement learning and reverse auction proposed in this paper solves the collaboration and competition problems between vehicles and edge servers, thus improving the efficiency and quality of tasks. The method has innovation and practicality, and provides new ideas and solutions for the development and application of vehicular network.
