---
UID: 20231207170526 
tags: 
source: 
cssclass: 
created: "2023-12-07 17:05"
updated: "2023-12-29 14:25"
---


### 综述

- 这篇文章[^31]是一篇综述，介绍了移动边缘网络的概念、架构、应用和挑战，以及相关的计算任务调度方法。<https://www.sciencedirect.com/science/article/abs/pii/S0140366419307273>
[^31]:Wang X, Wang X, Liang C, Huang C. A Survey on Mobile Edge Networks: Convergence of Computing, Caching and Communications. IEEE Access. 2017;5:6757-74. 

- 这篇文章[^32]是一篇综述，介绍了动态任务调度中使用强化学习的方法和应用。该文章分析了强化学习在不同领域和场景中的优势和挑战，并给出了未来的研究方向和展望。<https://link.springer.com/article/10.1007/s42979-020-00326-5>
[^32]:Shyalika C, Silva T, Karunananda A. Reinforcement Learning in Dynamic Task Scheduling: A Review. SN Computer Science. 2020 Sep;1(5):1-9. 

### 传统算法

- Wu Y, Wu J, Chen L, Yan J, Luo Y. Efficient task scheduling for servers with dynamic states in vehicular edge computing. Computer Communications. 2020 Jan 15;150:245-53. 这篇文章提出了一种考虑边缘服务器动态状态的车联网任务调度方法，目标是最小化任务的响应时间和服务器的能耗。该文章分析了移动边缘网络中涉及到的计算、缓存和通信三个方面的问题和解决方案，并给出了未来的研究方向和展望。 <https://www.sciencedirect.com/science/article/pii/S0140366419307273>
- Hu F, Lv L, Zhang T, Shi Y. Vehicular task scheduling strategy with resource matching computing in cloud-edge collaboration. IET Collaborative Intelligent Manufacturing. 2021 Mar 18. 这篇文章提出了一种基于资源匹配度的车联网任务调度策略，目标是最小化任务的完成时间和边缘服务器的计算单元数量。<https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/cim2.12023>
- Wu Y, Wu J, Chen L, Yan J, Luo Y. Efficient task scheduling for servers with dynamic states in vehicular edge computing. Computer Communications. 2020 Jan 15;150:245-53. 这篇文章提出了一种考虑边缘服务器动态状态的车联网任务调度方法，目标是最小化任务的响应时间和服务器的能耗。该方法利用了泊松过程和马尔可夫决策过程，建立了一个双层优化模型，并设计了一种基于贪心算法和动态规划算法的启发式算法，实现了任务的有效调度。 <https://www.sciencedirect.com/science/article/pii/S0140366419307273>
- Hu F, Lv L, Zhang T, Shi Y. Vehicular task scheduling strategy with resource matching computing in cloud-edge collaboration. IET Collaborative Intelligent Manufacturing. 2021 Mar 18. 这篇文章提出了一种基于资源匹配度的车联网任务调度策略，目标是最小化任务的完成时间和边缘服务器的计算单元数量。该策略利用了改进的混合遗传算法，结合了计算、存储和网络带宽资源的匹配度，生成了更优的解决方案，并利用了虚拟机技术，实现了计算单元的动态分配。<https://www.sciencedirect.com/science/article/pii/S1319157822001653>

### 强化学习

- Jamil B, Ijaz H, Shojafar M, Munir K. IRATS: A DRL-based intelligent priority and deadline-aware online resource allocation and task scheduling algorithm in a vehicular fog network. Ad Hoc Networks. 2023 Mar 15;141:103090. 这篇文章提出了一种基于深度强化学习的智能优先级和截止时间感知的在线资源分配和任务调度算法，目标是最小化任务的等待时间和延迟。该算法利用了近端策略优化算法，将资源分配问题建模为一个马尔可夫决策过程，并设计了一种分布式算法，实现了任务的有效卸载和资源的高效分配。<https://www.sciencedirect.com/science/article/pii/S1570870523000100>
- Li X, Zhang Y, Wang Y, Zhang J. Cluster-Enabled Cooperative Scheduling Based on Reinforcement Learning for Vehicular Networks. IEEE Transactions on Vehicular Technology. 2020 Oct 26;70(1):1018-30. 这篇文章提出了一种基于强化学习的车联网中的集群协作调度方法，目标是提高通信效率和可靠性。该方法利用了深度确定性策略梯度算法，将集群协作调度问题建模为一个连续动作空间的强化学习问题，并设计了一种集中式训练和分布式执行的算法，实现了任务的优化分配和通信资源的合理利用。<https://ieeexplore.ieee.org/document/9217939>

[[Learning to Bid with AuctionGym]]
[[Resource Management with Deep Reinforcement Learning]]



---

[[A Truthful Dynamic Workflow Scheduling Mechanism for Commercial Multicloud Environments]]

### 2.2 基于拍卖的调度

- 资源的市场化调度在分布式系统中是一个深入研究的领域。[10]发表了关于网格经济的调查报告。多种经济模型，如商品市场和拍卖，在分布式系统中得到了提出。例如，在[11]中研究了若干自私任务竞争分布式资源的拍卖机制。他们在理论上研究了均衡解在执行时间方面的质量。[12]提出了一个网格的分层非合作博弈模型，其中用户是自私的代理。
- 另外，[13]提出了基于众所周知的Vickrey-Clarke-Groves（VCG）机制的静态分布式系统的负载平衡机制。[14]提出了网格系统负载平衡问题的动态解决方案，采用了博弈理论方法，该机制将负载分散到每个计算节点上。他们将负载平衡建模为一个约束最小化问题，并提出了一个可以最小化任务的平均完成时间的算法。[3]和[4]中讨论的问题，机制只关心结果。此外，付款仅用于强制代理告诉事实。
- [15]的作者介绍了网格资源分配的动态市场模型。所提出的竞标算法基于“近视均衡策略”。他们分析了用户在重复的基于拍卖的机制中的理性策略，其中用户通过更新其竞标来寻找所需资源。[16]分析了反社会代理对相关机器上任务调度机制中其他代理造成损失的影响，而不是最大化自己的利润。该工作假设反社会代理知道赢家的竞标价值，这与我们工作中存在私人信息的假设相违背。[17]提出了在单一维度领域中的$(1+\epsilon)$-近似时间的真实机制（即解决方案保证在最优解的$(1+\epsilon)$-倍内）。
- 此外，[18]的作者为网格系统中任务调度提出了一个连续双重拍卖机制。[19]针对网格中自私组织的完成时间调度问题类似于囚徒困境博弈。他们得出结论，对社区进行严格控制是实现可接受性能所必需的。[20]中，我们提出了调度程序和资源管理器之间的谈判协议的新实例，使用基于市场的连续双重拍卖模型。
- 最后，在[21]中提出了云计算集群中批处理作业的基于市场的资源分配模型。此问题的社会成本是分配给用户的价值之和。




---

[[A Truthful Reverse-Auction Mechanism for Computation Offloading in Cloud-Enabled Vehicular Network]]

随着计算密集型应用程序的日益普及，虽然为人们带来了极大的便利，但也给车载网络和车载设备带来了巨大的负担 [4]，[6]。这些应用程序通常需要超出智能车辆能力范围的计算资源，这对应用程序所有者来说是一个主要问题 [7]。随着MCO和MEC的出现，车辆可以通过将应用程序上传到移动边缘云服务器来增强其计算能力。一些研究关注通过V2I通信进行计算卸载机制。在[10]中提出了一种能源高效的自适应资源调度器，用于利用TCP/IP连接的本地测量状态，以最大化整体通信加计算能效，同时满足应用程序诱导的最小传输速率、最大延迟和延迟抖动等硬质量服务要求。张等人[11]提出了一个分层框架，引入了邻近的备份计算服务器来补偿MEC服务器的计算资源不足。为了减少车辆的计算卸载延迟，研究了车载边缘网络中多车辆计算卸载游戏[12]。然而，他们没有考虑车载设备上的可用资源和车辆之间的机会性联系的优势。虽然系统性能仍有改进空间，但当前的V2I计算卸载可能会极大地加重已经过载的蜂窝网络负担。此外，RSU的信号覆盖限制（例如，在山区和农村地区等偏远地区）仍可能对卸载方案施加限制。

因此，基于绿色无线通信[13]，机会性卸载[7]被提出作为解决上述问题的解决方案，它利用由车辆间联系形成的机会性网络来进行应用程序卸载。==据我们所知，考虑服务价格的V2V计算卸载机制是一个涉及少量现有工作的新颖研究方向。然而，类似的机会性卸载问题可以归纳为两类：1）设备对设备（D2D）数据卸载和2）D2D计算卸载。对于前者，低数据传输延迟和内容流行度是主要目标，而频谱是最稀缺的资源。相反，在计算卸载情况下，必须同时考虑传输延迟和应用程序处理持续时间，在这种情况下，计算资源的严重限制成为一个大问题。对于后者，智能手机始终受到能量限制，而通常认为车辆具有强大的电池。此外，智能手机用户的移动性在某些情况下可能会被忽略，因为移动速度较低，而车辆移动速度较快，导致不同的信道条件和有限的接触持续时间。==陈等人[4]提出了延迟和能量高效的智能手机用户任务调度方法，但忽略了任务处理持续时间，也没有考虑价格策略。陈等人[14]提出了一个框架，在网络边缘实现移动用户的协作任务执行，主要关注能量效率。

由于云计算是一种按需付费的服务，最好考虑诸如与具有不同能力的服务提供商相关的个性化服务价格、应用程序所有者的成本考虑以及用户偏好等经济因素，在计算卸载市场上限制了车辆之间的接触持续时间在一定程度上也被忽视了。

拍卖在无线通信和认知无线电网络中广泛应用于交通卸载，构成了一种有助于有效价格发现和资源分配的流行交易形式。然而，很少有研究将拍卖机制应用于解决计算卸载市场中的资源分配问题以及激励问题。在[17]中，该模型仅考虑了移动资源的单个买方，因此省略了一般场景中多个买方之间的竞争。金等人[18]和[19]研究了移动云计算中云片的资源共享，并设计了有效的拍卖机制来保证投标者的真实性；然而，他们通过一对一匹配限制了拍卖机制，忽略了资源丰富设备在实际系统中支持多个资源需求买方的能力。王等人[20]设计了一个分布式拍卖机制，通过D2D通信公平分配任务，并确定资源的交易价格。作者还提出了一个支付评估过程，以防止投标者可能的不诚实行为。在王等人的另一项研究中[20]，在移动网络中考虑了异构和同质任务模型，设计了一个拍卖机制，证明了个体理性、真实性和计算效率的属性[21]。在[22]中，计算卸载问题被建模为一个两阶段拍卖，其中具有共同社会特征的远程宏蜂窝用户设备可以组成一个组，然后通过中继小蜂窝用户设备购买小蜂窝BS的计算资源。然而，通过中继进行卸载的传输可靠性难以保护。拍卖可以在一定程度上有效地应对MCO市场的挑战；因此，在云启用的车载网络框架下，如何利用车辆间的机会性联系和不同信道条件设计一个真实和个体理性的卸载机制是一个迫切需要仔细研究的问题。

---

[[Applications of Auction and Mechanism Design in Edge Computing A Survey]]

- 移动边缘计算：2014年，欧洲电信标准化协会（ETSI）首次提出了MEC的概念[30]，MEC的系统架构如图1所示。
  - 他们建议在无线接入网络（RAN）内部部署足够的计算能力、存储空间和服务环境到边缘网络。
- MEC的主要思想是将高度复杂和繁重的计算任务分发给相邻的边缘服务器，提供超低延迟的计算服务、更高的传输带宽、更少的能耗[18]，[31]，[32]等。
  - 通过将高度复杂和计算密集的任务迅速转移至边缘节点，可以极大地减轻终端用户的工作负载。

---

[[Deep Reinforcement Learning-Based Workload Scheduling for Edge Computing]]

当涉及边缘计算时，移动设备可以将大部分任务卸载到边缘服务器进行执行，这有效地解决了它们有限资源的问题，同时减少了核心网络的流量负荷。然而，不恰当的任务卸载不仅会导致边缘服务器之间工作负载不均衡，还会增加任务延迟和能耗。因此，在边缘服务器之间合理调度计算任务对于优化高资源效率的服务质量至关重要。调度研究旨在选择任务应该卸载的时间和地点。已经有大量工作致力于工作负载调度问题。 ^bcefad

- Santoro等人[11]提出了一个名为Foggy的软件平台，用于雾计算环境中的工作负载编排和资源协商。它根据计算、存储或网络资源安排任务的执行位置。
- Anas等人[12]考虑了计算利用率和访问概率，并基于排队理论开发了一个性能模型，以解决联合云环境中服务提供商之间的工作负载平衡问题。
- Ma等人[13]考虑了边缘节点之间的合作，并研究了旨在最小化服务响应时间以及移动边缘计算中外包流量的工作负载调度。他们提出了一种基于水填充的启发式工作负载调度算法，以减少计算复杂性。
- 模糊逻辑是解决边缘计算中工作负载调度问题的一种有效方法，近年来已经有所讨论。
- Sonmez等人[8]采用了基于模糊逻辑的方法来解决边缘计算系统中的工作负载编排问题。他们的方法考虑了被卸载任务的属性以及当前计算和网络资源的状态，并使用模糊规则来定义工作负载编排行动，涉及网络、计算和任务需求，以决定整个边缘计算系统内的任务执行位置。

基于模糊逻辑的方法需要预先定义各种模糊规则，这将耗费大量时间和精力来衡量。因此，为了减少手动干预，一些研究尝试采用机器学习方法进行工作负载调度。

- Nascimento等人[14]提出了基于强化学习（RL）的调度方法，用于基于云的科学工作流执行。RL是机器学习的一个分支，专注于如何通过在动态环境中学习来实现最优目标。在RL中，作为学习者的代理程序感知环境的当前状态，并选择要采取的行动。
- 在RL中，作为学习者的代理程序感知环境的当前状态，并选择要采取的行动。当执行动作时，代理程序将根据行动的效果从环境中接收奖励或惩罚。如果从环境中接收到奖励，代理程序将增加采取此行动以获取更多奖励的倾向。相反，如果接收到惩罚，代理程序将减少采取此行动的倾向。为了最大化累积奖励，代理程序需要平衡探索和利用步骤。在探索步骤中，代理程序尝试以前未选择过的行动，并探索新的状态以获取更高的奖励。在利用步骤中，代理程序采取迄今为止已经观察到的最佳行动[15]。

尽管基于RL的工作负载调度方法可以减少手动干预并在状态和动作空间较小的情况下有效解决问题，但在状态或动作空间非常大时，通过普通的强化学习几乎不可能获得准确的状态或动作值。[16]

- 结合深度学习和强化学习的DRL算法，如DQN [17]、DDPG [18]和PPO [19]，对于处理复杂性和高维度的问题变得非常有用。在这项工作中，我们提出了一种基于DRL的边缘计算工作负载调度方法，它可以从先前的行动中学习，并在没有环境数学模型的情况下实现最佳调度。同时，我们采用DQN算法来解决复杂性和高维度的工作负载调度问题，旨在平衡边缘服务器之间的工作负载，减少服务时间和失败任务率。

---

[[Dynamic Pricing On E-commerce Platform With Deep Reinforcement Learning A Field Experiment]]

- **动态定价研究综述**
  - 多年来的研究：统计学习与价格优化
  - 假定功能关系下的先前研究
    - 假定静态价格-需求关系：[15]
    - 实际情况下的问题：静态假设的不准确性：[16]
    - 引入随时间变化的动态需求函数：[17]
    - 考虑约束条件：[18]
  - 未知需求函数的动态定价
    - 参数化方法的尝试
      - 假设参数化的需求函数族：[19]
      - 历史购买数据学习：[20]
      - 贝叶斯动态定价策略：[21]
      - 错误地规定了需求族，收入可能会偏离最优值
      - 非参数化学习的深入研究：[22], [23], [24]
      - 凹凸函数性质在电子商务零售业的不适用性
  - 强化学习的引入
    - 应用于动态问题：[25], [26]
    - Q-learning, 的价格机器人以根据市场变化调整价格的可能性[9]
    - 时序差分在定价中的应用： [13]
    - 对单卖家和两个卖家的动态定价问题进行了建模，并在模拟环境中采用不同的强化学习算法[11]
    - 不同类型的异步多智能体强化学习方法确定市场情境下的竞争定价策略, [27] ^e396ec
    - 能源市场中的强化学习应用：[10], [12]
    - 使用神经网络逼近的Q-learning来在模拟环境中维持收入同时提高公平性。[8]
    - DNN仅用于离散价格的逼近，而在现实世界的市场中情况并非如此。

---

[[1. Projects/2023-05-07 基于多智能体强化学习和反向拍卖机制的车联网任务调度/参考文献/mmd/Multi-Objective_Workflow_Scheduling_With_Deep-Q-Network-Based_Multi-Agent_Reinforcement_Learning|Multi-Objective_Workflow_Scheduling_With_Deep-Q-Network-Based_Multi-Agent_Reinforcement_Learning]]

当在分布式平台上安排多任务工作流时，普遍认为这是一个 NP 难问题。因此，通过遍历型算法获得最优调度方案非常耗时。幸运的是，启发式和元启发式策略具有多项式复杂度，能够以可接受的最优性损失产生近似或接近最优解。 ^9f03a2

- Kaur 等人 [24] 修改了原始的细菌觅食优化算法（BFOA），提出了一种多目标细菌觅食优化算法（MOBFOA），考虑了帕累托最优前沿，旨在最小化流程时间、制造周期和资源使用成本。
- Zhang 等人 [25] 提出了一种双目标遗传算法（BOGA），能够优化能源节约和工作流可靠性，并获得接近最优的帕累托前沿。
- Casas 等人 [26] 提出了一种增强型的遗传算法与高效调谐（GA-ETI），用于云系统中的科学应用，能够优化工作流制造周期和成本。
- Verma 等人 [27] 提出了基于非支配排序的混合粒子群优化（HPSO）算法，用于工作流调度，能够优化执行时间和成本。
- Zhou 等人 [28] 提出了基于模糊支配排序的异构最早完成时间（FDHEFT）算法，能够最小化部署在 IaaS 云上的工作流的成本和制造周期。

然而，这些方法受到以静态全局视角为基础的先前专家知识的严重限制，无法恰当地描述工作流调度的动态过程。

最近，博弈论和强化学习（RL）模型和方法被广泛应用于多约束过程调度问题 [29, 30, 31, 32, 33, 34, 35]。人们相信博弈论中的均衡概念和多智能体训练方法在处理多约束和多目标优化问题方面具有很高的潜力。 ^0a55cc

- Duan 等人 [18] 提出了一种用于成本和制造周期优化的顺序协作博弈算法，同时满足大规模工作流调度的存储约束。
- Cui 等人 [22] 提供了一种基于强化学习的方法，用于云环境中不同时间提交的多优先级多工作流调度。
- Iranpour 等人 [17] 提出了一种基于模糊博弈理论模型的分布式负载平衡和准入控制算法，用于大规模 SaaS 云。
- Wu 等人 [20] 提出了一种改进的带加权适应值函数的 Q-learning 算法，用于云环境中完成时间和负载平衡的优化。

---

[[Performance and Cost-Efficient Spark Job Scheduling Based on Deep Reinforcement Learning in Cloud Computing Environments]]

深度强化学习（DRL）在作业调度中的应用是相对较新的。有一些工作试图解决云基础应用程序的不同SLA目标。

- Liu等人[28]开发了一个分层框架，用于在减少能源消耗和降低延迟的同时进行云资源分配。
  - 全局层使用Q-learning进行虚拟机资源分配。
  - 本地层使用基于LSTM的工作负载预测器和基于模型的RL进行本地服务器的功耗管理。
- Wei等人[29]提出了一种面向QoS的应用程序在云部署中的作业调度算法。
  - 使用DQN与目标网络和经验回放来提高算法的稳定性。
  - 主要目标是改善平均作业响应时间，同时最大化虚拟机资源利用率。
- DeepRM[14]使用REINFORCE，一种政策梯度深度RL算法，用于集群调度中的多资源打包。
  - 主要目标是最小化平均作业减慢。
  - 集群被假设为同构的，由于状态空间将所有集群资源视为一个大的CPU和内存块。
- Decima[15]使用政策梯度代理，处理Spark中每个作业的DAG调度问题，同时考虑相互依赖的任务。
  - 目标与DeepRM类似，致力于最小化平均作业减慢。
- Li等人[30]考虑了基于Actor Critic的算法，以处理Apache Storm中持续连续数据的处理。
  - 调度问题是将工作负载分配到特定的工作节点，目标是减少平均端到端元组处理时间。
  - 假设集群设置为同构，不考虑成本效率。
- DSS[17]是在云计算环境中自动化的大数据任务调度方法，结合了DRL和LSTM。
  - 自动预测每个传入大数据作业应被安排到哪个虚拟机，以提高大数据分析的性能，同时减少资源执行成本。
- Harmony[31]是一个由深度学习驱动的ML集群调度器，以最小化干扰和最大化平均作业完成时间的方式放置训练作业。
  - 使用基于Actor Critic的算法和作业感知的行动空间探索与经验重放。
  - 具有奖励预测模型，使用历史样本进行训练，用于未见放置生成奖励。
- Cheng等人[18]使用基于DQN的算法进行云中Spark作业的调度。
  - 主要目标是优化带宽资源成本，同时最小化节点和链接能源消耗。
- Spear[32]致力于最小化复杂DAG作业的完成时间，同时考虑任务依赖性和异构资源需求。
  - 利用蒙特卡洛树搜索（MCTS）进行任务调度，并训练DRL模型来指导MCTS中的扩展和回滚步骤。
- Wu等人[33]提出了一种基于深度CNN和基于价值函数的Q-learning的最佳任务分配方案。
  - 任务被分配到合适的物理节点上，目标是在满足任务要求的同时最大化长期收益。
- Thamsen等人[19]使用梯度赌博法方法提高Spark和Flink作业的资源利用率和作业吞吐量。
  - RL模型学习了在共享资源上不同类型作业的共同位置好处。

---

[[Price-Based Resource Allocation for Edge Computing A Market Equilibrium Approach]]

与一般云经济学和资源分配的现有作品不同，我们的设计目标是以一种公平且高效的方式从多个节点（例如ENs）向预算受限的代理商（即服务）分配资源，使每个代理商对她的资源分配感到满意，并确保高边缘资源利用率。所提出的模型还包含实际方面，例如，服务请求可以在不同的ENs上提供服务，并且服务需求可以灵活定义，而不是像拍卖模型中那样固定的捆绑。

- 最近的文献广泛探讨了边缘计算（EC）的潜在好处和许多技术方面。其中包括：
  - 利用混合边缘/雾计算/云系统来提高新兴应用性能，比如云游戏和医疗保健[11]，[12]。
  - 探索功耗和延迟感知的云端选择策略，用于多个云端环境中的计算卸载[13]。
  - 在考虑延迟约束的情况下，通过制定工作负载分配问题来权衡功耗和服务延迟[14]。
  - 为移动用户设计延迟感知的工作负载卸载方案，以最小化平均响应时间[15]。
  - 探索联合优化云端位置和用户到云端分配，以最小化服务延迟并考虑负载平衡[16]。
  - 提出统一的服务位置和请求分发框架，评估用户访问延迟与服务成本之间的权衡[17]。
  - 运用Stackelberg博弈和匹配理论研究三层边缘网络中的联合优化[18]。
- 另一个研究方向是MEC环境中通信和计算资源的联合分配[19]，[20]，[21]。其中包括：
  - 允许移动设备将计算任务卸载到资源丰富的服务器，以降低能耗和任务执行延迟[19]。
  - 考虑多用户同时卸载任务可能导致的问题，如干扰增加、数据速率降低和任务执行时间增加[20]。
  - 建议在集成框架中联合考虑卸载决策、无线资源和计算资源的分配和调度[21]。
- 此外，还有丰富的云资源分配和定价文献，包括：
  - 云提供商的利润最大化框架[34]，[35]，[36]。
  - 在云联盟中高效共享资源和利润的研究[37]，[38]，[39]。
  - 辅助用户选择多云市场中合适供应商的资源采购机制[40]。
  - 在[41]中，云提供商与多个服务之间的互动被建模为广义Nash博弈。
  - 该模型在[42]中扩展到多云多服务环境。
  - 在[43]中，将单云多服务资源提供和定价问题，包括平面、按需和临时虚拟机实例，作为Stackelberg博弈，以最大化提供商的收入同时最小化服务成本。

1. **拍卖理论在云资源分配研究中的广泛应用[44]，[45]，[46]。**
2. **典型系统组成：一个或多个云服务提供商和多个用户。**
3. **用户竞标流程：**
   - 用户提交竞标，包括对虚拟机（VM）类型和数量的期望资源捆绑以及愿意支付的价格给拍卖者。
4. **拍卖者角色：**
   - 拍卖者解决获胜者确定问题，以确定被接受的竞标。
5. **金额计算：**
   - 拍卖者计算每个获胜者需要支付的金额，以确保拍卖的真实性。
6. **拍卖目标：**
   - 拍卖中常见的目标是最大化社会福利或最大化云提供商的利润。
7. **资源获取：**
   - 只有竞拍成功的用户才能获得云资源。
8. **弹性用户需求：**
   - 大多数现有的拍卖模型不考虑弹性用户需求。
9. **用户行为假设：**
   - 先前的研究通常假设云用户是单一思维的，只对特定的资源捆绑感兴趣，对其他捆绑没有价值。

---

[[Resource Management with Deep Reinforcement Learning]]

- 强化学习（RL）已被用于各种学习任务，包括机器人学[25] [24]、工业制造[26]和电脑游戏[34]。
- 与我们的工作相关的是张和戴特里奇在NASA航天飞机任务前后分配人力资源的论文[42]。
  - 我们的作业调度设置与NASA任务相似，但关键区别在于我们的问题是在线问题，而NASA的任务是离线的（所有输入都事先知道）。
- 早期的工作使用RL进行交换机中的分散式数据包路由[8]，但问题规模较小，不需要神经网络技术。
- 最近，学习已应用于设计拥塞控制协议，包括大量的离线[37]或在线[13]实验。强化学习可为学习这些拥塞控制算法提供有用的框架。
- 在数据并行框架的普及推动下，集群调度得到广泛研究。
  - 几个调度器设计专注于解决特定问题或目标，如公平性[41] [16]、局部性[22]、包装[17]和滞后任务[5]。
- 尚未发现将强化学习应用于数据并行集群调度的相关工作。

---

[[Reverse Auction-based Computation Offloading and Resource Allocation in Mobile Cloud-Edge Computing]]


- MEC的计算任务处理通常基于分布式协作，核心是通过在边缘环境中有效分配计算、存储和通信资源实现动态任务调度[27]。
 ^6e9ecb


- 一些研究关注了MEC的激励机制设计[36]-[41]。 ^1c85cb
  - 在[36]中，作者提出了一种在线激励驱动的任务分配方案，最大化系统效用。
  - [37]中，作者利用最优价格的方案为MDs提供计算卸载服务，平衡了个体和整体系统的利益。
  - Liu等人在[38]中提出了CSO和ESO之间的两阶段Stackelberg博弈，使CSO能够根据ESO的状态信息分配计算任务。
  - 在[39]中，作者提出了一个分布式和部分分布式的激励机制，显著降低了节点的计算成本。
  - [40]中，作者提出了一种基于利润最大化的激励机制，最大化边缘服务器的利润，同时确保MDs的QoE。
  - 在[41]中，作者提出了一个在线激励机制，允许基站在不知道未来信息的情况下进行任务调度、资源分配和定价决策，以最大化系统效用。
- 一些基于拍卖的激励机制研究应用于MEC中的资源分配[42]-[48]。 ^8eae3c
  - Ma等人在[42]中提出了一个真实的组合双拍卖机制，平衡了预算，激励边缘服务器为附近的移动用户提供服务。
  - 在[43]中，作者引入了两种基于双拍卖的动态定价策略，用于确定MDs和边缘服务器之间的匹配对，以及满足经济属性的资源分配方案。
  - [44]中，提出了一个基于拍卖的机制，最大化总体社会福利，激励边缘节点将虚拟机资源分配给MDs进行计算卸载。
  - Sun等人在[45]中建模了边缘服务器和MDs之间的交互，以最大化系统的效率。
  - Le等人在[46]中提出了一个两阶段激励机制，结合了拍卖游戏，最小化车辆网络背景下的总网络延迟。
  - 在[47]中，研究了MEC中资源分配的拍卖问题，提出了基于双拍卖的多任务资源分配算法，改善系统效用。
  - He等人在[48]中提出了一种基于拍卖的在线激励机制，可以在不知道未来信息的情况下优化系统的长期效用。


---

[[Three Dynamic Pricing Schemes for Resource Allocation of Edge Computing for IoT Environment]]

- 最近进行了多项研究，以优化边缘计算服务器资源的定价方案[16]，[37]–[41]。
- 在[37]中，考虑了边缘计算系统中的均匀定价，用户在考虑能效和成本时，Kim等人[37]制定了用户和边缘卖家之间的Stackelberg博弈模型，并确定了最优单价以最大化卖家的收入。
- Jošilo和Dán [38]提出了一种算法，通过考虑固定价格政策下的能效、成本和延迟，找到用户之间的纳什均衡（NE）。
- 在[39]中，提出了考虑延迟和卖家成本的用户之间的Stackelberg博弈模型。他们还提出了两种算法，可以在使用统一和差异定价政策的情况下最大化卖家的收入。
- Yi等人[40]提出了一种差异定价方案，最大化了用户和卖家的整体满意度。
- 在[41]中，考虑了设备用户和边缘服务器之间的联盟的系统模型中，采用了差异定价。
- Chen等人[16]通过考虑联合通信和计算资源分配，引入了一种基于出价的拍卖式定价机制。

一些研究也考虑了边缘计算在特定情境下的定价方案：

- Xiong等人[42]–[44]制定了一个情境，其中移动用户使用边缘计算运行区块链应用来挖掘区块，并考虑了统一定价和差异定价。
- 在[45]中，讨论了边缘计算的双重拍卖系统。作者提出了两种不同的系统，基于盈亏平衡的双重拍卖和基于动态定价的双重拍卖，并分析了它们的效率。
- 在[46]中，将资源管理制定为双重拍卖博弈并进行了分析。
- [47]考虑了一个包含移动用户、边缘云和远程云的系统。
- [48]提出了在由具有容量限制的边缘节点和具有预算限制的服务组成的系统中引入一般均衡的概念。
- De Pellegrini等人[49]专注于一种情境，其中卖家向用户提供基于边缘云的缓存服务。由于提供者以固定价格购买缓存空间，因此考虑了卖家和用户之间的Stackelberg均衡（SE）。

所有上述研究都集中在设计具有特定场景或指标（如能效、延迟、公平性等）的最优定价机制。然而，以上研究没有提供多个定价政策之间的比较分析。此外，大多数关于边缘计算的研究仅考虑了单一的定价方案。因此，很难衡量各种定价方案的优劣势。



