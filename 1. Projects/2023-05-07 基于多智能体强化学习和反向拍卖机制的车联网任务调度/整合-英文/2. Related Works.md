---
date created: 2023-12-06 16:58
date updated: 2023-12-07 13:15
---

## 2. Related Works

==这部分还未整理, 需要调整==

- 相关工作
  - 综述车联网中的任务调度方法
  - 包括传统的优化方法和基于机器学习的方法
  - 比较它们的优缺点，指出现有方法的不足之处。

- 综述车联网中的任务调度方法
	- 传统算法, 三篇
	- 强化学习算法
资源的市场化调度, 拍卖机制的方法
基于机器学习和强化学习的方法

当涉及边缘计算时，移动设备可以将大部分任务卸载到边缘服务器进行执行，这有效地解决了它们有限资源的问题，同时减少了核心网络的流量负荷。然而，不恰当的任务卸载不仅会导致边缘服务器之间工作负载不均衡，还会增加任务延迟和能耗。因此，在边缘服务器之间合理调度计算任务对于优化高资源效率的服务质量至关重要。调度研究旨在选择任务应该卸载的时间和地点。已经有大量工作致力于工作负载调度问题。

MEC的计算任务处理通常基于分布式协作，核心是通过在边缘环境中有效分配计算、存储和通信资源实现动态任务调度

当在分布式平台上安排多任务工作流时，普遍认为这是一个 NP 难问题。因此，通过遍历型算法获得最优调度方案非常耗时。幸运的是，启发式和元启发式策略具有多项式复杂度，能够以可接受的最优性损失产生近似或接近最优解。

常用的方法主要包括两种不同的方法：模糊逻辑方法和机器学习方法。模糊逻辑方法需要预先定义各种模糊规则，这需要大量的时间和精力来设计，并且严重受到基于静态全局视角的先验专业知识的限制。

因此，为了减少手动干预，一些研究尝试使用机器学习方法进行工作负载调度。机器学习方法使用一些学习技术来动态调整任务调度策略。

然而，由于用于学习的数据集可能过时或无法推广到新环境中使用，采用在仿真环境中训练的强化学习代理是一个合适的选择。


传统资源调度方法通常基于集中式的优化模型，需要预先知道系统的参数和状态信息，然而在边缘云环境中，这些信息往往是不完全或不准确的，导致资源调度效率低下。为了解决这一问题，近年来出现了一些基于随机优化的分布式资源调度方法，它们可以根据实时的反馈信息动态地调整资源分配策略。目前已有一些针对边缘云计算场景的服务迁移和负载调度方法被提出
Wang等人（2017）提出了一个联合计算卸载、资源分配和内容缓存的优化框架，并设计了一个基于ADMM的分布式算法来提高具有移动边缘计算的无线蜂窝网络的收益[4]。
Rahul等人（2015）通过解耦原始MDP并采用李雅普诺夫优化技术，提出了一种高效、鲁棒、自适应且无需统计知识的边缘云服务迁移和负载调度算法[5]。
Ibrahim等人（2019）提出了一种考虑车辆移动性和任务延迟要求的边缘计算工作负载调度方法，并通过拉格朗日松弛技术求解[6]。
Zhang 等人（2022）考虑了多跳传输导向的车联网云动态工作流调度问题，提出了一种基于人工蜂群算法和贪心策略相结合的动态工作流调度方法[7]。
这些方法在一些特定场景下表现出了较好的效果，但也面临着一些挑战，例如如何保证算法的收敛性和稳定性，如何平衡探索和利用之间的权衡，如何减少通信开销和计算复杂度等。

Sonmez等人[^1]采用了基于模糊逻辑的方法来解决边缘计算系统中的工作负载编排问题。他们的方法考虑了被卸载任务的属性以及当前计算和网络资源的状态，并使用模糊规则来定义工作负载编排行动，涉及网络、计算和任务需求，以决定整个边缘计算系统内的任务执行位置。

[^1]:Sonmez C, Ozgovde A, Ersoy C (2019) Fuzzy Workload Orchestration for Edge Computing. IEEE Trans Netw Serv Manag 16(2):769–782. https://doi. org/10.1109/TNSM.2019.2901346



---


- 一些研究关注了MEC的激励机制设计。
  - 在[^3]中，作者提出了一种在线激励驱动的任务分配方案，最大化系统效用。
  - [^4]中，作者利用最优价格的方案为MDs提供计算卸载服务，平衡了个体和整体系统的利益。
  - 在[^5]中，作者提出了一个分布式和部分分布式的激励机制，显著降低了节点的计算成本。
[^3]: W. Hou, H. Wen, N. Zhang, J. Wu, W. Lei, and R. Zhao, "IncentiveDriven Task Allocation for Collaborative Edge Computing in Industrial Internet of Things," IEEE Internet Things J., vol. PP, no. 99, pp. 1-1, 2021.
[^4]: L. Li, T. Q. S. Quek, J. Ren, H. H. Yang, Z. Chen, and Y. Zhang, "An Incentive-Aware Job Offloading Control Framework for Multi-Access Edge Computing," IEEE Trans. Mobile Comput., vol. 20, no. 1, pp. 63-75, 1 Jan. 2021. 
[^5]: R. Chattopadhyay, and C. Tham, "Fully and Partially Distributed Incentive Mechanism for a Mobile Edge Computing Network," IEEE Trans. Mobile Comput., vol. PP, no. 99, pp. 1-1, 2020.


- 一些基于拍卖的激励机制研究应用于MEC中的资源分配。
  - Ma等人在[^6]中提出了一个真实的组合双拍卖机制，平衡了预算，激励边缘服务器为附近的移动用户提供服务。
  - 在[^7]中，作者引入了两种基于双拍卖的动态定价策略，用于确定MDs和边缘服务器之间的匹配对，以及满足经济属性的资源分配方案。
  - [^8]中，提出了一个基于拍卖的机制，最大化总体社会福利，激励边缘节点将虚拟机资源分配给MDs进行计算卸载。

[^6]: L. Ma, X. Wang, X. Wang, L. Wang, Y. Shi and M. Huang, "TCDA: Truthful Combinatorial Double Auctions for Mobile Edge Computing in Industrial Internet of Things," IEEE Trans. Mobile Comput., vol. PP, no. 99, pp. 1-1, 2021. 
[^7]: W. Sun, J. Liu, Y. Yue, and H. Zhang, "Double Auction-Based Resource Allocation for Mobile Edge Computing in Industrial Internet of Things, " IEEE Trans. Ind. Inf., vol. 14, no. 10, pp. 4692 - 4701, 2018. 
[^8]: G. Gao, M. Xiao, J. Wu, H. Huang, S. Wang, and G. Chen, "Auction based VM allocation for deadline-sensitive tasks in distributed edge cloud," IEEE Trans. Services Comput., vol. PP, no. 99, pp. 1-1, 2020.

---

最近，博弈论和强化学习（RL）模型和方法被广泛应用于多约束过程调度问题 [^9][^10]。人们相信博弈论中的均衡概念和多智能体训练方法在处理多约束和多目标优化问题方面具有很高的潜力。
[^9]: .Kang Yang, Rongyu Cao, Yueyuan Zhou, Jiawei Zhang, En Shao, Guangming Tan, "Deep Reinforcement Agent for Failure-aware Job scheduling in High-Performance Computing", _2021 IEEE 27th International Conference on Parallel and Distributed Systems (ICPADS)_, pp.442-449, 2021.

[^10]: Seiju Yasuda, Chonho Lee, Susumu Date, "An Adaptive Cloud Bursting Job Scheduler based on Deep Reinforcement Learning", _2021 International Conference on High Performance Big Data and Intelligent Systems (HPBD&IS)_, pp.217-224, 2021.

Nascimento等人[^2]提出了基于强化学习（RL）的调度方法，用于基于云的科学工作流执行。RL是机器学习的一个分支，专注于如何通过在动态环境中学习来实现最优目标。在RL中，作为学习者的代理程序感知环境的当前状态，并选择要采取的行动。

[^2]:Nascimento A, Olimpio V, Silva V, Paes A, de Oliveira D (2019) A Reinforcement Learning Scheduling Strategy for Parallel Cloud-Based Workflows. In: 2019 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW). pp 817–824. https://doi.org/10.1109/ IPDPSW.2019.00134

[^11]提出了一种基于强化学习的车联网中的集群协作调度方法，目标是提高通信效率和可靠性。该方法利用了深度确定性策略梯度算法，将集群协作调度问题建模为一个连续动作空间的强化学习问题，并设计了一种集中式训练和分布式执行的算法，实现了任务的优化分配和通信资源的合理利用。

[^11]: Li X, Zhang Y, Wang Y, Zhang J. Cluster-Enabled Cooperative Scheduling Based on Reinforcement Learning for Vehicular Networks. IEEE Transactions on Vehicular Technology. 2020 Oct 26;70(1):1018-30.

[^12]受到深度强化学习在人工智能问题上的最新进展的启发，我们提出了DeepRM，这是一个创新性的解决方案，解决了系统和网络中资源管理的问题，通过将具有多重资源需求的任务打包的问题转化为一个学习问题。初步结果表明，DeepRM的性能与最先进的启发式方法相媲美，适应不同条件，快速收敛，并通过经验学习制定了明智的策略。用于集群调度中的多资源打包。
  - 主要目标是最小化平均作业减慢。
  - 集群被假设为同构的，由于状态空间将所有集群资源视为一个大的CPU和内存块。
[^12]:Mao, Hongzi, Mohammad Alizadeh, Ishai Menache, and Srikanth Kandula. 2016. “Resource Management with Deep Reinforcement Learning.” In Proceedings of the 15th ACM Workshop on Hot Topics in Networks. doi:10.1145/3005745.3005750.

[^13]该文讨论了在线广告拍卖中的挑战，广告商必须在动态和反应灵敏的环境中为曝光机会进行策略性出价。它引入了一个统一的框架，涵盖了“学习出价”方法，包括基于价值、基于策略和双重稳健的公式，并介绍了AuctionGym，一个开源仿真环境，可用于在线广告拍卖中出价策略的可靠离线验证，实证证据支持了所提方法的有效性。
[^13]:Jeunen, Olivier, et al. Learning to Bid with AuctionGym.

不同类型的异步多智能体强化学习方法确定市场情境下的竞争定价策略, [^14]
[^14]: Erich Kutschinski, Thomas Uthmann, and Daniel Polani. Learning competitive pricing strategies by multi-agent reinforcement learning. Journal of Economic Dynamics and Control, 27(11-12):2207–2218, 2003.
