---
date created: 2023-12-06 16:58
---

## 2. Related Works

---

==这部分还未整理, 需要调整==

- 相关工作
  - 综述车联网中的任务调度方法
  - 包括传统的优化方法和基于机器学习的方法
  - 比较它们的优缺点，指出现有方法的不足之处。

为了解决车联网中的任务调度问题，许多研究者提出了各种各样的方法。这些方法可以分为两类：传统的优化方法和基于机器学习的方法。传统的优化方法通常建立一个数学模型来描述任务调度问题，然后利用一些算法来求解该模型，例如动态规划、分支定界、遗传算法等。这些方法的优点是可以保证找到最优或近似最优的解，但是也有一些缺点，例如模型假设过于简化、算法复杂度过高、对环境变化不敏感等。基于机器学习的方法则利用一些学习技术来自适应地调整任务调度策略，例如强化学习、深度神经网络、博弈论等。这些方法的优点是可以适应复杂和动态的环境，但是也有一些缺点，例如收敛速度慢、需要大量的数据和计算资源、难以保证最优性和稳定性等。
• Wu Y, Wu J, Chen L, Yan J, Luo Y. Efficient task scheduling for servers with dynamic states in vehicular edge computing. Computer Communications. 2020 Jan 15;150:245-53. 这篇文章提出了一种考虑边缘服务器动态状态的车联网任务调度方法，目标是最小化任务的响应时间和服务器的能耗。
• Hu F, Lv L, Zhang T, Shi Y. Vehicular task scheduling strategy with resource matching computing in cloud-edge collaboration. IET Collaborative Intelligent Manufacturing. 2021 Mar 18. 这篇文章提出了一种基于资源匹配度的车联网任务调度策略，目标是最小化任务的完成时间和边缘服务器的计算单元数量。
• Wang X, Wang X, Liang C, Huang C. A Survey on Mobile Edge Networks: Convergence of Computing, Caching and Communications. IEEE Access. 2017;5:6757-74. 这篇文章是一篇综述，介绍了移动边缘网络的概念、架构、应用和挑战，以及相关的计算任务调度方法。

1. <https://www.sciencedirect.com/science/article/pii/S0140366419307273>
2. <https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/cim2.12023>
3. <https://www.sciencedirect.com/science/article/abs/pii/S0140366419307273>

• Wu Y, Wu J, Chen L, Yan J, Luo Y. Efficient task scheduling for servers with dynamic states in vehicular edge computing. Computer Communications. 2020 Jan 15;150:245-53. 这篇文章提出了一种考虑边缘服务器动态状态的车联网任务调度方法，目标是最小化任务的响应时间和服务器的能耗。该方法利用了泊松过程和马尔可夫决策过程，建立了一个双层优化模型，并设计了一种基于贪心算法和动态规划算法的启发式算法，实现了任务的有效调度。
• Hu F, Lv L, Zhang T, Shi Y. Vehicular task scheduling strategy with resource matching computing in cloud-edge collaboration. IET Collaborative Intelligent Manufacturing. 2021 Mar 18. 这篇文章提出了一种基于资源匹配度的车联网任务调度策略，目标是最小化任务的完成时间和边缘服务器的计算单元数量。该策略利用了改进的混合遗传算法，结合了计算、存储和网络带宽资源的匹配度，生成了更优的解决方案，并利用了虚拟机技术，实现了计算单元的动态分配。
• Wang X, Wang X, Liang C, Huang C. A Survey on Mobile Edge Networks: Convergence of Computing, Caching and Communications. IEEE Access. 2017;5:6757-74. 这篇文章是一篇综述，介绍了移动边缘网络的概念、架构、应用和挑战，以及相关的计算任务调度方法。该文章分析了移动边缘网络中涉及到的计算、缓存和通信三个方面的问题和解决方案，并给出了未来的研究方向和展望。

1. <https://www.sciencedirect.com/science/article/pii/S0140366419307273>
2. <https://www.sciencedirect.com/science/article/pii/S1319157822001653>
3. <https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/cim2.12023>

• Li Y, Chen M, Wang J. Joint Computation Offloading and Resource Allocation Optimization in Vehicular Edge Computing. IEEE Transactions on Vehicular Technology. 2019 Nov 18;69(1):1048-60. 这篇文章提出了一种联合计算卸载和资源分配优化的车联网任务调度方法，目标是最小化车辆的总能耗。该方法利用了拉格朗日对偶理论和交替方向乘子法，将原问题分解为两个子问题，并设计了一种迭代算法，实现了任务的有效卸载和资源的高效分配。
• Zhang Y, Zhang J, Liu F, Guo S. Computation Offloading and Resource Allocation in Mixed Fog/Cloud Computing Systems with Min-Max Fairness Guarantee. IEEE Transactions on Communications. 2018 Apr 17;66(4):1594-609. 这篇文章提出了一种混合雾/云计算系统中的计算卸载和资源分配方法，目标是最大化系统的最小效用。该方法利用了凸优化理论和次梯度投影法，将原问题转化为一个凸优化问题，并设计了一种分布式算法，实现了任务的公平卸载和资源的均衡分配。
• Liu Y, Chen M, Guo S, Yin J. Task Offloading and Resource Allocation for Multiuser Mobile Edge Computing with Energy Harvesting Devices. IEEE Internet of Things Journal. 2019 Feb 25;6(3):4652-63. 这篇文章提出了一种考虑能量收集设备的多用户移动边缘计算中的任务卸载和资源分配方法，目标是最大化用户的总效用。该方法利用了随机优化理论和随机近似算法，将原问题转化为一个随机优化问题，并设计了一种在线算法，实现了任务的自适应卸载和资源的动态分配。

1. <https://www.sciencedirect.com/science/article/pii/S0140366419307273>
2. <https://www.sciencedirect.com/science/article/abs/pii/S0140366419307273>
3. <https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/cim2.12023>

• Shyalika C, Silva T, Karunananda A. Reinforcement Learning in Dynamic Task Scheduling: A Review. SN Computer Science. 2020 Sep;1(5):1-9. 这篇文章是一篇综述，介绍了动态任务调度中使用强化学习的方法和应用。该文章分析了强化学习在不同领域和场景中的优势和挑战，并给出了未来的研究方向和展望。
• Jamil B, Ijaz H, Shojafar M, Munir K. IRATS: A DRL-based intelligent priority and deadline-aware online resource allocation and task scheduling algorithm in a vehicular fog network. Ad Hoc Networks. 2023 Mar 15;141:103090. 这篇文章提出了一种基于深度强化学习的智能优先级和截止时间感知的在线资源分配和任务调度算法，目标是最小化任务的等待时间和延迟。该算法利用了近端策略优化算法，将资源分配问题建模为一个马尔可夫决策过程，并设计了一种分布式算法，实现了任务的有效卸载和资源的高效分配。
• Li X, Zhang Y, Wang Y, Zhang J. Cluster-Enabled Cooperative Scheduling Based on Reinforcement Learning for Vehicular Networks. IEEE Transactions on Vehicular Technology. 2020 Oct 26;70(1):1018-30. 这篇文章提出了一种基于强化学习的车联网中的集群协作调度方法，目标是提高通信效率和可靠性。该方法利用了深度确定性策略梯度算法，将集群协作调度问题建模为一个连续动作空间的强化学习问题，并设计了一种集中式训练和分布式执行的算法，实现了任务的优化分配和通信资源的合理利用。

1. <https://link.springer.com/article/10.1007/s42979-020-00326-5>
2. <https://www.sciencedirect.com/science/article/pii/S1570870523000100>
3. <https://ieeexplore.ieee.org/document/9217939>

本文主要研究的问题是车联网（IoV）中的工作流调度问题，即根据车辆的计算任务需求和移动特性，在移动边缘计算（MEC）环境中有效地分配和调度车辆的计算任务，以达到优化系统性能和用户体验的目的。车联网工作流调度涉及到多个方面的问题，如任务卸载策略、资源分配算法、激励机制设计、时延分析等。近年来，随着车联网技术和移动边缘计算技术的发展，车联网工作流调度问题受到了国内外学者的广泛关注，并取得了一些研究成果。为了对本课题有一个全面的了解，我们首先回顾了国内外相关领域的研究现状，主要包括以下几个方面：传统资源调度和随机优化的分布式资源调度方法,强化学习的车联网资源调度,强化学习移动边缘计算资源调度,强化学习服务器资源调度,车联网中的拍卖机制,强化学习定价策略。

## 二、 国内外相关研究现状综述

本文主要研究的问题是车联网（IoV）中的工作流调度问题，即根据车辆的计算任务需求和移动特性，在移动边缘计算（MEC）环境中有效地分配和调度车辆的计算任务，以达到优化系统性能和用户体验的目的。车联网工作流调度涉及到多个方面的问题，如任务卸载策略、资源分配算法、激励机制设计、时延分析等。近年来，随着车联网技术和移动边缘计算技术的发展，车联网工作流调度问题受到了国内外学者的广泛关注，并取得了一些研究成果。为了对本课题有一个全面的了解，我们首先回顾了国内外相关领域的研究现状，主要包括以下几个方面：传统资源调度和随机优化的分布式资源调度方法, 强化学习的车联网资源调度, 强化学习移动边缘计算资源调度, 强化学习服务器资源调度, 车联网中的拍卖机制, 强化学习定价策略。

### (1). 传统资源调度和随机优化的分布式资源调度方法

资源调度是指在有限的资源条件下，根据任务的需求和优先级，合理地分配和利用资源，以达到最优或次优的目标。资源调度问题在各种计算系统中都广泛存在，例如云计算、边缘计算、车联网等。传统的资源调度方法通常基于确定性或随机性的模型，通过数学规划、启发式算法、元启发式算法等技术来求解。传统资源调度方法通常基于集中式的优化模型，需要预先知道系统的参数和状态信息，然而在边缘云环境中，这些信息往往是不完全或不准确的，导致资源调度效率低下。为了解决这一问题，近年来出现了一些基于随机优化的分布式资源调度方法，它们可以根据实时的反馈信息动态地调整资源分配策略。目前已有一些针对边缘云计算场景的服务迁移和负载调度方法被提出，例如：Wang 等人（2017）提出了一个联合计算卸载、资源分配和内容缓存的优化框架，并设计了一个基于 ADMM 的分布式算法来提高具有移动边缘计算的无线蜂窝网络的收益[4]。Rahul 等人（2015）通过解耦原始 MDP 并采用李雅普诺夫优化技术，提出了一种高效、鲁棒、自适应且无需统计知识的边缘云服务迁移和负载调度算法[5]。Ibrahim 等人（2019）提出了一种考虑车辆移动性和任务延迟要求的边缘计算工作负载调度方法，并通过拉格朗日松弛技术求解[6]。Zhang 等人（2022）考虑了多跳传输导向的车联网云动态工作流调度问题，提出了一种基于人工蜂群算法和贪心策略相结合的动态工作流调度方法[7]。这些方法在一些特定场景下表现出了较好的效果，但也面临着一些挑战，例如如何保证算法的收敛性和稳定性，如何平衡探索和利用之间的权衡，如何减少通信开销和计算复杂度等。

在分布式资源调度问题中，基于随机优化的方法是一种常见的解决方案。这类方法利用随机性来处理不确定性，通过迭代更新可行解来逼近最优解。然而，基于随机优化的方法也存在一些局限性，例如收敛速度慢、计算开销大、对参数敏感等。因此，近年来出现了一种新的方法，即深度强化学习。深度强化学习是一种结合了深度神经网络和强化学习的技术，可以训练神经网络来快速准确地解决多目标优化问题[8] 。相比于基于随机优化的方法，深度强化学习具有以下优势：一是能够自动学习复杂的策略函数，无需人为设计或调整参数；二是能够利用大量数据和高效算法来提高学习效率和质量；三是能够适应动态变化的环境和目标，并实现在线决策和反馈。

### (2).强化学习的车联网资源调度

强化学习是一种基于试错学习和奖励反馈的机器学习方法，它可以让智能体在与环境交互的过程中自主地学习最优或近似最优的策略。强化学习具有以下几个特点：无需事先知道系统模型和参数；能够处理部分可观测和非马尔可夫性的环境；能够适应动态变化和不确定性的环境；能够实现长期目标和多目标之间的平衡。由于这些特点，强化学习被认为是一种适合于解决车联网资源调度问题的方法。强化学习的车联网资源调度是一种利用智能体的自主学习能力，根据车辆的状态、环境的变化和奖励信号，动态优化车辆之间和基础设施之间的通信资源分配的方法。强化学习的车联网资源调度可以提高车联网的性能，如降低时延、增加吞吐量、节省能耗等。强化学习的车联网资源调度面临着多个挑战，如高维状态空间、部分可观测性、非平稳性、多目标优化等。为了解决这些挑战，一些研究者提出了基于深度神经网络、多智能体协作、知识驱动等技术的强化学习算法，并在不同的场景中进行了验证和应用，如频谱共享、信道选择、功率控制、数据传输等。例如,Liu 等人（2019）设计了两种强化学习方法来优化计算卸载和资源分配问题[9]。Liu等人（2020）提出了一种基于优先级和价值函数的任务调度算法，考虑了任务之间的依赖性[10]。Song等人（2023）提出了一种基于潜在博弈理论和联邦深度强化学习的多异构服务器边缘计算卸载方法，能够有效地满足任务车辆用户的定制化服务需求[11]。Pang等人（2020）提出了一种考虑位置隐私保护的车联网计算资源协同调度策略，利用卡尔曼滤波预测车辆间距离，采用双重深度 Q 网络优化总成本[1]。然而，强化学习的车联网资源调度问题也存在一些挑战和难点，例如如何设计合适的状态空间、动作空间和奖励函数，如何处理高维度和连续性的状态和动作，如何实现多智能体之间的协调和博弈，如何提高学习效率和泛化能力等。

### (3). 强化学习移动边缘计算资源调度

移动边缘计算是一种将云计算的功能和服务延伸到网络边缘的技术，它可以为移动用户提供低延迟、高带宽、高可靠性的计算资源和服务。移动边缘计算中的资源调度问题是指如何在有限的计算资源和网络资源下，根据用户的任务需求和服务质量要求，合理地分配和调度任务在本地执行或卸载到边缘服务器或云服务器上执行。这是一个具有多约束、多目标、动态性和不确定性的优化问题，传统的优化方法难以有效地解决。因此，一些学者尝试使用强化学习来解决移动边缘计算中的资源调度问题，并取得了一定的成果。强化学习是一种基于智能体与环境交互学习最优行为策略的机器学习方法，可以适应不确定性和动态性，并且不需要先验知识或模型。近年来出现了许多基于强化学习的 MEC 资源调度方法，接下来对其中部分进行介绍。

例如，Zheng 等人（2022）使用深度 Q 网络（DQN）或双重深度 Q 网络（DDQN）来训练移动边缘服务器上的智能体，使其能够根据自身状态和环境状态选择最优或次优的任务卸载策略[12]；Chen J 等人（2021）使用深度确定性策略梯度算法（DDPG）来训练连续动作空间下的智能体，使其能够输出最优或次优的任务卸载比例或执行速度[13]；Chen M 等利用异步优势行动者-评论者算法 (A 3 C），提出了一个基于软件定义网络和信息中心网络的动态资源分配方案，以提高车辆网络的服务质量[14]; Peng 等使用多智能体强化学习（MARL）来训练多个边缘服务器之间的智能体，使其能够协作地进行车辆关联和资源分配决策[15]；Peng 等人（2022）提出了一种基于深度强化学习和有向无环图的依赖任务卸载策略，能够在多用户多服务器边缘计算环境中，灵活地选择合适的卸载目标服务器，并有效地减少服务延迟和终端能耗[16]。然而，强化学习在移动边缘计算中的资源调度问题仍然面临着一些问题和挑战，例如如何处理状态空间和动作空间的爆炸性增长，如何平衡探索和利用之间的权衡，如何解决多智能体之间的非平稳性和通信开销等。

### (4). 强化学习服务器资源调度

服务器资源调度是指在数据中心或云平台中，根据用户或应用程序的需求和服务质量要求，合理地分配和调度服务器上的计算资源、存储资源、网络资源等。这是一个涉及到数据中心或云平台的性能、效率、成本、节能等方面的重要问题。传统的服务器资源调度方法通常基于静态规划或启发式算法等技术来求解，但这些方法往往依赖于精确的系统模型和参数，难以适应复杂多变的环境。因此，一些学者利用强化学习来解决服务器资源调度问题，并取得了一些进展。强化学习服务器资源调度（Reinforcement Learning Server Resource Scheduling）是一种利用强化学习方法来优化服务器资源分配和任务执行的技术。它可以应对服务器状态的动态变化、多用户多任务的复杂需求、网络切片和边缘计算等新型网络架构的挑战，以及可再生能源和机器学习服务等新兴应用场景的特点。强化学习服务器资源调度的目标是在保证服务质量和满足约束条件的前提下，最大化系统效率和性能，最小化系统成本和延迟。强化学习服务器资源调度涉及多个层次和方面，包括资源配置、任务卸载、任务调度、并行度配置、协商策略等。近年来，有许多研究者提出了基于深度强化学习（Deep Reinforcement Learning）的服务器资源调度方法，并在不同的场景和数据集上进行了实验验证。

例如，有些学者使用强化学习来优化虚拟机（VM）或容器（Container）在物理机上的放置策略，以提高资源利用率和节约能耗[17–19]，其中Cheng 等人（2018）提出了一个基于深度强化学习的资源配置和任务调度系统，能够有效地降低云服务提供商的数据中心能耗和电费成本，并具有高效、可扩展、自适应和快速收敛等特点[17]，Zhao 等（2021）展示了如何利用深度强化学习在混合云环境中实现自适应的多目标任务调度，以最大化可再生能源的利用率和满足截止日期约束[19]；有些学者使用强化学习来优化服务器上的任务调度策略，以提高任务的执行效率和服务质量[20–25]，例如，Wu 等人（2020）针对服务器状态动态变化导致的资源分配不均衡问题，提出了一种基于深度强化学习和马尔可夫决策过程相结合的任务调度方法[23]，Rjoub等人（2021）提出了四种基于深度学习和强化学习的调度方法，并用真实数据集进行了实验比较。实验结果表明，深度强化学习结合长短期记忆网络（DRL-LSTM）的方法在减少任务执行成本和延迟方面优于其他三种方法[25]。然而，强化学习在服务器资源调度问题中也存在一些问题和挑战，例如如何处理大规模的状态空间和动作空间，如何处理部分可观测和非马尔可夫性的环境，如何处理多目标和多约束的优化问题，如何提高算法的收敛速度和稳定性等。

### (5).车联网中的拍卖机制

拍卖机制是一种经济学中常用的激励机制，它可以实现资源或服务的有效分配和价格的公平确定。在车联网中，拍卖机制可以应用于多种场景，例如车辆之间或车辆与基础设施之间的资源或服务交易，例如频谱、缓存、计算、数据等。近年来，许多学者研究了车联网中的拍卖机制，并设计了各种拍卖模型和算法。例如，有些学者使用密封双向拍卖（SDBA）来实现车辆之间的频谱共享和交易；有些学者使用组合双向拍卖（CDBA）来实现车辆之间的缓存共享和交易；有些学者使用反向拍卖（RAM）来实现车辆与边缘服务器之间的计算任务卸载和交易；有些学者使用多属性拍卖（MAA）来实现车辆与数据提供者之间的数据获取和交易。然而，车联网中的拍卖机制也面临着一些问题和挑战，例如如何保证拍卖机制的真实性、有效性、个体合理性、社会福利最大化等性质，如何处理参与者的自私或恶意行为，如何减少拍卖过程中的计算复杂度和通信开销等。

车联网是指通过无线通信技术，实现车辆、道路、交通设施等的信息交互和资源共享的网络。车联网可以提高道路安全，优化交通管理，提升驾驶体验，促进智能出行等。在车联网中，拍卖机制是一种有效的资源分配和价格发现的方法，可以解决供需不平衡、竞争不公平、信息不对称等问题。拍卖机制可以应用于车联网中的多种场景，例如路侧单元（RSU）的接入控制、频谱资源的分配、数据服务的交易等。接下来将对部分研究进行介绍。Vishalatchi 等人（2017）介绍了云计算中的虚拟机调度问题，以及一种基于拍卖机制的禁忌搜索算法来解决它。这可以作为一个基础和背景，让读者了解云计算中的资源分配问题和拍卖机制的作用。Ding 等人（2016）从云计算转向网格计算，介绍了网格计算无线网络中的动态资源分配问题，以及一种具有预测能力和多属性特征的新颖反向在线拍卖算法来解决它。这可以展示拍卖机制在另一种计算网络中的适用性和创新性。Liwang 等人（2019）从网格计算转向车联网，介绍了车联网中存在的计算卸载、资源共享和用户自私等问题，以及一种基于 VCG 原理的反向拍卖机制来优化计算卸载决策并满足经济属性。这可以展示拍卖机制在更具挑战性和前沿性的领域中的应用和效果。Zhang 等人（2022）进一步考虑了公共区块链网络对车联网资源分配问题的影响和支持，提出了一种利用反向拍卖模型和 VCG 机制激励车辆记录驾驶数据，并使用边缘计算节点支持区块链技术的真实拍卖机制。这可以展示拍卖机制在结合其他先进技术时能够产生更高效、安全、可信等优点。

### (6).强化学习定价策略

定价策略是指在市场交易中，根据供需关系、成本收益分析、竞争对手行为等因素，确定商品或服务的价格水平和变化规律。定价策略是一种重要的市场营销手段，它可以影响消费者的购买意愿和行为，从而影响供应商的收入和利润。在车联网中，定价策略可以应用于多种场景，例如车辆之间或车辆与基础设施之间的资源或服务交易，例如频谱、缓存、计算、数据等。近年来，许多学者研究了车联网中的定价策略，并设计了各种定价模型和算法。例如，有些学者使用基于需求函数的定价策略来实现车辆之间的频谱共享和交易；有些学者使用基于成本函数的定价策略来实现车辆之间的缓存共享和交易；有些学者使用基于效用函数的定价策略来实现车辆与边缘服务器之间的计算任务卸载和交易；有些学者使用基于博弈论的定价策略来实现车辆与数据提供者之间的数据获取和交易。然而，车联网中的定价策略也存在一些问题和挑战，例如如何根据市场环境和用户行为动态地调整价格，如何平衡供应商和消费者之间的利益，如何处理多方参与者之间的竞争和合作等。

强化学习定价策略是一种利用强化学习技术来优化定价决策的方法，它可以在不依赖用户响应函数的情况下，通过不断地探索和学习找到最优的定价策略，从而实现需求响应、能耗调度、竞争优势、成本效率等目标。强化学习定价策略在金融量化、电子市场、云计算等领域有着广泛的应用和研究。接下来将对使用强化学习进行定价的部分研究进行介绍。

Kutschinski 等人（2003）研究了电子市场中多智能体强化学习定价策略，并提出了一个分布式代理平台来模拟和评估不同竞争环境下的定价效果。随后，Kim 等人（2016）将强化学习技术应用到一个不确定微网环境中，解决了动态定价和能耗调度问题。Ghasemkhani 等人（2018）提出了一种不依赖用户响应函数的定价算法，通过强化学习找到最优定价策略，实现需求响应目标。Krasheninnikova 等人（2019）将强化学习算法扩展到保险领域，使用马尔可夫决策过程来解决保险续费价格调整的多目标优化问题。Islam 等人（2022）提出了一种基于深度强化学习的 Spark 作业调度算法，考虑了多个 SLA 目标，利用了云 VM 定价模型，动态调整了资源配置，用于解决云计算中的虚拟机（VM）调度问题。

综上所述，国内外相关领域的研究现状表明，车联网中的资源调度问题是一个具有重要意义和挑战性的课题，它涉及到多种技术和方法，例如传统优化、随机优化、强化学习、拍卖机制、定价策略等。这些技术和方法在一定程度上可以解决资源调度问题，但也存在一些不足或缺陷，需要进一步的研究和改进。因此，在本文中，我们将结合反向拍卖机制和多智能体深度强化学习，提出一种新颖的车联网工作流调度方法（RAM-DRL），并对其进行理论分析和仿真验证。下面将介绍本文研究的主要内容。

[[Learning to Bid with AuctionGym]]
[[Resource Management with Deep Reinforcement Learning]]

---

[[A Truthful Dynamic Workflow Scheduling Mechanism for Commercial Multicloud Environments]]

### 2.1 多云计算

- 多云计算领域的研究相对年轻，相关文献较少。在[6]中引入了“天空计算”概念，用于构建分布在多个云上的虚拟站点。[7]探讨了多个云计算环境中基于效用的联合的重要挑战和架构要素。同时在[8]和[9]中讨论了云经纪人的角色，负责将用户请求分配给多个提供商，以降低用户成本。

### 2.2 基于拍卖的调度

- 资源的市场化调度在分布式系统中是一个深入研究的领域。[10]发表了关于网格经济的调查报告。多种经济模型，如商品市场和拍卖，在分布式系统中得到了提出。例如，在[11]中研究了若干自私任务竞争分布式资源的拍卖机制。他们在理论上研究了均衡解在执行时间方面的质量。[12]提出了一个网格的分层非合作博弈模型，其中用户是自私的代理。

- 另外，[13]提出了基于众所周知的Vickrey-Clarke-Groves（VCG）机制的静态分布式系统的负载平衡机制。[14]提出了网格系统负载平衡问题的动态解决方案，采用了博弈理论方法，该机制将负载分散到每个计算节点上。他们将负载平衡建模为一个约束最小化问题，并提出了一个可以最小化任务的平均完成时间的算法。[3]和[4]中讨论的问题，机制只关心结果。此外，付款仅用于强制代理告诉事实。

- [15]的作者介绍了网格资源分配的动态市场模型。所提出的竞标算法基于“近视均衡策略”。他们分析了用户在重复的基于拍卖的机制中的理性策略，其中用户通过更新其竞标来寻找所需资源。[16]分析了反社会代理对相关机器上任务调度机制中其他代理造成损失的影响，而不是最大化自己的利润。该工作假设反社会代理知道赢家的竞标价值，这与我们工作中存在私人信息的假设相违背。[17]提出了在单一维度领域中的$(1+\epsilon)$-近似时间的真实机制（即解决方案保证在最优解的$(1+\epsilon)$-倍内）。

- 此外，[18]的作者为网格系统中任务调度提出了一个连续双重拍卖机制。[19]针对网格中自私组织的完成时间调度问题类似于囚徒困境博弈。他们得出结论，对社区进行严格控制是实现可接受性能所必需的。[20]中，我们提出了调度程序和资源管理器之间的谈判协议的新实例，使用基于市场的连续双重拍卖模型。

- 最后，在[21]中提出了云计算集群中批处理作业的基于市场的资源分配模型。此问题的社会成本是分配给用户的价值之和。

### 2.1 多云计算

- 多云计算领域的研究相对年轻，相关文献较少。在[6]中引入了“天空计算”概念，用于构建分布在多个云上的虚拟站点。[7]探讨了多个云计算环境中基于效用的联合的重要挑战和架构要素。同时在[8]和[9]中讨论了云经纪人的角色，负责将用户请求分配给多个提供商，以降低用户成本。

### 2.2 基于拍卖的调度

#### 拍卖机制与资源调度

- [10]发表了关于网格经济的调查报告，介绍了多种经济模型，如商品市场和拍卖，在分布式系统中得到了提出。
- [11]中研究了自私任务竞争分布式资源的拍卖机制，并在理论上研究了均衡解在执行时间方面的质量。
- [12]提出了一个网格的分层非合作博弈模型，其中用户是自私的代理。

#### 负载平衡和效用最大化

- [13]提出了基于Vickrey-Clarke-Groves（VCG）机制的静态分布式系统的负载平衡机制。
- [14]提出了网格系统负载平衡问题的动态解决方案，采用了博弈理论方法，并提出了最小化任务的平均完成时间的算法。
- [3]和[4]中讨论的问题，机制只关心结果，付款仅用于强制代理告诉事实。

#### 基于市场的资源分配和竞标策略

- [15]介绍了网格资源分配的动态市场模型和竞标算法基于“近视均衡策略”。
- [16]分析了反社会代理对任务调度机制中其他代理造成损失的影响。
- [17]提出了单一维度领域中的$(1+\epsilon)$-近似时间的真实机制。

#### 任务调度与博弈论

- [18]提出了网格系统中任务调度的连续双重拍卖机制。
- [19]针对网格中自私组织的完成时间调度问题，类似囚徒困境博弈。
- [20]提出了调度程序和资源管理器之间的基于市场的谈判协议的新实例。

### 2.3 云计算集群中的批处理作业

- [21]提出了基于市场的资源分配模型，社会成本是分配给用户的价值之和。

---

[[A Truthful Reverse-Auction Mechanism for Computation Offloading in Cloud-Enabled Vehicular Network]]

## 2. 相关工作

随着计算密集型应用程序的日益普及，虽然为人们带来了极大的便利，但也给车载网络和车载设备带来了巨大的负担 [4]，[6]。这些应用程序通常需要超出智能车辆能力范围的计算资源，这对应用程序所有者来说是一个主要问题 [7]。随着MCO和MEC的出现，车辆可以通过将应用程序上传到移动边缘云服务器来增强其计算能力。一些研究关注通过V2I通信进行计算卸载机制。在[10]中提出了一种能源高效的自适应资源调度器，用于利用TCP/IP连接的本地测量状态，以最大化整体通信加计算能效，同时满足应用程序诱导的最小传输速率、最大延迟和延迟抖动等硬质量服务要求。张等人[11]提出了一个分层框架，引入了邻近的备份计算服务器来补偿MEC服务器的计算资源不足。为了减少车辆的计算卸载延迟，研究了车载边缘网络中多车辆计算卸载游戏[12]。然而，他们没有考虑车载设备上的可用资源和车辆之间的机会性联系的优势。虽然系统性能仍有改进空间，但当前的V2I计算卸载可能会极大地加重已经过载的蜂窝网络负担。此外，RSU的信号覆盖限制（例如，在山区和农村地区等偏远地区）仍可能对卸载方案施加限制。

因此，基于绿色无线通信[13]，机会性卸载[7]被提出作为解决上述问题的解决方案，它利用由车辆间联系形成的机会性网络来进行应用程序卸载。据我们所知，考虑服务价格的V2V计算卸载机制是一个涉及少量现有工作的新颖研究方向。然而，类似的机会性卸载问题可以归纳为两类：1）设备对设备（D2D）数据卸载和2）D2D计算卸载。对于前者，低数据传输延迟和内容流行度是主要目标，而频谱是最稀缺的资源。相反，在计算卸载情况下，必须同时考虑传输延迟和应用程序处理持续时间，在这种情况下，计算资源的严重限制成为一个大问题。对于后者，智能手机始终受到能量限制，而通常认为车辆具有强大的电池。此外，智能手机用户的移动性在某些情况下可能会被忽略，因为移动速度较低，而车辆移动速度较快，导致不同的信道条件和有限的接触持续时间。陈等人[4]提出了延迟和能量高效的智能手机用户任务调度方法，但忽略了任务处理持续时间，也没有考虑价格策略。陈等人[14]提出了一个框架，在网络边缘实现移动用户的协作任务执行，主要关注能量效率。

在V2V计算卸载相关的车载网络中，孟等人[15]研究了一个云辅助的车载网络架构，其中包含远程云、本地云和VC的各自特点，并通过解决半马尔科夫决策过程获得了相应的最优方案，旨在最大化系统的预期平均奖励。为了提高网络容量和系统计算能力，于等人[16]将原始云无线接入网络扩展到集成本地云服务，其中有几个云片地理分布，提供了低成本、可扩展、自组织和有效的解决方案，并且还主要考虑了D2D和异构网络的关键技术，以增加云片的资源利用率。

由于云计算是一种按需付费的服务，最好考虑诸如与具有不同能力的服务提供商相关的个性化服务价格、应用程序所有者的成本考虑以及用户偏好等经济因素，在计算卸载市场上限制了车辆之间的接触持续时间在一定程度上也被忽视了。

拍卖在无线通信和认知无线电网络中广泛应用于交通卸载，构成了一种有助于有效价格发现和资源分配的流行交易形式。然而，很少有研究将拍卖机制应用于解决计算卸载市场中的资源分配问题以及激励问题。在[17]中，该模型仅考虑了移动资源的单个买方，因此省略了一般场景中多个买方之间的竞争。金等人[18]和[19]研究了移动云计算中云片的资源共享，并设计了有效的拍卖机制来保证投标者的真实性；然而，他们通过一对一匹配限制

了拍卖机制，忽略了资源丰富设备在实际系统中支持多个资源需求买方的能力。王等人[20]设计了一个分布式拍卖机制，通过D2D通信公平分配任务，并确定资源的交易价格。作者还提出了一个支付评估过程，以防止投标者可能的不诚实行为。在王等人的另一项研究中[20]，在移动网络中考虑了异构和同质任务模型，设计了一个拍卖机制，证明了个体理性、真实性和计算效率的属性[21]。在[22]中，计算卸载问题被建模为一个两阶段拍卖，其中具有共同社会特征的远程宏蜂窝用户设备可以组成一个组，然后通过中继小蜂窝用户设备购买小蜂窝BS的计算资源。然而，通过中继进行卸载的传输可靠性难以保护。拍卖可以在一定程度上有效地应对MCO市场的挑战；因此，在云启用的车载网络框架下，如何利用车辆间的机会性联系和不同信道条件设计一个真实和个体理性的卸载机制是一个迫切需要仔细研究的问题。

- **计算密集型应用与车载网络**
  - 应用的普及带来便利与压力 [4]，[6]
  - MCO和MEC提升车辆计算能力 [7]
  - 现有研究集中于V2I通信的计算卸载机制 [10]

- **机会性卸载解决方案**
  - 基于绿色无线通信的机会性卸载 [13]
  - V2V计算卸载机制与服务价格的研究空白 [7]

- **D2D数据与计算卸载**
  - 不同于D2D数据卸载的D2D计算卸载需求和挑战 [4]
  - 研究关注传输延迟、应用处理时间和计算资源限制 [14]

- **车载网络中的研究探索**
  - 云辅助的车载网络架构研究 [15]
  - 云无线接入网络扩展和本地云服务集成 [16]

- **经济因素与拍卖机制**
  - 考虑云计算服务价格和应用所有者成本 [13]
  - 拍卖机制在资源分配和激励问题上的应用研究 [17], [18], [19], [20], [21], [22]

- **拍卖机制的局限性与挑战**
  - 现有研究限制在单个买方或一对一匹配情况下 [17], [18], [19], [20], [21], [22]
  - 在车载网络中设计真实且个体理性的卸载机制的紧迫性 [22]

---

[[Applications of Auction and Mechanism Design in Edge Computing A Survey]]

移动边缘计算：2014年，欧洲电信标准化协会（ETSI）首次提出了MEC的概念[30]，MEC的系统架构如图1所示。他们建议在无线接入网络（RAN）内部部署足够的计算能力、存储空间和服务环境到边缘网络。MEC的主要思想是将高度复杂和繁重的计算任务分发给相邻的边缘服务器，提供超低延迟的计算服务、更高的传输带宽、更少的能耗[18]，[31]，[32]等。因此，通过将高度复杂和计算密集的任务迅速转移至边缘节点，可以极大地减轻终端用户的工作负载。此外，用户设备（例如工业物联网设备）的电池寿命和存储空间可以显著延长和扩展，因此用户设备可以运行各种延迟敏感或计算密集型的应用，如无人机[33]、智能城市[8]、AR/VR[9]等。值得注意的是，作为MEC的关键特性之一，上下文感知可以促进和改善用户设备的上下文感知服务。由于订阅者数据资源的低集中度和小规模，MEC中边缘服务器受到攻击的概率要小得多。此外，许多MEC服务器配备了身份验证、入侵检测和数据加密功能，可以有效解决安全和隐私问题[34]，[35]。这表明，对于隐私敏感和安全敏感的应用，MEC可以得到很好的支持并从MEC中获益良多。考虑到上述优势，毫无疑问，MEC消除了FC和MCC的缺点。随着物联网的蓬勃发展，越来越多的应用将得到MEC的支持[36]。

- 移动边缘计算：2014年，欧洲电信标准化协会（ETSI）首次提出了MEC的概念[30]，MEC的系统架构如图1所示。
- 他们建议在无线接入网络（RAN）内部部署足够的计算能力、存储空间和服务环境到边缘网络。
- MEC的主要思想是将高度复杂和繁重的计算任务分发给相邻的边缘服务器，提供超低延迟的计算服务、更高的传输带宽、更少的能耗[18]，[31]，[32]等。
- 通过将高度复杂和计算密集的任务迅速转移至边缘节点，可以极大地减轻终端用户的工作负载。
- 用户设备（例如工业物联网设备）的电池寿命和存储空间可以显著延长和扩展，因此用户设备可以运行各种延迟敏感或计算密集型的应用，如无人机[33]、智能城市[8]、AR/VR[9]等。
- 上下文感知作为MEC的关键特性之一，可以促进和改善用户设备的上下文感知服务。
- 由于订阅者数据资源的低集中度和小规模，MEC中边缘服务器受到攻击的概率要小得多。
- 许多MEC服务器配备了身份验证、入侵检测和数据加密功能，可以有效解决安全和隐私问题[34]，[35]。
- 对于隐私敏感和安全敏感的应用，MEC可以得到很好的支持并从MEC中获益良多。
- 考虑到上述优势，毫无疑问，MEC消除了FC和MCC的缺点。
- 随着物联网的蓬勃发展，越来越多的应用将得到MEC的支持[36]。

---

[[Deep Reinforcement Learning-Based Workload Scheduling for Edge Computing]]

当涉及边缘计算时，移动设备可以将大部分任务卸载到边缘服务器进行执行，这有效地解决了它们有限资源的问题，同时减少了核心网络的流量负荷。然而，不恰当的任务卸载不仅会导致边缘服务器之间工作负载不均衡，还会增加任务延迟和能耗。因此，在边缘服务器之间合理调度计算任务对于优化高资源效率的服务质量至关重要。调度研究旨在选择任务应该卸载的时间和地点。已经有大量工作致力于工作负载调度问题。

Santoro等人[11]提出了一个名为Foggy的软件平台，用于雾计算环境中的工作负载编排和资源协商。它根据计算、存储或网络资源安排任务的执行位置。Anas等人[12]考虑了计算利用率和访问概率，并基于排队理论开发了一个性能模型，以解决联合云环境中服务提供商之间的工作负载平衡问题。Ma等人[13]考虑了边缘节点之间的合作，并研究了旨在最小化服务响应时间以及移动边缘计算中外包流量的工作负载调度。他们提出了一种基于水填充的启发式工作负载调度算法，以减少计算复杂性。模糊逻辑是解决边缘计算中工作负载调度问题的一种有效方法，近年来已经有所讨论。Sonmez等人[8]采用了基于模糊逻辑的方法来解决边缘计算系统中的工作负载编排问题。他们的方法考虑了被卸载任务的属性以及当前计算和网络资源的状态，并使用模糊规则来定义工作负载编排行动，涉及网络、计算和任务需求，以决定整个边缘计算系统内的任务执行位置。

然而，基于模糊逻辑的方法需要预先定义各种模糊规则，这将耗费大量时间和精力来衡量。因此，为了减少手动干预，一些研究尝试采用机器学习方法进行工作负载调度。Nascimento等人[14]提出了基于强化学习（RL）的调度方法，用于基于云的科学工作流执行。RL是机器学习的一个分支，专注于如何通过在动态环境中学习来实现最优目标。在RL中，作为学习者的代理程序感知环境的当前状态，并选择要采取的行动。当执行动作时，代理程序将根据行动的效果从环境中接收奖励或惩罚。如果从环境中接收到奖励，代理程序将增加采取此行动以获取更多奖励的倾向。相反，如果接收到惩罚，代理程序将减少采取此行动的倾向。为了最大化累积奖励，代理程序需要平衡探索和利用步骤。在探索步骤中，代理程序尝试以前未选择过的行动，并探索新的状态以获取更高的奖励。在利用步骤中，代理程序采取迄今为止已经观察到的最佳行动[15]。

尽管基于RL的工作负载调度方法可以减少手动干预并在状态和动作空间较小的情况下有效解决问题，但在状态或动作空间非常大时，通过普通的强化学习几乎不可能获得准确的状态或动作值[16]。为解决这个问题，结合深度学习和强化学习的DRL算法，如DQN [17]、DDPG [18]和PPO [19]，对于处理复杂性和高维度的问题变得非常有用。在这项工作中，我们提出了一种基于DRL的边缘计算工作负载调度方法，它可以从先前的行动中学习，并在没有环境数学模型的情况下实现最佳调度。同时，我们采用DQN算法来解决复杂性和高维度的工作负载调度问题，旨在平衡边缘服务器之间的工作负载，减少服务时间和失败任务率。

当涉及边缘计算时，移动设备可以将大部分任务卸载到边缘服务器进行执行，这有效地解决了它们有限资源的问题，同时减少了核心网络的流量负荷。然而，不恰当的任务卸载不仅会导致边缘服务器之间工作负载不均衡，还会增加任务延迟和能耗。因此，在边缘服务器之间合理调度计算任务对于优化高资源效率的服务质量至关重要。调度研究旨在选择任务应该卸载的时间和地点。已经有大量工作致力于工作负载调度问题。

- Santoro等人[11]提出了一个名为Foggy的软件平台，用于雾计算环境中的工作负载编排和资源协商。它根据计算、存储或网络资源安排任务的执行位置。
- Anas等人[12]考虑了计算利用率和访问概率，并基于排队理论开发了一个性能模型，以解决联合云环境中服务提供商之间的工作负载平衡问题。
- Ma等人[13]考虑了边缘节点之间的合作，并研究了旨在最小化服务响应时间以及移动边缘计算中外包流量的工作负载调度。他们提出了一种基于水填充的启发式工作负载调度算法，以减少计算复杂性。模糊逻辑是解决边缘计算中工作负载调度问题的一种有效方法，近年来已经有所讨论。
- Sonmez等人[8]采用了基于模糊逻辑的方法来解决边缘计算系统中的工作负载编排问题。他们的方法考虑了被卸载任务的属性以及当前计算和网络资源的状态，并使用模糊规则来定义工作负载编排行动，涉及网络、计算和任务需求，以决定整个边缘计算系统内的任务执行位置。

基于模糊逻辑的方法需要预先定义各种模糊规则，这将耗费大量时间和精力来衡量。因此，为了减少手动干预，一些研究尝试采用机器学习方法进行工作负载调度。

- Nascimento等人[14]提出了基于强化学习（RL）的调度方法，用于基于云的科学工作流执行。RL是机器学习的一个分支，专注于如何通过在动态环境中学习来实现最优目标。在RL中，作为学习者的代理程序感知环境的当前状态，并选择要采取的行动。

尽管基于RL的工作负载调度方法可以减少手动干预并在状态和动作空间较小的情况下有效解决问题，但在状态或动作空间非常大时，通过普通的强化学习几乎不可能获得准确的状态或动作值。

- 结合深度学习和强化学习的DRL算法，如DQN [17]、DDPG [18]和PPO [19]，对于处理复杂性和高维度的问题变得非常有用。在这项工作中，我们提出了一种基于DRL的边缘计算工作负载调度方法，它可以从先前的行动中学习，并在没有环境数学模型的情况下实现最佳调度。同时，我们采用DQN算法来解决复杂性和高维度的工作负载调度问题，旨在平衡边缘服务器之间的工作负载，减少服务时间和失败任务率。

---

[[Dynamic Pricing On E-commerce Platform With Deep Reinforcement Learning A Field Experiment]]

**标题：动态定价研究综述**

多年来，动态定价领域进行了大量研究。我们参考[14]，其中对最近的发展进行了全面的回顾。它结合了两个研究领域：（1）统计学习，特别应用于估计需求问题，以及（2）价格优化。大多数先前的研究集中在假定决策者已知价格和需求之间的功能关系的情况下。[15] 被认为是第一个对产品价格-需求关系进行数学描述并解决数学问题以实现最优收入的研究。然而，它假设这种关系随时间是静态的，而在现实中通常并非如此。[16] 假设需求不仅是价格的函数，还是价格的时间导数，导致了随时间变化的动态需求函数。[17] 引入了一个随机模型来捕捉需求不确定性同时优化价格。[18] 考虑了诸如有限库存和有限规划时间等约束条件。

在实践中，预先描述需求往往是困难的。最近的许多研究侧重于对未知需求函数的动态定价。一些研究人员通过参数化方法解决了这个问题。[19] 假设要学习的需求函数族是参数化的。[20] 提出了一种从历史购买数据中学习的方法。[21] 利用贝叶斯动态定价策略来解决需求不确定性。然而，由于错误地规定了需求族，收入可能会偏离最优值。因此，最近的许多研究主要围绕非参数化方法展开。[22]、[23]、[24] 深入研究了学习的非参数化方法。但是，它们都假设收入函数严格凹凸并可微，而这在电子商务零售行业中可能并不成立，正如第3.1节所示。

随着计算能力的发展，强化学习（RL）被引入以解决动态问题（[25]、[26]）。[9] 演示了使用Q-learning来表达对可能价格的预期未来折现利润，形成所谓的价格机器人以根据市场变化调整价格的可能性。[13] 从收益管理的角度使用时序差分进行信息产品的动态定价。[11] 对单卖家和两个卖家的动态定价问题进行了建模，并在模拟环境中采用不同的强化学习算法。[27] 使用不同类型的异步多智能体强化学习方法确定市场情境下的竞争定价策略。[10] 和 [12] 利用强化学习优化能源市场的价格。[8] 建议使用神经网络逼近的Q-learning来在模拟环境中维持收入同时提高公平性。然而，所有这些先前的工作都是在简化的市场设置中进行的模拟，其中以收入定义的奖励效果良好。同时，DNN仅用于离散价格的逼近，而在现实世界的市场中情况并非如此。

- **动态定价研究综述**
  - 多年来的研究：统计学习与价格优化
  - 假定功能关系下的先前研究
    - 假定静态价格-需求关系：[15]
    - 实际情况下的问题：静态假设的不准确性：[16]
    - 引入随时间变化的动态需求函数：[17]
    - 考虑约束条件：[18]
  - 未知需求函数的动态定价
    - 参数化方法的尝试
      - 假设参数化的需求函数族：[19]
      - 历史购买数据学习：[20]
      - 贝叶斯动态定价策略：[21]
    - 非参数化方法的主要研究
      - 非参数化学习的深入研究：[22], [23], [24]
      - 凹凸函数性质在电子商务零售业的不适用性：第3.1节
  - 强化学习的引入
    - 应用于动态问题：[25], [26]
    - Q-learning和时序差分在定价中的应用：[9], [13]
    - 不同环境和模型中的强化学习应用：[11], [27]
    - 能源市场中的强化学习应用：[10], [12], [8]

---

[[Intelligent resource allocation management for vehicles network An A3C learning approach]]

在强化学习（RL）环境中，代理根据称为环境状态的信号做出决策。满足马尔可夫性质的强化学习任务称为马尔可夫决策过程（MDP）。特别地，如果动作空间和状态空间是有限的，则称为有限马尔可夫决策过程。MDP 被表示为一个元组 $𝑀 = (𝑆,𝐴,𝑝,𝑅)$，其中 $S$ 表示状态集合，$A$ 是动作集合，$p$ 是在执行动作 $a$ 时从状态 $s$ 转移到 $𝑠′$ 的转移概率。更具体地说，当代理执行动作 $𝑎 ∈ 𝐴$ 并获得奖励 $𝑟  ∈  𝑅$ 时，环境从状态 $𝑠  ∈  𝑆$ 转移到 $𝑠 ′∈  𝑆$。$R$ 是执行动作 $a$ 后获得的奖励。如果在强化学习环境中满足马尔可夫性质，这意味着转移到下一个状态 *𝑠*′ 仅由当前动作 $a$ 和状态 $s$ 决定。以下列出了接下来要使用的一些变量，也在 [表][1] 中展示出来。

### 2.1. 强化学习

强化学习分为两种类型：基于价值的强化学习和基于策略的强化学习。此外，演员-评论家（actor-critic）强化学习方法是基于价值和基于策略方法的结合。此外，根据环境元素（即状态转移概率和奖励函数）是否已知，强化学习可分为无模型强化学习和基于模型的强化学习。无模型强化学习最近成功应用于深度神经网络[[35]–[37]]。

#### 基于价值的强化学习

我们知道，Q-learning是一种知名的基于价值的强化学习算法，用于找到最优的动作-价值函数 *Q(s, a)*。而使用深度神经网络来逼近动作-价值函数 *Q(s, a; θ)* 被称为深度 Q 网络（DNQ）[38] 和异步 Q-learning [39]。基于价值的强化学习算法通过最小化均方误差损失来更新参数。

#### 基于策略的强化学习

文献[[40],[41]]介绍了基于策略的强化学习。与基于价值的强化学习相比，基于策略的算法可以直接优化策略。基于策略的核心思想是将状态映射到动作，然后优化参数以最大化长期回报。基于策略的算法优势在于可以具有随机策略，这可能对某些问题导致最优策略。基于策略方法的变种包括近端策略优化（PPO）[40]、信任区域策略优化（TRPO）[41] 等。图 1 展示了基于价值和基于策略的关系。

对于深度 Q-learning，代理将采用神经网络（NN）来定义 Q 函数，即 *Q(s, a; θ)*。参数 $θ$ 被视为神经网络的权重。通过更新参数 $θ$，每个周期内的 Q 网络被训练以逼近 *Q(s, a)*。尽管神经网络具有很大的灵活性，但它们也为 Q-learning 的稳定性付出了代价。最近，提出了深度 Q 网络，它用深度神经网络代替了 Q 函数的近似，已经证明具有更强的稳健性和更高的性能。为了将 Q-learning 转化为深度 Q-learning，需要满足一些条件：

1. 使用多层卷积网络，因为它们可以通过采用分层卷积滤波器从原始输入数据中提取高级特征，利用局部空间相关性。这可以使代理从过去的经验中学习。
2. 使用经验回放。在深度 Q 网络中，系统将数据 *e(t)* = {s(t), a(t), r(t), s(t + 1)} 存储到经验池 *D* = {*e*(1), ... , *e*(t)}。当训练开始时，代理随机从 $D$ 中抽样以更新网络的参数，而不是使用连续的样本。
3. 在训练过程中建立目标 Q 网络，用于计算损失函数。如果只使用一个网络来估计目标 Q 值和 Q 值，将会陷入反馈循环。因此，目标网络的权重是固定的，并定期更新，以确保稳定的训练。

- 在强化学习（RL）环境中，代理根据称为环境状态的信号做出决策。

- 满足马尔可夫性质的强化学习任务称为马尔可夫决策过程（MDP）。

- 如果动作空间和状态空间是有限的，则称为有限马尔可夫决策过程。

- MDP 被表示为一个元组 $𝑀 = (𝑆,𝐴,𝑝,𝑅)$。
  - $S$ 表示状态集合。
  - $A$ 是动作集合。
  - $p$ 是在执行动作 $a$ 时从状态 $s$ 转移到 $𝑠′$ 的转移概率。
  - 当代理执行动作 $𝑎 ∈ 𝐴$ 并获得奖励 $𝑟  ∈  𝑅$ 时，环境从状态 $𝑠  ∈  𝑆$ 转移到 $𝑠 ′∈  𝑆$。
  - $R$ 是执行动作 $a$ 后获得的奖励。

- 如果在强化学习环境中满足马尔可夫性质，转移到下一个状态 *𝑠*′ 仅由当前动作 $a$ 和状态 $s$ 决定。

- 强化学习类型：
  - 基于价值的强化学习和基于策略的强化学习。
  - 演员-评论家（actor-critic）强化学习方法结合了基于价值和基于策略的方法。
  - 强化学习根据环境元素是否已知可分为无模型强化学习和基于模型的强化学习，最近在深度神经网络中成功应用无模型强化学习。[[35]–[37]]

- 基于价值的强化学习：
  - Q-learning是一种知名的基于价值的强化学习算法，用于找到最优的动作-价值函数 *Q(s, a)*。
  - 使用深度神经网络逼近动作-价值函数 *Q(s, a; θ)* 被称为深度 Q 网络（DNQ）和异步 Q-learning。[[38], [39]]
  - 通过最小化均方误差损失来更新参数实现基于价值的强化学习。

- 基于策略的强化学习：
  - 文献介绍了基于策略的强化学习。[[40], [41]]
  - 基于策略的算法可以直接优化策略，将状态映射到动作，并优化参数以最大化长期回报。
  - 优势在于具有随机策略，可能对某些问题导致最优策略。
  - 变种包括近端策略优化（PPO）、信任区域策略优化（TRPO）等。

- 深度 Q-learning：
  - 代理采用神经网络（NN）定义 Q 函数 *Q(s, a; θ)*。
  - 更新参数 $θ$ 以逼近 *Q(s, a)*。
  - 提出了深度 Q 网络，用深度神经网络代替 Q 函数的近似，具有更强的稳健性和性能。[[38], [39]]
  - 转化为深度 Q-learning需满足条件：
    1. 使用多层卷积网络提取高级特征。
    2. 使用经验回放，将数据存储到经验池，并随机抽样更新网络参数。
    3. 建立目标 Q 网络计算损失函数，确保稳定的训练。

---

[[\[Learning to Bid with AuctionGym]]]

当今在线广告拍卖中，许多假设都很可能被违反。因此，第二高价拍卖机制不再能够最大化拍卖者的收入，所有主要广告交易所已经放弃了第二高价格式。希望参与此类拍卖的广告商现在需要决定竞标策略，因为以前行业标准的真实竞标策略已经变得次优。

许多假设中的常见违规情况之一是广告商拥有预算。[吴等人]采用了无模型的强化学习方法，在第二价格拍卖中学习单个标量“调节”参数以优化预算[40]，同时其他方法已经被提出以将更多KPI约束纳入目标[41]。相比之下，我们引入了一个基于赌博机的学习框架，适用于_任何_拍卖机制，这对于非第二高价拍卖中的剩余优化至关重要。此外，我们处理的竞标策略取决于每次机会的上下文协变量，具有很高的灵活性。

在第一价格拍卖中降低竞标被称为_竞标压缩_。当拍卖者向_所有_参与者公开获胜的出价时，这些数据可以被利用来学习最优策略[10]。然而，这些信息很少可得。[潘等人]提出了一个两步竞标压缩程序，包括（1）获胜率估计和（2）剩余最大化。他们采用逻辑回归模型配合二分搜索以进行快速推断[28]。其他工作直接对“最低获胜竞标价”的分布进行建模[19]——使用一系列估计器和高效的黄金分割搜索来进行推断[43]。正如我们将展示的那样，这些工作符合“学习竞标”的价值观（也被称为_模型_为基础的观点）。[张等人]在训练样本规模大时利用非参数方法的灵活性进行竞标压缩，报告相比参数方法的改进[42]。AuctionGym使我们能够重现这些见解，同时在一系列环境条件下提供了参数方法的性能额外视角。这使我们能够在低或高数据情况下、竞争强或弱以及模型更新频率高或低等可配置参数中实证地确定最优方法。

我们受到了更广泛的强化学习研究社区中模拟环境取得成功的启发[2]。特别是，我们借鉴了RecoGym模拟环境，旨在实现对_分配_步骤的赌博机优化，决定在特定上下文中应该展示哪个广告[30]。AuctionGym将这一步骤与_竞标_问题联合建模，决定在给定广告机会时应该出多少竞标。我们相信这为未来的研究方向开辟了令人兴奋的新途径，可以共同解决这两个问题。实际上，即使拍卖结果与分配的广告无关，拍卖结果对未来的训练数据和_探索_有很大影响。

当今在线广告拍卖中存在多个假设违规的情况：

- 第二价格拍卖机制不再能最大化拍卖者的收入，因此广告交易所已经放弃了这种格式 [38]。
- 广告商需要重新决定竞标策略，因为真实竞标策略不再是最佳选择。
- 在广告商拥有预算的情况下，常见的违规情况之一是在第二价格拍卖中采用无模型的强化学习方法优化预算 [40]。
- 引入了基于赌博机的学习框架，适用于任何拍卖机制，尤其对于非第二高价拍卖中的剩余优化至关重要，并且竞标策略灵活性高，取决于每次机会的上下文协变量。
- 在第一价格拍卖中，降低竞标被称为竞标压缩，利用获胜出价数据学习最优策略，但这些数据很少可得 [10, 28, 19, 43]。
- 提出了两步竞标压缩程序，包括获胜率估计和剩余最大化，采用逻辑回归模型进行快速推断。其他工作直接对最低获胜竞标价的分布进行建模，并采用各种估计器和高效搜索方法进行推断。
- AuctionGym模拟环境可重现这些见解，并提供参数方法在不同环境条件下的性能视角，使研究者能够在不同情况下确定最优方法。
- 受到RecoGym模拟环境的启发，AuctionGym将竞标问题与广告分配问题联合建模，为未来的研究方向提供了新途径，即使拍卖结果与分配的广告无关，拍卖结果仍然对未来训练数据和探索产生重大影响。

---

[[1. Projects/2023-05-07 基于多智能体强化学习和反向拍卖机制的车联网任务调度/参考文献/mmd/Multi-Objective_Workflow_Scheduling_With_Deep-Q-Network-Based_Multi-Agent_Reinforcement_Learning|Multi-Objective_Workflow_Scheduling_With_Deep-Q-Network-Based_Multi-Agent_Reinforcement_Learning]]

当在分布式平台上安排多任务工作流时，普遍认为这是一个 NP 难问题。因此，通过遍历型算法获得最优调度方案非常耗时。幸运的是，启发式和元启发式策略具有多项式复杂度，能够以可接受的最优性损失产生近似或接近最优解。例如，Kaur 等人 [24] 修改了原始的细菌觅食优化算法（BFOA），提出了一种多目标细菌觅食优化算法（MOBFOA），考虑了帕累托最优前沿，旨在最小化流程时间、制造周期和资源使用成本。Zhang 等人 [25] 提出了一种双目标遗传算法（BOGA），能够优化能源节约和工作流可靠性，并获得接近最优的帕累托前沿。Casas 等人 [26] 提出了一种增强型的遗传算法与高效调谐（GA-ETI），用于云系统中的科学应用。它能够优化工作流制造周期和成本。Verma 等人 [27] 提出了基于非支配排序的混合粒子群优化（HPSO）算法，用于工作流调度，能够优化执行时间和成本。Zhou 等人 [28] 提出了基于模糊支配排序的异构最早完成时间（FDHEFT）算法，能够最小化部署在 IaaS 云上的工作流的成本和制造周期。然而，这些方法受到以静态全局视角为基础的先前专家知识的严重限制，无法恰当地描述工作流调度的动态过程。

最近，博弈论和强化学习（RL）模型和方法被广泛应用于多约束过程调度问题 [29, 30, 31, 32, 33, 34, 35]。人们相信博弈论中的均衡概念和多智能体训练方法在处理多约束和多目标优化问题方面具有很高的潜力。例如，Duan 等人 [18] 提出了一种用于成本和制造周期优化的顺序协作博弈算法，同时满足大规模工作流调度的存储约束。Cui 等人 [22] 提供了一种基于强化学习的方法，用于云环境中不同时间提交的多优先级多工作流调度。Iranpour 等人 [17] 提出了一种基于模糊博弈理论模型的分布式负载平衡和准入控制算法，用于大规模 SaaS 云。Wu 等人 [20] 提出了一种改进的带加权适应值函数的 Q-learning 算法，用于云环境中完成时间和负载平衡的优化。

当在分布式平台上安排多任务工作流时，普遍认为这是一个 NP 难问题。因此，通过遍历型算法获得最优调度方案非常耗时。幸运的是，启发式和元启发式策略具有多项式复杂度，能够以可接受的最优性损失产生近似或接近最优解。

- Kaur 等人 [24] 修改了原始的细菌觅食优化算法（BFOA），提出了一种多目标细菌觅食优化算法（MOBFOA），考虑了帕累托最优前沿，旨在最小化流程时间、制造周期和资源使用成本。
- Zhang 等人 [25] 提出了一种双目标遗传算法（BOGA），能够优化能源节约和工作流可靠性，并获得接近最优的帕累托前沿。
- Casas 等人 [26] 提出了一种增强型的遗传算法与高效调谐（GA-ETI），用于云系统中的科学应用，能够优化工作流制造周期和成本。
- Verma 等人 [27] 提出了基于非支配排序的混合粒子群优化（HPSO）算法，用于工作流调度，能够优化执行时间和成本。
- Zhou 等人 [28] 提出了基于模糊支配排序的异构最早完成时间（FDHEFT）算法，能够最小化部署在 IaaS 云上的工作流的成本和制造周期。然而，这些方法受到以静态全局视角为基础的先前专家知识的严重限制，无法恰当地描述工作流调度的动态过程。

最近，博弈论和强化学习（RL）模型和方法被广泛应用于多约束过程调度问题 [29, 30, 31, 32, 33, 34, 35]。人们相信博弈论中的均衡概念和多智能体训练方法在处理多约束和多目标优化问题方面具有很高的潜力。

- Duan 等人 [18] 提出了一种用于成本和制造周期优化的顺序协作博弈算法，同时满足大规模工作流调度的存储约束。
- Cui 等人 [22] 提供了一种基于强化学习的方法，用于云环境中不同时间提交的多优先级多工作流调度。
- Iranpour 等人 [17] 提出了一种基于模糊博弈理论模型的分布式负载平衡和准入控制算法，用于大规模 SaaS 云。
- Wu 等人 [20] 提出了一种改进的带加权适应值函数的 Q-learning 算法，用于云环境中完成时间和负载平衡的优化。

---

[[Performance and Cost-Efficient Spark Job Scheduling Based on Deep Reinforcement Learning in Cloud Computing Environments]]

### 2.1 云虚拟机和数据中心中的调度

在云虚拟机中调度任务以及在数据中心调度虚拟机创建的问题已经被深入研究。PARIS[24]对不同类型的虚拟机中各种工作负载的性能进行了建模，以确定性能和节省成本之间的权衡。因此，这个模型可以用来选择最适合于某个特定工作负载的既能节省成本又能保证性能的虚拟机来运行云虚拟机中的任务。Yuan等人[25]提出了一种分布式绿色数据中心（DGDC）的双目标任务调度算法。他们制定了一个多目标优化方法，以最大化DGDC提供者的利润，并通过联合确定任务在多个ISP之间的分配和每个GDC的任务服务速率来最小化所有应用程序的平均任务丢失可能性。Zhu等人[26]提出了一种称为匹配和多轮分配（MMA）的调度方法，以优化所有提交任务的完成时间和总成本，同时考虑了安全性和可靠性约束。在本文中，我们解决的是大数据集群调度问题，这与在云虚拟机中调度任务或在数据中心进行虚拟机提供的问题不同。这种差异是由于Spark作业的性质和内存架构范式所导致的。

此外，来自Spark作业的执行器可能无法放入单个虚拟机中，调度器在创建执行器时应选择不同类型的虚拟机组合，以满足资源约束。此外，工作负载的性能也会根据放置策略（分散或整合）而变化，我们的目标是训练调度器，在不提供有关工作负载和集群动态性能模型的情况下选择最佳策略。

### 2.2 框架调度器

Apache Spark默认使用先进先出（FIFO）调度器，该调度器将作业的执行器分布式放置（分散）以减少单个工作节点（或如果考虑云部署，则是虚拟机）上的开销。虽然这种策略可以改善计算密集型工作负载的性能，但由于网络洗牌操作的增加，网络密集型工作负载可能会遭受性能开销。Spark也可以 consolodate 核心使用率以最小化集群中使用的总节点数。然而，它并不考虑虚拟机的成本和作业的运行时间。因此，可能会长时间使用昂贵的虚拟机，产生更高的虚拟机成本。基于Fair3和DRF[27]的调度器改善了集群中多个作业的公平性。但是，这些调度器并不能提高云部署集群中的成本效率等SLA目标。对于不同的工作负载类型，各种执行器放置策略可能是合适的，但是框架调度器无法支持这一点。这就是我们在我们的工作中所谓的精细级执行器放置。

### 2.3 性能模型和基于启发式的调度器

有一些工作试图改善Spark作业的调度不同方面。这些方法大多基于不同的工作负载和资源特性构建性能模型。然后，性能模型被用于资源需求预测，或者设计复杂的启发式算法以实现一个或多个目标。

Sparrow[7]是一个使用基于随机抽样的去中心化调度器，旨在改善默认的Spark调度。Quasar[8]是一个集群管理器，可以在满足用户提供的应用程序性能目标的同时最小化集群资源利用率。它使用协同过滤来找出不同资源对应用程序性能的影响。然后，这些信息被用于高效的资源分配和调度。Morpheus[9]从历史跟踪中估计作业性能，然后通过容器的紧凑放置来最小化集群资源使用成本。此外，Morpheus还可以动态地重新调整失败的作业以增加整体集群性能。Justice[10]使用每个作业的截止时间约束和历史作业执行跟踪进行入场控制和资源分配。它还自动适应工作负载变化，以为每个作业提供足够的资源，以便满足截止时间。OptEx[11]从分析信息中建模Spark作业的性能。然后性能模型用于组成成本高效的集群，同时只部署每个作业所需的最小虚拟机集。此外，假设每个作业具有相同的执行器大小，即虚拟机的总资源容量。Maroulis等人[12]利用DVFS技术调整CPU频率来调整传入工作负载以减少能源消耗。Li等人[13]还提供了一个能源高效的调度器，其中算法假设每个作业具有相同的执行器大小，这等同于虚拟机的总资源容量。

性能模型和基于启发式的方法存在的问题是：（1）性能模型严重依赖过去的数据，有时可能由于集群环境的各种变化而过时（2）调整或修改基于启发式的方法以纳入工作负载和集群变化是困难的。因此，许多研究人员正致力于使用RL方法以更高效、可扩展地解决调度问题。

### 2.4 基于DRL的调度器

深度强化学习（DRL）在作业调度中的应用是相对较新的。有一些工作试图解决云基础应用程序的不同SLA目标。

Liu等人[28]开发了一个分层框架，用于在减少能源消耗和降低延迟的同时进行云资源分配。全局层使用Q-learning进行虚拟机资源分配，而本地层使用基于LSTM的工作负载预测器和基于模型的RL进行本地服务器的功耗管理。Wei等人[29]提出了一种面向QoS的应用程序在云部署中的作业调度算法。他们使用DQN与目标网络和经验回放来提高算法的稳定性。主要目标是改善平均作业响应时间，同时最大化虚拟机资源利用率。DeepRM[14]使用REINFORCE，一种政策梯度深度RL算法，用于集群调度中的多资源打包。主要目标是最小化平均作业减慢。但是，由于状态空间将所有集群资源视为一个大的CPU和内存块，所以集群被假设为同构的。Decima[15]也使用政策梯度代理，其目标与DeepRM类似。这里，代理和环境都设计为处理Spark中每个作业的DAG调度问题，同时考虑相互依赖的任务。Li等人[30]考虑了基于Actor Critic的算法，以处理Apache Storm中持续连续数据的处理，并具有高可伸缩性。调度问题是将工作负载分配到特定的工作节点，目标是减少平均端到端元组处理时间。这项工作还假设集群设置为同构，并且不考虑成本效率。DSS[17]是在云计算环境中自动化的大数据任务调度方法，它结合了DRL和LSTM，以自动预测每个传入大数据作业应被安排到哪个虚拟机，以提高大数据分析的性能，同时减少资源执行成本。Harmony[31]是一个由深度学习驱动的ML集群调度器，可以以最小化干扰和最大化平均作业完成时间的方式放置训练作业。它使用基于Actor Critic的算法和作业感知的行动空间探索与经验重放。此外，它还有一个奖励预测模型，该模型使用历史样本进行训练，并用于为未见放置生成奖励。Cheng等人[18]使用基于DQN的算法进行云中Spark作业的调度。这项工作的主要目标是优化带宽资源成本，同时最小化节点和链接能源消耗。Spear[32]致力于最小化复杂DAG作业的完成时间，同时同时考虑任务依赖性和异构资源需求。Spear利用蒙特卡洛树搜索（MCTS）进行任务调度，并训练DRL模型来指导MCTS中的扩展和回滚步骤。Wu等人[33]提出了一种基于深度CNN和基于价值函数的Q-learning的最佳任务分配方案。在这里，任务被分配到合适的物理节点上，目标是在满足任务要求的同时最大化长期收益。Thamsen等人[19]使用梯度赌博法方法来提高Spark和Flink作业的资源利用率和作业吞吐量，其中RL模型学习了在共享资源上不同类型作业的共同位置好处。

总之，大多数现有方法主要关注性能改进。此外，这些工作还假设每个作业/任务只分配给一个虚拟机或工作节点。此外，许多工作还假设集群节点是同构的，在云部署时可能并非如此。因此，这些工作并不考虑在Spark作业调度中进行细粒度级别的执行器放置。相比之下，我们的调度代理可以在需要时将同一作业的执行器放置在不同的虚拟机中（以优化特定策略），并保证在所需资源上启动该作业的所有执行器。此外，我们的代理可以处理作业的不同执行器大小以及具有定价模型的不同虚拟机实例大小。此外，我们的代理可以被训练以优化单一目标，比如成本效率或性能改进。此外，我们的代理还可以被训练以在多个目标之间取得平衡。最后，提议的调度代理可以学习作业的固有特性，以找到适当的放置策略来改善目标对象，而不需要任何有关作业或集群的先验信息。表1显示了我们的工作与其他相关工作之间的比较总结。

- **云虚拟机和数据中心中的调度**
  - PARIS[24]对不同虚拟机中工作负载性能建模，权衡性能与成本。
  - DGDC[25]提出多目标任务调度算法，最大化利润并最小化任务丢失可能性。
  - MMA[26]方法优化任务完成时间和总成本，考虑安全性和可靠性。
  - 大数据集群调度问题与云虚拟机和数据中心调度不同，由于Spark作业性质和内存架构。

- **框架调度器**
  - Apache Spark默认使用FIFO调度器，但可能对网络密集型工作负载有性能开销。
  - 其他调度器改善了集群中多个作业的公平性，但未提高云部署集群的成本效率。

- **性能模型和基于启发式的调度器**
  - Sparrow[7]、Quasar[8]、Morpheus[9]、Justice[10]、OptEx[11]等方法改善调度，但对集群动态性能和工作负载变化适应困难。

- **基于DRL的调度器**
  - Liu[28]的分层框架、Wei[29]的面向QoS的调度算法、DeepRM[14]、Decima[15]、DSS[17]、Harmony[31]等尝试最小化成本、最大化性能等目标。

---

[[Price-Based Resource Allocation for Edge Computing A Market Equilibrium Approach]]

当然，我可以帮你将这段文字翻译成中文。

最近的文献广泛探讨了边缘计算（EC）的潜在好处和许多技术方面。首先，混合边缘/雾计算/云系统可以被利用来提高新兴应用的性能，比如云游戏和医疗保健[11]，[12]。在[13]中，A. Mukherjee等人提出了一种功耗和延迟感知的云端选择策略，用于多个云端环境中的计算卸载。在[14]中，作者们制定了一个工作负载分配问题，以在延迟约束条件下最小化系统能耗与服务延迟之间的权衡。[15]中针对云端网络提出了一种延迟感知的工作负载卸载方案，以最小化移动用户的平均响应时间。

在[16]中，M. Jia等人探讨了云端位置和用户到云端分配的联合优化，以最小化服务延迟，同时考虑负载均衡。[17]中提出了一个统一的服务位置和请求分发框架，以评估用户访问延迟和服务成本之间的权衡。参考文献[18]运用Stackelberg博弈和匹配理论研究了三层边缘网络中数据服务运营商（DSO）、数据服务订阅者（DSS）和一组ENs之间的联合优化，其中DSOs可以从不同的ENs获取计算资源来为它们的DSS提供服务。

最近的另一个主要研究方向集中在MEC环境中任务卸载的通信和计算资源的联合分配[19]，[20]，[21]。MEC允许移动设备将计算任务卸载到靠近或位于蜂窝基站的资源丰富的服务器，这可能会降低设备的能耗和任务执行延迟。然而，如果多个用户同时将他们的任务卸载到MEC服务器，这些好处可能会受到威胁。在这种情况下，用户不仅可能遭受严重的干扰，还可能收到非常少的EC资源，这将进一步降低数据速率，增加传输延迟，并导致服务器上高的任务执行时间。因此，在一个集成框架中应该联合考虑卸载决策、无线资源和计算资源的分配以及调度。

与现有文献不同，现有文献大多从单个网络运营商的角度优化整个系统的性能，我们则从博弈论和市场设计的角度考虑了EC资源分配问题[8]。特别是，我们研究如何从多个ENs向多个服务中分配资源，以一种公平且高效的方式。我们利用一种被授予诺贝尔奖的普遍均衡理论（General Equilibrium）来构建一个高效的基于市场的资源分配框架。虽然这个概念提出了100多年，直到1954年，Arrow和Debreu的开创性工作在温和条件下证明了ME的存在[4]。然而，他们基于不动点定理的证明是非构造性的，并没有给出计算均衡的算法[8]。最近，理论计算机科学家对理解普遍均衡概念的算法方面表现出了极大的兴趣。在过去的十年里，已经完成了各种计算ME的高效算法和复杂性分析[8]，[22]，[23]，[24]，[25]，[26]。需要注意的是，尽管存在结果已经被证明，但并没有一般性的技术来计算ME。

我们提出的模型受到Fisher市场[5]的启发，这是普遍均衡理论中交换市场模型的特例。一个_交换市场_模型由一组经济代理商交易不同类型的可分割商品组成。每个代理商都有一定数量的初始商品和一个表示其对不同商品捆绑的偏好的效用函数。在给定商品价格的情况下，每个代理商出售初始商品，然后使用收入购买他们能负担得起的最佳商品组合[4]，[8]。市场的目标是找到平衡价格和分配，以最大化每个代理商的效用，遵守预算约束，并且市场清算。在Fisher市场模型中，每个代理商只携带一定数量的初始货币，并希望在市场上购买可用的商品。我们将EC资源分配问题建模为Fisher市场。我们不仅展示了均衡分配的吸引人的公平性质，还引入了高效的分布式算法来找到ME。更重要的是，我们系统地设计了一个新的简单凸规划来捕捉市场，其中货币对买家具有内在价值，这超出了经典Fisher市场模型的范围。

实际上，关于云资源分配和定价有大量的文献[34]。在[35]，[36]中，作者为云提供

商提出了不同的利润最大化框架。参考文献[37]，[38]，[39]研究了如何在云联盟中在云提供商之间高效共享资源和利润。在[40]中引入了几种资源采购机制，以帮助用户选择多云市场中合适的供应商。在[41]中，云提供商与多个服务之间的互动被建模为广义Nash博弈。该模型在[42]中扩展到多云多服务环境。在[43]中，将单云多服务资源提供和定价问题，包括平面、按需和临时虚拟机实例，作为Stackelberg博弈，以最大化提供商的收入同时最小化服务成本。

拍卖理论已被广泛用于研究云资源分配[44]，[45]，[46]。一个典型的系统由一个或多个云和多个用户组成。首先，用户提交竞标，包括他们对VM类型和数量的期望资源捆绑以及他们愿意支付的价格，给拍卖者。然后，拍卖者解决一个获胜者确定问题来识别被接受的竞标。最后，拍卖者计算每个获胜者需要支付的金额，以确保真实性。在拍卖中，常见的目标是最大化社会福利或最大化云提供商的利润。此外，只有获胜者才能获得云资源。此外，大多数现有的拍卖模型不考虑弹性用户需求。例如，先前的研究通常假设云用户是单一思维的，他们只对特定的捆绑感兴趣，并且对其他捆绑没有价值。

与一般云经济学和资源分配的现有作品不同，我们的设计目标是以一种公平且高效的方式从多个节点（例如ENs）向预算受限的代理商（即服务）分配资源，使每个代理商对她的资源分配感到满意，并确保高边缘资源利用率。所提出的模型还包含实际方面，例如，服务请求可以在不同的ENs上提供服务，并且服务需求可以灵活定义，而不是像拍卖模型中那样固定的捆绑。

- 最近的文献广泛探讨了边缘计算（EC）的潜在好处和许多技术方面。其中包括：
  - 利用混合边缘/雾计算/云系统来提高新兴应用性能，比如云游戏和医疗保健[11]，[12]。
  - 探索功耗和延迟感知的云端选择策略，用于多个云端环境中的计算卸载[13]。
  - 在考虑延迟约束的情况下，通过制定工作负载分配问题来权衡功耗和服务延迟[14]。
  - 为移动用户设计延迟感知的工作负载卸载方案，以最小化平均响应时间[15]。
  - 探索联合优化云端位置和用户到云端分配，以最小化服务延迟并考虑负载平衡[16]。
  - 提出统一的服务位置和请求分发框架，评估用户访问延迟与服务成本之间的权衡[17]。
  - 运用Stackelberg博弈和匹配理论研究三层边缘网络中的联合优化[18]。

- 另一个研究方向是MEC环境中通信和计算资源的联合分配[19]，[20]，[21]。其中包括：
  - 允许移动设备将计算任务卸载到资源丰富的服务器，以降低能耗和任务执行延迟[19]。
  - 考虑多用户同时卸载任务可能导致的问题，如干扰增加、数据速率降低和任务执行时间增加[20]。
  - 建议在集成框架中联合考虑卸载决策、无线资源和计算资源的分配和调度[21]。

- 与以往研究不同，该文献考虑了EC资源分配问题的博弈论和市场设计视角[8]。具体包括：
  - 利用普遍均衡理论构建市场资源分配框架，追求公平和高效[8]。
  - 提出基于Fisher市场的模型，展示了均衡分配的公平性质，并引入高效的分布式算法[8]。
  - 系统设计新的简单凸规划来捕捉具有买家内在价值的市场[8]。

- 此外，还有丰富的云资源分配和定价文献，包括：
  - 云提供商的利润最大化框架[34]，[35]，[36]。
  - 在云联盟中高效共享资源和利润的研究[37]，[38]，[39]。
  - 辅助用户选择多云市场中合适供应商的资源采购机制[40]。
  - 通过拍卖理论研究云资源分配，最大化社会福利或提供商利润[44]，[45]，[46]。
