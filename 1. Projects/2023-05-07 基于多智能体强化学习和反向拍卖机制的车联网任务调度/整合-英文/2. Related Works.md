---
date created: 2023-12-06 16:58
date updated: 2023-12-08 14:41
---

## 2. Related Works


When it comes to edge computing, mobile devices often offload most tasks to edge servers for execution, effectively addressing limitations in resources and alleviating pressure on core network traffic [^31]. However, improper task offloading may result in uneven workloads on edge servers, leading to increased task latency and energy consumption. Therefore, judiciously scheduling computing tasks among edge servers is crucial to optimize service quality and resource efficiency.

Common scheduling methods in edge cloud computing environments include fuzzy logic and machine learning. Various distributed resource scheduling methods based on random optimization have emerged [^24][^26][^27][^1], dynamically adjusting resource allocation strategies based on real-time feedback information, demonstrating good performance in specific scenarios. However, these methods face challenges, such as the need to predefine fuzzy rules, requiring significant time for design, and being limited by a static global perspective. In contrast to fuzzy logic methods, traditional machine learning methods utilize existing scheduling data to learn dynamic task scheduling strategies, reducing manual intervention. Nevertheless, the dataset used for learning may not adapt to new environments. Therefore, reinforcement learning agents trained in simulation environments have become a suitable solution to address the challenges of environmental changes [^32].

In recent years, scholars in the auction mechanism field have focused on researching issues such as imbalance in supply and demand, unfair competition, and information asymmetry in vehicular networks. They strive to design various mechanisms and algorithms to achieve fair determination of resource prices and efficient allocation. In the field of Mobile Edge Computing (MEC), research has shown various developments. For instance, W. Hou et al. [^3] proposed an online incentive-driven task allocation scheme aimed at maximizing system utility. Meanwhile, the distributed incentive mechanism proposed by R. Chattopadhyay and C. Tham [^5] significantly reduces node computation costs. Additionally, auction-based incentive mechanisms have been widely applied to MEC resource allocation. For example, L. Ma et al. [^6] designed a practical combined double auction mechanism to incentivize edge servers to provide services to nearby mobile users. W. Sun et al. [^7] proposed a resource allocation scheme based on double auctions, applicable to the Industrial Internet of Things environment, aiming to maximize overall social welfare. G. Gao et al. [^8], based on auction mechanisms, proposed a solution for virtual machine allocation for deadline-sensitive tasks in distributed edge clouds. Zhou, H et al. [^42] introduced a reverse auction-based computation offloading and resource allocation mechanism for mobile edge computing to minimize the cost of cloud service centers.

Reinforcement learning (RL) methods have been widely applied to multi-constraint process scheduling problems [^9][^10]. These methods leverage equilibrium concepts from game theory and multi-agent training techniques to address multi-constraint and multi-objective optimization problems. Nascimento et al. [^2] proposed an RL-based scheduling method for cloud-based scientific workflow execution. Reinforcement learning focuses on how agents achieve optimal goals by learning in dynamic environments. Mao et al. [^12] introduced DeepRM, addressing system resource management issues by transforming time-sensitive resource demand task scheduling into a reinforcement learning problem. In vehicular networks, Li et al. [^11] proposed an RL-based cluster collaboration scheduling method aiming to improve communication efficiency and reliability.

This paper combines reinforcement learning and reverse auction mechanisms, aiming to optimize resource utilization and bidding strategies through reinforcement learning methods by learning in dynamic environments. It also utilizes reverse auction mechanisms to achieve resource allocation, enhancing vehicle utility and promoting fair competition. Combining these approaches can make vehicle scheduling more intelligent and efficient, reducing congestion and resource waste, thus optimizing the overall efficiency and user satisfaction of the transportation system.

[^41]: Applications of Auction and Mechanism Design in Edge Computing A Survey
[^42]:Zhou, H., Wu, T., Chen, X., Guo, D., & Wu, J. (n.d.). Reverse Auction-based Computation Offloading and Resource Allocation in Mobile Cloud-Edge Computing.
[^31]:Wang X, Wang X, Liang C, Huang C. A Survey on Mobile Edge Networks: Convergence of Computing, Caching and Communications. IEEE Access. 2017;5:6757-74.
[^32]:Shyalika C, Silva T, Karunananda A. Reinforcement Learning in Dynamic Task Scheduling: A Review. SN Computer Science. 2020 Sep;1(5):1-9.
[^24]:  WANG C, LIANG C, YU F R, et al. Computation Offloading and Resource Allocation in Wireless Cellular Networks With Mobile Edge Computing[J/OL]. IEEE Transactions on Wireless Communications, 2017, 16(8): 4924-4938. DOI:10.1109/TWC.2017.2703901.
[^25]:  URGAONKAR R, WANG S, HE T, et al. Dynamic service migration and workload scheduling in edge-clouds[J/OL]. Performance Evaluation, 2015, 91: 205-228. DOI:10.1016/j.peva.2015.06.013.
[^26]:  SORKHOH I, EBRAHIMI D, ATALLAH R, et al. Workload Scheduling in Vehicular Networks With Edge Cloud Capabilities[J/OL]. IEEE Transactions on Vehicular Technology, 2019, 68(9): 8472-8486. DOI:10.1109/TVT.2019.2927634.
[^27]:  ZHANG Q. Multihop Transmission-Oriented Dynamic Workflow Scheduling in Vehicular Cloud[J/OL]. Wireless Communications and Mobile Computing, 2022, 2022: 1-14. DOI:10.1155/2022/2033644.
[^1]:Sonmez C, Ozgovde A, Ersoy C (2019) Fuzzy Workload Orchestration for Edge Computing. IEEE Trans Netw Serv Manag 16(2):769–782. <https://doi>. org/10.1109/TNSM.2019.2901346
[^3]: W. Hou, H. Wen, N. Zhang, J. Wu, W. Lei, and R. Zhao, "IncentiveDriven Task Allocation for Collaborative Edge Computing in Industrial Internet of Things," IEEE Internet Things J., vol. PP, no. 99, pp. 1-1, 2021.
[^4]: L. Li, T. Q. S. Quek, J. Ren, H. H. Yang, Z. Chen, and Y. Zhang, "An Incentive-Aware Job Offloading Control Framework for Multi-Access Edge Computing," IEEE Trans. Mobile Comput., vol. 20, no. 1, pp. 63-75, 1 Jan. 2021.
[^5]: R. Chattopadhyay, and C. Tham, "Fully and Partially Distributed Incentive Mechanism for a Mobile Edge Computing Network," IEEE Trans. Mobile Comput., vol. PP, no. 99, pp. 1-1, 2020.
[^6]: L. Ma, X. Wang, X. Wang, L. Wang, Y. Shi and M. Huang, "TCDA: Truthful Combinatorial Double Auctions for Mobile Edge Computing in Industrial Internet of Things," IEEE Trans. Mobile Comput., vol. PP, no. 99, pp. 1-1, 2021.
[^7]: W. Sun, J. Liu, Y. Yue, and H. Zhang, "Double Auction-Based Resource Allocation for Mobile Edge Computing in Industrial Internet of Things, " IEEE Trans. Ind. Inf., vol. 14, no. 10, pp. 4692 - 4701, 2018.
[^8]: G. Gao, M. Xiao, J. Wu, H. Huang, S. Wang, and G. Chen, "Auction based VM allocation for deadline-sensitive tasks in distributed edge cloud," IEEE Trans. Services Comput., vol. PP, no. 99, pp. 1-1, 2020.
[^9]: .Kang Yang, Rongyu Cao, Yueyuan Zhou, Jiawei Zhang, En Shao, Guangming Tan, "Deep Reinforcement Agent for Failure-aware Job scheduling in High-Performance Computing", *2021 IEEE 27th International Conference on Parallel and Distributed Systems (ICPADS)*, pp.442-449, 2021.
[^10]: Seiju Yasuda, Chonho Lee, Susumu Date, "An Adaptive Cloud Bursting Job Scheduler based on Deep Reinforcement Learning", *2021 International Conference on High Performance Big Data and Intelligent Systems (HPBD&IS)*, pp.217-224, 2021.
[^2]:Nascimento A, Olimpio V, Silva V, Paes A, de Oliveira D (2019) A Reinforcement Learning Scheduling Strategy for Parallel Cloud-Based Workflows. In: 2019 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW). pp 817–824. <https://doi.org/10.1109/> IPDPSW.2019.00134
[^11]: Li X, Zhang Y, Wang Y, Zhang J. Cluster-Enabled Cooperative Scheduling Based on Reinforcement Learning for Vehicular Networks. IEEE Transactions on Vehicular Technology. 2020 Oct 26;70(1):1018-30.
[^12]:Mao, Hongzi, Mohammad Alizadeh, Ishai Menache, and Srikanth Kandula. 2016. “Resource Management with Deep Reinforcement Learning.” In Proceedings of the 15th ACM Workshop on Hot Topics in Networks. doi:10.1145/3005745.3005750.
[^13]:Jeunen, Olivier, et al. Learning to Bid with AuctionGym.
[^14]: Erich Kutschinski, Thomas Uthmann, and Daniel Polani. Learning competitive pricing strategies by multi-agent reinforcement learning. Journal of Economic Dynamics and Control, 27(11-12):2207–2218, 2003.
