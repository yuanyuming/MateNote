---
date created: 2023-12-06 16:58
date updated: 2023-12-08 11:01
---

## 2. Related Works

==这部分还未整理, 需要调整==

- 相关工作
  - 综述车联网中的任务调度方法
  - 包括传统的优化方法和基于机器学习的方法
  - 比较它们的优缺点，指出现有方法的不足之处。

- 综述车联网中的任务调度方法
  - 传统算法
  - 强化学习算法

- 资源的市场化调度, 拍卖机制的方法

- 基于机器学习和强化学习的方法

### 1. 多智能体强化学习在车联网任务调度中的应用

- **Literature Review:** 回顾先前关于多智能体强化学习在车联网任务调度方面的研究。
- **Multi-Agent RL Algorithms:** 总结不同多智能体强化学习算法在车联网任务调度中的应用和效果。
- **Advantages and Challenges:** 讨论多智能体强化学习在车联网任务调度中的优势和挑战。

### 2. 反向拍卖在车联网任务调度中的应用

- **Survey of Reverse Auctions:** 综述反向拍卖在车联网领域的应用情况。
- **Reverse Auction Mechanisms:** 总结不同反向拍卖机制在车联网任务调度中的应用和效果。
- **Comparison with Traditional Methods:** 比较反向拍卖和传统方法在车联网任务调度中的效率和适用性。

### 3. 结合多智能体强化学习与反向拍卖的研究

- **Integrated Approaches:** 总结将多智能体强化学习与反向拍卖相结合的研究成果。
- **Benefits of Integration:** 探讨整合两种方法在车联网任务调度中带来的好处和优势。
- **Case Studies or Simulations:** 如果有的话，列举相关研究中的案例研究或模拟实验结果。

### 4. 未来研究方向

- **Open Challenges:** 总结当前领域中尚未解决的问题和挑战。
- **Potential Research Directions:** 提出未来可能的研究方向和拓展点。
- **Emerging Technologies:** 讨论新兴技术如何影响车联网任务调度，例如边缘计算、区块链等。

当涉及边缘计算时，移动设备通常将大部分任务卸载到边缘服务器执行，有效解决资源有限性和核心网络流量压力的问题。[^31]然而，不当的任务卸载可能导致边缘服务器工作负载不均衡，增加任务延迟和能源消耗。因此，合理调度计算任务在边缘服务器之间至关重要，有助于优化服务质量和资源效率。调度研究致力于选择任务的卸载时间和位置，并通过启发式、元启发式策略来解决 NP 难问题。

常用的调度方法包括模糊逻辑和机器学习两种方式。模糊逻辑方法需要预先定义模糊规则，耗费大量时间设计，并受到静态全局视角的限制。相反，机器学习方法利用学习技术动态调整任务调度策略，减少了手动干预。然而，学习所用的数据集可能无法适应新环境，因此在仿真环境中训练的强化学习代理成为一个合适的选择，以应对环境变化的挑战。[^32]当考虑边缘云环境下资源调度的挑战时，传统的集中式优化模型常常面临信息不完整或不准确的问题，降低了调度效率。为了解决这一问题，近年涌现了基于随机优化的分布式资源调度方法，能够根据实时反馈信息动态调整资源分配策略。

当考虑边缘云环境下资源调度的挑战时，传统的集中式优化模型常常面临信息不完整或不准确的问题，降低了调度效率。为了解决这一问题，近年涌现了基于随机优化的分布式资源调度方法，能够根据实时反馈信息动态调整资源分配策略。在边缘云计算环境中，已出现多种服务迁移和负载调度方法，如Wang等人（2017）提出的联合计算卸载、资源分配和内容缓存的优化框架[^24]，Rahul等人（2015）提出的高效鲁棒且无需统计知识的边缘云服务迁移和负载调度算法[^25]，以及Ibrahim等人（2019）结合考虑车辆移动性和任务延迟要求的边缘计算工作负载调度方法[^26]。另外，Zhang等人（2022）提出了一种多跳传输导向的车联网云动态工作流调度方法[^27]，而Sonmez等人[^1]则采用基于模糊逻辑的方法解决边缘计算系统中的工作负载编排问题。这些方法在特定场景下表现出良好效果，但也面临着挑战，例如确保算法收敛性和稳定性，平衡探索与利用的权衡，以及减少通信开销和计算复杂度等问题。

当涉及车联网中的拍卖机制研究时，学者们着眼于解决供需不平衡、竞争不公平和信息不对称等问题，并致力于设计各种模型和算法以实现资源价格的公平确定和有效分配。近年来，在移动边缘计算（MEC）领域的研究呈现出多个方面的发展。其中，有提出一种在线激励驱动的任务分配方案，旨在最大化系统效用[^3]；还有利用最优价格为移动设备提供计算卸载服务，平衡了个体和整体系统的利益[^4]；此外，还有提出的分布式激励机制显著降低了节点的计算成本[^5]。同时，基于拍卖的激励机制也被广泛应用于MEC资源分配。一些研究设计了真实的组合双拍卖机制，激励边缘服务器为附近移动用户提供服务[^6]；另一些引入了双拍卖的动态定价策略，用于资源分配并满足经济属性[^7]；还有一些拍卖机制旨在最大化总体社会福利，激励边缘节点将虚拟机资源分配给移动设备进行计算卸载[^8]。

当今，博弈论和强化学习（RL）模型与方法在多约束过程调度问题中得到广泛应用[^9][^10]。这些方法利用博弈论中的均衡概念和多智能体训练技术，有望解决多约束和多目标优化问题。Nascimento等人[^2]提出了基于强化学习（RL）的调度方法，用于基于云的科学工作流执行。强化学习专注于代理程序如何通过在动态环境中学习来实现最优目标。Li等人[^11]提出了一种基于强化学习的车联网中的集群协作调度方法，旨在提高通信效率和可靠性。他们利用了深度确定性策略梯度算法，将集群协作调度问题建模为连续动作空间的强化学习问题，并设计了一种集中式训练和分布式执行的算法，以优化任务分配和通信资源利用。Mao等人[^12]提出了DeepRM，解决了系统和网络资源管理问题，将多重资源需求任务打包问题转化为强化学习问题。Jeunen等人[^13]讨论了在线广告拍卖的挑战，提出了一个框架，涵盖了不同的“学习出价”方法，包括基于价值、基于策略和双重稳健的公式。他们还介绍了AuctionGym，一个用于在线广告拍卖中出价策略的可靠离线验证的开源仿真环境，实证证据支持了所提方法的有效性。Kutschinski等人[^14]则利用不同类型的异步多智能体强化学习方法确定市场情境下的竞争定价策略。

本文通过MARL促进多个智能体协作学习，适应不断变化的环境，优化资源利用和策略演化；而反向拍卖则通过动态定价、资源配置和降低信息不对称，提高车辆利用率并促进公平竞争。结合两者，能够使车辆调度更智能高效，减少拥堵和资源浪费，从而优化整体交通系统的运行效率。

[^31]:Wang X, Wang X, Liang C, Huang C. A Survey on Mobile Edge Networks: Convergence of Computing, Caching and Communications. IEEE Access. 2017;5:6757-74.
[^32]:Shyalika C, Silva T, Karunananda A. Reinforcement Learning in Dynamic Task Scheduling: A Review. SN Computer Science. 2020 Sep;1(5):1-9.
[^24]:  WANG C, LIANG C, YU F R, et al. Computation Offloading and Resource Allocation in Wireless Cellular Networks With Mobile Edge Computing[J/OL]. IEEE Transactions on Wireless Communications, 2017, 16(8): 4924-4938. DOI:10.1109/TWC.2017.2703901.
[^25]:  URGAONKAR R, WANG S, HE T, et al. Dynamic service migration and workload scheduling in edge-clouds[J/OL]. Performance Evaluation, 2015, 91: 205-228. DOI:10.1016/j.peva.2015.06.013.
[^26]:  SORKHOH I, EBRAHIMI D, ATALLAH R, et al. Workload Scheduling in Vehicular Networks With Edge Cloud Capabilities[J/OL]. IEEE Transactions on Vehicular Technology, 2019, 68(9): 8472-8486. DOI:10.1109/TVT.2019.2927634.
[^27]:  ZHANG Q. Multihop Transmission-Oriented Dynamic Workflow Scheduling in Vehicular Cloud[J/OL]. Wireless Communications and Mobile Computing, 2022, 2022: 1-14. DOI:10.1155/2022/2033644.
[^1]:Sonmez C, Ozgovde A, Ersoy C (2019) Fuzzy Workload Orchestration for Edge Computing. IEEE Trans Netw Serv Manag 16(2):769–782. <https://doi>. org/10.1109/TNSM.2019.2901346
[^3]: W. Hou, H. Wen, N. Zhang, J. Wu, W. Lei, and R. Zhao, "IncentiveDriven Task Allocation for Collaborative Edge Computing in Industrial Internet of Things," IEEE Internet Things J., vol. PP, no. 99, pp. 1-1, 2021.
[^4]: L. Li, T. Q. S. Quek, J. Ren, H. H. Yang, Z. Chen, and Y. Zhang, "An Incentive-Aware Job Offloading Control Framework for Multi-Access Edge Computing," IEEE Trans. Mobile Comput., vol. 20, no. 1, pp. 63-75, 1 Jan. 2021.
[^5]: R. Chattopadhyay, and C. Tham, "Fully and Partially Distributed Incentive Mechanism for a Mobile Edge Computing Network," IEEE Trans. Mobile Comput., vol. PP, no. 99, pp. 1-1, 2020.
[^6]: L. Ma, X. Wang, X. Wang, L. Wang, Y. Shi and M. Huang, "TCDA: Truthful Combinatorial Double Auctions for Mobile Edge Computing in Industrial Internet of Things," IEEE Trans. Mobile Comput., vol. PP, no. 99, pp. 1-1, 2021.
[^7]: W. Sun, J. Liu, Y. Yue, and H. Zhang, "Double Auction-Based Resource Allocation for Mobile Edge Computing in Industrial Internet of Things, " IEEE Trans. Ind. Inf., vol. 14, no. 10, pp. 4692 - 4701, 2018.
[^8]: G. Gao, M. Xiao, J. Wu, H. Huang, S. Wang, and G. Chen, "Auction based VM allocation for deadline-sensitive tasks in distributed edge cloud," IEEE Trans. Services Comput., vol. PP, no. 99, pp. 1-1, 2020.
[^9]: .Kang Yang, Rongyu Cao, Yueyuan Zhou, Jiawei Zhang, En Shao, Guangming Tan, "Deep Reinforcement Agent for Failure-aware Job scheduling in High-Performance Computing", *2021 IEEE 27th International Conference on Parallel and Distributed Systems (ICPADS)*, pp.442-449, 2021.
[^10]: Seiju Yasuda, Chonho Lee, Susumu Date, "An Adaptive Cloud Bursting Job Scheduler based on Deep Reinforcement Learning", *2021 International Conference on High Performance Big Data and Intelligent Systems (HPBD&IS)*, pp.217-224, 2021.
[^2]:Nascimento A, Olimpio V, Silva V, Paes A, de Oliveira D (2019) A Reinforcement Learning Scheduling Strategy for Parallel Cloud-Based Workflows. In: 2019 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW). pp 817–824. <https://doi.org/10.1109/> IPDPSW.2019.00134
[^11]: Li X, Zhang Y, Wang Y, Zhang J. Cluster-Enabled Cooperative Scheduling Based on Reinforcement Learning for Vehicular Networks. IEEE Transactions on Vehicular Technology. 2020 Oct 26;70(1):1018-30.
[^12]:Mao, Hongzi, Mohammad Alizadeh, Ishai Menache, and Srikanth Kandula. 2016. “Resource Management with Deep Reinforcement Learning.” In Proceedings of the 15th ACM Workshop on Hot Topics in Networks. doi:10.1145/3005745.3005750.
[^13]:Jeunen, Olivier, et al. Learning to Bid with AuctionGym.
[^14]: Erich Kutschinski, Thomas Uthmann, and Daniel Polani. Learning competitive pricing strategies by multi-agent reinforcement learning. Journal of Economic Dynamics and Control, 27(11-12):2207–2218, 2003.

---

你的文献综述非常全面，涵盖了边缘计算、资源调度、拍卖机制和强化学习等多个领域的研究进展。以下是一些建议，可帮助你在Related Works部分进行修改：

1. **结构调整：** 尝试按照主题或者相关性对文献进行重新排序，这样读者可以更清晰地理解不同主题之间的关联和发展。

2. **文献引用：** 如果可能的话，考虑添加文献引用的具体内容，比如作者名和年份，这样读者可以更方便地追溯相关研究。

3. **简洁明了：** 尽量保持每段话的长度适中，确保表达清晰简洁，避免出现过多复杂的句子结构，有助于读者更容易理解你的论述。

4. **增加实证支持：** 如果可能的话，在描述每种方法时，可以加入一些已有研究的实证支持或者案例，以加强对这些方法有效性的描述。

5. **关注未来研究方向：** 在最后，可以考虑添加一些关于未来研究方向或者当前该领域尚未解决的挑战，为读者展示这个领域的发展前景。

示例修改：

**原文：**
"常用的调度方法包括模糊逻辑和机器学习两种方式..."

**修改建议：**
"在资源调度方面，常见的方法包括基于模糊逻辑和机器学习。模糊逻辑需要预先定义规则，而机器学习方法则动态调整任务调度策略，减少了手动干预。然而，机器学习方法的数据集可能无法适应新环境..."

这样的修改可以使得文献综述更易读，也更直接地突出了你想要表达的观点。
