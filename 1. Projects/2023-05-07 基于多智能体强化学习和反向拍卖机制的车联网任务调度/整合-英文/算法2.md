---
UID: 20231208212623 
tags: 
source: 
cssclass: 
created: "2023-12-08 21:26"
updated: "2023-12-29 14:25"
---
Algorithm 2: Multi-Agent Policy Update using PPO

Input: Edge servers set $\mathcal S$, experience replay buffer $\mathcal D$
Output: Updated policy parameters for each server

1. Initialize policy network parameters $\theta_i$ for each server $\mathbf s_i \in \mathcal S$
2. Initialize other hyperparameters, e.g., learning rate, discount factor, clip parameter, etc.
3. for each training iteration do
   1. Sample a mini-batch of experiences from $\mathcal D$: $\{(s_i, a_i, r_i, s'_i)\}$
   2. for each server $\mathbf s_i \in \mathcal S$ do
      1. Compute advantage estimates $A_i$ using the collected experiences
      2. Compute policy probabilities for selected actions using the current policy $\pi_{\theta_i}(a_i|s_i)$
      3. Compute old policy probabilities for selected actions using the old policy $\pi_{\theta_{i_{old}}}(a_i|s_i)$
      4. Compute the ratio $r_i = \frac{\pi_{\theta_i}(a_i|s_i)}{\pi_{\theta_{i_{\text{old}}}}(a_i|s_i)}$
      5. Compute the clipped surrogate objective: $L_i(\theta_i) = \mathbb{E}\left[\min(r_i A_i, \text{clip}(r_i, 1-\epsilon, 1+\epsilon) A_i)\right]$
   3. Update the policy network parameters $\theta_i$ using gradient ascent on $L_i(\theta_i)$
   4. end for
   5. Update policy parameters: $\theta_{i_{\text{old}}} \leftarrow \theta_i$
4. end for