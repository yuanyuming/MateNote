---
tags:
cssclass:
source:
created: "2023-08-28 09:24"
updated: "2023-09-11 16:52"
---
## 5. Performance Evaluation

Computing offloading is a technique that improves the performance and energy efficiency of vehicles in the Internet of Vehicles (IoV) by transferring some computation tasks to edge servers. The goal of computing offloading is to maximize the utility of vehicles, while ensuring the profitability and user experience of edge servers. To achieve this, edge servers need to design an appropriate bidding strategy, which means deciding the amount of each price offered based on the needs, urgency and constraints of the task.

Learning the bidding strategy involves complex environment modeling, incomplete information and dynamic optimization problems, which are crucial for choosing the verification method. Offline verification and online verification are common methods, but they have advantages and disadvantages. Offline verification uses historical data to perform counterfactual estimation, but the results may be affected by data bias, counterfactual assumptions and environmental changes, and may not satisfy the Donsker theorem. On the other hand, online verification tests the performance of the bidding strategy directly in real-time environment through experiments, which is more reliable, but also more costly, time-consuming and risky.

Reinforcement learning is a method that learns optimal behavior through intelligent agents interacting with environments. It has wide applications in fields such as games, robots and recommendation systems. Although it has achieved success in practice, research communities have pointed out its limitations and suggested that a reliable simulation environment is a major breakthrough in recent years. Simulation environment provides a platform that is close to real environment but not constrained or affected by it, allowing researchers to quickly develop, test and improve reinforcement learning algorithms. In some related fields, such as recommendation systems, simulation environment has been widely used as an effective evaluation mechanism.

For the verification of server pricing strategy for computing offloading in IoV environment based on the above model, we developed a corresponding reinforcement learning environment. In the bidding strategy learning process, we use reinforcement learning to assist edge servers to dynamically decide each price based on task characteristics and constraints according to their own needs.

### Simulation environment design and parameter setting

To conduct the simulation experiments for the reinforcement learning method for vehicle task offloading based on multi-agent reinforcement learning and reverse auction mechanism (RATO) in the Internet of Vehicles (IoV) environment, we developed a Python-based open-source simulation environment called VehicleJobScheduling, based on the system modeling of previous studies. The simulation environment aims to simulate the process of vehicle task offloading and provide a series of common evaluation indicators. The design of VehicleJobScheduling was inspired by Pettingzoo, which is a popular multi-agent reinforcement learning framework that defines a standard interface, allowing different algorithms to be compared in the same environment.

VehicleJobScheduling simulation environment consists of three main components: vehicles, servers and environment generator. Vehicles act as task initiators, according to their resource needs, connection restrictions and budgets, choose appropriate servers for task offloading and pay corresponding fees. Servers act as task executors, according to their resource status and cost, report prices for received task offloading requests and execute accepted tasks. Environment generator is responsible for generating task and server parameters, including task arrival rate, features, connection restrictions, as well as server number, type, resource vector, cost and so on. Users can set these parameters to explore different scenarios and strategies of task offloading.

Our environment supports Agent Environment Cycle (ACE) mode, which is an interface suitable for sequential round-based environments that can handle any multi-agent reinforcement learning algorithm. In ACE mode, each agent represents an edge server, receives vehicle task offloading requests from vehicles, uses its own resources to complete task offloading and obtains profit. In each time step, one or more vehicles send task offloading requests to a set of servers that meet their connection restrictions and wait for servers to return prices. The environment selects a server as the current round’s actor based on connection restrictions. The actor server calculates the price for the received task offloading request based on its own resource usage situation, task features and pricing strategy and returns it to the vehicle. When all servers that meet connection restrictions have returned prices after completing pricing, the vehicle selects one or more servers with the lowest prices that meet its needs and budget conditions for trading. The transaction is successful when the vehicle sends the task to the winner server and pays the corresponding amount and waits for execution results to return. The server obtains profit by allocating resources and executing task offloading. If no server’s price meets the vehicle’s condition then the vehicle cannot complete task offloading , does not pay any amount.

To evaluate the performance and effect of task offloading ,the environment provides some indicators ,including load balance ,server profit ,vehicle utility and task completion rate .Load balance measures the degree of resource utilization balance among all servers ,server profit reflects the economic benefit obtained from task offloading by servers ,vehicle utility reflects the service quality obtained from task offloading by vehicles ,task completion rate reflects the success rate of task offloading .These indicators help evaluate different strategies of task offloading .



