---
cssclass: null
source: null
created: 2023-08-28 09:24
updated: 2023-09-11 16:52
date updated: 2023-12-03 16:35
---

## 5. Performance Evaluation

Computational offloading is a technique in vehicular networks aimed at enhancing vehicle performance and conserving energy by transferring a portion of computational tasks to edge servers. The objective of computational offloading is to maximize vehicle utility while ensuring the profitability of edge servers and user experience. To achieve this, edge servers need to design appropriate bidding strategies, determining the bid amount for each transaction based on task requirements, urgency, and their own resource constraints.

Learning bidding strategies involves complex environment modeling, incomplete information, and dynamic optimization problems, making the choice of validation methods crucial. Offline validation and online validation are common approaches, each with its pros and cons. Offline validation relies on historical data for counterfactual estimation, but results may be influenced by data biases, counterfactual assumptions, and environmental changes, potentially violating the Goodhart's Law. In contrast, online validation directly tests the performance of bidding strategies in a real environment through experiments, offering more reliability. However, it comes with higher costs, time consumption, and some level of risk.

Reinforcement learning, as a method for learning optimal behavior through interactions between agents and environments, has shown promising applications in various fields such as gaming, robotics, and recommendation systems. Despite its practical successes, the research community has acknowledged its limitations, emphasizing the significance of reliable simulation environments as a major recent advancement. Simulation environments provide a platform that closely resembles the real environment but is free from its constraints and influences, enabling researchers to rapidly develop, test, and enhance reinforcement learning algorithms. In certain relevant domains, such as recommendation systems, simulation environments have been widely adopted as effective evaluation mechanisms.

For the validation of server bidding strategies in the context of computational offloading in vehicular networks, we have developed a corresponding reinforcement learning environment based on the aforementioned model. In the process of bidding strategy learning, we leverage reinforcement learning to assist edge servers in dynamically determining bid amounts based on task offloading features and their own constraints, aiming to maximize utility or profitability.

### Simulation environment design and parameter setting

To validate the effectiveness of the proposed Vehicle-to-Edge task Offloading (RATO) method based on multi-agent reinforcement learning and reverse auction mechanism, we developed an open-source simulation environment named "VehicleJobScheduling" using Python. This simulation environment aims to simulate the process of vehicle task offloading and provides a set of commonly used evaluation metrics. In its design, we drew inspiration from PettingZoo, a popular multi-agent reinforcement learning framework that defines a standard interface, enabling different algorithms to be compared within the same environment.

: PettingZoo: Gym for Multi-Agent Reinforcement Learning

The VehicleJobScheduling simulation environment is primarily composed of three components: vehicles, servers, and environment generator. In this environment, vehicles act as task initiators, selecting an appropriate server for task offloading based on resource requirements, connection constraints, and budget, and subsequently paying the corresponding fee. Servers, on the other hand, act as task executors, quoting and executing accepted task offloading requests based on resource availability and cost. The environment generator is responsible for generating parameters for tasks and servers, including task arrival rate, features, connection constraints, as well as the number, type, resource vectors, and costs of servers. Users can explore different task offloading scenarios and strategies by setting these parameters.

Our simulation environment supports the Agent Environment Cycle (ACE) mode, an interface suitable for sequential turn-based environments that can accommodate any multi-agent reinforcement learning algorithm. In ACE mode, each agent represents an edge server, receiving task offloading requests from vehicles, using its own resources to complete task offloading, and earning profits. At each time step, one or more vehicles send task offloading requests to the set of servers that meet their connection constraints, waiting for server quotations. The environment, based on the connection constraints, selects a server as the current round's actor. This server calculates task quotations based on its resource usage, task features, and quotation strategy, and returns them to the vehicles. Once all servers meeting the connection constraints have submitted their quotations, vehicles select one or more servers with the lowest quotations that simultaneously meet their requirements and budget conditions for transactions. After a successful transaction, vehicles send tasks to the winning server, pay the corresponding amount, and wait for the execution results. Servers gain profits, allocate resources for tasks, and execute task offloading. If no server's quotation meets the vehicle's conditions, the vehicle is unable to complete task offloading and incurs no cost.

To evaluate the performance and effect of task offloading ,the environment provides some indicators ,including load balance ,server profit ,vehicle utility and task completion rate .Load balance measures the degree of resource utilization balance among all servers ,server profit reflects the economic benefit obtained from task offloading by servers ,vehicle utility reflects the service quality obtained from task offloading by vehicles ,task completion rate reflects the success rate of task offloading .These indicators help evaluate different strategies of task offloading .

We abstracted the task unloading requests for vehicles, representing them as task features with four dimensions: priority, resource requirements, duration, and task connection constraints. The priority of tasks ranges from 0 to 10, influencing the budget allocation for vehicle-task assignments. The maximum requests for CPU and memory per task are 24 and 100, respectively. Task duration is set from 1 to 10 time slots, each slot representing 1 minute, resulting in a maximum execution time of 10 minutes per task. Task connection constraints range from 0 to 3, indicating the number of servers to which a task unloading request can be sent based on channel conditions.

In our experiments, we chose [Azure Dedicated Hosts](https://learn.microsoft.com/en-us/azure/virtual-machines/dedicated-hosts) as edge servers, offering various virtual machine specifications and prices. We selected three different Dedicated Host SKUs representing large, medium, and small edge servers. The cost from cloud service providers is calculated based on a three-year plan, while market prices are derived from Azure's US East [Container Instances](https://azure.microsoft.com/en-us/pricing/details/container-instances/) pricing. Table 1 presents the specifications and prices of the chosen Dedicated Hosts.

| Dedicated Host SKUs | vCPUs | RAM       | 3 year plan |
| ------------------- | ----- | --------- | ----------- |
| Mdsv2MedMem-Type1   | 192   | 2,048 GiB | $ 5.81      |
| Easv5-Type1         | 112   | 768 GiB   | $ 3.49      |
| Fsv2-Type2          | 72    | 144 GiB   | $ 2.05      |

Table 1 Chosen Dedicated Hosts

| Parameter                      | Range                                     |
| ------------------------------ | ----------------------------------------- |
| Task arrival rate distribution | Poisson distribution                      |
| Tasks per unit time            | 20                                        |
| Task duration                  | 1 to 10 time slots, each slot is 1 minute |
| Task priority                  | 0 to 10                                   |
| Task CPU requests              | Maximum 24                                |
| Task memory requests           | Maximum 100                               |
| Task connection constraints    | 0 to 3                                    |

Table 2 Environmental hyperparameters

| Agent Model Type                 | PPO Agent                    | PPO LTSM Agent                      |
| -------------------------------- | ---------------------------- | ----------------------------------- |
| Network architecture             | Multi-Layer Perceptron (MLP) | MLP + Long Short-Term Memory (LSTM) |
| Fully connected layer parameters | [384,384,384]                | [384,384,384]                       |
| LSTM unit size                   | -                            | 256                                 |
| Activation function              | tanh                         | tanh                                |

Table 3 Reinforcement learning agent parameters

In this study, we employed two different reinforcement learning agent models, namely the PPO Agent and PPO LTSM Agent. Both are based on the Proximal Policy Optimization (PPO) algorithm, differing in their network structures. The PPO Agent utilizes a Multi-Layer Perceptron (MLP) with three fully connected layers, each with 384 neurons and a tanh activation function. The PPO LTSM Agent extends the PPO Agent's model by incorporating an LSTM layer with a unit size of 256, enhancing the model's memory capacity to handle and predict time-correlated data. We applied these two models to the RATO method to compare their performance in different scenarios.

This configuration allows us to consider diverse vehicular networking scenarios in simulation, encompassing tasks of varying types and durations, as well as dynamic interactions between vehicles and edge servers. Subsequently, we will conduct a numerical analysis of the RATO method in this simulation environment to comprehensively evaluate its performance in different scenarios. We will specifically focus on the impact of different server pricing strategies on system effectiveness to gain deeper insights into the advantages and applicability of the proposed method.

### Convergence Analysis of Deep Reinforcement Learning Agents

![[Pasted image 20231128145930.png]]

Figure 1: Mean Reward Estimations of PPO and PPO+LTSM over the Last 448,000 Iterations

In this section, we assess the convergence performance of two approaches employing Proximal Policy Optimization (PPO) reinforcement learning agents—PPO and PPO+LTSM—in the context of vehicular network resource offloading. We utilize two multi-agent reinforcement learning algorithms, PPO and PPO+LTSM, to train multiple agents with the objective of maximizing the server's revenue within a single round. The server's revenue reflects the effectiveness of task offloading and serves as the reward function for the agents. We conduct training for the agents over 4,480,000 iterations and evaluate the algorithms every 40,000 iterations, calculating the average reward of the training policy over 10 test runs. Figure 1 illustrates the reward evaluation during the training process, while Table 1 presents the mean and standard deviation of the rewards for PPO and PPO+LTSM over the last 448,000 iterations.

| Algorithm | Average Reward | Standard Deviation |
| --------- | -------------- | ------------------ |
| PPO       | $1.68×10^7$    | $2.65×10^5$        |
| PPO+LTSM  | $1.72×10^7$    | $1.12×10^5$        |

Table 1 summarizes the variance and mean of the rewards for the last 10% of iterations during the training process for PPO and PPO+LTSM.

It can be observed that PPO+LTSM has an average reward of $1.72×10^7$, approximately 2.4% higher than PPO's $1.68×10^7$. Additionally, PPO+LTSM exhibits a standard deviation of $1.12×10^5$, approximately 57.7% lower than PPO's $2.65×10^5$. These results indicate that not only does PPO+LTSM achieve a higher average reward, but it also demonstrates greater stability with significantly reduced fluctuations. Consequently, PPO+LTSM exhibits superior convergence performance in vehicular network resource offloading. In conclusion, the RATO method demonstrates commendable convergence in various task offloading scenarios, rapidly learning optimal or near-optimal bidding strategies, thereby enhancing server revenue and vehicle utility. This validates the effectiveness and robustness of the RATO method, highlighting the advantages and applicability of reinforcement learning and reverse auction mechanisms.

### Experimental Results and Analysis

This paper introduces a vehicular network task offloading method based on multi-agent reinforcement learning and reverse auction mechanism, referred to as RATO. Comparative analyses were conducted through simulation experiments with other benchmark methods. The study considered three scenarios with task arrival numbers of 10, 20, and 40, employing four different server bidding strategies: Fixed Bid (FB), Random Bid (RB), PPO-based Bid (PPO), and PPO+LSTM-based Bid (PPO+LSTM). Four metrics—Load Balance, Server Earning, Vehicle Utility, and Complete Rate—were utilized to evaluate the performance of different scheduling strategies under varying arrival numbers. Ten independent simulation experiments were conducted for each arrival number, using five random seeds for simulation. The mean and standard deviation of each metric were calculated and analyzed.

The objective of this study is to compare the performance of different bidding strategies in a task offloading system and analyze the advantages of the RATO method. Four bidding strategies were considered:

- **Fixed Strategy**: In this strategy, servers use market prices as a reference and determine bids solely based on task features and duration.
- **Random Strategy**: Servers use market prices as a reference and randomly sample a coefficient within a certain range to determine bids.
- **PPO Strategy**: Servers employ the PPO reinforcement learning algorithm to dynamically determine bids based on their resource usage and task features through a Multi-Layer Perceptron (MLP).
- **PPO+LSTM Strategy**: Building upon the PPO strategy, servers add a Long Short-Term Memory (LSTM) layer to capture temporal information and dynamically bid using a combination of an MLP.

|          | Load Balance | Server Earning (10^6) | Vehicle Utility (10^6) | Complete Rate |
| -------- | ------------ | --------------------- | ---------------------- | ------------- |
| Random   | 0.19±0.012   | 0.55±0.00037          | 0.55±0.037             | 0.76±0.014    |
| Fixed    | 0.20±0.005   | 1.60±0.00007          | 0.76±0.047             | 0.91±0.010    |
| PPO      | 0.20±0.011   | 1.49±0.00004          | 0.78±0.036             | 0.92±0.010    |
| PPO+LTSM | 0.18±0.010   | 1.53±0.00004          | 0.83±0.063             | 0.93±0.012    |

Table 4: Experimental Results for Arrival Number 10

|          | Load Balance | Server Earning (10^6) | Vehicle Utility (10^6) | Complete Rate |
| -------- | ------------ | --------------------- | ---------------------- | ------------- |
| Random   | 0.22±0.007   | 2.74±0.00036          | 0.99±0.033             | 0.72±0.008    |
| Fixed    | 0.23±0.004   | 3.05±0.00014          | 1.43±0.040             | 0.90±0.005    |
| PPO      | 0.22±0.007   | 2.95±0.00011          | 1.53±0.060             | 0.91±0.007    |
| PPO+LTSM | 0.21±0.006   | 3.06±0.00009          | 1.62±0.036             | 0.93±0.009    |

Table 5: Experimental Results for Arrival Number 20

|          | Load Balance | Server Earning (10^6) | Vehicle Utility (10^6) | Complete Rate |
| -------- | ------------ | --------------------- | ---------------------- | ------------- |
| Random   | 0.22±0.006   | 4.80±0.00100          | 1.64±0.034             | 0.63±0.007    |
| Fixed    | 0.21±0.004   | 4.73±0.00071          | 2.26±0.034             | 0.73±0.010    |
| PPO      | 0.21±0.003   | 4.68±0.00077          | 2.29±0.054             | 0.72±0.009    |
| PPO+LTSM | 0.20±0.006   | 4.66±0.00075          | 2.34±0.044             | 0.73±0.009    |

Table 6: Experimental Results for Arrival Number 40

In this section, we present the simulation experiment results in tabular and graphical form to showcase the performance of different strategies under varying task arrival numbers. Tables 1, 2, and 3 provide the mean and standard deviation of each metric for task arrival numbers 10, 20, and 40.

In the horizontal comparison, we delve into the analysis of four distinct bidding strategies to comprehensively evaluate their performance across various metrics. The PPO+LTSM strategy excels in all metrics, such as Load Balance, Server Earning, Vehicle Utility, and Task Completion Rate, with values of 0.21±0.006, 3.06±0.00009, 1.62±0.036, and 0.93±0.009, respectively, outperforming all other strategies. The Fixed strategy performs well in server earnings but exhibits lower performance in Load Balance, Vehicle Utility, and Task Completion Rate, indicating lower user satisfaction. The Random strategy performs consistently poorly across all metrics. While the PPO strategy slightly lags behind the Fixed strategy in server earnings, it outperforms in other metrics. However, compared to PPO+LTSM, PPO exhibits higher variance in all four metrics, indicating relatively greater instability. Overall, the PPO+LTSM strategy demonstrates superior long-term performance.

In the vertical comparison, we closely observe the four bidding strategies under different task arrival numbers. As the task arrival number increases, server earnings and utilization generally rise, while the completion rate experiences a downward trend, possibly due to server overload preventing additional tasks. For the scenario with a task arrival number of 10, where tasks are relatively scarce, all strategies, except for the Random strategy, exhibit similar performance in Load Balance and Task Completion Rate. PPO+LSTM shows a slight advantage in Vehicle Utility and Load Balance, demonstrating better adaptability and flexibility. The higher income of the Fixed strategy may be attributed to its fixed bids aligning with the budgets of most tasks and encountering less competition among servers. When the task arrival number increases to 20, PPO+LTSM surpasses other strategies in all metrics as the competition between servers intensifies. The Load Balance value is higher at this point, suggesting increased server competition and potential load imbalance. However, as the task arrival rate further increases, server load remains consistently high, leading to relatively balanced loads. In the scenario with a task arrival number of 40, PPO+LTSM continues to maintain its lead, achieving better Load Balance, Vehicle Utility, and Task Completion Rate. The Random strategy attains the highest income in this scenario, possibly due to its adaptability to different tasks and competitive environments, avoiding excessive competition and price wars. Nevertheless, it is essential to note that the Random strategy may lack stability over the long term, and its occasional good performance could be a result of randomness. In real-world applications, strategies with stronger adaptability and stability are preferable. In summary, under different task arrival numbers, the PPO+LTSM strategy performs optimally in Load Balance and Vehicle Utility, demonstrating superior adaptability and performance. It maximizes long-term rewards, effectively balancing various system metrics, and providing balanced optimization for system performance under different task arrival numbers.

In conclusion, this paper conducted simulation experiments to compare the performance of different bidding strategies under varying task arrival numbers and analyzed the advantages of the PPO+LTSM strategy. The study reveals that the PPO+LTSM strategy utilizes LSTM to capture temporal information, enabling more rational decision-making and achieving the most effective load balance.
