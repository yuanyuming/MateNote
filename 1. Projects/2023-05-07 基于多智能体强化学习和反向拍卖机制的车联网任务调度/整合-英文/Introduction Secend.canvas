{
	"nodes":[
		{"id":"828f8d6eef86ddf6","type":"text","text":"### A DRL Agent for Jointly Optimizing Computation Offloading and Resource Allocation in MEC\n\nChen, Juan, Huanlai Xing, Zhiwen Xiao, Lexi Xu和Tao Tao. 《A DRL Agent for Jointly Optimizing Computation Offloading and Resource Allocation in MEC》. _IEEE Internet of Things Journal_ 8, 期 24 (2021年12月15日): 17508–24. [https://doi.org/10.1109/JIOT.2021.3081694](https://doi.org/10.1109/JIOT.2021.3081694).\n\n\nTraditional approaches, such as convex approximation [5], [6], game theory [7]–[9], and metaheuristics [4], [10], are not suitable for addressing dynamic JCORA problems in MEC. These methods usually suffer from exponentially increasing search space and heavy computational burdens, especially in large-scale scenarios. Adopting deep reinforcement learning (DRL) to handle JCORA problems has attracted increasingly more research interests because of the powerful deep neural networks (DNNs) [3], [12]–[20]. Deep deterministic policy gradient (DDPG) [16], [18] shows excellent potential when coping with dynamic optimization problems with a continuous action space. More details can be found in Section II. \n\nIn the traditional DDPG, both the actor and critic networks are based on fully connected networks (FCNs). However, FCNs generally suffer from two drawbacks. One is the huge number of trainable parameters, which increases the difficulty for training and significantly consumes computing resources. The other is that FCNs only extract the global discriminative state and action policy features, ignoring the temporal variation of task sequences that may contain useful shapelets for function approximation. Moreover, experience transitions are uniformly sampled from the replay buffer. This approach treats all experiences equally, regardless of their significance. However, ignoring the importance of valuable experiences usually leads to poor stability and slow convergence during training. \n\nTo overcome the disadvantages above, this article proposes temporal attentional deterministic policy gradient (TADPG), an improved DDPG agent for tackling the decentralized JCORA problem in a dynamic MEC environment. Running a TADPG agent on each MD requires fewer control costs between this MD and its corresponding MEC server. It is thus more suitable for large-scale scenarios, compared with those centralized JCORA methods. Our main contributions are summarized as follows.\n\n- Traditional approaches for dynamic Joint Computation Offloading and Resource Allocation (JCORA) problems in Mobile Edge Computing (MEC) include convex approximation [5], [6], game theory [7]–[9], and metaheuristics [4], [10].\n- These methods face challenges such as exponentially increasing search space and heavy computational burdens, particularly in large-scale scenarios.\n- The use of deep reinforcement learning (DRL) to tackle JCORA problems has gained increasing research interest, leveraging the power of deep neural networks (DNNs) [3], [12]–[20].\n- Deep Deterministic Policy Gradient (DDPG) [16], [18] has shown excellent potential in dealing with dynamic optimization problems with a continuous action space.\n- More details on these approaches can be found in Section II.\n- In traditional Deep Deterministic Policy Gradient (DDPG), both the actor and critic networks rely on fully connected networks (FCNs).\n- FCNs have two significant drawbacks: a large number of trainable parameters, making training difficult and consuming computing resources, and the limitation of extracting only global discriminative state and action policy features.\n- FCNs neglect the temporal variation of task sequences, potentially overlooking useful shapelets for function approximation.\n- Additionally, in traditional DDPG, experience transitions are uniformly sampled from the replay buffer, treating all experiences equally regardless of their significance.\n- This uniform sampling approach ignores the importance of valuable experiences, often resulting in poor stability and slow convergence during training.\n- To address the drawbacks mentioned earlier, this article introduces Temporal Attentional Deterministic Policy Gradient (TADPG), an enhanced agent derived from DDPG.\n- TADPG is designed to handle the decentralized Joint Computation Offloading and Resource Allocation (JCORA) problem within a dynamic Mobile Edge Computing (MEC) environment.\n- The TADPG agent is deployed on each Mobile Device (MD), resulting in lower control costs between the MD and its corresponding MEC server.\n- This makes TADPG more suitable for large-scale scenarios compared to centralized JCORA methods.\n- The main contributions of this work can be summarized as follows.\n","x":-140,"y":-693,"width":527,"height":1761},
		{"id":"4377f960d6d505c5","type":"text","text":"### A Truthful Reverse-Auction Mechanism for Computation Offloading in Cloud-Enabled Vehicular Network\n\nLiwang, Minghui, Shijie Dai, Zhibin Gao, Yuliang Tang和Huaiyu Dai. 《A Truthful Reverse-Auction Mechanism for Computation Offloading in Cloud-Enabled Vehicular Network》. _IEEE Internet of Things Journal_ 6, 期 3 (2019年6月): 4214–27. [https://doi.org/10.1109/JIOT.2018.2875507](https://doi.org/10.1109/JIOT.2018.2875507).\n\n\nHowever, mobile edge servers may still experience signal coverage limitations and resource constraints in cases of high user density, especially during high-traffic periods. Therefore, mobile device cloud (MDC) [7], [8] technology has been applied as a strategy for offloading computation-intensive applications to nearby mobile devices with idle resources. Compared with location-fixed mobile edge clouds, the offloading scheme in an MDC environment possesses advantages of infrastructure independency and economic efficiency.\n\nIn this paper, we investigate a Vickrey–Clarke–Groves (VCG)-based reverse auction mechanism for V2V computation offloading that addresses the aforementioned issues while considering specific features such as opportunistic connections and different V2V channel conditions in cloud-enabled vehicular networks. Specifically, we envision an offloading market containing several auction groups1 with multiple buyers (service requestors) and sellers (service providers). Groups are managed by centralized brokers (RSUs) with innovative policies that can preserve truthfulness and individual rationality. This paper makes the following contributions.\n\n- **Introduction:**\n  - Mobile edge servers face signal coverage limitations and resource constraints, especially during high user density and traffic periods.\n  - Mobile device cloud (MDC) technology is applied for offloading computation-intensive applications to nearby mobile devices with idle resources.\n  - Advantages of MDC include infrastructure independency and economic efficiency.\n- **Research Objective:**\n  - Investigating a Vickrey–Clarke–Groves (VCG)-based reverse auction mechanism for V2V computation offloading in cloud-enabled vehicular networks.\n  - Addressing issues such as signal coverage limitations, resource constraints, and specific features like opportunistic connections and different V2V channel conditions.\n- **Auction Mechanism:**\n  - Envisioning an offloading market with multiple auction groups, each containing buyers (service requestors) and sellers (service providers).\n  - Groups are managed by centralized brokers (RSUs) with innovative policies ensuring truthfulness and individual rationality.\n- **Contributions of the Paper:**\n  - Investigates VCG-based reverse auction mechanism for V2V computation offloading.\n  - Addresses issues in cloud-enabled vehicular networks, including opportunistic connections and varied V2V channel conditions.\n  - Envisions an offloading market with centralized brokers managing auction groups for economic efficiency.\n\n\n![[Pasted image 20231219141124.png]]\nTABLE II OPTIMAL VCG-BASED REVERSE AUCTION\n\n![[Pasted image 20231219141141.png]]\nTABLE III PROPOSED MAT C H I N G ALGORITHM IN REVERSE AUCTION","x":-140,"y":1115,"width":527,"height":2186},
		{"id":"bdc8ca778f55da5e","type":"text","text":"### Deep Reinforcement Learning Based Resource Management for Multi-Access Edge Computing in Vehicular Networks\n\nPeng, Haixia, 和Xuemin Shen. 《Deep Reinforcement Learning Based Resource Management for Multi-Access Edge Computing in Vehicular Networks》. *IEEE Transactions on Network Science and Engineering* 7, 期 4 (2020年10月1日): 2416–28. <https://doi.org/10.1109/TNSE.2020.2978856>.\n\n![[Pasted image 20231219142015.png]]\n\n![[Pasted image 20231219142031.png]]\n\nTo address the challenges on implementing MEC-based vehicular networks, many research works have been performed recently, including design of architecture, task offloading scheme, resource management scheme, and so on. For example, the MEC-based hierarchical vehicular network framework, comprised of vehicle level’s on-board computing/ storing resources and server level’s resources (resources placed at the MEC and cloud-computing servers), has been investigated in [10], [12], [14]–[16]. To better manage the spectrum/computing/storing resources among and make task offloading decisions to vehicle users, task offloading and resource management schemes have been proposed in [10], [15]–[17]. Since task offloading and spectrum/computing resource allocation are coupled with each other, the objectives of the most existing works have been achieved by jointly optimizing these two parts with traditional optimization methods [10], [15]. However, only one or two dimensions of resources have been considered in most of the existing schemes, which cannot be directly adopted to support some vehicular applications where high dimensional resources are involved, such as the computing tasks generated by the leading vehicle for platoon/convoy control [7]. Moreover, there are also some works focusing on multi-dimensional resources management in the scenarios with low mobility users [18], [19]. For MEC-based vehicular networks, the computational complexity of multi-dimensional resource management problem increases due to the high vehicle mobility and time-varying demand on resources, therefore increasing the time consumption on the resource management scheme itself. Thus, it is infeasible to adopt the pure optimization approachbased schemes to achieve multi-dimensional resource management in MEC-based vehicular networks, especially for the scenarios with delay-sensitive applications. How to design practical and QoS-oriented multi-dimensional resource management schemes for the MEC-based vehicular networks still needs efforts.\n\nAs is known, artificial intelligence (AI) technology, especially reinforcement learning (RL), can be exploited to solve resource management problems quickly [20]–[23]. Q-learning [16], [24], deep Q-learning [25]–[27], actor-critic [18], [28], and other deep RL algorithms have been widely exploited for resource management in wireless communication networks. Inspired by the existing works and considering the dynamic vehicular network environment caused by high vehicle mobility and heterogeneous applications, we investigate how to exploit deep RL to jointly manage the spectrum, computing, and storing resources to support delay-sensitive applications in the MEC-based vehicular network [12] in this paper. Specifically, the main contributions of this work can be summarized as follows,\n\n","x":419,"y":-693,"width":527,"height":2782},
		{"id":"ec67400a03415c26","type":"text","text":"### DRL-Based Long-Term Resource Planning for Task Offloading Policies in Multiserver Edge Computing Networks\n\nLi, Haiyuan, Karcius Day R. Assis, Shuangyi Yan和Dimitra Simeonidou. 《DRL-Based Long-Term Resource Planning for Task Offloading Policies in Multiserver Edge Computing Networks》. *IEEE Transactions on Network and Service Management* 19, 期 4 (2022年12月): 4151–64. <https://doi.org/10.1109/TNSM.2022.3191748>.\n\nIn previous studies, some of the task offloading algorithms assumed infinite computing resources on the MEC server for offloading operations [6], [7]. However, exploding computingintensive tasks and the imbalanced request distribution are challenging the computing capability of MEC servers in bearer networks [8], [9]. The limited MEC resources require offload policies to consider long-term rewards and bring collaboration between multiple MECs. On the other side, to improve the utilization of limited MEC server resources, a large body of literature has focused on designing offloading resource management algorithms based on centralized [7], [10]–[12] and distributed approaches [8], [13], [14].\n\nAccording to the number of servers, MEC systems in the current papers are divided into single-server [13], [15]–[17] or multiple-server [9]–[11], [18], [19] systems. Meanwhile, the techniques to tackle computation offloading and resource allocation in those models are classified into optimizationbased [10], [11], [13], [19] or machined learning based [16], [17], [20], respectively.\n\nRegarding the single-server system, Lyu et al. [13]introduced a semi-distributed approach that jointly optimizes offloading decisions and resource allocation problems. With a heuristic offloading algorithm (HODA), they achieved superior system utility with an acceptable complexity of O(K 3). Nevertheless, the single-server system cannot deal with the coexistence between idle and over occupied servers because of the inevitable unbalanced request distribution [21], [22]. Under the same system model, Li et al. [16] designed a centralizedbased DRL-based model to simultaneously resolve offloading decisions and computing resource allocations. However, excess users will cause the explosion of the action space of the DRLbased algorithm, which causes the model fail to converge. In comparison, Chen et al. [7] applied multi-agent policybased DRL model that distributed the artificial intelligence (AI) agent to the edge devices to reduce the dimension of action space. However, this approach comes at the expense of complexity and might bring burdens on the devices because of their weak computing abilities.\n\nIn order to further improve the workload carrying capability of the MEC network, multiple access in RAN was introduced as a promising technology to achieve the multi-server system. Li et al. [10], Nduwayezu et al. [12] and Xue and An [11] explored the centralized joint optimization approach for the multi-access system. First fit (FF) algorithm [23] was used to select the offloading destination for the offloading requests in the network. In comparison, Apostolopoulos et al. proposed a distributed approach towards determining the optimal data oflfoading of each user within a multiple MEC servers system by non-cooperative game among the users [8]. However, their assumption that all edge devices have access to the same set of servers neglected the geographical proximity factor in practical networks. In contrast, Kan et al. [24] designed a model where edge devices can only connect to their proximate servers. The multi-server system was achieved by relaying the workload among servers via the wired interface (Mp3) which is designed and standardized for the workload transition between MEC servers by the European Telecommunications Standards Institute (ETSI) [25]. Furthermore, Qian et al. designed a distributed algorithm and a centralized online DRL-based solution for statistic and dynamic channel scenarios, respectively, which can realized the selection of access point with minimized transmission cost [14].\n\nAlthough extensive research has been carried out on the offloading management technology in single and multiple server scenarios, few studies have been reported with consideration of long-term resource balance for offloading operations. Offloading without long-term resource balance will prefer instant high reward at the expense of the resource shorting for following offloading requests.\n\nIn summary, three critical issues need to be resolved in designing efficient offloading management policies. First, the prior joint offloading algorithms in multi-server systems ignored the geographic proximity between edge devices and servers. FF-based solutions did not fundamentally deal with the unbalanced traffic distribution. Second, the possible action space explosion in DRL-based solutions remained unclear. Last but not least, the lack of consideration of long-term reward in the current studies hindered further improvement in network resource utilization. To overcome these obstacles, the main contributions of this paper are as follows:\n\nTo the best of our knowledge, this is the first time that the optimization-based method and DRL have been geared together to solve two interrelated subproblems and find the policies that maximize long-term offloading benefits. Regarding the joint solution between edge devices in one time slot, our results show that there is a great benefit in the execution time by sorting the execution order of requests and reusing the released resources for the on processing workloads. In addition, a period of continuous time slots is taken to illustrate the long-term reward of random allocation, over allocation, and three DRL-based allocation policies. We prove that over-allocation used by the prior studies can not get reliable results as it pursues higher rewards at the expense of future loss. In comparison, DRL-based algorithms can adapt to the diversity of future network states and obtain resource allocation strategies with better performance. Furthermore, of these three DRL algorithms, compared to single-agent DQN, the cooperative multi-agent model is able to more accurately account for interactions between servers on limited resources and achieve higher average rewards. In the end, the value of server cooperation is also justified.","x":979,"y":-693,"width":527,"height":2205},
		{"id":"93ea1152bc6b1264","type":"text","text":"### Efficient Task Scheduling for Servers with Dynamic States in Vehicular Edge Computing\n\nWu, Yalan, Jigang Wu, Long Chen, Jiaquan Yan和Yuchong Luo. 《Efficient Task Scheduling for Servers with Dynamic States in Vehicular Edge Computing》. *Computer Communications* 150 (2020年1月): 245–53. <https://doi.org/10.1016/j.comcom.2019.11.019>.\n\nIn VEC, it is critical to save energy consumption and accelerate the computation process by offloading a particular task to the roadside unit (RSU) which is placed with an edge server [7]. Therefore, most existing works in VEC focus on the algorithm design of task offloading, to minimize the response time of tasks or energy consumption. Authors in [5,8] propose adaptive algorithms for task offloading to minimize the average response time of tasks. Authors in [9] propose an efficient algorithm by contract theoretical modeling to minimize the network delay. Authors in [10] propose an intelligent offloading ∗ Corresponding author. E-mail address: <asjgwucn@outlook.com> (J. Wu). system by leveraging deep reinforcement learning to maximum the total computation and communication rates. Authors in [11] propose an energy-efficient algorithm for resource allocation in VEC. In addition, the offloading algorithm is proposed to minimize the weighted sum of energy consumption and latency [12].\n\nDue to geographical difference of vehicles, the tasks are distributed in different locations. Therefore, the load of edge servers is serious unbalanced, which results in high latency of tasks and energy consumption. However, the task scheduling algorithm in edge servers is only considered in few existing works on VEC. A heuristic algorithm for energy-efficient scheduling is proposed to minimize the energy consumption [7]. A scheduling algorithm based on ant colony optimization is proposed to minimize the completion time of jobs [13]. An algorithm for partial offloading scheduling and power allocation is proposed to minimize the weighted sum of the execution delay and energy consumption in mobile edge computing [14]. Meanwhile, authors in [15] propose a task scheduling algorithm for the applications with computation intensive and time-sensitive to minimize the completion time of tasks.\n\nThe RSUs, placed with edge servers, are densely distributed in the VEC system to guarantee the quality of service. Besides, the spatial distribution of vehicles has much difference. For example, vehicles are mainly distributed in city, especially in hot spots, instead of the remote districts. Thus, the task requests are greatly varied in space. For saving energy, some RSUs switch their state into sleep state, when there are few task requests [16–19]. Authors in [20,21] propose algorithms to maximize the energy efficiency by designing the sleeping strategies for base stations. Authors in [22] propose an efficient algorithm to minimize the energy consumption by jointing the cell association and on–off scheme. Authors in [23] propose a novel user-centric energyaware mobility management scheme to minimize the delay of tasks in the mobile edge computing, in which the candidate base stations randomly switch their states between sleep and work.\n\nTo the best of our knowledge, the existing works in VEC mainly target on the area in the algorithm designing for task offloading, only a few of them investigate the algorithms for task scheduling. Meanwhile, none of the existing works in VEC consider that the edge servers may turn to sleep state in some cases. Therefore, in this paper, we investigate the problem of scheduling the tasks in VEC to minimize the response time, in the case of that the edge servers may switch their state between sleep and work according to the traffic of the requests. The contributions of this paper are summarized as follows.\n\n- We propose a novel problem of task scheduling for minimizing the total delay of tasks in VEC, with considering that the edge servers in RSUs can dynamically switch their states between sleep and work. Meanwhile, we model task requests generated by vehicles as an independent Poisson stream, and we model an edge server in the RSU as a simple M/M/1 queuing system. Besides, the proposed problem of minimizing the total delay of tasks is formulated and the NP-hardness is proved in this paper.\n- To solve the proposed problem, we contribute a greedy algorithm by carefully choosing the edge server so that the response time of the task arrived at the current time is minimum. Meanwhile, we customize a tabu search algorithm, which can successfully refine the solution generated by the greedy algorithm.\n- We also propose a deep Q-network based algorithm by utilizing the deep reinforcement learning algorithm to learn the optimal scheduling policy for minimizing the total delay of tasks, without having a priori knowledge of dynamic statistics.\n- Simulation results show that, our proposed algorithms outperform the random algorithm also proposed in this paper in terms of the total response time of tasks. Especially, the deep Q-network based algorithm performs better than the other algorithms in terms of the total response time of tasks. For example, for the case of that the maximum tolerant response time of each task is 14 s, the total response time of tasks decreases by 24.13%, 28.73% and 35.95%, compared with the customized tabu search algorithm, the greedy algorithm and the random algorithm, respectively.","x":979,"y":1575,"width":527,"height":1955},
		{"id":"59e271b6d698153c","type":"text","text":"### A Computing Offloading Resource Allocation Scheme Using Deep Reinforcement Learning in Mobile Edge Computing Systems\n\nLi, Xuezhu. 《A Computing Offloading Resource Allocation Scheme Using Deep Reinforcement Learning in Mobile Edge Computing Systems》. _Journal of Grid Computing_ 19, 期 3 (2021年9月): 35. [https://doi.org/10.1007/s10723-021-09568-w](https://doi.org/10.1007/s10723-021-09568-w).\n\nMEC can effectively reduce the delay cost of service delivery and network operation by deploying servers close to mobile users. And it has been certified by European 5G Infrastructure Public Private Partnership (PPP) as one of key technologies of the next-generation 5G network. The advantages of MEC include low latency, high bandwidth, real time wireless network information and location awareness [7,8]. The MEC-based Internet of Vehicles (IoV) allows mobile vehicles to offload computing tasks to network edge nodes for processing, which helps to achieve the ultra-low latency requirements of IoV [9]. MEC sinks cloud services to the edge of wireless access network and provides computing services near moving vehicles [10]. However, in the MEC-based vehicle-connected network, there are various computationally intensive and delay sensitive computational tasks. Moreover, each task has different resource requirements, including computing resources required for task execution and communication resources required for task transmission. In this case, a suitable strategy is needed to control offloading tasks and ensure the normal operation of system. Considering the impact of offloading decision and resource allocation on computing offloading performance. In response to the above problems, a computing offloading resource allocation scheme using reinforcement learning in a MEC system is proposed,\n\n- MEC (Mobile Edge Computing) effectively reduces the delay cost of service delivery and network operation by deploying servers close to mobile users.\n- It has been certified by the European 5G Infrastructure Public Private Partnership (PPP) as a key technology for the next-generation 5G network.\n- Advantages of MEC include low latency, high bandwidth, and real-time wireless network information and location awareness [7,8].\n- MEC-based Internet of Vehicles (IoV) enables mobile vehicles to offload computing tasks to network edge nodes, addressing the ultra-low latency requirements of IoV [9].\n- MEC sinks cloud services to the edge of the wireless access network, providing computing services near moving vehicles [10].\n- In MEC-based vehicle-connected networks, computationally intensive and delay-sensitive tasks exist, each with different resource requirements (computing and communication resources).\n- A suitable strategy is essential to control offloading tasks and ensure the normal operation of the system.\n- Considering the impact of offloading decisions and resource allocation on computing offloading performance, a computing offloading resource allocation scheme using reinforcement learning in a MEC system is proposed.\n","x":419,"y":2123,"width":527,"height":1178},
		{"id":"8a7016c5db0d8166","type":"text","text":"\n### Dependency-Aware Task Scheduling in Vehicular Edge Computing\n\nLiu, Yujiong, Shangguang Wang, Qinglin Zhao, Shiyu Du, Ao Zhou, Xiao Ma和Fangchun Yang. 《Dependency-Aware Task Scheduling in Vehicular Edge Computing》. *IEEE Internet of Things Journal* 7, 期 6 (2020年6月): 4961–71. <https://doi.org/10.1109/JIOT.2020.2972041>.\n\n![[Pasted image 20231219151604.png]]\n\nThere has been some research works on VEC. Zhang et al. [12] proposed a task offloading scheme based on the Stackelberg game theory to maximize the utilities of both vehicles and MEC servers. Zhang et al. [13] introduced an efficient predictive combination-mode offloading mechanism to reduce the offloading cost. Dai et al. [14] developed a joint optimal VEC server selection and offloading algorithm to maximize the system utility. Zhou et al. [15] proposed an energy-efficient resource allocation algorithm based on the alternating direction method of multipliers. Zhu et al. [16] developed a dynamic task allocation solution to ensure the quality of service. The prior studies assume that applications consisting of independent tasks are offloaded to RSUs for execution. However, task execution order depends on task dependency and the effect of task dependency on the execution time of applications has not been considered in the previous research works. For an augmented vehicular reality system with the following major components: object tracking, model mapping, object recognition, perspective transformation, and merging processing, there are some task dependencies among the components, e.g., only after one vehicle is tracked, the surrounding environmental model of the vehicle can be built and only after one vehicle is recognized, the process of perspective transformation and merging processing can be executed. To ensure that multiple vehicular applications can be completed in time, it is necessary to take task dependency into account for task scheduling policies design.\n\nTo overcome the above drawbacks, in this article, we consider the task dependency and the completion time constraints when scheduling tasks into multiple MEC servers. The goal of the work is to identify task scheduling decision that minimizes the average completion time of multiple applications, subject to their respective completion time constraints. We first present a VEC architecture. Then, we specify the completion time constraint of each application and the task dependency requirements of tasks. Finally, we propose an efficient task scheduling algorithm to minimize the average completion time of multiple applications.\n\nThe main contributions of this article are as follows.\n\n1. We propose a VEC architecture which consists of multiple vehicles, multiple RSUs, and multiple MEC servers. Each vehicle has a computation-intensive and delay-sensitive application. Each RSU is equipped with multiple MEC servers. Multiple vehicles can offload their computation-intensive and delay-sensitive applications to MEC servers on RSUs for execution where applications are independent of each other but tasks (belonging to the same application) have processing dependence.\n2. We formalize the task scheduling decision problem as an optimization problem which is NP-hard, and then propose an efficient multiple applications multiple tasks scheduling (MAMTS) algorithm to solve the optimization problem. Furthermore, we prioritize multiple applications to meet their respective completion time constraints and prioritize multiple tasks for satisfying their processing dependency requirements.\n3. We evaluate the proposed task scheduling algorithm with extensive simulations. The simulation results show that our proposed algorithm can significantly reduce the average completion time of multiple applications compared with benchmark algorithms.","x":1541,"y":-693,"width":527,"height":2444},
		{"id":"e283e054643c750e","type":"text","text":"### A Collaborative Scheduling Strategy for IoV Computing Resources Considering Location Privacy Protection in Mobile Edge Computing Environment\nPang, Meiyu, Li Wang和Ningsheng Fang. 《A Collaborative Scheduling Strategy for IoV Computing Resources Considering Location Privacy Protection in Mobile Edge Computing Environment》. Journal of Cloud Computing 9, 期 1 (2020年12月): 52. https://doi.org/10.1186/s13677-020-00201-x.\n\nWith the continuous improvement of relevant standards and continuous increase of intelligent vehicles, it is foreseeable that more and more vehicles will realize network interconnection by relevant protocols in the future. With the increasing number of vehicles, road hazards have become a problem that must be faced in the development of IoV [10]. Besides, the communication transmission of vehicle safety business has higher timeliness and reliability requirements. In some application scenarios of IoV, such as automatic driving, the delay requirement even needs to be lower than 10 ms. This makes the research on the transmission strategy of IoV security services more and more important [11,12]. In the vehicle communication process based on IEEE 802.11P and LTE-V protocols, channel congestion, channel interference, shadow fading and intelligent computing processing are main factors that affect the communication performance of vehicles. How to schedule computing resources and communication resources in IoV to improve the communication performance of vehicle safety business has important research value. Besides, the proposed scheduling strategy is based on an IoV system of multi-area multi-user multi-MEC server. A vehicle distance prediction method based on Kalman filtering is proposed combined with the mobility of IoV users in this paper. Furthermore, the total cost of communication delay and energy consumption of all users is formulated as the optimization goal, the Double DQN algorithm is used to solve the optimal scheduling strategy for minimizing the total consumption cost of system.\n\n- **Background** The continuous improvement of relevant standards and the increase of intelligent vehicles indicate a future where more vehicles will achieve network interconnection through relevant protocols.\n- **Challenge** With the growing number of vehicles, addressing road hazards becomes a crucial challenge in the development of the Internet of Vehicles (IoV) [10].\n\t- Vehicle safety business communication requires high timeliness and reliability, especially in scenarios like automatic driving where the delay requirement may need to be lower than 10 ms [11,12].\n\t- In the vehicle communication process based on IEEE 802.11P and LTE-V protocols, factors such as channel congestion, channel interference, shadow fading, and intelligent computing processing significantly affect communication performance.\n- **Importance** Research on the transmission strategy of IoV security services gains importance, particularly in scheduling computing and communication resources to enhance vehicle safety business communication performance [11,12].\n- **Solution** The proposed scheduling strategy is based on an IoV system with multi-area, multi-user, and multi-Mobile Edge Computing (MEC) server configuration.\n\t- A vehicle distance prediction method based on Kalman filtering is introduced, considering the mobility of IoV users.\n\t- The optimization goal is formulated as minimizing the total cost of communication delay and energy consumption for all users.\n\t- The Double Deep Q-Network (DQN) algorithm is employed to solve the optimal scheduling strategy for minimizing the total consumption cost of the system.\n","x":1541,"y":1790,"width":527,"height":1390},
		{"id":"60cf73ff2492e3e8","type":"text","text":"### Multi-Agent Reinforcement Learning Based Resource Management in MEC- and UAV-Assisted Vehicular Networks\n\nPeng, Haixia, 和Xuemin Shen. 《Multi-Agent Reinforcement Learning Based Resource Management in MEC- and UAV-Assisted Vehicular Networks》. *IEEE Journal on Selected Areas in Communications* 39, 期 1 (2021年1月): 131–41. <https://doi.org/10.1109/JSAC.2020.3036962>.\n\nConsidering the stringent delay requirements of some applications and huge latency on wireless and wired communications between a vehicle and the cloud computing server, offloading vehicles’ tasks to the cloud computing server is sometimes inapplicable [13]–[15]. Meanwhile, physically increasing the amount of onboard resources would result in a serious increase in the manufacturing cost of vehicles. The MEC server, as an extension of the cloud computing server, shifts computing and caching capabilities close to user devices [16]–[19], and allows vehicles to offload tasks to it via multiple wireless communication technologies. By saving the time consumption on communications between the MEC and the cloud computing server, the sensitive delay requirement of an offloaded task can be satisfied. However, the amount of computing/caching resources is generally preset in each MEC-mounted base station (BS) while the resource demand from vehicle users is time-varying, underused or wasted resource issues remain in the vehicular scenarios supported by only MEC-mounted BSs, especially when there exists bursty traffic caused by some social activities or events. Taking the flexibility advantage of UAVs, mounting MEC servers in UAVs can help to address the above issues by dispatching the MEC-mounted UAVs to assist the designated BSs [11], [20]. Related applications have been also considered in different projects launched by many leading companies [21].\n\nTo implement MEC- and UAV-assisted vehicular networks, many efforts have been made recently. Some of them have been focused on the deployment of MEC-mounted UAVs. For example, [22] has studied how to deploy and schedule the MEC-mounted UAVs to support vehicular applications. Resource management, as another research emphasis, has also attracted lots of attention from the existing works, where most of them adopt the optimization and reinforcement learning (RL) methods. In [12], the transmit powers of vehicles and the trajectories of UAVs have been jointly optimized to maximize the resource efficiency on MEC-mounted UAVs. In [23], a deep RL based adaptive computation offloading method has been proposed to balance the tradeoff between energy consumption and data transmission delay in an MEC-based vehicular network. In [24], a framework using MEC-mounted UAVs has been proposed to support mobile users in the extended 5G network, and an RL method is adopted to manage the resources carried by the UAV. To jointly manage the spectrum, computing, and caching resources available to an MEC-mounted BS, deep deterministic policy gradient (DDPG)- and hierarchical DDPG (HDDPG)-based schemes have been proposed in [25]. However, only vehicular networks supported either by MEC-mounted BSs or UAVs have been studied by most of the existing works. How to perform efficient resource allocation to support applications with various resource demand and heterogeneous quality-of-service (QoS) requirements in vehicular networks with MEC-mounted BSs and UAVs still needs efforts.\n\nIn this paper, we investigate multi-dimensional resource management in the MEC- and UAV-assisted vehicular networks, where MEC servers are mounted at a macro eNodeB (MeNB) and in some UAVs to provide resource access to vehicles with limited onboard resources. Inspired by existing works [23]–[26], we adopt RL methods to achieve real-time resource management in the considered scenario. Considering the sensitive delay requirements of some vehicular applications (e.g., autonomous driving) and the wireless transmission time between a UAV and a controller, installing a central controller either at the MeNB or an edge node to enable a centralized resource management scheme is infeasible sometimes. Thus, we develop a distributed cooperative scheme based on a multi-agent RL method to manage the multi-dimensional resources available to the MEC-mounted MeNB and UAVs. The main contributions of this work are summarized as follows,\n\n1. To support as many offloaded tasks as possible while satisfying their QoS requirements, we formulate an individual optimization problem to each MEC server to jointly manage the MEC-mounted MeNB’s and UAVs’ spectrum, computing, and caching resources;\n2. Because of the vehicle association pattern variables, the formulated problems are coupled with each other and non-convex. To rapidly solve these problems to satisfy the sensitive delay requirements of the offloaded tasks, we transform each formulated problem according to the main idea of RL;\n3. We convert the transformed problems as a multi-agent problem by letting each MEC server act as an agent and develop a multi-agent DDPG (MADDPG) algorithm to solve it. Through training the MADDPG model offline, the vehicle association and resource allocation decisions can be made in real time by each MEC server.\n","x":-2845,"y":52,"width":527,"height":1845},
		{"id":"38d1769d3407d63b","type":"text","text":"\n- Considering the stringent delay requirements of some applications and huge latency on wireless and wired communications between a vehicle and the cloud computing server, offloading vehicles’ tasks to the cloud computing server is sometimes inapplicable [13]–[15].\n- Meanwhile, physically increasing the amount of onboard resources would result in a serious increase in the manufacturing cost of vehicles.\n- The MEC server, as an extension of the cloud computing server, shifts computing and caching capabilities close to user devices [16]–[19].\n- It allows vehicles to offload tasks to it via multiple wireless communication technologies.\n- By saving the time consumption on communications between the MEC and the cloud computing server, the sensitive delay requirement of an offloaded task can be satisfied.\n- However, the amount of computing/caching resources is generally preset in each MEC-mounted base station (BS) while the resource demand from vehicle users is time-varying, underused, or wasted resource issues remain in the vehicular scenarios supported by only MEC-mounted BSs, especially when there exists bursty traffic caused by some social activities or events.\n- Taking the flexibility advantage of UAVs, mounting MEC servers in UAVs can help to address the above issues by dispatching the MEC-mounted UAVs to assist the designated BSs [11], [20].\n- Related applications have been also considered in different projects launched by many leading companies [21].","x":-2149,"y":3,"width":495,"height":670},
		{"id":"b0aa8cc67ab76ff0","type":"text","text":"\n- To implement MEC- and UAV-assisted vehicular networks, recent efforts have been made, with a focus on the deployment of MEC-mounted UAVs.\n- [22] has studied how to deploy and schedule the MEC-mounted UAVs to support vehicular applications.\n- Resource management is another research emphasis, with most works adopting optimization and reinforcement learning (RL) methods.\n- In [12], the transmit powers of vehicles and the trajectories of UAVs are jointly optimized to maximize the resource efficiency on MEC-mounted UAVs.\n- [23] proposes a deep RL-based adaptive computation offloading method to balance the tradeoff between energy consumption and data transmission delay in an MEC-based vehicular network.\n- [24] introduces a framework using MEC-mounted UAVs to support mobile users in the extended 5G network, with an RL method adopted to manage the resources carried by the UAV.\n- [25] proposes deep deterministic policy gradient (DDPG)- and hierarchical DDPG (HDDPG)-based schemes to jointly manage spectrum, computing, and caching resources available to an MEC-mounted BS.\n- However, most existing works have studied vehicular networks supported either by MEC-mounted BSs or UAVs.\n- There is still a need for efforts in efficiently allocating resources to support applications with various resource demands and heterogeneous quality-of-service (QoS) requirements in vehicular networks with MEC-mounted BSs and UAVs.","x":-2149,"y":698,"width":495,"height":640},
		{"id":"5911a2a71cba17fe","type":"text","text":"### Deep Reinforcement Learning for Offloading and Resource Allocation in Vehicle Edge Computing and Networks\n\nLiu, Yi, Huimin Yu, Shengli Xie和Yan Zhang. 《Deep Reinforcement Learning for Offloading and Resource Allocation in Vehicle Edge Computing and Networks》. *IEEE Transactions on Vehicular Technology* 68, 期 11 (2019年11月): 11158–68. <https://doi.org/10.1109/TVT.2019.2935450>.\n\nIn MEC based IoT network, the devices can offload all/part of the computation tasks to the MEC server which can speed up the processing of the tasks and save energy for devices [15]–[17]. Then, the main technical problem becomes whether/when/how many computation tasks should be offloaded. Numerous literatures are devoted to design the optimal strategy to solve this problem under different performance requirements [18]–[21]. Considering the long-time energy efficiency while using IoT network, an efficient edge computing infrastructure is proposed in [18]. Due to stochastic task arrivals and wireless channels, congested air interface, and prohibitive feedbacks from thousands of IoT devices, authors in [19] generate asymptotically optimal schedules tolerant to out-of-date network knowledge, thereby relieving stringent requirements on feedbacks. The optimal schedule and energy efficient resource allocation policies for MEC are proposed in [20] and [21], respectively.\n\nSince the vehicle is an important type of User Equipment (UE) in IoT system, the vehicular edge computing network structure and related resource allocation methods are studied by many researchers [22]–[27]. C. Wang et al., propose a scalable SDN-enabled MEC architecture that integrates a heterogeneous vehicular network to decrease the overall delay and offload the traffic load from the backbone network [22]. To reduce both the latency and the transmission cost of the computation offloading, a cloud-based MEC offloading framework is proposed for vehicular networks in [23]. In [24], based on the MEC-assisted vehicular offloading mode, a target server selection policy is presented to improve task offloading reliability in the case of vehicular data transmission failure. Some novel MEC-based vehicular networks are proposed in [25], [26], in which the computation offloading policies are carefully designed according to different scenarios. The reinforcement learning is used for long-term resource provision in vehicular cloud to deal with dynamic demands for the resources and stringent QoS requirements [27]. However, in the existing literatures, the vehicles play the role as users in the MEC network in which the edge servers are statically deployed and may cause “service hole” due to the explosion of service requests of tremendous number of UEs.\n\nThe focus of this paper is to design a Vehicle Edge Computing (VEC) network in which the vehicles are able to provide computation services as well as the traditional edge servers. As the traditional edge server, generally connected to road side units, small-cell base stations, etc., has fixed locations, the proposed architecture can extend the computation services range and improve flexibility of the MEC network. Then, we propose an efficient computation offloading scheme for UEs while considering the delay of the computation tasks generated by UEs. Accordingly, we formulate an optimization problem to maximize the total utility of the proposed VEC network.\n\nTo solve the problem, the stochastic traffic and communication uncertainty in vehicular communication environment should be carefully addressed. The Q-learning method is one of the model-free reinforcement learning methods which are not based on the environment elements are already known [32]. Such feature makes Q-learning method suitable for solving the proposed problem in the vehicle edge computing network. The crucial part of Q-learning is to accurately and efficiently estimate the Q value, which may lead to the curse of dimensionality as the increasing of state space. Deep reinforcement learning (DRL), which approximates the Q-function by using deep neural networks, has more advantageous than Q-learning for providing greater performance and more robust learning [33]–[36]. Hence, the proposed problem is further formulated as a semi-Markov process and two reinforcement learning methods: Q-learning based method and deep reinforcement learning (DRL) method are proposed to determine the policies of computation offloading and resource allocation.\n\nIn this paper, we propose a VEC network to enhance the flexibility and scalability of the MEC based IoT system with main contributions summarized as follows:\n\n1. We propose a vehicle edge computing network architecture in which the vehicles can provide computation services for UEs as well as the traditional edge server.\n2. We propose an efficient offloading scheme for the vehicle edge computing network while considering both delay and limited computation capabilities of vehicles and edge servers. Accordingly, we formulate an optimization problem to maximize the total utility of the vehicle edge computing network.\n3. Taking into account of the stochastic traffic and uncertain communication conditions, we reformulated the proposed problem as a semi-Markov process and propose Q-learning based reinforcement learning method and DRL method to find the policies of computation offloading and resource allocation.","x":-1384,"y":273,"width":527,"height":1977},
		{"id":"1b11274524513b64","x":-2149,"y":1405,"width":495,"height":523,"type":"text","text":"\n- In this paper, the focus is on investigating multi-dimensional resource management in MEC- and UAV-assisted vehicular networks.\n- MEC servers are mounted at a macro eNodeB (MeNB) and in some UAVs to provide resource access to vehicles with limited onboard resources.\n- RL methods are adopted, inspired by existing works [23]–[26], to achieve real-time resource management in the considered scenario.\n- Due to sensitive delay requirements in certain vehicular applications (e.g., autonomous driving) and the wireless transmission time between a UAV and a controller, installing a central controller at the MeNB or an edge node for centralized resource management is sometimes infeasible.\n- To address this, a distributed cooperative scheme is developed based on a multi-agent RL method to manage the multi-dimensional resources available to the MEC-mounted MeNB and UAVs.\n- The main contributions of this work are summarized as follows:"},
		{"id":"3c8c5072975c7288","x":-2149,"y":1991,"width":495,"height":405,"type":"text","text":"1. To support as many offloaded tasks as possible while satisfying their QoS requirements, we formulate an individual optimization problem to each MEC server to jointly manage the MEC-mounted MeNB’s and UAVs’ spectrum, computing, and caching resources;\n2. Because of the vehicle association pattern variables, the formulated problems are coupled with each other and non-convex. To rapidly solve these problems to satisfy the sensitive delay requirements of the offloaded tasks, we transform each formulated problem according to the main idea of RL;\n3. We convert the transformed problems as a multi-agent problem by letting each MEC server act as an agent and develop a multi-agent DDPG (MADDPG) algorithm to solve it. Through training the MADDPG model offline, the vehicle association and resource allocation decisions can be made in real time by each MEC server."},
		{"id":"9f1946b76addeacb","x":-812,"y":505,"width":250,"height":60,"type":"text","text":"Certainly! Here is the text organized as an unordered list:\n\n- In MEC-based IoT networks, devices have the capability to offload all or part of computation tasks to the MEC server, enhancing task processing speed and saving device energy [15]–[17].\n- The primary technical challenge revolves around determining whether, when, and how many computation tasks should be offloaded.\n- A significant body of literature is dedicated to designing optimal strategies to address this problem under various performance requirements [18]–[21].\n- For long-term energy efficiency in IoT networks, [18] proposes an efficient edge computing infrastructure.\n- Considering stochastic task arrivals, wireless channel variations, congested air interfaces, and prohibitive feedback from numerous IoT devices, [19] generates asymptotically optimal schedules tolerant to out-of-date network knowledge, relieving stringent requirements on feedback.\n- Optimal scheduling solutions and energy-efficient resource allocation policies for MEC are proposed in [20] and [21], respectively."},
		{"id":"9f76515ec3bc8475","x":-812,"y":852,"width":250,"height":60,"type":"text","text":"Certainly! Here is the text organized as an unordered list:\n\n- Researchers have extensively explored the vehicular edge computing network structure and resource allocation methods, given the significant role of vehicles as User Equipment (UE) in IoT systems [22]–[27].\n- C. Wang et al. propose a scalable SDN-enabled MEC architecture integrating a heterogeneous vehicular network to decrease overall delay and offload traffic load from the backbone network [22].\n- To address both latency and transmission cost of computation offloading, a cloud-based MEC offloading framework is proposed for vehicular networks in [23].\n- In [24], a target server selection policy is presented, based on the MEC-assisted vehicular offloading mode, aiming to improve task offloading reliability in the case of vehicular data transmission failure.\n- Novel MEC-based vehicular networks are proposed in [25], [26], where computation offloading policies are carefully designed for different scenarios.\n- Reinforcement learning is employed for long-term resource provision in vehicular clouds, addressing dynamic demands for resources and stringent Quality of Service (QoS) requirements [27].\n- However, in existing literature, vehicles predominantly serve as users in the MEC network, where edge servers are statically deployed and may lead to \"service holes\" due to an explosion of service requests from a tremendous number of User Equipments (UEs)."}
	],
	"edges":[
		{"id":"11bd23db47de2651","fromNode":"60cf73ff2492e3e8","fromSide":"right","toNode":"38d1769d3407d63b","toSide":"left"},
		{"id":"3a8ff5eb2782b36e","fromNode":"60cf73ff2492e3e8","fromSide":"right","toNode":"b0aa8cc67ab76ff0","toSide":"left"},
		{"id":"e423a83dcf14242d","fromNode":"60cf73ff2492e3e8","fromSide":"right","toNode":"1b11274524513b64","toSide":"left"},
		{"id":"6f4111b2f137f784","fromNode":"60cf73ff2492e3e8","fromSide":"right","toNode":"3c8c5072975c7288","toSide":"left"}
	]
}