---
UID: 20231207104250
aliases: null
source: null
cssclass: null
created: 2023-12-07T00:00:00.000Z
date updated: 2023-12-07 13:28
---






---

为了解决这个问题，我们需要考虑任务、执行方案和执行资源之间的匹配程度，以及执行方案之间的竞争和合作程度。为了实现任务卸载，我们可以借借鉴一些先进的技术和方法，例如容器技术、微服务架构、多智能体强化学习和反向拍卖机制等。
本文旨在提出一种基于多智能体强化学习和逆拍卖机制的车联网任务调度方法，该方法可以根据任务、车辆和边缘服务器之间的资源匹配程度动态分配每个任务的执行计划，并通过激励机制确保车辆和边缘服务器之间的合作和公平性。本文认为这种方法可以有效提高任务调度的效率和性能，降低任务执行过程中产生的总成本。
在车联网中，任务调度面临着多方面的挑战，例如车辆的高速移动性、网络的动态变化、任务的时效性和复杂性、资源的有限性和异构性等。这些挑战使得任务调度的决策过程变得复杂和不确定，需要考虑多个因素和约束，如任务的特征, 网络的状态, 服务器的负载等。由于传统的任务调度方法存在模型假设过于简化、算法复杂度过高、对环境变化不敏感等问题，它们往往不能适应车联网的复杂环境，因此需要引入更智能和灵活的方法，如强化学习、拍卖机制等。现有的用于任务调度的强化学习算法大多使用一个中心部署的超级智能体用以调度, 在调度中需要额外维护个服务器的信息, 提高通信复杂度和降低了系统的灵活性,
这篇论文提出了一种创新的方法，结合了多智能体强化学习和反向拍卖机制，来解决车联网中的任务调度问题即根据车辆的计算任务需求和移动特性，在移动边缘计算（MEC）环境中有效地分配和调度车辆的计算任务，以达到优化系统性能和用户体验的目的。该方法的目标是通过动态分配计算密集型服务的执行计划，实现服务、车辆和边缘服务器之间的资源匹配，同时通过激励机制促进车辆和边缘服务器之间的合作，提高任务调度的效率和性能，降低总成本。 本文中提出的多智能体强化学习方法在每个边缘服务器中分布式部署了智能体, 最终根据反向拍卖机制决定卸载, 降低了通信成本和系统复杂度, 提高了灵活性.

由于云计算是一种按需付费的服务，最好考虑诸如与具有不同能力的服务提供商相关的个性化服务价格、应用程序所有者的成本考虑以及用户偏好等经济因素，在计算卸载市场上限制了车辆之间的接触持续时间在一定程度上也被忽视了。


人们相信博弈论中的均衡概念和多智能体训练方法在处理多约束和多目标优化问题方面具有很高的潜力。


可以从先前的行动中学习，并在没有环境数学模型的情况下实现最佳调度。同时，我们采用DQN算法来解决复杂性和高维度的工作负载调度问题，旨在平衡边缘服务器之间的工作负载，减少服务时间和失败任务率。


通过强化学习学习竞价策略, 再通过反向拍卖机制决定胜者和支付.


任务卸载涉及到多个参与者之间的资源分配问题，即如何将车辆的任务分配给合适的边缘服务器，并使得各方都能能获得最大化的效用。为了解决这一问题，本文借借鉴了经济学中的反向拍卖（Reverse Auction）理论，提出了一种基于反向拍卖的任务卸载方法（Reverse Auction-based Task Offloading, RATO）。反向拍卖是一种一买方多卖方的拍卖形式，适用于买方市场（Buyer’s Market）的交易活动。在反向拍卖中，买方提出自己的需求和预算，卖方根据自身的成本和利润，竞争性地给出自己的报价。最终，买方选择一个或多个报价最低且满足需求的卖方进行交易。反向拍卖可以有效降低买方的采购成本，提高卖方之间的竞争力，实现资源的优化分配。

---

在这个背景下，深度强化学习（DRL）成为一种关键的方法，特别是针对具有高维状态和动作空间的问题。DRL基于神经网络和马尔可夫决策过程（MDP）模型，使得智能体能够与环境交互，学习最优策略以最大化长期奖励。对于多智能体环境，引入了多智能体强化学习（MARL）问题，其在评估和生成策略方面具有不同的方法，如基于价值、基于策略和演员-评论者类型。MARL方法因其适应性、鲁棒性、可扩展性和分布性等优势，在众多领域取得了广泛应用，为复杂、动态和不确定的环境提供了高效和稳定的解决方案。

在许多复杂的动态优化问题中，传统的强化学习方法很难处理高维状态和动作空间，因此深度强化学习（DRL）应运而生。DRL是一种基于神经网络和马尔可夫决策过程（MDP）模型的强化学习方法，它可以使一个或多个智能体与环境交互，并学习最优策略以最大化它们的长期奖励。当一个环境中有多个智能体时，DRL成为多智能体强化学习（MARL）问题，需要每个智能体根据自己和其他智能体的行为调整自己的策略，以实现合作或竞争的目标。MARL方法可以分为基于价值、基于策略和演员-评论者类型，它们使用不同的方式来评估或生成策略。MARL具有适应性、鲁棒性、可扩展性和分布性等许多优势，可适应复杂、动态和不确定的环境，提高系统的效率和稳定性。MARL在许多领域都有广泛应用，如机器人协作、交通控制、电力调度、社会模拟等。

本文使用多智能体深度强化学习（MADRL）来训练部署在每个MEC服务器上的策略网络，使其能够根据用户的任务需求和自身状态计算接受任务的长期奖励，并根据最大化奖励的原则出价。与其他机器学习方法相比，DRL可以直接从环境中获得反馈信号，并通过持续的探索和利用优化自己的行为。DRL不需要预定义特征或标签，也不需要大量的先验知识或假设，因此更适用于处理车联网等复杂、动态和不确定的环境。

---

在车联网中，正拍卖和逆拍卖都有各自的应用场景和优势，但它们也面临一些问题和挑战。逆拍卖可以激励车辆用户共享资源或执行任务，如频谱共享、缓存共享、计算任务卸载、数据采集等。这些服务或任务请求可以由平台发起，也可以由车辆用户或边缘服务器发起到平台。平台可以使用逆拍卖找到愿意以最低价格提供服务或执行任务的车辆用户或服务器，并与它们签署标准合同，从而为平台节省成本。逆拍卖也可以用于将一些社会福利任务分配给车辆用户，如交通管理、环境监测、数据收集等。平台可以根据任务需求和预算在逆拍卖中发布任务信息，并选择出价最低且符合标准的车辆用户来执行任务。这可以使平台在有效实现任务目标的同时，激励车辆用户参与社会福利活动并获得相应的奖励。本文将设计一种基于机制设计理论的逆拍卖机制，用于将车辆用户的任务分配给MEC服务器，并激励它们以最低价格提供服务或执行任务。为了实现这种逆拍卖机制，本文还将使用深度强化学习来训练部署在每个MEC服务器上的策略网络，使其能够根据用户的任务需求和自身状态计算接受任务所产生的长期收益，并根据收益最大化的原则向用户出价。这可以确保资源分配过程的真实性、效率和个体合理性，并在车辆用户和MEC服务器之间实现合作优化。
