## 4. Method of MADRL and Reverse-Auction

To address the workflow scheduling problem in edge computing for vehicular networks, we propose a method based on multi-agent reinforcement learning and reverse auction mechanism. We first model the problem as an online and NP-hard problem, since the task offloading in vehicular networks can be regarded as a temporal multi-dimensional knapsack problem. Then, we use reinforcement learning to deal with complex dynamic high-dimensional decision-making problems. We adopt multi-agent reinforcement learning to achieve distributed resource allocation, avoiding the communication overhead and information inaccuracy of centralized management, and improving the stability of the system. We also propose a task offloading method based on reverse auction for solving the resource allocation problem in vehicular networks. Reverse auction is a one-buyer-multi-seller auction mechanism, suitable for trading activities in the buyer market. The buyer submits its demand and task priority, and the seller makes a bid based on its own resource usage and task features. The buyer only chooses one seller to trade with. Reverse auction can effectively reduce the buyer's procurement costs, improve the competitiveness of sellers, and achieve optimal resource allocation.
### Multi-agent reinforcement learning environment modeling

In the task offloading modeling of Chapter 3, edge servers are defined as agents, each of which learns a policy that maximizes its own reward through interaction with the environment. Because in a multi-agent environment, the action of one agent affects the decision of other agents. In order to describe this situation, we use a partially observable Markov decision process (POMDP) to model multi-agent reinforcement learning. A POMDP consisting of K agents can be defined as $$ (\mathbf S,\mathbf O_1,\dots,\mathbf O_K,\mathbf A_1,\dots,\mathbf A_K, \mathbf R_1,\dots,\mathbf R_K, \mathbf P_{ss'}, \mathbf P_{o|s}) $$ where $\mathbf S$ is the state space, $\mathbf O_i$ is the observation space of the $i$-th agent, $\mathbf A_i$ is the action space of the $i$-th agent, $\mathbf R_i$ is the reward function of the $i$-th agent, $\mathbf P_{ss'}$ is the state transition probability, and $\mathbf P_{o|s}$ is the observation likelihood. In each time interval, the edge server, as an agent, observes the observation $o_i\in\mathbf O_i$ generated by the current environment, and selects a suitable action $a_i\in\mathbf A_i$ according to the policy $\pi_i(a_i|o)$. Then, it obtains the corresponding reward $r_i=\mathbf R_i(s,a_1,\dots,a_K)$. In the subsequent parts of this chapter, we will introduce in detail how to define $\mathbf S,\mathbf O,\mathbf A,\mathbf R$ and how to design a multi-agent reinforcement learning environment.

**State Space.** The state space is a description of the entire system, including all the information in the system. In this study, we are focusing on multi-agent reinforcement learning, so the state space should include the states of all server clusters and task queues. The state space can be represented by a set $\mathbf S$, as follows: $$ \mathcal S=\{\mathcal B,\mathcal S, \mathbf r_{\mathcal B}\} $$ where $\mathcal B$ represents the seller servers, $\mathcal S$ represents the task requests submitted by the buyer vehicles, $\mathbf r_{\mathcal B}=\{r_{\mathbf s_1},\dots,r_{\mathbf s_m}\}$ represents the resource usage of the seller servers, and $r_{\mathbf s_i}=(r_{\mathbf s_i}(1),\dots,r_{\mathbf s_i}(T))$.

**Observation Space.** The observation space is the subset of the state space that an agent can observe. In this study, we assume that there is no additional communication or interaction between agents. Therefore, the observation space for agent $i$ is the resource usage of its server from the current time $t$ to a certain time in the future $\mathbf r_{\mathbf s_i,t}=(r_{\mathbf s_i}(t),\dots,r_{\mathbf s_i}(t+\epsilon))$ and the task execution features $\mathbf f_j$ of the task $\theta_j$ that is waiting to be quoted by the agent. Therefore, the observation space for edge server $i$ is $$O_i=\{r_{\mathbf s_i,t},\mathbf f_j\}$$ This means that each agent can only make decisions based on its own server load and the features of the current task, and cannot obtain information about the state of other agents.

**Action Space.** The action space is the set of actions that an agent can execute in the environment. In multi-agent reinforcement learning, each agent can take actions to affect the state of the environment, thereby earning rewards. Different tasks may require different types of action spaces, such as discrete or continuous. In this study, we consider a multi-agent server bidding task, where each agent represents an edge server that needs to bid on received tasks to compete for task allocation. We assume that each agent's action space is a numeric value in the range $[1,2]$, representing its floating coefficient for bidding on a task. This means that each agent can choose any real number in the range $[1,2]$ as its bidding coefficient $a_i$. This gives the agent more freedom and flexibility to adjust its bidding strategy. We can use a range $\mathbf A_i$ to represent the action space of the $i$th agent, as follows: $$ \mathbf A_i=[1,2] $$ When the environment receives the value of $a_i$ from agent $i$, then its final bid for the current task $\theta_j$ waiting to be auctioned is: $$ q_i = a_i \times \mathbf p \cdot \mathbf r_j \times \tau_j $$ where $\mathbf p$ is the market average price vector of resources, $\mathbf r_j$ is the resource usage, and $\tau_j$ is the resource usage time. However, not all servers can bid on any task, because tasks have execution server restrictions $\mathcal S_j \subseteq \mathcal S$, which means that only servers in $\mathcal S_j$ can meet the requirements of task $j$. Therefore, in time step $t$, the action space of the entire system is: $$ A_t=\{a_1 \sigma_{j,1},\dots,a_m\sigma_{j,m}\} $$ where $a_i\sigma_{j,i}$ represents the bidding coefficient of the $i$-th server for the task $\theta_j$ that is currently being requested. If a server does not meet the execution server conditions for task $\theta_j$, then its corresponding bidding coefficient is 0, indicating that it cannot take any action. When bidding on task $\theta_j$, the environment only collects the bids of the servers that meet the conditions for the task, and ignores the bids of other servers.

**Rewards.** The reward function defines the agent's goal and evaluation criteria, describing the immediate reward that the agent receives when it takes an action and transitions from one state to another. The agent will use the reward function to judge whether its action is beneficial or harmful to the achievement of its goal, and then adjust its behavior strategy to maximize its cumulative reward. In this paper, we consider a multi-agent server bidding task, where each agent represents an edge server that needs to bid on received tasks to compete for task allocation. The goal of the edge server is to maximize its profit, which is the difference between task payment and server cost. Therefore, we designed the following reward function: $$ \mathbf R_i = p_{t,i}-c_i $$ This reward function represents the reward that the $i$-th edge server receives in one time step. It consists of two parts: $p_{t,i}$ represents the sum of task payments completed by the edge server in one time step, and $c_i$ represents the fixed cost of the edge server in one time step.

### Reverse Auction-based Task Offloading Method
Task offloading involves the resource allocation problem among multiple participants, that is, how to assign the tasks of vehicles to suitable edge servers and make all parties achieve maximum utility. To solve this problem, this paper draws on the reverse auction theory in economics and proposes a reverse auction-based task offloading method (RATO). Reverse auction is a form of auction with one buyer and multiple sellers, suitable for transactions in a buyer's market. In a reverse auction, the buyer proposes his demand and budget, and the sellers compete to offer their bids based on their own costs and profits. Finally, the buyer selects one or more sellers with the lowest bids that meet his demand to trade. Reverse auction can effectively reduce the buyer's procurement cost, increase the competitiveness among sellers, and achieve optimal resource allocation.

This paper will introduce the roles and process of reverse auction in this model respectively.

**Buyer** is a vehicle in vehicular network that has task offloading demand and is willing to pay for its demand. At a certain moment $t$, the vehicle evaluates its connection status and initiates a task offloading request for task $\theta_j$ to the edge servers that meet its connection constraints, and sends its task attributes $\mathbf{f}_j$ to the server set $\mathcal S_j$ that meets the connection constraints to request bids.

**Seller** is an edge server in vehicular network. The edge server can receive the task offloading request from the vehicle and use its own resources to complete the task offloading and obtain revenue. The seller server $\mathbf s_i$'s characteristics can be defined as $\mathbf{s}_i = (\mathbf{r}_i, c_i)$, where resource vector $\mathbf{r}_i$ and cost $c_i$. After receiving the bid request, the server determines whether it can accept the task offloading. If it can accept it, it will use the reinforcement learning learned policy to bid for the task based on the current observation of available resources $\mathbf r_{s_i,t}$ and task characteristics $\mathbf f_j$.

To simplify the problem, this paper makes the following basic assumptions:

* Assume that time is divided into slots, tasks can only be submitted in one slot, and task duration is an integer number of slots.
* Vehicles can evaluate their connection status based on their own state, and obtain the edge server constraint conditions for tasks during the auction and offloading time.
* There is information asymmetry in the offloading market:
  * The seller's characteristics and maintenance costs are not visible to the buyer. Increase the competitiveness of the auction and improve the buyer's utility.
  * No seller knows other participants' bids, nor do sellers collude with each other.
  * The buyer's budget cannot be disclosed to the seller.
* During the auction process, only one task participates at a time. After the task auction is over, enter the next task's auction in this slot. Prevent sellers from colluding to raise prices.
* A buyer can only offload his task to one seller, but a seller can provide services to multiple buyers at the same time.
* The result of task offloading is consistent for vehicles in terms of utility.

In the cloud computing-based vehicular network scenario, we consider the case of $m$ sellers $\mathcal S$ and $n$ buyers $\mathcal B$. Each buyer represents a vehicle, which communicates with the edge server through its own communication unit, and generates corresponding constraints according to the communication status with the edge server, so as to select a suitable seller when it needs to perform task offloading. Each seller represents an edge server, which can connect with other edge servers and the Internet through wired links, and return the offloading results. At the same time, the edge server can also return the task execution results and transmit data between vehicles through wired connections. Algorithm 1 is a specific description of the reverse auction process.

Algorithm 1: Task offloading method based on reverse auction
Input: Vehicle set $\mathcal B$, server set $\mathcal S$
Output: Task offloading result $\mathcal R$
1. Initialization: For each server $\mathbf s_i \in \mathcal S$, initialize its resource vector $\mathbf r_i$, cost $c_i$, and bidding strategy $\pi_i$; initialize the offloading task set $\mathcal U=\{\mathcal U_1,\dots,\mathcal U_T\}$.
2. For each time slot $t=1,2,\dots,T$, perform the following steps:
   1. For each server $\mathbf s_i \in \mathcal S$ in the server cluster, update its resource usage and settle the revenue of the previous time slot $p_{t-1,i}-c_i$
   2. Select the buyer vehicle request $\mathbf b_j$ from $\mathcal U_t$ in chronological order, then perform the following steps:
      1. The vehicle evaluates its own connection status and generates the server constraint vector $\mathbf \sigma_j$ for the task.
      2. The vehicle sends the task offloading request $\mathbf f_j$ to the set of servers that meet the constraint condition $\mathcal S_j = \{\mathbf s_i | \forall \mathbf s_i \in \mathcal S, \sigma_{j,i} = 1\}$.
      3. For each server $\mathbf s_i \in \mathcal S_j$, if its available resource $r_{s_i}(t)$ can meet the resource demand of the task $r_{\theta_j}(t)$, then perform the following steps:
         1. The server calculates its own bid for the task $q_{j,i}$ according to its own resource usage situation $\mathbf r_{s_i,t}$ and task characteristics $\mathbf f_j$ in the time period $[t,t+\epsilon]$, as well as the bidding strategy $\pi_i$.
         2. The server returns the bid $q_{j,i}$ to the vehicle.
      4. The vehicle collects all the bids from the servers $\mathcal Q_j = \{q_{j,i} | \forall \mathbf s_i \in \mathcal S_j\}$ and sorts them in non-decreasing order.
      5. The vehicle selects the lowest bidder as the winner $\mathbf s_k =\mathop{\arg\min}\limits_{\mathbf s_i}\ q_{j,i}$, and checks whether its bid $p_j = q_{j,k}$ satisfies the budget condition $p_j \leq \psi_j$. If not, then this auction fails and the task offloading fails; if yes, then this auction succeeds and the task offloading succeeds, and perform the following steps:
         1. The vehicle sends the task to the winning server $\mathbf s_k$ and pays $p_j$, waiting for the execution result to return.
         2. The server $\mathbf s_k$ obtains revenue, allocates resources for the task and performs task offloading, updates its own resource usage situation $\mathbf r_{s_i,t}$
      6. Update task offloading result $\mathcal R$
3. When reaching total time step $T$, offloading ends

