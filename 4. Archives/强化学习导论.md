
#Todo
- [ ] 摘录
- [ ] 整理
# `ris:InkBottle`Notes
## 学习的本质
人是通过与环境交互进行学习
婴儿玩耍时挥舞双臂,四处张望,运动感知使其和外部环境直接联结
这种联结可以产生大量信息,告诉我们事情之间的因果关系,特定动作的后果,达到目标的方式

交互学习几乎是所有学习和智能理论的基本思想
强化学习研究交互中学习的计算性方法
主要探究:
- 理想化的学习场景
- 评估不同学习方法的有效性
而不是建立关于人与动物如何学习的理论
相较于其他机器学习方法,强化学习更侧重于以交互目标为导向进行学习

## 强化学习
学习做什么(如何把当前情景映射为动作)才能使得数值化收益信号最大化
学习者不会被告知应该采取什么动作,而是必须通过尝试发现哪些动作会产生最丰厚的收益
动作不仅仅会影响即时收益,也会影响下一情景,从而影响随后收益

强化学习最显著特征:
- 试错
- 延迟收益

强化学习和有监督学习不同
- 有监督学习从外部监督者提供的带标注的训练集中进行学习
	- 标注:针对当前情景,系统应该做出正确的动作,也可以看作对当前情景分类的标签
	- 让系统具备推断或泛化能力,能响应不同的情景并做出正确的动作,哪怕这个情景没有出现过
- 在交互问题中,不可能获得所有情境下即正确又有代表性的实例
	- 在未知领域,若要收益最大,需能够从自身经验学习

强化学习和无监督学习不同
- 无监督学习是寻找未标注数据中隐含结构的过程
- 强化学习的目的是最大化收益信号而不是找出数据的隐含结构
- 通过智能体的经验揭示其结构在强化学习中是有益的,但不能解决最大化收益信号的强化学习问题

### 强化学习特征
试探与开发之间的折中权衡
为获得大量收益,强化学习智能体会偏向于选择过去产生有效收益的动作
为了发现这些动作需要尝试未选择过的动作
智能体必须开发已有经验来获取收益,同时也要进行试探,使得未来可以获得更好的动作选择空间
智能体必须尝试各种各样的动作,并且逐渐筛选最好的动作

明确考虑了目标导向的智能体与不确定的环境交互整个问题
很多研究方法只考虑子问题,而忽视子问题在更大情景下的适用性
- 机器学习很多研究与有监督学习有关,没有明确说明关系是如何发挥作用
- 已有研究针对总体目标的规划理论,但没有考虑规划在实时决策中的作用,也没有考虑规划所需的预测模型的来源

强化学习从完整的,交互式的,明确导向的智能体出发
- 有一个明确的目标
- 能感知环境的各个方面
- 可以选择动作影响所处的环境

当强化学习涉及规划时,必须处理规划和事实动作之间的相互影响,如何获取和改善环境模型
强化学习涉及监督学习时,有某些特定因素可以决定对于智能体那些因素是重要的,哪些不是

强化学习利用参数化近似法解决运筹学和控制论研究中维度灾难问题
强化学习与心理学和神经科学之间有强相互哦用

基于一般规则的方法,如搜索学习,弱方法
基于知识的方法,强方法

## 强化学习要素
- 策略
- 收益信号
- 价值函数
- 模型(可选,对环境建立)
### 策略
定义了学习智能体在特定时间的行为方式
策略是环境状态到动作的映射
对应于心理学”刺激-反应”的规则关联或关联关系
强化学习智能体的核心
策略可能是环境所在状态和智能体所采取的动作随机函数

### 收益信号
定义了强化学习中的目标
每一步,环境向强化学习智能体发送称为**收益**的标量数值
收益信号是改变策略的主要基础
收益信号可能是环境状态和在此基础上所采取动作随机函数
表明短时间内什么是好的

### 价值函数
表示从长远角度的收益
一个状态的价值是一个智能体从这个状态开始,对将来累计总收益的期望

### 对环境建立的模型
对环境反应模式的模拟
允许对外部环境行为进行推断
环境模型被用于**规划**
规划,在真正经历之前,先考虑未来可能发生的各种情景,从而预先决定采取何种动作

使用环境模型和规划来解决强化学习 问题的方法,被称为**有模型的方法**
简单的**无模型方法**则是直接试错

### 收益与价值
从某种意义,收益更加重要,没有收益就没有价值,评估价值的唯一目的就是获得更多收益
在制定和评估策略时,最关心的是价值.动作选择是基于对价值判断做出的.
我们寻求能带来最高价值而不是最高收益状态的动作,这样可以得到最大的累积收益
确定价值比确定收益困难许多,需要综合评估,并根据智能体在整个过程观察到的收益序列重新估计
价值评估方法是几乎所有强化学习算法最重要组成部分

## 局限性和适用范围
强化学习十分依赖状态这个概念,作为策略和价值函数的输入,也作为模型的输入和输出
告诉智能体当前环境如何
大多数强化学习方法建立在对价值函数的估计

一些优化算法,遗传算法,遗传规划,模拟退火算法也可以用于解决强化学习问题,不用显式计算价值函数.选择获取了最多收益的策略及其变种来产生下一代的策略,之后循环更新.称为进化方法
如果策略空间充分小,或者很好结构化以找到好的策略,或者有充分的时间进行搜索,进化方法是有效
进化方法在智能体不能精确感知环境状态的问题上具有优势

我们关注的强化学习方法,是在与环境互动中学习的一类方法
我们相信考虑个体交互动作诸多细节的学习方法,会比进化方法更加高效
进化方法忽略了强化学习问题中一些有用的结构
- 忽略所求策略是状态到动作函数这一事实
- 没有注意个体在生命周期中经历过那些状态,采取哪些动作
在一些情形下,这些信息容易引起误导(错误的状态),但更多情形下,这些信息可以让搜索更高效


# `fas:ShoePrints`Cue
- 强化学习是一种对目标导向的学习与决策问题进行理解和自动化处理的计算方法
	- 强调智能体通过与环境直接互动来学习
	- 不需要可效仿的监督信号或对周围环境的完全建模
	- 与其他计算方法相比具有不同范式
- 强化学习是第一个严格意义上解决从环境中互动学习以达到长期目标这一计算问题的领域
- 强化学习使用马尔科夫链决策过程的形式化框架,使用状态,动作,收益定义学习型智能体与环境互动的过程
	- 力图简单地表示人工智能问题若干重要特征
		- 对因果关系认知
		- 对不确定性认知
		- 对显示目标存在性认知
- 价值与价值函数是强化学习方法重要特征
	- 价值函数对于策略空间有效搜索十分重要
	- 相比于进化方法以对完整策略反复评估为引导对策略空间直接搜索
		- 使用价值函数是强化学习方法与进化方法不同之处

# `ris:Loader`Summary
- 这里写总结

>[!link]
>[[2022-09-21 强化学习]]
>- 
