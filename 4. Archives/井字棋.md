
#Todo

# `ris:InkBottle`Notes
## 游戏过程
两个玩家轮流在一个3×3棋盘下棋.
一方为X,乙方为O
直到其中一方在行,列,对角线上占据三个子,则该方获胜
如果期盼占满后没有任何一方有三个连着的棋子,则游戏为平局
因为完美的玩家可以永远不输,我们假设挑战不完美的玩家,其有时出错使得我们有机会获胜

## 经典方法
- 博弈论中极大极小算法是不正确的
	- 假设了对手会按照某种特定方式玩游戏
	- 一个极大极小玩家不会让游戏陷入可能会输的状态
- 序列决策问题经典优化算法,如动态规划
	- 可以对任意对手计算出一个最优解,但需要输入对手完整明确的说明,包括对手在某种状态下每一步棋的概率
	- 我们假设这样的先验信息对问题来说不可知
	- 另一方面,这些信息可以从与对手多次交手经验中估计
	- 能做到最好的情况
		- 学习一个对手走棋动作的模型,以一定置信度,用动态规划方法近似对手模型计算一个最优解
	- 与本书强化学习方法相近
- 进化方法,直接在可能的策略空间中,寻找一个可以高概率赢对手的策略
	- 一个策略是一种指导玩家在每一种相应状态下如何走棋的规则
	- 对于每种考虑的策略,获胜概率可以从与对手多次博弈中估算
	- 这种评估方式可以告诉我们下一种应该考虑的策略
	- 经典的进化方法会在策略空间中进行爬山搜索,不断生成和评估新的尝试以获得提升最大的策略
	- 可能使用类似遗传算法的进化方法维护和评估由一组策略组成的集合

## 使用价值函数
1. 建立一个数字表,每一格表示一个游戏可能的状态,每一个数字表示对获胜概率的最新估计,这个估计就是状态的价值,整个表是通过学习得到的价值函数,如果状态A取胜的概率高于状态B,A价值更高或认为A更好
2. 和对手重复多次游戏,为了选择动作,我们要检查每种走棋动作(对应棋盘未落子的一个空格)可能产生的状态,在表格中查找他们当前的价值
3. 大多时候我们贪心行动,选择价值最高的动作.
4. 偶尔会从其他动作随机选择,称为试探性走棋行动
5. 走棋序列在游戏中可被绘制成如下树状结构
6. 在博弈过程中,我们不断改变自身所处状态价值
	1. 为做到这些,我们在贪心动作后得到状态对应价值回溯更新,到动作之前的状态
	2. 我们对早先状态价值向后面状态的价值方向移动一个增量
	3. $S_t$表示贪心动作之前的状态,$S_{t+1}$表示转移后的状态,$S_t$的价值用$V(S_t)$表示
		1. 更新过程可以写成$V(S_t)\gets V(S_t) +\alpha[V(S_{t+1}-V(S_t))]$
		2. $\alpha$为小的正分数,称为步长参数,影响学习速率
		3. 时序差分的例子,价值变化更新的部分依赖于两个不同时刻状态的价值差
	4. 如果步长参数随着时间推移适当减小,对于任何固定对手,会收敛于最优策略下真正获胜概率
	5. 每一步行动是对这个对手最佳行动
	6. 该方法收敛针对这个对手博弈最佳策略
	7. 如果步长参数没有随时间推移降低到0,玩家也可以很好对抗缓慢改变他们对手
 
![[Pasted image 20220921165229.png]]
图:井字棋走棋行动序列.
- 实现表示博弈中所采取的动作,虚线表示强化学习考虑但未做的动作
- 第二步是一个试探性举措,不是局部最优的,未采用$e^{*}$ 动作
- 试探动作不会导致学习,但我们其他动作会产生箭头所示自底向上回溯更新

进化方法和基于价值函数方法的差别
- 评估策略的进化方法需要固定一个策略并且和对手博弈多次,或者与对手的仿真模型进行大量模拟博弈
	- 获胜的频率是对该策略获胜概率的无偏估计,可以用来指导下一次策略选择
	- 每一次策略的改变基于多次博弈,只有每局比赛最后结果会被考虑,博弈中间过程被忽略
		- 如果玩家获胜,认为所有动作都有功劳,与具体动作无关
- 价值函数方法允许我们对单个状态进行评估
- 进化方法和价值函数方法都是对策略空间进行搜索,但是学习价值函数的过程利用了博弈过程的可用信息

强化学习的特性
- 强化学习强调在与环境交互的过程中学习
- 强化学习有明确的目标,正确的动作需要规划和预测,这样才能考虑每次选择的长期影响


# `fas:ShoePrints`Cue
- 这里写线索

# `ris:Loader`Summary
- 这里写总结

>[!link]
>
>- 
