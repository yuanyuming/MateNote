
#Todo

# `ris:InkBottle`Notes
两条同样源远流长的主线
- 源于动物学习心理学的试错法
	- 人工智能早期工作
	- 20世纪80年代激发强化学习复兴
- 最优控制问题以及价值函数和动态规划的解决方案
	- 很大程度不涉及机器学学习
- 时序差分方法

## 最优控制
起源于20世纪50年代末,描述涉及控制器问题,设计目标是使得动态系统随时间变化的某种度量最大化或最小化
20世纪50年代 运用了动态系统的状态和价值函数,“最优回报函数”,定义了一个函数方程,贝尔曼方程.通过求解这个方程来解决最优控制问题的离散随机版本称为马尔可夫决策过程(MDP)
动态规划被普遍认为是解决一般随机最优控制问题唯一可行方法,遭受了贝尔曼”维度灾难”,计算需求随着状态变量增加呈指数级增长,但仍然更有效,使用更广泛.20世纪50年代末期,动态规划被全面开发.
对最优控制和动态规划之间联系认知过程发展缓慢.动态规划作为离线计算,依赖于精确的系统模型和贝尔曼方程的解析解,最简单形态是沿时间线反推进的计算,导致很难看出如何被进行前向计算学习过程利用.
Witten论证了动态规划和学习方法之间紧密的相互关系,动态规划理解神经和认知机制的相关性
动态规划与在线学习的首次完全整合Chris Watkins 1989年的研究,他利用MDP形式对强化学习的方式至今广泛使用.
Dimitri Bertsekas 和 John Tsitsiklis (1996) 神经动态规划,动态规划和人工神经网络的结合.近似动态规划.用强化学习弥补动态规划典型缺陷
最优控制的工作在某种意义是强化学习的工作
强化学习定义为解决强化学习问题的任何有效途径,这些问题与最优控制问题相关,尤其是可以形式化为马尔可夫决策过程的随机最优控制问题

## 试错学习思想
19世纪50年代 Alexander Bain 对摸索和实验学习方法的讨论
巴普洛夫认为强化是动物行为模式的增强,源于动物受到增强剂刺激后与另一刺激或反应形成的短暂关系
后来强化也包括弱化过程,适用于对刺激事件忽略或终止
试错学习思想在计算机中最早出现在人工智能可行性思考
1948年,图灵 快乐痛苦系统
> 当达到没有预设动作的状态时，随机选择一些没有遇到过的数据，记录并试探性地应用这些数据。如果发生了痛苦刺激，停止所有动作试探。如果发生了愉悦刺激，则一直保持动作试探(Turing.1948)。

1954 Farley 和 Clark 描述了一种通过试错学习的神经网络学习机器数字化仿真程序,之后从强化学习转向有监督学习
神经网络中试错学习中的行为选择基本特征是基于评估反馈的

自动学习机对试错学习和现代强化学习有更直接的影响,又称为k臂赌博机算法
提高获得收益概率简单无需大内存的机器
分类器系统,救火队算法,遗传算法
## 统计学习理论
心理学产生,被经济学领域采纳,引发强化学习研究热潮

## 时序差分学习
源于动物学习心理学
次级强化物,与初级强化物(食物,疼痛等)配对,并产生强化属性的刺激物
# `fas:ShoePrints`Cue
- 这里写线索

# `ris:Loader`Summary
- 这里写总结

>[!link]
>
>- 
