> 有些人担心人工智能会让人类觉得自卑，但是实 际上，即使是看到一朵花，我们也应该或多或少 感到一些自愧不如。−艾伦·凯

# 神经元模型
- 大脑由神经元构成
	- 每个神经元
		- 树突获取输入信号
		- 轴突传递输出信号
	- 神经元之间相互连接构成神经网络
	- 形成人脑感知和意识基础
![[Pasted image 20221128103738.png]]
自动学习器
- 神经元输入向量$\mathbf{x}=[x_1,x_2,…,x_n]^T$ 
- 经过函数映射 $f_\theta:\mathbf{x}\to y$ 得到输出y
	- $\theta$:$f$ 自身参数
		- 确定神经元状态,处理逻辑
	- 对于任何采样点存在观测误差$\epsilon$ 
		- 假设$\epsilon \sim \mathscr{N}(\mu,\sigma^2)$  
		- 采集的样本符合
			- $y=wx+b+\epsilon$
		- 减少观测误差引起的估计偏差
			- 采样多组数据样本集合$\mathbb{D}=\{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),…,(x^{(n)},y^{(n)})\}$ 
			- 找出一条直线使得所有采样点到直线误差(Loss)最小
				- 总误差$\mathscr{L}=\frac{1}{n}\sum^n_{i=1}(wx^{(i)}+b-y^{(i)})^2$ 
				- $w*,b*=\underset{w,b}{argmin} \ \mathscr{L}$ (MSE,均方误差)
![[Pasted image 20221128104101.png]]

# 优化方法
- 解析解(Closed-form Solution)
	- 通过严格公式导出的精确解
- 数值解(Numerical Solution)
	- 借助数值方法优化
	- “搜索”,“试错”

梯度下降法(Gradient Descent)
- 梯度
	- 函数对各个自变量的偏导数(Partial Derivative)组成的向量
- 函数在各处的梯度方向$\
# 实战: 线性模型

# 线性回归
