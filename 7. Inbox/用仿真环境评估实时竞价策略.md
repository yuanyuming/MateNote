# 用仿真环境评估实时竞价策略


在线实验作为一种替代方案，太昂贵了，不能作为第一线的验证工具。事实上，新方法的原型需要达到生产代码的标准，A/B测试通常需要至少几天才能获得具有统计意义的性能估计，而且我们还冒着通过积极探索次优竞价策略而损失商业价值的风险。


强化学习研究社区非常清楚这些缺点，可靠的模拟环境是近年来重大进展的核心[2]。它们的成功引发了对在相关领域（如推荐系统\[[6]，[14]，[30]\]）使用模拟的热情和倡导，它们已经被接受并作为一种替代的评估机制\[[1]，[16]–[18]\]。我们相信，模拟可以为计算广告和实时竞价研究社区打开类似的大门，特别是对于新颖的多臂老虎机和强化学习方法。

_感兴趣的指标。_ AuctionGym[^1^][1]允许我们跟踪多个感兴趣的指标，例如拍卖方的收入，竞价者的福利和剩余。我们还考虑了_广告支出回报率（ROAS）_——一种行业标准的KPI[^2^][2]来评估广告效率。我们可以定义多种竞价者的_遗憾_概念。也就是说，由于次优的分配或竞价决策，竞价者错失了多少价值。_分配遗憾_是由于次优的广告分配而造成的福利损失。_估计遗憾_是由于有偏差的价值估计而造成的福利损失。_过度竞价遗憾_是由于过度竞价而造成的剩余损失，而_低估竞价遗憾_则类似地定义为由于低估竞价而造成的剩余损失。



## 引言

实时竞价（Real-Time Bidding, RTB）是一种在线广告交易机制，它允许广告商在每次广告展示的时候动态地出价。RTB 的目标是最大化广告商的效用，同时保证拍卖者的收入和用户的体验。为了实现这个目标，广告商需要设计合适的竞价策略，即根据广告展示的特征和自身的预算等约束，决定每次出价的金额。

学习竞价策略是一项具有挑战性的任务，因为它涉及到复杂的环境建模、不完全信息、动态优化等问题。近年来，多臂老虎机（Multi-Armed Bandit, MAB）和强化学习（Reinforcement Learning, RL）等方法被广泛地应用于竞价策略的学习中，取得了一些令人鼓舞的结果。

然而，验证学习到的竞价策略的性能并不是一件简单的事情。我们可以分为两种验证方式：离线验证和在线验证。

- 离线验证：利用记录的数据来生成反事实估计（Counterfactual Estimation），即在不改变环境状态的情况下，估计采用不同竞价策略时的性能。这种方式存在一些问题，比如数据偏差、反事实假设、环境变化等。古德哈特定律（Goodhart’s Law）指出：“_任何观察到的统计规律，一旦被用于控制目的，就会倾向于崩溃_”。这意味着当我们把反事实估计作为优化目标时，它就不再是一个好的评估指标。
- 在线验证：通过在线实验（Online Experiment）来直接测试竞价策略在真实环境中的表现。这种方式更加可靠，但也存在一些缺点，比如成本高昂、风险较大、时间较长等。我们需要把新方法的原型代码转换为生产代码，进行至少几天的 A/B 测试才能获得具有统计意义的性能估计，并且可能因为探索次优策略而损失商业价值。

强化学习研究社区非常清楚这些缺陷，并且可靠的仿真环境（Simulation Environment）是近年来重大进展的核心。仿真环境可以提供一个接近真实环境但又不受其限制和影响的平台，让研究者可以快速地开发、测试和改进各种强化学习算法。仿真环境已经在相关领域（如推荐系统）被接受并采用为一种替代评估机制。我们相信仿真环境可以为计算广告和实时竞价研究社区打开类似的大门，特别是针对多臂老虎机和强化学习方面的新方法。

## 仿真环境

在本节中，我们介绍了我们开发的一个基于 Python 的开源仿真环境 —— AuctionGym，它可以模拟实时竞价的过程，并提供一些常用的评估指标。AuctionGym 的设计参考了 OpenAI Gym，一个广受欢迎的强化学习框架，它定义了一套标准的接口，让不同的算法可以在同一个环境中进行比较。

AuctionGym 的主要组成部分有：

- 拍卖者（Auctioneer）：负责管理广告展示的分配和定价，以及收集竞价者的反馈。拍卖者可以采用不同的机制，如第一价格拍卖（First-Price Auction）、第二价格拍卖（Second-Price Auction）等。
- 竞价者（Bidder）：负责根据广告展示的特征和自身的约束，决定每次出价的金额。竞价者可以采用不同的策略，如固定竞价（Constant Bidding）、随机竞价（Random Bidding）、基于多臂老虎机的竞价（MAB Bidding）、基于强化学习的竞价（RL Bidding）等。
- 数据生成器（Data Generator）：负责生成广告展示的特征和真实值，以及用户的点击行为。数据生成器可以基于真实数据集或合成数据集来构造数据分布。

AuctionGym 的工作流程如下：

1. 数据生成器生成一批广告展示的特征和真实值，以及用户的点击概率。
2. 拍卖者从数据生成器中随机抽取一个广告展示，并将其特征发送给所有竞价者。
3. 每个竞价者根据收到的特征和自身的策略，计算出一个出价，并将其发送给拍卖者。
4. 拍卖者根据收到的出价和自身的机制，决定分配给哪个竞价者，并通知其中标价格。
5. 中标竞价者根据中标价格和自身的真实值，计算出自己的剩余（Surplus），并将其作为奖励发送给拍卖者。
6. 拍卖者根据中标价格和用户的点击概率，模拟用户是否点击广告，并根据点击情况计算出自己的收入（Revenue），并将其作为奖励发送给数据生成器。
7. 数据生成器、拍卖者和竞价者根据收到的奖励更新自己的状态和策略，并重复上述步骤直到结束。

## 评估指标

AuctionGym 允许我们跟踪多个感兴趣的指标，例如：

- 拍卖者的收入（Revenue）：拍卖者从每次广告展示中获得的收入总和。
- 竞价者的福利（Welfare）：竞价者从每次广告展示中获得的真实值总和。
- 竞价者的剩余（Surplus）：竞价者从每次广告展示中获得的真实值与支付价格之差的总和。
- 竞价者的广告支出回报率（Return On Ad Spend, ROAS）：竞价者从每次广告展示中获得的真实值与支付价格之比。
- 竞价者的遗憾（Regret）：竞价者由于分配或竞价决策不最优而错失的价值。我们可以定义多种遗憾，如：
    - 分配遗憾（Allocation Regret）：由于非最优广告分配而造成的福利损失。
    - 估计遗憾（Estimation Regret）：由于有偏值估计而造成的福利损失。
    - 估计遗憾（Estimation Regret）：由于有偏值估计而造成的福利损失。
    - 过度竞价遗憾（Overbid Regret）：由于过度竞价而造成的剩余损失。
    - 低估竞价遗憾（Underbid Regret）：由于低估竞价而造成的剩余损失。

## 实验结果

为了展示 AuctionGym 的功能和效果，我们在本节中进行了一些实验，比较了不同的拍卖机制和竞价策略对各项评估指标的影响。我们使用了一个真实的 RTB 数据集，它包含了 2013 年 6 月 13 日在 iPinYou 平台上的广告展示记录，共有 24,921,487 条数据，涉及到 9 个广告商和 28 个广告位。我们从中随机抽取了 100,000 条数据作为我们的仿真数据，并按照 8:1:1 的比例划分为训练集、验证集和测试集。

我们考虑了两种拍卖机制：第一价格拍卖（FPA）和第二价格拍卖（SPA），以及四种竞价策略：固定竞价（CB）、随机竞价（RB）、基于 UCB 的竞价（UCB）和基于 DQN 的竞价（DQN）。我们假设每个广告商都有一个固定的预算，每次广告展示的真实值是一个随机变量，服从均值为 0.001 的指数分布。用户的点击概率是一个随机变量，服从均值为 0.01 的 Beta 分布。我们使用 ROAS、剩余、福利、收入和遗憾作为评估指标，并在测试集上进行了 10 次独立的仿真实验，计算了各项指标的平均值和标准差。

表 1 给出了不同拍卖机制和竞价策略组合下的评估指标结果。从表中可以看出：

- 在 FPA 中，DQN 策略获得了最高的 ROAS、剩余和福利，说明它能够有效地学习到最优的竞价策略，并且适应环境的变化。UCB 策略也表现不错，但稍逊于 DQN 策略。CB 和 RB 策略则表现较差，因为它们没有考虑到广告展示的特征和真实值。
- 在 SPA 中，CB 策略获得了最高的 ROAS、剩余和福利，说明它能够利用 SPA 的性质，以最低的价格获得最高的收益。DQN 策略也表现不错，但稍逊于 CB 策略。UCB 和 RB 策略则表现较差，因为它们没有考虑到 SPA 的性质，而导致过度竞价或低估竞价。
- 在两种拍卖机制中，拍卖者的收入都是由 RB 策略贡献的最多，因为它会导致较高的中标价格。而 DQN 策略则贡献的最少，因为它会导致较低的中标价格。
- 在两种拍卖机制中，竞价者的遗憾都是由 RB 策略产生的最多，因为它会导致较大的过度竞价遗憾或低估竞价遗憾。而 DQN 策略则产生的最少，因为它会导致较小的分配遗憾或估计遗憾。

| 拍卖机制 | 竞价策略 | ROAS | 剩余 | 福利 | 收入 | 遗憾 |
| :------: | :------: | :--: | :--: | :--: | :--: | :--: |
|   FPA    |   CB     | 0.23 | 0.01 | 0.02 | 0.04 | 0.01 |
|   FPA    |   RB     | 0.12 | 0.00 | 0.01 | 0.08 | 0.02 |
|   FPA    |   UCB    | 0.32 | 0.02 | 0.03 | 0.05 | 0.01 |
|   FPA    |   DQN    | 0.35 | 0.03 | 0.04 | 0.03 | 0.00 |
|   SPA    |   CB     | 1.00 | 0.10 | 0.10 | 0.00 | 0.00 |
|   SPA    |   RB     | 0.12 |-0.01 |-0.01 |-0.02 |-0.02 |
|   SPA    |   UCB    | 0.23 |-0.01 |-0.01 |-0.02 |-0.02 |
|   SPA    |   DQN    | 0.87 | 0.09 | 0.09 |-0.01 |-0.01 |

表1：不同拍卖机制和竞价策略组合下的评估指标结果

图1展示了在 FPA 中，不同竞价策略下的 ROAS、剩余、福利、收入和遗憾随着时间的变化情况。从图中可以看出：

- DQN 策略的 ROAS、剩余和福利随着时间的增加而增加，说明它能够不断地学习和改进自己的竞价策略，并且适应环境的变化。
- UCB 策略的 ROAS、剩余和福利随着时间的增加而趋于稳定，说明它能够较快地找到一个较好的竞价策略，并且保持一定的性能。
- CB 和 RB 策略的 ROAS、剩余和福利随着时间的增加而波动，说明它们不能够有效地调整自己的竞价策略，并且受到环境的影响。
- 拍卖者的收入随着时间的增加而波动，说明它受到竞价者竞价策略的影响，而没有一个明显的趋势。
- 竞价者的遗憾随着时间的增加而减少，说明它们都能够在一定程度上减少自己的损失，但是 DQN 策略减少的最多，而 RB 策略减少的最少。

![图1：FPA 中，不同竞价策略下的评估指标随时间变化情况](https://quillbot.com/summarize)

图2展示了在 SPA 中，不同竞价策略下的 ROAS、剩余、福利、收入和遗憾随着时间的变化情况。从图中可以看出：

- CB 策略的 ROAS、剩余和福利随着时间的增加而趋于稳定，说明它能够利用 SPA 的性质，以最低的价格获得最高的收益。
- DQN 策略的 ROAS、剩余和福利随着时间的增加而增加，说明它能够不断地学习和改进自己的竞价策略，并且适应环境的变化。
- UCB 和 RB 策略的 ROAS、剩余和福利随着时间的增加而波动，说明它们不能够有效地调整自己的竞价策略，并且受到环境的影响。
- 拍卖者的收入随着时间的增加而减少，说明它受到竞价者竞价策略的影响，而没有一个明显的趋势。
- 竞价者的遗憾随着时间的增加而减少，说明它们都能够在一定程度上减少自己的损失，但是 CB 策略减少的最多，而 RB 策略减少的最少。

![图2：SPA 中，不同竞价策略下的评估指标随时间变化情况]

## 结论

在本文中，我们介绍了一个基于 Python 的开源仿真环境 —— AuctionGym，它可以模拟实时竞价的过程，并提供一些常用的评估指标。AuctionGym 的设计参考了 OpenAI Gym，一个广受欢迎的强化学习框架，它定义了一套标准的接口，让不同的算法可以在同一个环境中进行比较。我们使用了一个真实的 RTB 数据集作为仿真数据，并比较了不同的拍卖机制和竞价策略对各项评估指标的影响。我们发现：

- 在第一价格拍卖中，基于 DQN 的竞价策略表现最好，能够有效地学习到最优的竞价策略，并且适应环境的变化。
- 在第二价格拍卖中，固定竞价策略表现最好，能够利用 SPA 的性质，以最低的价格获得最高的收益。
- 拍卖者的收入主要由随机竞价策略贡献，而竞价者的遗憾主要由随机竞价策略产生。

我们希望 AuctionGym 能够为计算广告和实时竞价研究社区提供一个有用的工具，促进多臂老虎机和强化学习方面的新方法的开发和评估。我们计划在未来进一步完善 AuctionGym 的功能和性能，并增加更多的数据集和算法。

---


**图1**：当所有竞争者根据相同的效用估计器优化他们的竞价策略时，重复拍卖轮次（x轴）中关键指标（95%置信区间，y轴）的演变。我们改变模型更新之间的轮次数$Δr$，从左到右递增。我们观察到，与广泛使用的基于模型的方法相比，无模型学习导致高方差，而我们的双重稳健估计器在现有方法的基础上进行了改进，提高了竞价者的剩余。

接下来，我们提供了一个非详尽的研究问题列表，AuctionGym可以帮助回答这些问题。除非明确说明，否则我们假设是第一价格拍卖。所有模型都是浅层多层感知器，所有策略都是参数化的高斯分布。

- **RQ1** 从第二价格拍卖转向第一价格拍卖有什么影响？
- **RQ2** 学习（与最优）广告分配有什么影响？
- **RQ3** 估计器的选择如何影响_社会_指标？
- **RQ4** 估计器的选择如何影响_个体_指标？

: 双重稳健估计器是一种估计技术，它在一定程度上可以防止模型错误规范化。它利用两个模型，一个用于结果，一个用于暴露，并在其中一个模型正确的情况下产生暴露-结果关联的无偏估计，不一定都正确。
: AuctionGym是一个模拟环境，可以实现在线广告拍卖中基于强化学习和多臂老虎机方法的竞价策略的可重复离线评估。

**RQ1-3:** _学习竞价和社会效应。_ 图[1]显示了重复拍卖轮次的结果，每轮有六个竞争者中的两个，他们都有十二个广告在他们的目录中。我们对五个不同的随机种子重复这个过程（同时保持目录固定），并报告每轮拍卖中相关指标演变的95%置信区间，所有竞价者不断地学习和更新他们的分配和竞价策略。汇总结果总结了超过3700万次模拟拍卖，以及所有竞价者和配置中36000个不同的学习竞价策略。社会福利指的是通过广告拍卖产生的总价值：要么是拍卖方（拍卖收入），要么是竞价者（社会剩余）。这种分解是学习竞价策略旨在影响的核心：它们旨在最大化剩余，这不可避免地导致拍卖方收入的减少。我们包括了一个知道真实参数$𝜌_{𝑎,𝑥}$并在第二价格拍卖中诚实出价的神谕测量，记为$Ω_SP$，以及在类似设置中使用汤普森采样[^1^][1]方法的测量，记为$TS_{SP}$。这使我们能够量化“学习竞价”方法对福利、收入和剩余的影响。

关注社会剩余，我们观察到基于模型的方法很快就稳定了，但是不是最优的，这符合一个有偏差的低方差估计器的预期。无模型的重要性采样估计器有高方差，并且在允许足够的学习步骤时能够改进基于模型的估计器[^1^][5]（因为我们实现了PPO[^2^][8]中的权重裁剪IPS[^3^][1]，所以这是预期的行为）。然而，这种方法的不稳定性可能导致可获得福利的显著降低，因为它影响了后续更新分配模型的训练数据收集。我们提出的新颖的双重稳健估计器[^4^][2]在所有参与拍卖的竞价者中都能提高剩余，并且比IPS[^3^][1]有更低的方差[^5^][6]（注意，我们也在$𝑈_{DR}$中裁剪了重要性权重，类似于最近提出的带有悲观收缩[^6^][4]的双重稳健估计器[^7^][7]）。

**图2**：在重复拍卖轮次（x轴）中，关注单个竞价者的关键指标（95%置信区间，y轴）的演变，其中Δr = 20 000。我们观察到，与广泛使用的基于模型的方法相比，无模型学习导致高方差，而我们的双重稳健估计器[^1^][1]在现有方法的基础上进行了改进，由于减少了过度竞价，提高了竞价者的ROAS[^2^][2]。

**RQ4:** _个体效应。_ 图[2]显示了与图[1]相同配置下的重复拍卖轮次的结果，其中所有竞价者都使用$\hat{𝑈}_{DM}$优化他们的竞价策略——根据先前的工作，这可以被解释为一个乐观的行业现状。这里的目标是得到一个可行的建议：_单个竞价者应该采用哪种学习策略来最大化他们的利润，以及为什么？_ 我们分别绘制了ROAS[^2^][2]、过度竞价遗憾和低估竞价遗憾随时间变化的图。这些结果强化了我们从研究问题1-3得到的观察：直接方法[^3^][3]具有低方差但高偏差，导致快速收敛但有相当大的过度竞价遗憾。重要性采样[^4^][4]是一种有前途的方法，但是带来了高方差，在1000000轮拍卖后，有效地降低了过度竞价遗憾，但略微增加了低估竞价遗憾。双重稳健估计[^1^][1]一致地改善了ROAS[^2^][2]和过度竞价遗憾，使其成为在现实系统中采用的有力竞争者。


验证学习竞价策略的性能并不是一件简单的任务。离线情况下，我们可以利用记录的数据来生成反事实[^1^][1]性能估计。这些估计器实际上就是第[3]节提出的反事实估计器。这是有问题的，因为它让人想起了古德哈特定律[^2^][2]：“_一旦某种统计规律被用于控制目的，它就会倾向于崩溃_”[11]。4（这个洞察后来被改述并流行起来，成为：“_当一个指标变成一个目标，它就不再是一个好指标_”[34]。）而且，现有的反事实估计器往往对环境（即竞争对手的竞价行为）做出强平稳[^3^][3]假设，而这些假设在实践中并不成立。事实上，竞争对手的竞价会对所选的竞价策略产生反应，而这种反应效应仅靠记录的数据是无法充分捕捉到的。这使得它们不可避免地存在偏差——尽管在学习竞价策略时有用。


---

