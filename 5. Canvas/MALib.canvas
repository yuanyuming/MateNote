{
	"nodes":[
		{"type":"text","text":"-   关键组件：\n    -   Policy：作为一个行为接口，封装了参数化一个代理策略的模型。\n    -   Evaluator：评估Policy的性能。\n    -   RolloutWorkerManager：负责一群RolloutWorker，用于采样和更新Policy。\n    -   AgentInterfaceManager：负责一群AgentInterface，用于与环境交互和管理Policy。\n-   对于嵌套的强化学习算法，我们将其分为AgentInterfaceManager和RolloutWorkerManager的协调。\n-   我们将所有这些组件打包为一个场景，隔离了组件交互的细节。","id":"92a9c496f0fbfe7b","x":-140,"y":-360,"width":485,"height":360},
		{"type":"file","file":"0.Attach/0.4.Picture/Pasted image 20230415113119.png","id":"229bd9046eba5a9a","x":-211,"y":-600,"width":628,"height":220},
		{"type":"text","text":"## Scenarios\n- 一个场景定义了一个训练实例，它将所有的MALib组件和资源管理结合在一起。\n- 用户可以通过继承Scenario类来创建自己的场景，我们已经在malib.scenarios包下实现了两个标准的场景。\n- 要将一个场景部署为一个实例，你只需要实现一个场景实例，比如PSROScenario，然后将它加载到位于malib.runner下的runner接口。\n\n```\nfrom malib.runner import run\nfrom malib.scenarios.psro_scenario import PSROScenario\n\nscenario = PSROScenario(\n    name=f\"psro_{env_id}\",\n    log_dir=runtime_logdir,\n    algorithms=algorithms,\n    env_description=env_description,\n    training_config=training_config,\n    rollout_config=rollout_config,\n    # control the outer loop.\n    global_stopping_conditions={\"max_iteration\": 50},\n    agent_mapping_func=agent_mapping_func,\n    # for the training of best response.\n    stopping_conditions={\n        \"training\": {\"max_iteration\": int(1e4)},\n        \"rollout\": {\"max_iteration\": 100},\n    },\n)\n\nrun(scenario)\n```\n\n### Available Scenarios[](https://malib.readthedocs.io/en/latest/concepts.html#available-scenarios \"Permalink to this heading\")\n\n-   [Multi-agent RL Scenario](https://malib.readthedocs.io/en/latest/scenarios/marl-scenario.html#marl-scenario-doc)\n    \n-   [PSRO Scenario](https://malib.readthedocs.io/en/latest/scenarios/psro-scenario.html#psro-scenario-doc)\n    \n-   [League Training Scenario](https://malib.readthedocs.io/en/latest/scenarios/league-scenario.html#league-training-doc)","id":"b728298ac869869a","x":-300,"y":40,"width":645,"height":940},
		{"type":"text","text":"## Reinforcement Learning Algorithms\n- MALib支持基于人口的学习算法，它们运行嵌套的强化学习过程。\n- 为了更好地与高层次的基于人口的优化协调，MALib将传统的强化学习算法分为三个关键概念，即策略、训练器和AgentInterface。","id":"34ea1cb12e336597","x":-300,"y":1100,"width":470,"height":200},
		{"type":"text","text":"## Policy\n-   策略是定义代理在环境中如何行动的Python类。代理查询策略来确定行动。在一个环境中，可能有多个策略，其中一些可以与多个环境代理相关联。\n-   目前，策略的实现与Tianshou库兼容。但是，仅适用于PyTorch实现。策略的定制对用户来说非常方便，因为我们已经将策略抽象为两种主流的实现，即基于价值的和基于策略梯度的。\n\n```python\nclass A2CPolicy(PGPolicy):\n    def __init__(\n        self,\n        observation_space: spaces.Space,\n        action_space: spaces.Space,\n        model_config: Dict[str, Any],\n        custom_config: Dict[str, Any],\n        **kwargs\n    ):\n        super().__init__(\n            observation_space, action_space, model_config, custom_config, **kwargs\n        )\n\n        preprocess_net: nn.Module = self.actor.preprocess\n        if isinstance(action_space, spaces.Discrete):\n            self.critic = discrete.Critic(\n                preprocess_net=preprocess_net,\n                hidden_sizes=model_config[\"hidden_sizes\"],\n                device=self.device,\n            )\n        elif isinstance(action_space, spaces.Box):\n            self.critic = continuous.Critic(\n                preprocess_net=preprocess_net,\n                hidden_sizes=model_config[\"hidden_sizes\"],\n                device=self.device,\n            )\n        else:\n            raise TypeError(\n                \"Unexpected action space type: {}\".format(type(action_space))\n            )\n\n        self.register_state(self.critic, \"critic\")\n\n    def value_function(self, observation: torch.Tensor, evaluate: bool, **kwargs):\n        \"\"\"Compute values of critic.\"\"\"\n\n        with torch.no_grad():\n            values, _ = self.critic(observation)\n        return values.cpu().numpy()\n```","id":"fc7ba7b206ef155a","x":-500,"y":1366,"width":820,"height":1234},
		{"type":"text","text":"## Trainer\n-   一个训练器定义了一个策略的损失计算和特定的训练逻辑，用户可以加载一个策略实例和训练配置来执行训练。\n\n```\nfrom mailb.rl.dqn import DQNTrainer, DEFAULT_CONFIG\n\ntrainer = DQNTrainer(\n    training_config=DEFAULT_CONFIG[\"training_config\"],\n    policy_instance=policy\n)\n\nloss = trainer(buffer=Batch(**data))\n```\n\n[malib.rl.common package](https://malib.readthedocs.io/en/latest/api/malib.rl.common.html)","id":"c7b8f6d588b920cb","x":-500,"y":2660,"width":660,"height":420},
		{"type":"text","text":"## AgentInterface\n管理策略池及其依赖，安排策略组合的训练，与远程服务器交互数据和参数，实现不同的训练范式和分布式策略。","id":"911b127ff68fa3ca","x":-500,"y":3180,"width":365,"height":160},
		{"type":"file","file":"0.Attach/0.4.Picture/Pasted image 20230415125848.png","id":"06171a1fc0cf4b2a","x":-90,"y":3237,"width":673,"height":140},
		{"type":"text","text":"## AgentInterface管理\n基于策略池的管理单元，实现训练或进化接口，支持人口隔离和多智能体同时训练，使用TrainingManager和RolloutWorker进行管理。","id":"e037e6a43fb82186","x":-500,"y":3440,"width":410,"height":160},
		{"type":"text","text":"## Rollout管理\n使用RolloutWorkerManger创建独立的rollout工作器，封装actor池和推理CS实例。","id":"b85f70ff488e53f4","x":-500,"y":3660,"width":445,"height":185},
		{"type":"file","file":"0.Attach/0.4.Picture/Pasted image 20230415130357.png","id":"2df27aacd835a1f8","x":-20,"y":3683,"width":1676,"height":140},
		{"type":"text","text":"##  Rollout Worker\n- 执行模拟任务，创建actor池，考虑探索和利用的需求。\n- 考虑环境向量化技术，支持共享策略服务器或独立策略副本，详见环境和推理机制部分。\n```\nactor_pool = ActorPool(\n    [\n        self.inference_client_cls.remote(\n            env_desc,\n            ray.get_actor(settings.OFFLINE_DATASET_ACTOR),\n            max_env_num=num_env_per_thread,\n            use_subproc_env=rollout_config[\"use_subproc_env\"],\n            batch_mode=rollout_config[\"batch_mode\"],\n            postprocessor_types=rollout_config[\"postprocessor_types\"],\n            training_agent_mapping=agent_mapping_func,\n        )\n        for _ in range(num_threads + num_eval_threads)\n    ]\n)\n```","id":"22419e87cf378d7b","x":-490,"y":3880,"width":830,"height":520},
		{"type":"text","text":"## 人口评估\n基于策略组合评估和Meta Solvers，使用Payoff Manager给出的策略组合。","id":"f28de30db05d770e","x":-490,"y":4460,"width":385,"height":163},
		{"type":"text","text":"## population-based reinforcement learning algorithm\n一种利用多个智能体同时进行强化学习的方法，它可以通过动态调整智能体的超参数和模型，来寻找最优的策略和多样性。它可以解决强化学习中的探索问题，也可以获得多个优秀且不同的策略。\n","id":"6b486ac0ff18c154","x":-760,"y":-260,"width":406,"height":212},
		{"type":"file","file":"0.Attach/0.4.Picture/Pasted image 20230415130804.png","id":"2398cf03d9bdb7ef","x":-55,"y":4480,"width":612,"height":200}
	],
	"edges":[]
}