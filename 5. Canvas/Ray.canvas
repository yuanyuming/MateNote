{
	"nodes":[
		{"type":"text","text":"#  Ray\nPython AI框架, 并行 分布式\n\n## Ray 扩展\n\n- Batch\n- Model 训练\n- 超参数调参\n- Model 伺服\n- 强化学习\n\n## Getting Started\n\n- 基础\n- 安装\n- 实验\n\n## 进阶\n\n- AI 运行时\n- Ray Core\n- Ray 集群","id":"d49021dbd7e0f5c1","x":-125,"y":-280,"width":285,"height":660},
		{"type":"text","text":"# Overview\n\n## Components\n\n- 可扩展库\n\t- 数据处理\n\t- 分布式训练\n\t- 超参数调参\n\t- 强化学习\n\t- Model 伺服\n- 并行化扩展Python程序\n- 与现有工具K8s. AWS, GCP, Azure 集成\n\n## Distributed System Key Process\n\n- Orchestration\n\t- Managing the compens of distributed system\n- Scheduling\n\t- Coordinate the task\n- Fault Tolerance\n\t- Ensure the task complete\n- Auto Scaling\n\t- Adjust the resource allocation dynamically\n\n## Usage\n\n- Batch inference\n- Parallel Traning\n- Model serving\n- Distributed training of large models\n- Parallel hyperparameter tuning experiments\n- Reinforcement Learning\n- ML Platform\n\n## Framework\n\n![[Pasted image 20230704151435.png]]\n\n1. Ray AI: Runtime toolkit for ML applications\n2. Ray Core: Scale applications and accelerate workloads\n3. Ray Cluster: fixed-size or autoscale, according to the request\n\n## Product\n\n### Ray Air\nScale machine learning workloads\n- data processing\n- model training\n- tuning\n- reinforcement learning\n- model serving\n- ...\n\n### Ray Core\nBulid distributed application\nParallelize single machine code\n\n### Ray Clusters\nDeploy large-scale workloads\n\n","id":"7f703f5163265f09","x":180,"y":-280,"width":500,"height":1960},
		{"type":"text","text":"# Getting Started\n\n- Ray AI Runtime\n\t- scale end-to-end ML applications\n- Ray Libraries\n\t- scale single ML workloads\n- Ray Core\n\t- Scale General Python applications\n- Ray Clusters\n\t- Deploy to the cloud\n- Debug and Monitor\n\n","id":"177311d27d127974","x":700,"y":-280,"width":520,"height":400},
		{"type":"text","text":" ## Ray AI Runtime(Ray AIR)\n\n### Installation\n```shell\npip install -U \"ray[air]\"\n```\n\n### Data Processing\n\nLoad Data\n```python\nimport ray\n\n# Load data.\ndataset = ray.data.read_csv(\"s3://anonymous@air-example-data/breast_cancer.csv\")\n\n# Split data into train and validation.\ntrain_dataset, valid_dataset = dataset.train_test_split(test_size=0.3)\n\n# Create a test dataset by dropping the target column.\ntest_dataset = valid_dataset.drop_columns(cols=[\"target\"])\n```\n\nPreprocess\n```python\n# Create a preprocessor to scale some columns.\nfrom ray.data.preprocessors import StandardScaler\n\npreprocessor = StandardScaler(columns=[\"mean radius\", \"mean texture\"])\n```\n\n### Scale training\nuse Ray's wrapper library `xgboost_ray`\n```shell\npip install xgboost_ray\n```\n\ntrain\n```python\nfrom ray.air.config import ScalingConfig\nfrom ray.train.xgboost import XGBoostTrainer\n\ntrainer = XGBoostTrainer(\n    scaling_config=ScalingConfig(\n        # Number of workers to use for data parallelism.\n        num_workers=2,\n        # Whether to use GPU acceleration.\n        use_gpu=False,\n        # Make sure to leave some CPUs free for Ray Data operations.\n        _max_cpu_fraction_per_node=0.9,\n    ),\n    label_column=\"target\",\n    num_boost_round=20,\n    params={\n        # XGBoost specific params\n        \"objective\": \"binary:logistic\",\n        # \"tree_method\": \"gpu_hist\",  # uncomment this to use GPUs.\n        \"eval_metric\": [\"logloss\", \"error\"],\n    },\n    datasets={\"train\": train_dataset, \"valid\": valid_dataset},\n    preprocessor=preprocessor,\n)\nbest_result = trainer.fit()\nprint(best_result.metrics)\n```\n\n### Tune the hyperparameters\nconfigure the parameter\n```python\nfrom ray import tune\n\nparam_space = {\"params\": {\"max_depth\": tune.randint(1, 9)}}\nmetric = \"train-logloss\"\n```\nRun tuning\n```python\nfrom ray.tune.tuner import Tuner, TuneConfig\n\ntuner = Tuner(\n    trainer,\n    param_space=param_space,\n    tune_config=TuneConfig(num_samples=5, metric=metric, mode=\"min\"),\n)\nresult_grid = tuner.fit()\nbest_result = result_grid.get_best_result()\nprint(\"Best result:\", best_result)\n```\n\n### Use the model\nbatch prediction `BatchPredictor`\n```python\nfrom ray.train.batch_predictor import BatchPredictor\nfrom ray.train.xgboost import XGBoostPredictor\n\n# You can also create a checkpoint from a trained model using\n# `XGBoostCheckpoint.from_model`.\ncheckpoint = best_result.checkpoint\n\nbatch_predictor = BatchPredictor.from_checkpoint(checkpoint, XGBoostPredictor)\n\npredicted_probabilities = batch_predictor.predict(test_dataset)\npredicted_probabilities.show()\n# {'predictions': 0.9970690608024597}\n# {'predictions': 0.9943051934242249}\n# {'predictions': 0.00334902573376894}\n# ...\n```","id":"4a00e6e118145612","x":-40,"y":1960,"width":623,"height":2673},
		{"type":"text","text":"## Ray Libraries\nUse individual libraries\n\n### Data: Scalable Datasets (Ray Data)\n**Ray Data**: load exchange data, data transformation `map`, `fliter`, `repartition`\n\ninstall `Ray Data` `Dask`\n```shell\npip install -U \"ray[data]\" dask\n```\n\ncreating synthetic data `ray.data.range()`, `ray.data.from_items()`\n\n```python\nimport ray\n\n# Create a Dataset of Python objects.\nds = ray.data.range(10000)\n# -> Dataset(num_blocks=200, num_rows=10000, schema=<class 'int'>)\n\nds.take(5)\n# -> [0, 1, 2, 3, 4]\n\nds.schema()\n# <class 'int'>\n\n# Create a Dataset from Python objects, which are held as Arrow records.\nds = ray.data.from_items([\n        {\"sepal.length\": 5.1, \"sepal.width\": 3.5,\n         \"petal.length\": 1.4, \"petal.width\": 0.2, \"variety\": \"Setosa\"},\n        {\"sepal.length\": 4.9, \"sepal.width\": 3.0,\n         \"petal.length\": 1.4, \"petal.width\": 0.2, \"variety\": \"Setosa\"},\n        {\"sepal.length\": 4.7, \"sepal.width\": 3.2,\n         \"petal.length\": 1.3, \"petal.width\": 0.2, \"variety\": \"Setosa\"},\n     ])\n# Dataset(num_blocks=3, num_rows=3,\n#         schema={sepal.length: float64, sepal.width: float64,\n#                 petal.length: float64, petal.width: float64, variety: object})\n\nds.show()\n# -> {'sepal.length': 5.1, 'sepal.width': 3.5,\n#     'petal.length': 1.4, 'petal.width': 0.2, 'variety': 'Setosa'}\n# -> {'sepal.length': 4.9, 'sepal.width': 3.0,\n#     'petal.length': 1.4, 'petal.width': 0.2, 'variety': 'Setosa'}\n# -> {'sepal.length': 4.7, 'sepal.width': 3.2,\n#     'petal.length': 1.3, 'petal.width': 0.2, 'variety': 'Setosa'}\n\nds.schema()\n# -> sepal.length: double\n# -> sepal.width: double\n# -> petal.length: double\n# -> petal.width: double\n# -> variety: string\n```\n\nDataset can create from:\n- Filesystem supported by pyarrow. \n- Ray Object Store\n- Ray-compatible distributed DataFrames\n\n```python\n# Create from CSV.\nds = ray.data.read_csv(\"s3://anonymous@air-example-data/iris.csv\")\n# Dataset(num_blocks=1, num_rows=150,\n#         schema={sepal length (cm): double, sepal width (cm): double, \n#         petal length (cm): double, petal width (cm): double, target: int64})\n\n# Create from Parquet.\nds = ray.data.read_parquet(\"s3://anonymous@air-example-data/iris.parquet\")\n# Dataset(num_blocks=1, num_rows=150,\n#         schema={sepal.length: double, sepal.width: double, \n#         petal.length: double, petal.width: double, variety: string})\n```\n\nusing `.map()` transform dataset in parallel. Also support `.filter()`, `.flat_map()`\n\n```python\nimport pandas\n\n# Create 10 blocks for parallelism.\nds = ds.repartition(10)\n# Dataset(num_blocks=10, num_rows=150,\n#         schema={sepal.length: float64, sepal.width: float64,\n#                 petal.length: float64, petal.width: float64, variety: object})\n\n# Find rows with sepal.length < 5.5 and petal.length > 3.5.\ndef transform_batch(df: pandas.DataFrame) -> pandas.DataFrame:\n    return df[(df[\"sepal.length\"] < 5.5) & (df[\"petal.length\"] > 3.5)]\n\ntransformed_ds = ds.map_batches(transform_batch, batch_format=\"pandas\")\n# Dataset(num_blocks=10, num_rows=3,\n#         schema={sepal.length: float64, sepal.width: float64,\n#                 petal.length: float64, petal.width: float64, variety: object})\n\ntransformed_ds.show()\n# -> {'sepal.length': 5.2, 'sepal.width': 2.7,\n#     'petal.length': 3.9, 'petal.width': 1.4, 'variety': 'Versicolor'}\n# -> {'sepal.length': 5.4, 'sepal.width': 3.0,\n#     'petal.length': 4.5, 'petal.width': 1.5, 'variety': 'Versicolor'}\n# -> {'sepal.length': 4.9, 'sepal.width': 2.5,\n#     'petal.length': 4.5, 'petal.width': 1.7, 'variety': 'Virginica'}\n```\n\n### Train: Distributed Training (Ray Train)\n\n#### Pytorch\ninstall\n```shell\npip install -U \"ray[train]\" torch torchvision\n```\n\nSet up dataset model\n```python\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\n\ndef get_dataset():\n    return datasets.FashionMNIST(\n        root=\"/tmp/data\",\n        train=True,\n        download=True,\n        transform=ToTensor(),\n    )\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28 * 28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n        )\n\n    def forward(self, inputs):\n        inputs = self.flatten(inputs)\n        logits = self.linear_relu_stack(inputs)\n        return logits\n```\n\ndefine a single worker training function\n```python\ndef train_func():\n    num_epochs = 3\n    batch_size = 64\n\n    dataset = get_dataset()\n    dataloader = DataLoader(dataset, batch_size=batch_size)\n\n    model = NeuralNetwork()\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n\n    for epoch in range(num_epochs):\n        for inputs, labels in dataloader:\n            optimizer.zero_grad()\n            pred = model(inputs)\n            loss = criterion(pred, labels)\n            loss.backward()\n            optimizer.step()\n        print(f\"epoch: {epoch}, loss: {loss.item()}\")\n```\n\nconvert this function to distributed mutli-worker training function\n- use `ray.train.torch.prepare_model`, `ray.train.torch.prepare_data_loader` utility functions to set up the model and data.\n- can automatically wraps model with `DistributeDataParallel` , place it on right device, add `DistributedSampler` to Dataloader.\n```python\nfrom ray import train\n\ndef train_func_distributed():\n    num_epochs = 3\n    batch_size = 64\n\n    dataset = get_dataset()\n    dataloader = DataLoader(dataset, batch_size=batch_size)\n    dataloader = train.torch.prepare_data_loader(dataloader)\n\n    model = NeuralNetwork()\n    model = train.torch.prepare_model(model)\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n\n    for epoch in range(num_epochs):\n        for inputs, labels in dataloader:\n            optimizer.zero_grad()\n            pred = model(inputs)\n            loss = criterion(pred, labels)\n            loss.backward()\n            optimizer.step()\n        print(f\"epoch: {epoch}, loss: {loss.item()}\")\n```\n\nInstantiate a `TorchTrainer`\nuse it run the new training function\n```python\nfrom ray.train.torch import TorchTrainer\nfrom ray.air.config import ScalingConfig\n\n# For GPU Training, set `use_gpu` to True.\nuse_gpu = False\n\ntrainer = TorchTrainer(\n    train_func_distributed,\n    scaling_config=ScalingConfig(num_workers=4, use_gpu=use_gpu)\n)\n\nresults = trainer.fit()\n```\n\n\n#### TensorFlow\ninstall\n```shell\npip install -U \"ray[train]\" tensorflow\n```\n\nsetup dataset and model\n```python\nimport numpy as np\nimport tensorflow as tf\n\ndef mnist_dataset(batch_size):\n    (x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n    # The `x` arrays are in uint8 and have values in the [0, 255] range.\n    # You need to convert them to float32 with values in the [0, 1] range.\n    x_train = x_train / np.float32(255)\n    y_train = y_train.astype(np.int64)\n    train_dataset = tf.data.Dataset.from_tensor_slices(\n        (x_train, y_train)).shuffle(60000).repeat().batch(batch_size)\n    return train_dataset\n\n\ndef build_and_compile_cnn_model():\n    model = tf.keras.Sequential([\n        tf.keras.layers.InputLayer(input_shape=(28, 28)),\n        tf.keras.layers.Reshape(target_shape=(28, 28, 1)),\n        tf.keras.layers.Conv2D(32, 3, activation='relu'),\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(128, activation='relu'),\n        tf.keras.layers.Dense(10)\n    ])\n    model.compile(\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n        optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\n        metrics=['accuracy'])\n    return model\n```\n\nsingle worker\n```python\ndef train_func():\n    batch_size = 64\n    single_worker_dataset = mnist_dataset(batch_size)\n    single_worker_model = build_and_compile_cnn_model()\n    single_worker_model.fit(single_worker_dataset, epochs=3, steps_per_epoch=70)\n```\n\nconvert to distributed multiworker\n- set global batch size\n- chooese distributed training strategy. use `MultiWorkerMirroredStrategy`\n\n```python\nimport json\nimport os\n\ndef train_func_distributed():\n    per_worker_batch_size = 64\n    # This environment variable will be set by Ray Train.\n    tf_config = json.loads(os.environ['TF_CONFIG'])\n    num_workers = len(tf_config['cluster']['worker'])\n\n    strategy = tf.distribute.MultiWorkerMirroredStrategy()\n\n    global_batch_size = per_worker_batch_size * num_workers\n    multi_worker_dataset = mnist_dataset(global_batch_size)\n\n    with strategy.scope():\n        # Model building/compiling need to be within `strategy.scope()`.\n        multi_worker_model = build_and_compile_cnn_model()\n\n    multi_worker_model.fit(multi_worker_dataset, epochs=3, steps_per_epoch=70)\n```\n\nInstantiate a `TensorflowTrainer` with 4 workers, and use it to run the new training function.\n```python\nfrom ray.train.tensorflow import TensorflowTrainer\nfrom ray.air.config import ScalingConfig\n\n# For GPU Training, set `use_gpu` to True.\nuse_gpu = False\n\ntrainer = TensorflowTrainer(train_func_distributed, scaling_config=ScalingConfig(num_workers=4, use_gpu=use_gpu))\n\ntrainer.fit()\n```\n\n### Tune: Hypermeter tuning (Ray Tune)\nScale hypermeter  tuning\ninstall\n```shell\npip install -U \"ray[tune]\"\n```\n\ngird search\n```python\nfrom ray import tune\n\n\ndef objective(config):  # ①\n    score = config[\"a\"] ** 2 + config[\"b\"]\n    return {\"score\": score}\n\n\nsearch_space = {  # ②\n    \"a\": tune.grid_search([0.001, 0.01, 0.1, 1.0]),\n    \"b\": tune.choice([1, 2, 3]),\n}\n\ntuner = tune.Tuner(objective, param_space=search_space)  # ③\n\nresults = tuner.fit()\nprint(results.get_best_result(metric=\"score\", mode=\"min\").config)\n```\n\n\n### Serve: Scale Model Serving (Ray Serve)\ninstall\n```shell\npip install -U \"ray[serve]\" scikit-learn\n```\n\nexample of serving scikit-learn gradient boosting classifier\n```python\nimport requests\nfrom starlette.requests import Request\nfrom typing import Dict\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nfrom ray import serve\n\n\n# Train model.\niris_dataset = load_iris()\nmodel = GradientBoostingClassifier()\nmodel.fit(iris_dataset[\"data\"], iris_dataset[\"target\"])\n\n\n@serve.deployment(route_prefix=\"/iris\")\nclass BoostingModel:\n    def __init__(self, model):\n        self.model = model\n        self.label_list = iris_dataset[\"target_names\"].tolist()\n\n    async def __call__(self, request: Request) -> Dict:\n        payload = (await request.json())[\"vector\"]\n        print(f\"Received http request with data {payload}\")\n\n        prediction = self.model.predict([payload])[0]\n        human_name = self.label_list[prediction]\n        return {\"result\": human_name}\n\n\n# Deploy model.\nserve.run(BoostingModel.bind(model))\n\n# Query it!\nsample_request_input = {\"vector\": [1.2, 1.0, 1.1, 0.9]}\nresponse = requests.get(\n    \"http://localhost:8000/iris\", json=sample_request_input)\nprint(response.text)\n```\n\n### RLlib : Reinforcement Learning\nhigh scalability, unified API\ninstall\n```shell\npip install -U \"ray[rllib]\" tensorflow  # or torch\n```\n\nPPO example\n```python\nimport gymnasium as gym\nfrom ray.rllib.algorithms.ppo import PPOConfig\n\n\n# Define your problem using python and Farama-Foundation's gymnasium API:\nclass SimpleCorridor(gym.Env):\n    \"\"\"Corridor in which an agent must learn to move right to reach the exit.\n\n    ---------------------\n    | S | 1 | 2 | 3 | G |   S=start; G=goal; corridor_length=5\n    ---------------------\n\n    Possible actions to chose from are: 0=left; 1=right\n    Observations are floats indicating the current field index, e.g. 0.0 for\n    starting position, 1.0 for the field next to the starting position, etc..\n    Rewards are -0.1 for all steps, except when reaching the goal (+1.0).\n    \"\"\"\n\n    def __init__(self, config):\n        self.end_pos = config[\"corridor_length\"]\n        self.cur_pos = 0\n        self.action_space = gym.spaces.Discrete(2)  # left and right\n        self.observation_space = gym.spaces.Box(0.0, self.end_pos, shape=(1,))\n\n    def reset(self, *, seed=None, options=None):\n        \"\"\"Resets the episode.\n\n        Returns:\n           Initial observation of the new episode and an info dict.\n        \"\"\"\n        self.cur_pos = 0\n        # Return initial observation.\n        return [self.cur_pos], {}\n\n    def step(self, action):\n        \"\"\"Takes a single step in the episode given `action`.\n\n        Returns:\n            New observation, reward, terminated-flag, truncated-flag, info-dict (empty).\n        \"\"\"\n        # Walk left.\n        if action == 0 and self.cur_pos > 0:\n            self.cur_pos -= 1\n        # Walk right.\n        elif action == 1:\n            self.cur_pos += 1\n        # Set `terminated` flag when end of corridor (goal) reached.\n        terminated = self.cur_pos >= self.end_pos\n        truncated = False\n        # +1 when goal reached, otherwise -1.\n        reward = 1.0 if terminated else -0.1\n        return [self.cur_pos], reward, terminated, truncated, {}\n\n\n# Create an RLlib Algorithm instance from a PPOConfig object.\nconfig = (\n    PPOConfig().environment(\n        # Env class to use (here: our gym.Env sub-class from above).\n        env=SimpleCorridor,\n        # Config dict to be passed to our custom env's constructor.\n        # Use corridor with 20 fields (including S and G).\n        env_config={\"corridor_length\": 28},\n    )\n    # Parallelize environment rollouts.\n    .rollouts(num_rollout_workers=3)\n)\n# Construct the actual (PPO) algorithm object from the config.\nalgo = config.build()\n\n# Train for n iterations and report results (mean episode rewards).\n# Since we have to move at least 19 times in the env to reach the goal and\n# each move gives us -0.1 reward (except the last move at the end: +1.0),\n# we can expect to reach an optimal episode reward of -0.1*18 + 1.0 = -0.8\nfor i in range(5):\n    results = algo.train()\n    print(f\"Iter: {i}; avg. reward={results['episode_reward_mean']}\")\n\n# Perform inference (action computations) based on given env observations.\n# Note that we are using a slightly different env here (len 10 instead of 20),\n# however, this should still work as the agent has (hopefully) learned\n# to \"just always walk right!\"\nenv = SimpleCorridor({\"corridor_length\": 10})\n# Get the initial observation (should be: [0.0] for the starting position).\nobs, info = env.reset()\nterminated = truncated = False\ntotal_reward = 0.0\n# Play one episode.\nwhile not terminated and not truncated:\n    # Compute a single action, given the current observation\n    # from the environment.\n    action = algo.compute_single_action(obs)\n    # Apply the computed action in the environment.\n    obs, reward, terminated, truncated, info = env.step(action)\n    # Sum up rewards for reporting purposes.\n    total_reward += reward\n# Report results.\nprint(f\"Played 1 episode; total-reward={total_reward}\")\n```\n\n","id":"d7da86bfd01ee998","x":720,"y":1960,"width":760,"height":10802},
		{"type":"text","text":"## Ray Core\n\nTurn functions and classes easily into Ray task and actors\n\n### Parallelize Function\ninstall\n```shell\npip install -U \"ray\"\n```\n\n1. Import Ray and initialize `ray.init()`\n2. decorate function `@ray.remote`\n3. call function with `.remote()`\n4. remote call yields a future(Ray object reference), fetch with `ray.get`\n\n```python\nimport ray\nray.init()\n\n@ray.remote\ndef f(x):\n    return x * x\n\nfutures = [f.remote(i) for i in range(4)]\nprint(ray.get(futures)) # [0, 1, 4, 9]\n```\n\n### Parallelize Class\n\ninstantiate class with Ray actor, execute remote method call, maintain internal state\n\ninstall\n```shell\npip install -U \"ray\"\n```\n\nexample\n```python\nimport ray\nray.init() # Only call this once.\n\n@ray.remote\nclass Counter(object):\n    def __init__(self):\n        self.n = 0\n\n    def increment(self):\n        self.n += 1\n\n    def read(self):\n        return self.n\n\ncounters = [Counter.remote() for i in range(4)]\n[c.increment.remote() for c in counters]\nfutures = [c.read.remote() for c in counters]\nprint(ray.get(futures)) # [1, 1, 1, 1]\n```\n\n## Ray Cluster\n\nDeploy applications on cluster\n\n### AWS\n\nexample.py\n```python\nimport sys\nimport time\nfrom collections import Counter\n\nimport ray\n\n\n@ray.remote\ndef get_host_name(x):\n    import platform\n    import time\n\n    time.sleep(0.01)\n    return x + (platform.node(),)\n\n\ndef wait_for_nodes(expected):\n    # Wait for all nodes to join the cluster.\n    while True:\n        num_nodes = len(ray.nodes())\n        if num_nodes < expected:\n            print(\n                \"{} nodes have joined so far, waiting for {} more.\".format(\n                    num_nodes, expected - num_nodes\n                )\n            )\n            sys.stdout.flush()\n            time.sleep(1)\n        else:\n            break\n\n\ndef main():\n    wait_for_nodes(4)\n\n    # Check that objects can be transferred from each node to each other node.\n    for i in range(10):\n        print(\"Iteration {}\".format(i))\n        results = [get_host_name.remote(get_host_name.remote(())) for _ in range(100)]\n        print(Counter(ray.get(results)))\n        sys.stdout.flush()\n\n    print(\"Success!\")\n    sys.stdout.flush()\n    time.sleep(20)\n\n\nif __name__ == \"__main__\":\n    ray.init(address=\"localhost:6379\")\n    main()\n```\n\ncluster.yaml\n```yaml\n# An unique identifier for the head node and workers of this cluster.\ncluster_name: default\n\n# The maximum number of workers nodes to launch in addition to the head\n# node.\nmax_workers: 2\n\n# The autoscaler will scale up the cluster faster with higher upscaling speed.\n# E.g., if the task requires adding more nodes then autoscaler will gradually\n# scale up the cluster in chunks of upscaling_speed*currently_running_nodes.\n# This number should be > 0.\nupscaling_speed: 1.0\n\n# This executes all commands on all nodes in the docker container,\n# and opens all the necessary ports to support the Ray cluster.\n# Empty string means disabled.\ndocker:\n    image: \"rayproject/ray-ml:latest-gpu\" # You can change this to latest-cpu if you don't need GPU support and want a faster startup\n    # image: rayproject/ray:latest-cpu   # use this one if you don't need ML dependencies, it's faster to pull\n    container_name: \"ray_container\"\n    # If true, pulls latest version of image. Otherwise, `docker run` will only pull the image\n    # if no cached version is present.\n    pull_before_run: True\n    run_options:   # Extra options to pass into \"docker run\"\n        - --ulimit nofile=65536:65536\n\n    # Example of running a GPU head with CPU workers\n    # head_image: \"rayproject/ray-ml:latest-gpu\"\n    # Allow Ray to automatically detect GPUs\n\n    # worker_image: \"rayproject/ray-ml:latest-cpu\"\n    # worker_run_options: []\n\n# If a node is idle for this many minutes, it will be removed.\nidle_timeout_minutes: 5\n\n# Cloud-provider specific configuration.\nprovider:\n    type: aws\n    region: us-west-2\n    # Availability zone(s), comma-separated, that nodes may be launched in.\n    # Nodes will be launched in the first listed availability zone and will\n    # be tried in the subsequent availability zones if launching fails.\n    availability_zone: us-west-2a,us-west-2b\n    # Whether to allow node reuse. If set to False, nodes will be terminated\n    # instead of stopped.\n    cache_stopped_nodes: True # If not present, the default is True.\n\n# How Ray will authenticate with newly launched nodes.\nauth:\n    ssh_user: ubuntu\n# By default Ray creates a new private keypair, but you can also use your own.\n# If you do so, make sure to also set \"KeyName\" in the head and worker node\n# configurations below.\n#    ssh_private_key: /path/to/your/key.pem\n\n# Tell the autoscaler the allowed node types and the resources they provide.\n# The key is the name of the node type, which is just for debugging purposes.\n# The node config specifies the launch config and physical instance type.\navailable_node_types:\n    ray.head.default:\n        # The node type's CPU and GPU resources are auto-detected based on AWS instance type.\n        # If desired, you can override the autodetected CPU and GPU resources advertised to the autoscaler.\n        # You can also set custom resources.\n        # For example, to mark a node type as having 1 CPU, 1 GPU, and 5 units of a resource called \"custom\", set\n        # resources: {\"CPU\": 1, \"GPU\": 1, \"custom\": 5}\n        resources: {}\n        # Provider-specific config for this node type, e.g. instance type. By default\n        # Ray will auto-configure unspecified fields such as SubnetId and KeyName.\n        # For more documentation on available fields, see:\n        # http://boto3.readthedocs.io/en/latest/reference/services/ec2.html#EC2.ServiceResource.create_instances\n        node_config:\n            InstanceType: m5.large\n            # Default AMI for us-west-2.\n            # Check https://github.com/ray-project/ray/blob/master/python/ray/autoscaler/_private/aws/config.py\n            # for default images for other zones.\n            ImageId: ami-0387d929287ab193e\n            # You can provision additional disk space with a conf as follows\n            BlockDeviceMappings:\n                - DeviceName: /dev/sda1\n                  Ebs:\n                      VolumeSize: 140\n                      VolumeType: gp3\n            # Additional options in the boto docs.\n    ray.worker.default:\n        # The minimum number of worker nodes of this type to launch.\n        # This number should be >= 0.\n        min_workers: 0\n        # The maximum number of worker nodes of this type to launch.\n        # This takes precedence over min_workers.\n        max_workers: 2\n        # The node type's CPU and GPU resources are auto-detected based on AWS instance type.\n        # If desired, you can override the autodetected CPU and GPU resources advertised to the autoscaler.\n        # You can also set custom resources.\n        # For example, to mark a node type as having 1 CPU, 1 GPU, and 5 units of a resource called \"custom\", set\n        # resources: {\"CPU\": 1, \"GPU\": 1, \"custom\": 5}\n        resources: {}\n        # Provider-specific config for this node type, e.g. instance type. By default\n        # Ray will auto-configure unspecified fields such as SubnetId and KeyName.\n        # For more documentation on available fields, see:\n        # http://boto3.readthedocs.io/en/latest/reference/services/ec2.html#EC2.ServiceResource.create_instances\n        node_config:\n            InstanceType: m5.large\n            # Default AMI for us-west-2.\n            # Check https://github.com/ray-project/ray/blob/master/python/ray/autoscaler/_private/aws/config.py\n            # for default images for other zones.\n            ImageId: ami-0387d929287ab193e\n            # Run workers on spot by default. Comment this out to use on-demand.\n            # NOTE: If relying on spot instances, it is best to specify multiple different instance\n            # types to avoid interruption when one instance type is experiencing heightened demand.\n            # Demand information can be found at https://aws.amazon.com/ec2/spot/instance-advisor/\n            InstanceMarketOptions:\n                MarketType: spot\n                # Additional options can be found in the boto docs, e.g.\n                #   SpotOptions:\n                #       MaxPrice: MAX_HOURLY_PRICE\n            # Additional options in the boto docs.\n\n# Specify the node type of the head node (as configured above).\nhead_node_type: ray.head.default\n\n# Files or directories to copy to the head and worker nodes. The format is a\n# dictionary from REMOTE_PATH: LOCAL_PATH, e.g.\nfile_mounts: {\n#    \"/path1/on/remote/machine\": \"/path1/on/local/machine\",\n#    \"/path2/on/remote/machine\": \"/path2/on/local/machine\",\n}\n\n# Files or directories to copy from the head node to the worker nodes. The format is a\n# list of paths. The same path on the head node will be copied to the worker node.\n# This behavior is a subset of the file_mounts behavior. In the vast majority of cases\n# you should just use file_mounts. Only use this if you know what you're doing!\ncluster_synced_files: []\n\n# Whether changes to directories in file_mounts or cluster_synced_files in the head node\n# should sync to the worker node continuously\nfile_mounts_sync_continuously: False\n\n# Patterns for files to exclude when running rsync up or rsync down\nrsync_exclude:\n    - \"**/.git\"\n    - \"**/.git/**\"\n\n# Pattern files to use for filtering out files when running rsync up or rsync down. The file is searched for\n# in the source directory and recursively through all subdirectories. For example, if .gitignore is provided\n# as a value, the behavior will match git's behavior for finding and using .gitignore files.\nrsync_filter:\n    - \".gitignore\"\n\n# List of commands that will be run before `setup_commands`. If docker is\n# enabled, these commands will run outside the container and before docker\n# is setup.\ninitialization_commands: []\n\n# List of shell commands to run to set up nodes.\nsetup_commands: []\n    # Note: if you're developing Ray, you probably want to create a Docker image that\n    # has your Ray repo pre-cloned. Then, you can replace the pip installs\n    # below with a git checkout <your_sha> (and possibly a recompile).\n    # To run the nightly version of ray (as opposed to the latest), either use a rayproject docker image\n    # that has the \"nightly\" (e.g. \"rayproject/ray-ml:nightly-gpu\") or uncomment the following line:\n    # - pip install -U \"ray[default] @ https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-3.0.0.dev0-cp37-cp37m-manylinux2014_x86_64.whl\"\n\n# Custom commands that will be run on the head node after common setup.\nhead_setup_commands: []\n\n# Custom commands that will be run on worker nodes after common setup.\nworker_setup_commands: []\n\n# Command to start ray on the head node. You don't need to change this.\nhead_start_ray_commands:\n    - ray stop\n    - ray start --head --port=6379 --object-manager-port=8076 --autoscaling-config=~/ray_bootstrap_config.yaml --dashboard-host=0.0.0.0\n\n# Command to start ray on worker nodes. You don't need to change this.\nworker_start_ray_commands:\n    - ray stop\n    - ray start --address=$RAY_HEAD_IP:6379 --object-manager-port=8076\n```\n\nlaunch cluster\n```shell\nray submit cluster.yaml example.py --start\n```\n\n\n## Debug and Monitor\n\n### Ray Dashboard\nvisual interface\ninstall\n```shell\npip install -U \"ray[default]\"\n```\n\n### Ray State API\ninstall\n```shell\npip install -U \"ray[default]\"\n```\n\ntask\n```python\n    import ray\n    import time\n\n    ray.init(num_cpus=4)\n\n    @ray.remote\n    def task_running_300_seconds():\n        print(\"Start!\")\n        time.sleep(300)\n    \n    @ray.remote\n    class Actor:\n        def __init__(self):\n            print(\"Actor created\")\n    \n    # Create 2 tasks\n    tasks = [task_running_300_seconds.remote() for _ in range(2)]\n\n    # Create 2 actors\n    actors = [Actor.remote() for _ in range(2)]\n\n    ray.get(tasks)\n```\n\nstatistics of task\n```shell\nray summary tasks\n```\n\n```\n    ======== Tasks Summary: 2022-07-22 08:54:38.332537 ========\n    Stats:\n    ------------------------------------\n    total_actor_scheduled: 2\n    total_actor_tasks: 0\n    total_tasks: 2\n\n\n    Table (group by func_name):\n    ------------------------------------\n        FUNC_OR_CLASS_NAME        STATE_COUNTS    TYPE\n    0   task_running_300_seconds  RUNNING: 2      NORMAL_TASK\n    1   Actor.__init__            FINISHED: 2     ACTOR_CREATION_TASK\n```","id":"aaab8e05a33f2583","x":1640,"y":1940,"width":764,"height":8523},
		{"type":"file","file":"0.Attach/0.4.Picture/Pasted image 20230724165912.png","id":"8e39256cceea1f26","x":-1760,"y":4114,"width":400,"height":398},
		{"type":"text","text":"## Scaling Multi-Agent Reinforcement Learning\n\n### The Reason\n\n- natural decomposition\n- more scalable learning\n\t- decomposing actions and obsversion reduces dimensionality and increase training data\n\t- play a similar role to the approach, increase the learning efficiency in single-agent setting\n\t- lead to learning of policies that more transferable\n\n![[Pasted image 20230717162154.png]]\n_**Figure 1**: Single-agent approaches (a) and (b) in comparison with multi-agent RL (c)._\n\n## Examples\n\n- [Traffic congestion reduction](https://flow-project.github.io/)\n\t- intelligently controlling the speed of a few autonomous vehicles can increase traffic flow\n\t- ![[Pasted image 20230717163002.png]]\n\t- https://lh5.googleusercontent.com/-Z6cEw5si6xVEydePd4Ml28IS2Grd5X8Fiax-w9g7ASFRfd_OvVRvorF69wb8VIJInvj1LIO3j-9hWFtky_kq84zc_C3FCK9MrVoAvS6mUfXOagzTjJjr-W8-AQF1TV_prE257NC\n- [Antenna tilt control](https://www.kt.tu-darmstadt.de/media/kt/publikationen_1/10/17/WPC_16_Dandanov.pdf)\n\t- joint configuration of celluar base station can be optimized according to the distribution of ussage and topology of the local environment.\n\t- each base station can be modeled as an agent\n\t- ![[Pasted image 20230717162956.png]]\n- [OpenAI Five](https://blog.openai.com/openai-five/)\n\t- Dota 2 AI that compete against human.\n\t- Each agent is implemented a separate neural network policy and trained together with large-scale PPO\n\n### RLlib\ndistributed algorithms\n- A2C/A3C\n- PPO\n- IMPALA\n- DQN\n- DDPG\n- Ape-X\n\nreduce friction and make multi-agent easy\n\n### Difficulty Of Support Multi-Agent \n\n- environment non-stationarity\n\t- ![[Pasted image 20230717170039.png]]\n\t- _**Figure 2**: Non-stationarity of environment: Initially (a), the red agent learns to regulate the speed of the traffic by slowing down. However, over time the blue agents learn to bypass the red agent (b), rendering the previous experiences of the red agent invalid._\n\t- this is problematic because from single agent view, blue agents are \"part of the environment\"\n\t- the environment is dynamical changing from perpective of single agent, and violates the Markov assumptions required for the convergence of Q-learning algorithms such as DQN.\n\t- Algorithms take other agents's action into account during RL training.\n\t\t- LOLA, RIAL, Q-MIX\n\t\t- centralized for training, decentralized execution\n\t\t- policy networks may have dependencies on each other\n\t\t- Q-MIX\n\t\t\t- ![[Pasted image 20230718092028.png]]\n\t\t\t- _The Q-mix network architecture, from [QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning](https://arxiv.org/abs/1803.11485). Individual Q-estimates are aggregated by a monotonic mixing network for efficiency of final action computation._\n\t\t- Policy-gradient algorithms(A3C, PPO), struggle in multi-agent settings. The credit assignment problem become harder with more agents.\n\t\t\t- ![[Pasted image 20230718092454.png]]\n\t\t\t- _**High variance of advantage estimates**: In this traffic gridlock situation, it is unclear which agents' actions contributed most to the problem -- and when the gridlock is resolved, from any global reward it will be unclear which agents get credit._\n\t\t- One approach is model the effect of other agents on environment with **centralized value functions**.\n\t\t\t- MA-DDPG, the variability of individual agent advantage estimates can be greatly reduced, by taking into account the actions of other agents.\n\t\t\t\t- ![[Pasted image 20230718093314.png]]\n\t\t\t\t- _**The MA-DDPG architecture**, from [Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments](https://arxiv.org/abs/1706.02275). Policies run using only local information at execution time, but may take advantage of global information at training time._\n\nTwo different challenge and approachess.\nIn many settings training multi-agent policies using single-agent RL algorithms can yield surprisingly strong results.\nOpenAI Five has been successful using only a combination of large-scale [PPO](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#proximal-policy-optimization-ppo) and a [specialized network model](https://neuro.cs.ut.ee/the-use-of-embeddings-in-openai-five/). The only considerations for multi-agent are the annealing of a “team spirit” hyperparameter that influences the level of shared reward, and a shared “max-pool across Players” operation in the [model](https://s3-us-west-2.amazonaws.com/openai-assets/dota_benchmark_results/network_diagram_08_06_2018.pdf) to share observational information.\n\n## Multi-agent training in RLlib\n\nPrinciples to multi-agent settings\n- **Policies are represented as objects**\n\t- Gradient-based algorithms declare a policy graph object\n\t\t- include\n\t\t\t- policy model $\\pi_\\theta(o_t)$ \n\t\t\t- trajectory postprocessing function $post_\\theta(traj)$ \n\t\t\t- policy loss $L(\\theta;X)$ \n\t\t- ussage\n\t\t\t- execute environment rollouts (querying $\\pi_\\theta$)\n\t\t\t- collate(校对) experiences (applying $post_\\theta$)\n\t\t\t- improve policy (desceding $L(\\theta;X$) )\n- **Policy objects are black boxes**\n\t- To support multi-agent\n\t\t- manage the creation and execution of multiple policy graphs\n\t\t- add together the loss during policy optimization\n\t- Policy graph object is treated as black box\n\t\t- can be implemented in TensorFlow and Pytorch\n\t\t- internally share variables and layers for Q-MIX, MADDPG\n\n## Multi-agent environment model\n\nIn multi-agent environment,there can be multiple acting entities per step.\n\n### Example: Traffic Control Scenario\n\n![[Pasted image 20230718103555.png]]\n_RLlib multi-agent environments can model multiple independent agents that come and go over time. Different policies can be assigned to agents as they appear._\n\n- Each agents can act at different time-scales\n- Agents can come and go from the environment as time progresses\n\nEnvironment is formalized in the  [MultiAgentEnv](https://github.com/ray-project/ray/blob/master/python/ray/rllib/env/multi_agent_env.py) interface, returns observations and rewards from multiple ready agents per step.\n\n```python\n# Example: using a multi-agent env\n> env = MultiAgentTrafficEnv(num_cars=20, num_traffic_lights=5)\n\n# Observations are a dict mapping agent names to their obs. Not all\n# agents need to be present in the dict in each time step.\n> print(env.reset())\n{\n    \"car_1\": [[...]],\n    \"car_2\": [[...]],\n    \"traffic_light_1\": [[...]],\n}\n\n# Actions should be provided for each agent that returned an observation.\n> new_obs, rewards, dones, infos = env.step(\nactions={\"car_1\": ..., \"car_2\": ...})\n\n# Similarly, new_obs, rewards, dones, infos, etc. also become dicts\n> print(rewards)\n{\"car_1\": 3, \"car_2\": -1, \"traffic_light_1\": 0}\n\n# Individual agents can early exit; env is done when \"__all__\" = True\n> print(dones)\n{\"car_2\": True, \"__all__\": False}\n```\n\n [observation space](https://github.com/openai/gym/tree/master/gym/spaces)\n- Discrete\n- Box\n- Dict\n- Tuple\n\n## Multiple levels of API Support\n\n\nRLlib nodels agents and policies as objects bound to each other during an episode.\nUser can leverage this abstraction to varying degrees. From single-agent shared policy, to multiple policies, to fully customized policy optimization.\n\n![[Pasted image 20230718142417.png]]\n_The multi-agent execution model in RLlib compared with single-agent execution._\n\n### Level 1: Multiple agents, shared policy\n\nIf all agents are homogenous, it is possible to ues existing single-agent algorithns for training. Then it only needs to internally aggregate(总和) the experiences of different agents prior to optimization.\nThe change is minimal on user side:\n\n#### From Single-agent\n```python\nregister_env(\"your_env\", lambda c: YourEnv(...))\ntrainer = PPOAgent(env=\"your_env\")\nwhile True:\n    print(trainer.train())  # distributed training step\n```\n\n#### To Multi-agent\n```python\nregister_env(\"your_multi_env\", lambda c: YourMultiEnv(...))\ntrainer = PPOAgent(env=\"your_multi_env\")\nwhile True:\n    print(trainer.train())  # distributed training step\n```\n\n__NOTE__: The _PPO Agent_ is more like a \"Trainer\" than an \"Agent\"\n\n### Level 2: Multiple agents, Multiple policies\n\nThe definition of which agent is handled by which policy is required to handle multiple policies. Via _Policy Mapping Function_, RLlib can assign agents in the env to a particular policy.\n\n#### A example of hierarchical control,\nSupervisor agents assign work to teams of worker agents they oversee(监管). It has the configuration of a single supervisor policy and an ensemble of two worker polices.\n\n```python\ndef policy_mapper(agent_id):\n    if agent_id.startswith(\"supervisor_\"):\n        return \"supervisor_policy\"\n    else:\n        return random.choice([\"worker_p1\", \"worker_p2\"])\n```\n\nwe bind supervisor agents to the single supervisor policy, and randomly divide other agents between an ensemble of two different worker policies. These assignments are done when the agent fist enter the episode, and persist for the rest. We need to define policy configurations. This can be done as part of the top-level agent configuration.\n\n```python\ntrainer = PPOAgent(env=\"control_env\", config={\n    \"multiagent\": {\n        \"policy_mapping_fn\": policy_mapper,\n        \"policy_graphs\": {\n            \"supervisor_policy\":\n                (PPOPolicyGraph, sup_obs_space, sup_act_space, sup_conf),\n            \"worker_p1\": (\n                (PPOPolicyGraph, work_obs_s, work_act_s, work_p1_conf),\n            \"worker_p2\":\n                (PPOPolicyGraph, work_obs_s, work_act_s, work_p2_conf),\n        },\n        \"policies_to_train\": [\n             \"supervisor_policy\", \"worker_p1\", \"worker_p2\"],\n    },\n})\nwhile True:\n    print(trainer.train())  # distributed training step\n```\n\nThis generates a configuration of the Figure 2. You can custom policy graph class for each policy, and policy config dicts. This allows any supported customization (e.g., [custom models and preprocessors](https://ray.readthedocs.io/en/latest/rllib-models.html#custom-models)) to be used and wholesale definition of a new class of policy.\n\n_Advanced examples:_\n\n- [Sharing layers across policies](https://ray.readthedocs.io/en/latest/rllib-env.html#variable-sharing-between-policies)\n- [Implementing a centralized critic](https://github.com/ray-project/ray/blob/master/python/ray/rllib/examples/multiagent_two_trainers.py)\n\n### Level 3: Custom training strategies\n\nFor advanced application or research, it is inevitable to run into the linitations of framework.\n\n#### Example\n\nSuppose some agents learning with PPO, others with DQN.\nThis can be done by swapping weights between two different trainers ( example [here](https://github.com/ray-project/ray/blob/master/python/ray/rllib/examples/multiagent_two_trainers.py)), but this won't scale with more types of algorithns, or you want to use experience to train model of env at same time.\n\nWe can fall back to RLlib's underlying system Ray to distribute computations as needed.\n##### Ray parallel primitives.\n- [Tasks](https://ray.readthedocs.io/en/latest/tutorial.html), which are Python functions executed asynchronously via `func.remote()`\n- [Actors](https://ray.readthedocs.io/en/latest/actors.html), which are Python classes created in the cluster via `class.remote()`. Actor methods can be called via `actor.method.remote()`.\n\n##### RLlib\nProvide toolkit for distributed RL training.\n- [Policy graphs](https://ray.readthedocs.io/en/latest/rllib-concepts.html) (as seen in previous examples)\n- [Policy evaluation](https://ray.readthedocs.io/en/latest/rllib-concepts.html#policy-evaluation): the PolicyEvaluator class manages the environment interaction loop that generates [batches](https://github.com/ray-project/ray/blob/master/python/ray/rllib/evaluation/sample_batch.py) of experiences. When created as Ray actors, it can be used to gather experiences in a distributed way.\n- [Policy optimization](https://ray.readthedocs.io/en/latest/rllib-concepts.html#policy-optimization): these implement distributed strategies for improving policies. You can use one of the existing optimizers or go with a custom strategy.\n\nYou can create [policy evaluators](https://ray.readthedocs.io/en/latest/rllib-concepts.html#policy-evaluation) to gather multi-agent rollouts, and then process the batches as needed to improve the policies:\n\n```python\n# Initialize a single-node Ray cluster\nray.init()\n\n# Create local instances of your custom policy graphs\nsup, w1, w2 = SupervisorPolicy(), WorkerPolicy(), WorkerPolicy()\n\n# Create policy evaluators (Ray actor processes running in the cluster)\nevaluators = []\nfor i in range(16):\n    ev = PolicyEvaluator.as_remote().remote(\n        env_creator=lambda ctx: ControlEnv(),\n        policy_graph={\n            \"supervisor_policy\": (SupervisorPolicy, ...),\n            \"worker_p1\": ..., ...},\n        policy_mapping_fn=policy_mapper,\n        sample_batch_size=500)\n    evaluators.append(ev)\n\nwhile True:\n    # Collect experiences in parallel using the policy evaluators\n    futures = [ev.sample.remote() for ev in evaluators]\n    batch = MultiAgentBatch.concat_samples(ray.get(futures))\n    # >>> print(batch)\n    # MultiAgentBatch({\n    #     \"supervisor_policy\": SampleBatch({\n    #          \"obs\": [[...], ...], \"rewards\": [0, ...], ...\n    #      }),\n    #     \"worker_p1\": SampleBatch(...),\n    #     \"worker_p2\": SampleBatch(...),\n    # })\n    your_optimize_func(sup, w1, w2, batch)  # Custom policy optimization\n    # Broadcast new weights and repeat\n    for ev in evaluators:\n        ev.set_weights.remote({\n            \"supervisor_policy\": sup.get_weights(),\n            \"worker_p1\": w1.get_weights(),\n            \"worker_p2\": w2.get_weights(),\n         })\n```\n\nRLlib provides several levels of APIs targeted at progressively more requirements of customizability.\n- Highest level provides \"Out of the Box\" training progress.\n- Also retain the option of piece toghter own algorithms and training strategies from the core multi-agent absreactions.\n\nTo get started, here are a couple intermediate level scripts that can be run directly: [`multiagent_cartpole.py`](https://github.com/ray-project/ray/blob/master/python/ray/rllib/examples/multiagent_two_trainers.py), [`multiagent_two_trainers.py`](https://bair.berkeley.edu/).\n\n## Performance\n\nRLlib is designed to scale to large clusters. But also apply optimizations for single core efficiency.\n","id":"30c5ba69c993f155","x":2640,"y":1960,"width":640,"height":9093},
		{"type":"text","text":"  # RLlib Design\nhttps://docs.ray.io/en/latest/rllib/rllib-concepts.html\n\nIn RLlib, **Algorithm**'s learns how to solve problem in **environment**. The **Algorithm** use **policices** to select actions. Given a **Policy**, **rollouts** throughout an **environment** produce **sample batches** (**trajectories**) of experience.  You can also customize **training_steps**.\n\n## Environment\n\nSimplest definition of RL.\n> An **agent** interact with an **environment** and receives a **reward**.\n![[Pasted image 20230719094136.png]]\n\nEnvironment is agent's world, a simulation of the problem to be solved.\n\nEnvironment consists:\n1. All possible actions(**action space**)\n2. A complete description of environment, nothing hidden(**state space**)\n3. An observation by the agent of certain parts of the state (**observation space**)\n4. **Reward**, feedback the agent receives per action\n\n**Policy**, the model that tries to maximize the expected sum over all future **rewards**.The **policy** is a function mapping environment's observations to an action to take, $\\mathbb{\\pi}(s(t)) \\rightarrow a(t)$.\n\nThe RL learning process:\n![[Pasted image 20230719095250.png]]\n\n- The RL simulation feedback loop repeatedly collects data\n\t- for one or multiple policies\n- Trains the policies on these data\n\t- keep policies weights in sync\n- Collected data contains observations, taken actions, received rewards and **done** flags (indicating boundaries of different episodes)\n\n**episode** or **rollout**(in RLlib): the iteration of action, reward, next state, train, repeat, until the end state.\n\n## Algorithms\n\nAlgorithms bring RLlib components together. Each **Algorthms** class managed by **AlgorthmConfig**. Algorthm sets up its rollout workers and optimizers, collects training metrics. Also implement [Tune Trainable API](https://docs.ray.io/en/latest/tune/key-concepts.html#tune-60-seconds) for easy experiment management.\n\n### Interact with algorithm\n\n#### Basic RLlib Algorithm\n\n```python\n# Configure.\nfrom ray.rllib.algorithms.ppo import PPOConfig\nconfig = PPOConfig().environment(env=\"CartPole-v1\").training(train_batch_size=4000)\n\n# Build.\nalgo = config.build()\n\n# Train.\nwhile True:\n    print(algo.train())\n```\n\n#### RLlib Algorithms and Tune\n```python\nfrom ray import tune\n\n# Configure.\nfrom ray.rllib.algorithms.ppo import PPOConfig\nconfig = PPOConfig().environment(env=\"CartPole-v1\").training(train_batch_size=4000)\n\n# Train via Ray Tune.\ntune.run(\"PPO\", config=config)\n```\n\n#### RLlib Command Line\n\n```shell\nrllib train --run=PPO --env=CartPole-v1 --config='{\"train_batch_size\": 4000}'\n```\n\n### Distributed Workflow\nThe Algorithm class coordinate the distributed workflow of running rollouts and optimizing policies.\n\n![[Pasted image 20230719103531.png]]\n\nRLlib uses [Ray actors](https://docs.ray.io/en/latest/rllib/actors.html) to scale training from a single core to many thousands of cores in a cluster. You can [configure the parallelism](https://docs.ray.io/en/latest/rllib/rllib-training.html#specifying-resources) used for training by changing the `num_workers` parameter. Check out our [scaling guide](https://docs.ray.io/en/latest/rllib/rllib-training.html#scaling-guide) for more details here.\n\n## Policies\n\n[How To Customize Policies](https://docs.ray.io/en/latest/rllib/rllib-concepts.html#policies) \n\nPolicies are Python classes define how an agent acts.\n[Rollout workers](https://docs.ray.io/en/latest/rllib/rllib-concepts.html#policy-evaluation) query the policy to determine agent actions.\n\nIn a [Farama-Foundation Gymnasium](https://docs.ray.io/en/latest/rllib/rllib-env.html#gymnasium) environment, there is a single agent and policy. In [vector envs](https://docs.ray.io/en/latest/rllib/rllib-env.html#vectorized), policy inference is for multiple agents at once, and in [multi-agent](https://docs.ray.io/en/latest/rllib/rllib-env.html#multi-agent-and-hierarchical), there may be multiple policies, each controlling one or more agents:\n![[Pasted image 20230719104756.png]]\n\nPolicies can be implemented using any framework.[Ploicy Base Class](https://github.com/ray-project/ray/blob/master/rllib/policy/policy.py)\n\nFor TensorFlow and PyTorch, RLlib has [build_tf_policy](https://docs.ray.io/en/latest/rllib/rllib-concepts.html#building-policies-in-tensorflow) and [build_torch_policy](https://docs.ray.io/en/latest/rllib/rllib-concepts.html#building-policies-in-pytorch) helper functions that let you define a trainable policy with a functional-style API.\n```python\ndef policy_gradient_loss(policy, model, dist_class, train_batch):\n    logits, _ = model.from_batch(train_batch)\n    action_dist = dist_class(logits, model)\n    return -tf.reduce_mean(\n        action_dist.logp(train_batch[\"actions\"]) * train_batch[\"rewards\"])\n\n# <class 'ray.rllib.policy.tf_policy_template.MyTFPolicy'>\nMyTFPolicy = build_tf_policy(\n    name=\"MyTFPolicy\",\n    loss_fn=policy_gradient_loss)\n```\n\n## Policy Evaluation\n\n[RolloutWorker](https://github.com/ray-project/ray/blob/master/rllib/evaluation/rollout_worker.py) produce batches of experiences. Calling `worker.sample()` on a worker instance, or `worker.sample.remote()` in parallel on worker instances created as Ray actors (see [WorkerSet](https://github.com/ray-project/ray/blob/master/rllib/evaluation/worker_set.py)).\n\nExample of creating a set of rollout workers and using them gather experiences in parallel.\nThe trajectories are concatenated, the policy learns in trajectory batch, then boardcast the policy weights to workers for the next round of rollouts.\n```python\n# Setup policy and rollout workers.\nenv = gym.make(\"CartPole-v1\")\npolicy = CustomPolicy(env.observation_space, env.action_space, {})\nworkers = WorkerSet(\n    policy_class=CustomPolicy,\n    env_creator=lambda c: gym.make(\"CartPole-v1\"),\n    num_workers=10)\n\nwhile True:\n    # Gather a batch of samples.\n    T1 = SampleBatch.concat_samples(\n        ray.get([w.sample.remote() for w in workers.remote_workers()]))\n\n    # Improve the policy using the T1 batch.\n    policy.learn_on_batch(T1)\n\n    # The local worker acts as a \"parameter server\" here.\n    # We put the weights of its `policy` into the Ray object store once (`ray.put`)...\n    weights = ray.put({\"default_policy\": policy.get_weights()})\n    for w in workers.remote_workers():\n        # ... so that we can broacast these weights to all rollout-workers once.\n        w.set_weights.remote(weights)# Setup policy and rollout workers.\nenv = gym.make(\"CartPole-v1\")\npolicy = CustomPolicy(env.observation_space, env.action_space, {})\nworkers = WorkerSet(\n    policy_class=CustomPolicy,\n    env_creator=lambda c: gym.make(\"CartPole-v1\"),\n    num_workers=10)\n\nwhile True:\n    # Gather a batch of samples.\n    T1 = SampleBatch.concat_samples(\n        ray.get([w.sample.remote() for w in workers.remote_workers()]))\n\n    # Improve the policy using the T1 batch.\n    policy.learn_on_batch(T1)\n\n    # The local worker acts as a \"parameter server\" here.\n    # We put the weights of its `policy` into the Ray object store once (`ray.put`)...\n    weights = ray.put({\"default_policy\": policy.get_weights()})\n    for w in workers.remote_workers():\n        # ... so that we can broacast these weights to all rollout-workers once.\n        w.set_weights.remote(weights)\n```\n\n\n## Sample Batches\n\nAll data in RLlib is interchanged in [sample batches](https://github.com/ray-project/ray/blob/master/rllib/policy/sample_batch.py). \nSample Batches encode fragments of a trajectory.\nRLlib collects batches of size **rollout_fragment_length** from rollout workers, and concatenates these batches into batch of size **train_batch_size** as input of SGD.\n\nExample of Sample Batch. All values are kept in arrays, allows efficient encoding and transmission.\n\n```python\nsample_batch = { 'action_logp': np.ndarray((200,), dtype=float32, min=-0.701, max=-0.685, mean=-0.694),\n    'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.495),\n    'dones': np.ndarray((200,), dtype=bool, min=0.0, max=1.0, mean=0.055),\n    'infos': np.ndarray((200,), dtype=object, head={}),\n    'new_obs': np.ndarray((200, 4), dtype=float32, min=-2.46, max=2.259, mean=0.018),\n    'obs': np.ndarray((200, 4), dtype=float32, min=-2.46, max=2.259, mean=0.016),\n    'rewards': np.ndarray((200,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n    't': np.ndarray((200,), dtype=int64, min=0.0, max=34.0, mean=9.14)\n}\n```\n\nIn [multi-agent mode](https://docs.ray.io/en/latest/rllib/rllib-concepts.html#policies-in-multi-agent), sample batches are collected separately for each individual policy. These batches are wrapped up together in a `MultiAgentBatch`, serving as a container for the individual agents’ sample batches.\n\n## Training Step Method\n(_Algorithm.training_step()_)\n\n>[!note]\n> \n>[ray core methods](https://docs.ray.io/en/latest/ray-core/walkthrough.html#core-walkthrough)\n>[rollout worker reference docs](https://docs.ray.io/en/latest/rllib/package_ref/evaluation.html#rolloutworker-reference-docs) : `SampleBatch` (and its more advanced sibling: the `MultiAgentBatch`), `RolloutWorker`, and `Algorithm`\n> custom algorithms should familiarize themselves with the [Policy](https://docs.ray.io/en/latest/rllib/rllib-concepts.html#rllib-policy-walkthrough) and [Model](https://docs.ray.io/en/latest/rllib/rllib-models.html#rllib-models-walkthrough) classes.\n\n`training_step()` method of the `Algorithm` class defines the repeatable execution logic.\nuse `training_step()` to express how you want to coordinate the collection of samples from the environment(s), the movement of this data to other parts of the algorithm, and the updates and management of your policy’s weights across the different distributed components.\n \n**In short, a developer will need to override/modify the ``training_step`` method if they want to make custom changes to an existing algorithm, write their own algo from scratch, or implement some algorithm from a paper.**\n\nThe `Algorithm`’s `training_step()` method is called:\n1. when the `train()` method of `Algorithm` is called (e.g. “manually” by a user that has constructed an `Algorithm` instance).\n2. when an RLlib Algorithm is being run by Ray Tune. `training_step()` will be continuously called till the [ray tune stop criteria](https://docs.ray.io/en/latest/tune/api/execution.html#tune-run-ref) is met.\n\n### Example of VPG (“vanilla policy gradient”)\n\nIllustrate how to use `training_step()` to implement this algorithm.\n\nThe **vanilla policy gradient** :\n1. Sampling (collect data from env)\n2. Updating Policy (learning behavior)\n3. Boardcasting updated policy's weights (make sure distributed units have the same weights)\n4. Metrics reporting (returning relevant stats, with performance and runtime)\n\n```python\ndef training_step(self) -> ResultDict:\n    # 1. Sampling.\n    train_batch = synchronous_parallel_sample(\n                    worker_set=self.workers,\n                    max_env_steps=self.config[\"train_batch_size\"]\n                )\n\n    # 2. Updating the Policy.\n    train_results = train_one_step(self, train_batch)\n\n    # 3. Synchronize worker weights.\n    self.workers.sync_weights()\n\n    # 4. Return results.\n    return train_results\n```\n\nThe `training_step` method is framework agnostic. DL framework specific should only add to  [Policy](https://docs.ray.io/en/latest/rllib/rllib-concepts.html#rllib-policy-walkthrough) (e.g. in its loss function(s)) and [Model](https://docs.ray.io/en/latest/rllib/rllib-models.html#rllib-models-walkthrough) (e.g. tf.keras or torch.nn neural network code) classes\n\n### Break Down `training step()`\n\nFirst step, collect trajectory data from the environment.\n```python\ntrain_batch = synchronous_parallel_sample(\n                    worker_set=self.workers,\n                    max_env_steps=self.config[\"train_batch_size\"]\n                )\n```\n- `self.workers` : a set of `RolloutWorkers` created in the `Algorithm`'s `setup()` method.\n\t- `WorkerSet` is covered in greater depth on the [WorkerSet documentation page](https://docs.ray.io/en/latest/rllib/package_ref/evaluation.html#workerset-reference-docs).\n- `synchronous_parallel_sample` : parallel sampling in a blocking fashion across multiple rollout workers (returns once all rollout workers are done sampling). It returns one final MultiAgentBatch resulting from concatenating n smaller MultiAgentBatches (exactly one from each remote rollout worker).\n\nThen, the `train_batch` is passed to another utility function: `train_one_step`.\n```python\ntrain_results = train_one_step(self, train_batch)\n```\nMethods like `train_one_step` and `multi_gpu_train_one_step` are used for training.Further documentation with examples can be found on the [train ops documentation page](https://docs.ray.io/en/latest/rllib/package_ref/utils.html#train-ops-docs).\nThe training updates on the policy are only applied to its version inside `self.workers.local_worker`. Each WorkSet has n remote workers and one \"local worker\", and each worker holds a copy of the policy.\n\nWhen updated the local policy, synchronizing the weights for all remote workers.\n```python\nself.workers.sync_weights()\n```\n- By calling `self.workers.sync_weights()`, weights are broadcasted from the local worker to the remote workers.\n\t- See [rollout worker reference docs](https://docs.ray.io/en/latest/rllib/package_ref/evaluation.html#rolloutworker-reference-docs) for further details.\n\nFinally, a dictionary is expected to be returned that contains the results of the training update. It maps keys of type `str` to values that are of type `float` or to dictionaries of the same form, allowing for a nested structure.\na results dictionary could map policy_ids to learning and sampling statistics for that policy:\n```python\n{\n   'policy_1': {\n                 'learner_stats': {'policy_loss': 6.7291455},\n                 'num_agent_steps_trained': 32\n              },\n   'policy_2': {\n                'learner_stats': {'policy_loss': 3.554927},\n                'num_agent_steps_trained': 32\n              },\n}\n```\n\n### Training Step Method Utilities\nIf you would like to work with the various `training_step` methods or implement your own, familiarize with following concepts:\n\n- [Sample Batch](https://docs.ray.io/en/latest/rllib/core-concepts.html#sample-batches): `SampleBatch` and `MultiAgentBatch` are used for storing trajectory data in RLlib. All of our RLlib abstractions (policies, replay buffers, etc.) operate on these two types.\n- [Rollout Workers](https://docs.ray.io/en/latest/rllib/package_ref/evaluation.html#rolloutworker-reference-docs): An abstraction that wraps a policy (or policies in the case of multi-agent) and an environment.\n\t- Collect experiences by calling `sample()` method and train policies by calling `learn_on_batch()` method.\n\t- By default, a set of workers are used for sampling and training. `WorkerSet` object inside of `setup` is called when an RLlib algorithm is created. The `WorkerSet` has a `local_worker` and `remote_workers` if `num_workers > 0` in the experiment config.Typically, use `local_worker` for training and `remote_workers` for sampling.\n- [Train Ops](https://docs.ray.io/en/latest/rllib/package_ref/utils.html#train-ops-docs): Improve the policy and update workers.\n\t- `train_one_step`, takes in as input a batch of experiences and emits a `ResultDict` with metrics as output.\n\t- `multi_gpu_train_one_step`, for training with GPUs, \n\t- These methods use the `learn_on_batch` method of rollout workers to complete the training update.\n- [Replay Buffers](https://docs.ray.io/en/latest/rllib/rllib-replay-buffers.html#replay-buffer-reference-docs): RLlib provides [a collection](https://github.com/ray-project/ray/tree/master/rllib/utils/replay_buffers) of replay buffers that can be used for storing and sampling experiences.","id":"6d4f1facbcc2ec18","x":3520,"y":1960,"width":680,"height":9120}
	],
	"edges":[]
}